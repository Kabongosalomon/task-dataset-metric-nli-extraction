<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STAN: SPATIO-TEMPORAL ADVERSARIAL NETWORKS FOR ABNORMAL EVENT DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangmin</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="laboratory">Image and Video Systems Lab</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hak</forename><forename type="middle">Gu</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="laboratory">Image and Video Systems Lab</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Man</forename><surname>Ro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="laboratory">Image and Video Systems Lab</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STAN: SPATIO-TEMPORAL ADVERSARIAL NETWORKS FOR ABNORMAL EVENT DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Abnormal event detection</term>
					<term>adversarial learning</term>
					<term>spatio-temporal features</term>
					<term>interpretation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel abnormal event detection method with spatio-temporal adversarial networks (STAN). We devise a spatio-temporal generator which synthesizes an inter-frame by considering spatio-temporal characteristics with bidirectional ConvLSTM. A proposed spatio-temporal discriminator determines whether an input sequence is realnormal or not with 3D convolutional layers. These two networks are trained in an adversarial way to effectively encode spatio-temporal features of normal patterns. After the learning, the generator and the discriminator can be independently used as detectors, and deviations from the learned normal patterns are detected as abnormalities. Experimental results show that the proposed method achieved competitive performance compared to the state-of-the-art methods. Further, for the interpretation, we visualize the location of abnormal events detected by the proposed networks using a generator loss and discriminator gradients.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With a rapid development of storage and video acquisition devices, the video surveillance system is widely used for privacy protection, process monitoring in factories, criminal tracking, etc. Most of the surveillance scenes captured for a long time are meaningless and normal. It is time-consuming and labor-intensive for people to watch long hours of meaningless scenes. To address the problem, automatic abnormal event (i.e., meaningful moment) detection in videos has increasingly attracted attention in image processing and computer vision fields.</p><p>There were many existing methods for automatically detecting abnormal events in videos. In <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, trajectory based abnormal event detection methods which utilize the dynamic information of objects were proposed. Hu et al. <ref type="bibr" target="#b0">[1]</ref> proposed method for learning statistical motion patterns with multiobject tracking. Zhou et al. <ref type="bibr" target="#b1">[2]</ref> proposed statistical model to detect abnormal behaviors using Kanade-Lucas-Tomasi Feature Tracker (KLT). However, trajectory based methods are * Corresponding author (ymro@ee.kaist.ac.kr) not robust to occlusions and crowded scenes since most of the object tracking algorithms are vulnerable to such environments. In <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, handcraft feature based abnormal event detection methods were proposed. Kaltsa et al. <ref type="bibr" target="#b2">[3]</ref> utilized a descriptor created from Histograms of Oriented Gradients (HOG) and Histograms of Oriented Swarms (HOS). In <ref type="bibr" target="#b3">[4]</ref>, distributions of spatio-temporal oriented energy were used to model behavior. Although the handcraft feature based methods are more robust to occlusions and complex scenes than trajectory based methods, they require prior knowledge to design proper features for various events.</p><p>In recent years, deep learning has attracted attention in many challenging tasks with better performance compared to the traditional methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. In this flow, several methods using deep learning have been proposed for abnormal event detection. Hasan et al. <ref type="bibr" target="#b7">[8]</ref> proposed the convolutional autoencoder based method for learning temporal regularity. In <ref type="bibr" target="#b8">[9]</ref>, the ConvLSTM <ref type="bibr" target="#b9">[10]</ref> was employed to consider the spatiotemporal characteristics of event patterns. Recently, generative adversarial networks (GAN) based method <ref type="bibr" target="#b10">[11]</ref> was proposed. In <ref type="bibr" target="#b10">[11]</ref>, one GAN is trained to generate an optical flow map from a frame while the other GAN is trained to generate a frame from an optical flow map. Although the method of <ref type="bibr" target="#b10">[11]</ref> achieved good performance by using GAN, its performance could be sensitive to the quality of the estimated optical flow map. It could not be robust to scenes with occlusions in which it is difficult to well estimate the optical flow map.</p><p>In this paper, we propose the spatio-temporal adversarial networks (STAN) which learn the spatio-temporal features of normal patterns. Learning the characteristics of abnormal events is very difficult because the definition of such events is not spatially or temporally bounded. Thus, it is reasonable to learn normal patterns and consider deviations from the learned patterns as abnormalities. With the proposed STAN, we directly generate spatio-temporal scene without using the optical flow map. The STAN contains two networks which are the spatio-temporal generator and the spatio-temporal discriminator. The proposed generator consists of three network modules which are a spatial encoder for encoding spatial features of frames, a bidirectional ConvLSTM for encoding temporal feature of the scene, and a spatial decoder for generating an inter-frame. By using the bidirectional configuration, the spatio-temporal features of normal patterns can be encoded in forward and backward directions. The proposed discriminator is a 3D convolutional neural network for determining whether an input sequence is real or not.</p><p>By the adversarial learning, the generator tries to generate the inter-frame in order to deceive the discriminator while the discriminator tries to discriminate between a real sequence and a fake sequence. Note that the fake sequence includes the generated inter-frame. At testing time, abnormal events are detected by using a pixel-wise loss for the generator and an adversarial loss for the discriminator.</p><p>Experimental results show that the proposed method performs competitively compared to the state-of-the-art methods, especially on complex scenes with large objects and frequent occlusions. In addition, we visually interpret the proposed networks by locating detected abnormal events through both the generator and the discriminator.</p><p>The remainder of this paper is organized as follows. Section 2 describes the proposed method in detail. In Section 3, experimental results are shown for qualitative and quantitative evaluation. Finally, conclusions are drawn in Section 4. <ref type="figure">Fig. 1</ref> shows the overall procedure of the proposed STAN at training time. The proposed spatio-temporal generator generates the inter-frame by considering neighboring frames. Then, consecutive frames including the generated inter-frame are considered as a fake sequence while a real sequence contains only real frames. Through the adversarial learning, the spatiotemporal discriminator is trained to determine whether the input sequence is real or fake. The generator is trained to generate the inter-frame that can fool the discriminator. Details are described in following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Spatio-temporal generator</head><p>The role of the proposed generator is to generate a clear frame for the normal scene and a distorted frame for the abnormal scene. In video data, each frame has a high correlation with neighboring frames. We devise the bidirectional ConvLSTM to generate the inter-frame by considering the spatio-temporal characteristics of the neighboring frames. With the bidirectional configuration, the spatio-temporal features of normal patterns are encoded effectively. As a result, scenes with spatio-temporal abnormal patterns can be sensitively detected.</p><p>The proposed generator consists of the spatial encoder, the bidirectional ConvLSTM, and the spatial decoder. By the spatial encoder, latent spatial features are encoded for representing visual information of each frame. The spatial features of the previous 5 frames and the later 5 frames are fed to the forward ConvLSTM and the backward ConvLSTM respectively, in order to encode the spatio-temporal features. Then, the hidden states and the cell states resulting from each Con-vLSTM module are concatenated in pairs to combine forward and backward features. As shown in <ref type="figure">Fig. 1</ref>, the combined ConvLSTM outputs the latent feature of the inter-frame. Finally, the inter-frame is generated by the spatial decoder.</p><p>To generate the desirable inter-frame, a realism loss and a pixel-wise loss are designed for the generator. By minimizing the realism loss, the generator can force the discriminator to fail to classify the fake sequence as fake. Let G ? and D ? denote the generator function with parameter ? and the discriminator function with parameter ?, respectively. Let X t ? R 224?224?1 andX t ? R 224?224?1 denote the t-th real frame and the t-th generated frame by G ? , respectively. The realism loss can be written as</p><formula xml:id="formula_0">real (?; t) = ? log(D ? (? t )),<label>(1)</label></formula><p>where? t ? R 11?224?224?1 is the fake sequence including t-th generated frameX t . In addition, by minimizing the pixel-wise loss between the t-th real frame X t and the t-th generated frameX t , the generated frame resembles the real frame in pixel-level. The pixel-wise loss can be written as</p><formula xml:id="formula_1">pixel (?; t) = G ? (X t?5 , ..., X t?1 , X t+1 , ..., X t+5 ) ? X t 2 .<label>(2)</label></formula><p>Finally, the total loss for the generator is defined as a combination of the two losses, which can be written as</p><formula xml:id="formula_2">L G (?) = t?batch real (?; t) + ? pixel (?; t),<label>(3)</label></formula><p>where ? is a hyper-parameter to balance the realism loss and the pixel-wise loss. </p><formula xml:id="formula_3">Conv1 5?5/ 112?112?16 (2, 2) 3D Conv1 5?5?5/ 7?112?112?32 Conv2 5?5/ 56?56?32 (1, 2, 2) (2, 2) Conv3 3?3/ 28?28?64 (2, 2) 3D Conv1 3?5?5/ 5?56?56?64 Conv4 3?3/ 28?28?128 (1, 2, 2) (1, 1) Forward 3?3/ 28?28?64 3D Conv3 3?3?3/ 3?28?28?128 ConvLSTM (1, 1) (1, 2, 2) Backward 3?3/ 28?28?64 3D Conv4 3?3?3/ 1?14?14?256 ConvLSTM (1, 1) (1, 2, 2) Combined 3?3/ 28?28?128 3D Conv5 1?3?3/ 1?7?7?512 ConvLSTM (1, 1) (1, 2, 2) DeConv1 3?3/ 28?28?64 (1, 1) 3D Conv5 1?3?3/ 1?7?7?1 DeConv2 3?3/ 56?56?32 (1, 1, 1) (2, 2) DeConv3 5?5/ 112?112?16 (2, 2) DeConv4 5?5/ 224?224?1 (2, 2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Spatio-temporal discriminator</head><p>The proposed spatio-temporal discriminator consists of 3D convolutional layers. The 3D CNN can reliably determine whether the input sequence is real or fake by considering both spatial and temporal characteristics of the scene <ref type="bibr" target="#b11">[12]</ref>. The discriminator outputs 7 ? 7 probability map for employing the local adversarial loss <ref type="bibr" target="#b12">[13]</ref>.</p><p>To distinguish the real sequence and the fake sequence, an adversarial loss is designed for the discriminator. By minimizing the loss, the discriminator can discriminate between the real sequence and the fake sequence. The adversarial loss for the discriminator can be written as</p><formula xml:id="formula_4">L D (?) = t?batch ? log(1 ? D ? (? t )) ? log(D ? (S t )),<label>(4)</label></formula><p>where S t ? R 11?224?224?1 is the real sequence. The first term, ? log(1 ? D ? (? t )), allows the discriminator to classify the fake sequence as fake. The second term, ? log(D ? (S t )), allows the discriminator to classify the real sequence as real.</p><p>The proposed discriminator plays two roles. First, during the adversarial learning, the discriminator helps the generator learn the spatio-temporal features of normal patterns. Second, after the adversarial learning, the discriminator alone can detect abnormal events. The discriminator is trained to determine only real sequence containing normal events as real. Thus, the discriminator considers the real sequence containing abnormal events as not real. Consequently, the second term in (4) has a higher value for video sequences including abnormal events and can be used to detect abnormalities. <ref type="table" target="#tab_1">Table 1</ref> shows architecture details of the networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Abnormality score</head><p>To quantify detected abnormalities, we devise a novel abnormality score by using the losses of the generator and the discriminator. The generator and the discriminator complement detection results of each other. Thereby, robust abnormality detection can be achieved. The proposed abnormality loss can be defined as</p><formula xml:id="formula_5">s (t) = G ? (X t?5 , ..., X t?1 , X t+1 , ..., X t+5 ) ? X t 2 ? ? s log(D ? (S t )),<label>(5)</label></formula><p>where ? s is a hyper-parameter to balance the generator detection and the discriminator detection. Then, by normalizing s (t), abnormality score s(t) at t-th frame can be written as</p><formula xml:id="formula_6">s(t) = s (t) ? min t s (t) max t s (t) ? min t s (t)</formula><p>.</p><p>Note that video sequences containing abnormal events have higher abnormality scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>For experiments, we used three datasets: UCSD Ped1 <ref type="bibr" target="#b13">[14]</ref>, UCSD Ped2 <ref type="bibr" target="#b13">[14]</ref> and Avenue <ref type="bibr" target="#b14">[15]</ref> datasets. The training sets of these datasets contain only normal events while the testing sets contain both normal and abnormal events. The UCSD Ped1 dataset consists of 34 clips for training and 36 clips for testing. The UCSD Ped2 dataset consists of 16 clips for training and 12 clips for testing. The Avenue dataset consists of 16 clips for training and 21 clips for testing. The UCSD Ped1 and Ped2 datasets contain the motion of small objects in a broad area. On the other hand, the Avenue dataset contains complex motion of large objects and frequent occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation details</head><p>We used the Adam <ref type="bibr" target="#b15">[16]</ref> to optimize the networks with a learning rate of 0.0002 and a batch size of 3. At training time, first, only the generator was trained to minimize the pixel-wise loss. Then, the generator and the discriminator were trained in the adversarial way to minimize L G (?) and L D (?) alternately. We set ? as 1 in <ref type="formula" target="#formula_2">(3)</ref> and ? s as maximum value ratio divided by 10 of the two terms in <ref type="bibr" target="#b4">(5)</ref>. At the end of the generator and the discriminator, the tanh and the sigmoid were used as activation functions respectively. For all the other parts, the exponential linear unit (ELU) was used.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Performance evaluation</head><p>To evaluate the performance of the proposed method, two evaluation metrics were employed. First, we employed the frame-level evaluation <ref type="bibr" target="#b22">[23]</ref> based on the area under curves (AUC). Second, we employed event-level evaluation <ref type="bibr" target="#b7">[8]</ref> based on the number of detected events. <ref type="table" target="#tab_2">Table 2</ref> shows frame-level performance compared with handcraft and sparsity based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> and deep learning based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. The AUC values of the other methods were taken from each paper and <ref type="bibr" target="#b18">[19]</ref>. Interestingly, the proposed method even with G ? or D ? showed respectably high AUC values on the Avenue. The proposed method with both G ? and D ? outperformed the other methods including state-of-the-art methods. On the other hand, the performance on the UCSD Ped1 showed a bit lower than that on the other datasets. We observed that the UCSD Ped1 included several corrupted frames in testing clips as shown in <ref type="figure">Fig. 2</ref>. By the proposed method, these corrupted frames were detected as abnormal events even though such frames were denoted as normal events in ground truth labels. <ref type="table" target="#tab_3">Table 3</ref> shows event-level performance comparisons with correct detection, false alarm, and precision. The proposed method achieved higher precision results compared to the other methods in event-level evaluation.  works. <ref type="figure" target="#fig_0">Fig. 3(c)</ref> shows visualization results obtained from the pixel-wise loss between the real frame and the generated frame. The proposed generator could detect the abnormal event regions such as a car, a bicycle, and a jumping person. Further, we visualized gradients of the discriminator using guided backpropagation <ref type="bibr" target="#b23">[24]</ref>. <ref type="figure" target="#fig_0">Fig. 3(d)</ref> shows gradients of activation at the output of the discriminator with respect to the input real sequences including the abnormal events. We observed that magnitude of the gradients was significantly high around the abnormal event regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Interpretation of the proposed networks</head><p>Through these results, we confirmed that each network performs abnormal event detection with the proper basis. The proposed networks could not only determine whether each frame is abnormal or not, but also detect where abnormal events occur at each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper, we proposed the novel generative model based abnormal event detection method. To effectively represent the spatio-temporal features of normal patterns, the spatiotemporal generator and the spatio-temporal discriminator were trained in the adversarial way. As a result, the proposed method achieved competitive performance compared to the state-of-the-art methods. In particular, the proposed method far outperformed other methods on the dataset containing complex motion and frequent occlusions. Further, by visualizing the abnormal event regions detected by the proposed method, we could interpret how the STAN determine abnormal event at each scene.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3</head><label>3</label><figDesc>shows localization and visualization of abnormal events detected by the proposed method for interpreting the net-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Examples of the corrupted frames in UCSD Ped1. Localization and visualization results of the abnormal events on UCSD Ped1 (first row), UCSD Ped2 (Second row), and Avenue (third row). (a) Real frame, (b) Generated frame, (c) Abnormality visualization by the generator, and (d) Abnormality visualization by the discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Architectures of the proposed networks.</figDesc><table><row><cell></cell><cell>Generator</cell><cell></cell><cell cols="2">Discriminator</cell></row><row><cell>Layer</cell><cell>Filter/ Output Size Stride (h?w?c)</cell><cell>Layer</cell><cell>Filter/ Stride</cell><cell>Output Size (l?h?w?c)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Frame-level performance of the proposed method.</figDesc><table><row><cell></cell><cell></cell><cell>AUC (%)</cell><cell></cell></row><row><cell>Method</cell><cell>UCSD Ped1</cell><cell>UCSD Ped2</cell><cell>Avenue</cell></row><row><cell>MPPCA [17]</cell><cell>59.0</cell><cell>69.3</cell><cell>N / A</cell></row><row><cell>Social force (SF) [18]</cell><cell>67.5</cell><cell>55.6</cell><cell>N / A</cell></row><row><cell>MPPCA + SF [14]</cell><cell>68.8</cell><cell>61.3</cell><cell>N / A</cell></row><row><cell>Detection at 150fps [15]</cell><cell>91.8</cell><cell>N / A</cell><cell>80.9</cell></row><row><cell>AMDN [19]</cell><cell>92.1</cell><cell>90.8</cell><cell>N / A</cell></row><row><cell>Conv-AE [8]</cell><cell>81.0</cell><cell>90.0</cell><cell>70.2</cell></row><row><cell>ConvLSTM-AE [20]</cell><cell>75.5</cell><cell>88.1</cell><cell>77.0</cell></row><row><cell>Unmasking [21]</cell><cell>68.4</cell><cell>82.2</cell><cell>80.6</cell></row><row><cell>Optical flow-GAN [11]</cell><cell>97.4</cell><cell>93.5</cell><cell>N / A</cell></row><row><cell>Stacked RNN [22]</cell><cell>N / A</cell><cell>92.2</cell><cell>81.7</cell></row><row><cell>Proposed method (G?)</cell><cell>81.6</cell><cell>95.9</cell><cell>86.6</cell></row><row><cell>Proposed method (D?)</cell><cell>71.6</cell><cell>86.2</cell><cell>81.8</cell></row><row><cell>Proposed method (G? and D?)</cell><cell>82.1</cell><cell>96.5</cell><cell>87.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Event-level performance of the proposed method.</figDesc><table><row><cell>Method</cell><cell cols="2">Correct Detection / False Alarm</cell><cell cols="2">Precision (%)</cell></row><row><cell></cell><cell>UCSD</cell><cell>UCSD</cell><cell>UCSD</cell><cell>UCSD</cell></row><row><cell></cell><cell>Ped1</cell><cell>Ped2</cell><cell>Ped1</cell><cell>Ped2</cell></row><row><cell>IT-AE [8]</cell><cell>36 / 11</cell><cell>12 / 3</cell><cell>76.6</cell><cell>80.0</cell></row><row><cell>Conv-AE [8]</cell><cell>38 / 6</cell><cell>12 / 1</cell><cell>86.3</cell><cell>92.3</cell></row><row><cell>Proposed method (G? and D?)</cell><cell>37 / 3</cell><cell>12 / 0</cell><cell>92.5</cell><cell>100.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A system for learning statistical motion patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1450" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unusual event detection in crowded scenes by trajectory analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="1300" to="1304" />
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Swarm intelligence for detecting interesting events in crowded environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vagia</forename><surname>Kaltsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Briassouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leontios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael Gerasimos</forename><surname>Hadjileontiadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strintzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2153" to="2166" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anomalous behaviour detection using spatiotemporal oriented energies, subset inclusion histogram comparison and eventdriven processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zaharescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="563" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-objective based spatio-temporal feature representation learning robust to expression intensity variations for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wissam</forename><surname>Dae Hoe Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyeok</forename><surname>Baddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Man</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Anomaly detection in video using predictive convolutional long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jefferson</forename><surname>Ryan Medel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Savakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00390</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucio</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Webb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Viral Bhalodia, and Nuno Vasconcelos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
	<note>Anomaly detection in crowded scenes</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: A space-time mrf for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Detecting anomalous events in videos by learning deep representations of appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Remembering history with convolutional lstm for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="439" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2914" to="2922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
