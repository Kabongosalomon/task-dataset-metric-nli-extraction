<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Uncertainty Estimation for Semantic Segmentation in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yu</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Ting</forename><surname>Hsu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yueh</forename><surname>Chiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Fan</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Umbo Computer Vision</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Uncertainty Estimation for Semantic Segmentation in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Uncertainty</term>
					<term>Segmentation</term>
					<term>Video</term>
					<term>Efficient</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Uncertainty estimation in deep learning becomes more important recently. A deep learning model can't be applied in real applications if we don't know whether the model is certain about the decision or not. Some literature proposes the Bayesian neural network which can estimate the uncertainty by Monte Carlo Dropout (MC dropout). However, MC dropout needs to forward the model N times which results in N times slower. For real-time applications such as a self-driving car system, which needs to obtain the prediction and the uncertainty as fast as possible, so that MC dropout becomes impractical. In this work, we propose the region-based temporal aggregation (RTA) method which leverages the temporal information in videos to simulate the sampling procedure. Our RTA method with Tiramisu backbone is 10x faster than the MC dropout with Tiramisu backbone (N = 5). Furthermore, the uncertainty estimation obtained by our RTA method is comparable to MC dropout's uncertainty estimation on pixel-level and frame-level metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, deep learning has become a powerful tool in various applications. The uncertainty estimation in deep learning has got more attention as well. Some applications need not only the prediction of the model but also the confidence of this prediction. For instance, in the biomedical field, the confidence of cancer diagnosis is essential for doctors to make the decision. For self-driving car system to avoid accidents, the model should know what situation haven't seen before and then return to human control. There are some methods of uncertainty estimation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2]</ref> for deep learning have been proposed, but most of them need to sample several times, which is harmful to real-time applications. Slow inference of uncertainty estimation is an important issue before applying on real-time applications.</p><p>In general, neural networks can only generate prediction instead of uncertainty. Lack of uncertainty estimation is a shortcoming of neural networks. Bayesian neural networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7]</ref> solve this problem by modeling the posterior of networks weights. But they often increase computation cost and the number of model parameters. Recently, Gal et al. <ref type="bibr" target="#b9">[10]</ref> propose dropout as an approximation technique without increasing parameters which is easy to implement called MC dropout. Though MC dropout is useful and powerful, the inference is very slow because it needs to perform N (e.g., N = 50) stochastic forward pass through the network and average the results to obtain the prediction and uncertainty. Therefore, our work proposed utilizing video's temporal information to speed up the inference and also maintain the performance. For video segmentation, we can make good use of the temporal information based on the properties of video continuity. We propose two main methods called temporal aggregation (TA) and region-based temporal aggregation (RTA). For static objects in videos, calculating the average output of N consecutive frames has the same effect as utilizing MC dropout with N samples. Hence, we propose TA method that approximates the sampling procedure of MC dropout by calculating the moving average of the outputs in consecutive frames (see <ref type="figure" target="#fig_0">Fig. 1</ref>). To obtain the correct aggregation for moving objects in videos, we utilize optical flow to catch the flow of each pixel in the frame and aggregate each pixel's output depending on the flow. This TA method can also be used to calculate any kinds of uncertainty estimation function, i.e, Entropy, Bald. In this way, we can speed up MC dropout 10 times. The specific speed up rate is depend on backbone model. For larger backbone, our method can speed up even more. Furthermore, we designed RTA based on TA. For some objects with large displacements in videos, the large shift of pixels might result in poor flow estimation and lead to wrong prediction and uncertainty estimation. Thus, RTA can dynamically assign multiplying factor, which is used to decide the weight of incoming data, depending on the reconstruction error for every pixel. For pixels that have large reconstruction error, we shall assign higher multiplying factor so that they will rely more on themselves rather than the previous prediction. With the benefits of RTA, we can get better prediction and uncertainty estimation.</p><p>In this paper, we mainly contribute three points:</p><p>-We propose temporal aggregation (TA) method to solve the slow speed problem of MC dropout. We speed up more than 10 times comparing with MC dropout. -We propose region-based temporal aggregation (RTA) method to further improve the performance of TA by considering the flow accuracy. With our RTA method, we get comparable accuracy in video segmentation on CamVid dataset with only less than 2% drop on mean IoU metric. -We obtain nice uncertainty estimation which is evaluated in pixel-level and frame-level metric. Our uncertainty estimation even outperforms MC dropout on frame-level metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>First, We will introduce uncertainty estimation methods in Sec. 2.1. Next, some important segmentation models will be mentioned in Sec. 2.2. Finally, we introduce some works leverage the temporal information in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Uncertainty Estimation</head><p>Uncertainty is an important issue for some current decision-making tasks, i.e., self-driving car, drone, robotics. We can just blindly assume that the prediction of the model is accurate but sometimes the truth is not. To really understand what a model doesn't know is a critical issue nowadays. It helps us to know how much we can trust the prediction of the model. However, the majority of the segmentation works cannot generate a probabilistic output with a measure of model uncertainty. Bayesian neural networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7]</ref> is a well-known method that model uncertainty in neural networks. They turn deep learning model into a probabilistic model by learning the distribution over networks weights. Bayesian neural network's prediction is hard to obtain. Variational inference <ref type="bibr" target="#b11">[12]</ref> is often used to approximate the posterior of the model. Blundell et al. <ref type="bibr" target="#b1">[2]</ref> model a Gaussian distribution over weights in the neural networks rather than having a single fixed value, however, each weight should contain mean and variance to represent a Gaussian distribution that doubles the number of parameters. Recently, Gal et al. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref> use dropout as an approximation of variational inference. When testing time, they keep dropping neurons, which can be interpreted as adding a Bernoulli distribution over the weights. This technique called MC dropout that has been successfully used in camera relocalisation <ref type="bibr" target="#b20">[21]</ref> and segmentation <ref type="bibr" target="#b19">[20]</ref>. However, it still needs to sample model many times to estimate uncertainty. In this work, we propose leveraging video temporal information to speed up the MC dropout sampling process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic Segmentation</head><p>Semantic image segmentation that uses convolutional neural networks has achieved several breakthroughs in recent years. It is a pixel-wise labeling task that classifies every pixel into defined class. Long et al. <ref type="bibr" target="#b23">[24]</ref>, popularize CNN architectures for dense predictions without any fully connected layers. This method allowed segmentation maps to be generated for an image of any size and was also much faster compared to the patch classification approach. Ronneberger et al. <ref type="bibr" target="#b29">[30]</ref> propose U-net, which is an encoder-decoder architecture that focuses on improving more accurate boundaries. Howard et al. <ref type="bibr" target="#b14">[15]</ref> combined the ideas of MobileNets Depthwise Separable Convolutions with UNet to build a high speed, low parameter Semantic Segmentation model. PSP-Net <ref type="bibr" target="#b33">[34]</ref> uses ResNet as the backbone and utilizes global information from pyramid layers to provide more accurate semantics. DeepLab <ref type="bibr" target="#b4">[5]</ref> replaced fully connected CRF(conditional random field) to the last layer of CNN for improving the performance. In this work, we select Bayesian SegNet <ref type="bibr" target="#b0">[1]</ref> and Tiramusi <ref type="bibr" target="#b17">[18]</ref> to demonstrate our idea. Both methods are encoder-decoder architecture. Tiramisu is the state-of-the-art of CamVid dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Leverage Temporal Information</head><p>Previously, some works make use of superpixels <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref> , patches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29]</ref>, object proposal <ref type="bibr" target="#b27">[28]</ref>, optical flow <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref> as temporal information to reduce the computational complexity. Furthermore, video segmentation has gained significant improvement based on temporal information. Among all these temporal information, the most recent works heavily rely on optical flow. Srivastava et al. <ref type="bibr" target="#b32">[33]</ref> use the image in one stream, and optical flow in the other stream to recognize actions in the video. Simonyan et al. <ref type="bibr" target="#b31">[32]</ref> simultaneously predict pixel-wise object segmentation and optical flow in videos. Cheng et al. <ref type="bibr" target="#b5">[6]</ref> emphasize temporal information at the frame level instead of the final box level to improve detection accuracy. To enhance the reference feature map, they utilize optical flow network the work of Zhu et al. <ref type="bibr" target="#b34">[35]</ref> to estimate the motions between nearby frames and the reference frame. They then aggregate feature maps warping from nearby frames to the reference frame according to the flow motion. Briefly speaking, all these works utilize optical flow appropriately in video tasks. To the best of our knowledge, we are the first work that uses optical flow as temporal information to speed up uncertainty estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We first give a brief introduction of Bayesian neural networks with Monte Carlo dropout (MC) in Sec. 3.1. Next, we introduce our temporal aggregation Monte Carlo dropout (TA-MC) in Sec. 3.2. Finally, we propose a region-based temporal aggregation Monte Carlo dropout (RTA-MC) which can further improve both the accuracy and uncertainty estimation in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary: Bayesian Neural Network with MC dropout</head><p>Bayesian neural networks are probabilistic models that do not learn a set of deterministic parameters but a distribution over those parameters. It aims to learn the posterior distribution of the neural network's weights W given training data X and Y .</p><p>The posterior distribution, which is denoted as p (W |X, Y ), usually cannot be evaluated analytically. Variational inference is often used to approximate the posterior distribution. Given an approximating distribution over the network's weights, q (W ), we minimize the Kullback-Leibler (KL) divergence between p (W |X, Y ) and q (W ).</p><formula xml:id="formula_0">KL (q (W ) p (W |X, Y ))<label>(1)</label></formula><p>Dropout variational inference is a useful technique for approximating posterior distribution. Dropout can be viewed as using the Bernoulli distribution as the approximation distribution q (W ). At testing time, the prediction can be approximated by sampling model N times which is referred as Monte Carlo dropout (MC).</p><formula xml:id="formula_1">p (y * |x * , X, Y ) ? 1 N N n=1 p (y * |x * ,? n )<label>(2)</label></formula><p>The uncertainty of the classification can be obtained by several functions:</p><p>(a) Entropy <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_2">H [y|x, X, Y ] = ? c p (y = c|x, X, Y ) log p (y = c|x, X, Y )<label>(3)</label></formula><p>(b) BALD <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_3">I [y, ?|x, X, Y ] = H [y|x, X, Y ] ? E p(?|X,Y ) [H [y|x, ?]]<label>(4)</label></formula><p>(c) Variation ratio <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_4">variation-ratio [x] = 1 ? max y p (y|x, X, Y )<label>(5)</label></formula><p>(d) Mean standard deviation (Mean STD) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>:</p><formula xml:id="formula_5">? c = E q(?) p (y = c|x, ?) 2 ? E q(?) [p (y = c|x, ?)] 2 (6) ? (x) = 1 c c ? c<label>(7)</label></formula><p>Optical flow ( ) Output ( ) Bayesian neural networks with MC dropout can obtain better performance and uncertainty estimation. However, it requires to sample N times (e.g., N = 50) for predicting each image, which is N times slower than the original network. For real-time applications such as self-driving cars, which needs to obtain the prediction and uncertainty estimation as fast as possible, so that MC dropout becomes impractical. In this work, we propose temporal aggregation MC dropout to speed up the MC dropout process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Temporal Aggregation MC Dropout (TA-MC)</head><p>Our temporal aggregation MC dropout (TA-MC) method utilizes the temporal property in videos. Since a video contains consecutive frames, same objects may appear in many different frames and thus will be forwarded by the Bayesian model repeatedly. If a video contains static frames (i.e., a static scene observed by a static camera), the average output of N consecutive frames is the same as MC dropout with N samples. For video segmentation, though the frames are not static (i.e., the objects in the scene and the camera are both moving), the consecutive frames are still similar. The objects are often shifted slightly in the next frame. Hence, by warping each pixel to the new position in the next frame, we can aggregate the outputs of the pixels in consecutive frames correctly. Notations. Given a video V = {I 1 , I 2 , ..., I t , ..., I T } where I t is the t th frame and T is the length of the video, the outputs of the Bayesian neural network are denoted as O = {O 1 , O 2 , ..., O t , ..., O T }. Note that O are the outputs without MC sampling, which means each frame is forwarded by Bayesian model (with</p><formula xml:id="formula_6">I1 Flow ( ?1? ) Warping Function ( ( ?1 , ?1? )) I2 I2 - Reconstruction Error ( ) ( )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiplying Factor</head><p>Previous Frame ( ?1 ) Current Frame ( ) <ref type="figure">Fig. 3</ref>: Region-based temporal aggregation (RTA). We design a step function to acquire dynamic multiplying factor ? for improving the TA method. For regions that have wrong optical flow estimation (i.e., the reconstruction error is greater than a threshold ?), we use a larger multiplying factor to let the pixels rely more on itself rather than the previous predictions. See more detailed in Sec. <ref type="bibr">3.3.</ref> dropout) for only one time. To get the aggregated predictions, we calculate the optical flow between consecutive frames F = {F 1?2 , F 2?3 , ..., F T ?1?T } where F t?t+1 indicates the optical flow from frame I t to frame I t+1 . Aggregated Prediction. The prediction P t for each frame I t is obtained by calculating the weighted moving average of the outputs</p><formula xml:id="formula_7">O 1:t = {O 1 , O 2 , ..., O t }: P t = O t if t = 1, O t ? ? + W (P t?1 , F t?1?t ) ? (1 ? ?) otherwise,<label>(8)</label></formula><p>where W (?) is a pixel-wise warping function that moves the input values (e.g., P t?1 ) to their new positions depending on the given optical flow (e.g., F t?1 ). The output of W (?) has the same dimension as the input. ? is a multiplying factor which decides the weights of the incoming data and previous data. The whole system of our TA-MC dropout for video segmentation is shown in <ref type="figure" target="#fig_1">Fig. 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Region-Based Temporal Aggregation MC Dropout (RTA-MC)</head><p>Our TA-MC dropout works for most of the case; however, when the optical flow estimation is wrong, it will cause the uncertainty estimation inaccurate. For some regions that contain fast moving objects or occlusion, the optical flow may not be accurate. Bad flow estimation results in calculating moving average on the wrong patch and thus getting wrong prediction and uncertainty for those pixels.</p><p>To solve this problem, we propose the region-based temporal aggregation (RTA) that can dynamically assign different multiplying factor (? in Eq. 8) for every pixel depending on its reconstruction error. The reconstruction error E of warping I t?1 to I t is derived from the pixel-wise difference of the warped frame W (I t?1 , F t?1?t ) and I t .</p><formula xml:id="formula_8">E = |I t ? W (I t?1 , F t?1?t ) |.<label>(9)</label></formula><p>where E is a matrix contains pixel-wise reconstruction error and E ij ? [0, 255]. For a pixel that has large reconstruction error, we will give it a higher multiplying factor ? err since the optical flow may be inaccurate and thus the prediction should rely more on itself rather than the previous predictions. We design a decision function A (E) that decides ? for every pixel depending on the reconstruction error (see <ref type="figure">Fig. 3</ref>).</p><formula xml:id="formula_9">? ij = A (E ij ) = ? acc if E ij ? ?, ? err otherwise, ? = {? ij } ij<label>(10)</label></formula><p>where ? ij is the multiplying factor for the pixel in position (i, j). A(?) is a step function with a threshold ? which is a hyper-parameter deciding whether the optical flow has bad estimation or not. ? acc and ? error are also hyper-parameters which indicate the multiplying factor for good flow estimation and bad flow estimation, respectively. ? err should be higher than ? acc (e.g., ? acc = 0.2 and ? err = 0.7). Then, we simply replace ? in Eq. 8 with ? in Eq. 10 to obtain our region-based temporal aggregation MC dropout (RTA-MC). By applying our RTA method, the mismatch aggregation will be attenuated and can further improve the prediction and the uncertainty estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we describe the dataset that used for the video segmentation in Sec. <ref type="bibr" target="#b3">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We apply the Bayesian SegNet model <ref type="bibr" target="#b19">[20]</ref> and Tiramisu model <ref type="bibr" target="#b17">[18]</ref> to demonstrate our TA and RTA method. We train both models by the same setting in the original paper. We set ? in Eq. 8 for TA-MC dropout to 0.2. For RTA-MC dropout, we use threshold ? as 10 to determine whether the flow estimation is wrong. Note that the reconstruction error of flow is in the range of [0, 255] and the average error is about 2. Hence, we choose the threshold ? slightly higher than the average error. ? acc and ? err in Eq. 10 are set to 0.2 and 0.7, respectively. We use FlowNet 2.0 <ref type="bibr" target="#b15">[16]</ref> for our optical flow estimation. We implement all methods in Pytorch <ref type="bibr" target="#b25">[26]</ref> framework and experiment on GTX 1080 for time measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results of Video Segmentation</head><p>We compare MC dropout with our TA-MC dropout and RTA-MC dropout on CamVid dataset by two models. We first show the performance of the video segmentation on several different metrics: (1) pixel-wise classification accuracy on every class, (2) class average accuracy (class avg.), (3) overall pixel-wise classification accuracy (global avg.), (4) mean intersection over union (mean IoU) and <ref type="bibr" target="#b4">(5)</ref>  Our TA-MC dropout can reach comparable accuracy, and the RTA-MC dropout further improves the performance with only 1.2% drop on mean IoU metric. For Tiramisu, our methods also can reach comparable accuracy.</p><p>In the case of comparable accuracy, our method can further speed up the inference time. Since our methods only need to forward one time for each frame; while MC dropout needs to sample N times. For SegNet, we can obtain almost 11 times speed up. Note that our TA-MC dropout and RTA-MC dropout can perform the same speed as the only difference between them is the multiplying factor (? in Eq. 8) which doesn't affect the speed. The inference time of the RTA-MC dropout mainly contains the inference time of the Bayesian SegNet model and the FlowNet 2.0 model which are 0.04 seconds and 0.13 seconds, respectively. FlowNet 2.0 model takes 70% of the whole inference time. If we use the bigger segmentation model, we can get a better improvement in the speed. Therefore, we use Tiramisu model which is the state-of-the-art model in CamVid but 6x slower than SegNet to show better speed up ratio. For Tiramisu, Our method can achieve 31x faster than MC dropout sample 50 times. To fairly compare inference time, we reduce the MC dropout sample time to 5 times. The accuracy becomes the same as our methods. In this case, our methods are still 10x faster than MC dropout. This table shows that in the same accuracy level, our methods can speed up inference time 10x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results of Uncertainty Estimation</head><p>We evaluate the uncertainty estimation in pixel-level and frame-level metrics. Pixel-level Metric. The pixel-level evaluation is inspired by Precision-Recall Curve metric in <ref type="bibr" target="#b21">[22]</ref>. This curve shows the accuracy of the remained pixels as removing pixels with uncertainty larger than different percentile thresholds. De- tail explanation is in <ref type="figure">Fig. 4</ref>. A reliable uncertainty estimation should let the PR-Curve monotonically decrease. We compare MC dropout and our TA and RTA method with different uncertainty function in <ref type="figure" target="#fig_2">Fig. 5</ref>. Left four figures are the results of SegNet backbone. Right four figures are the results of Tiramisu backbone. All results show that as the recall percentage drop from 1 to 0.5, the mean IoU of all methods monotonically increase which means the uncertain pixels is correlated to misclassified pixels. Although MC dropout has the highest accuracy almost at all percentage, our TA-based methods are still comparable to MC dropout. TA-MC and RTA-MC have similar results in PR-Curve, but at the frame-level metric, RTA-MC will outperform TA-MC.</p><p>Frame-level Metric. Pixel-level metric is good to evaluate the uncertainty estimation. However, pixel-wise uncertainty estimation is hard to leverage in real applications. For example, active learning system wants to find which frame is valuable to be labeled rather than decides which pixel should be labeled. Here, we propose frame-level uncertainty metrics to show that our uncertainty estimation can work well and faster in real applications. The procedure is shown in <ref type="figure">Fig. 6</ref>. First, frames are ranked by the error of prediction as the ground truth ranking sequence. Then, we rank frames by the uncertainty estimation and evaluate the uncertainty ranking sequence by two metrics: Kendall tau <ref type="bibr" target="#b22">[23]</ref> and Ranking IoU. Kendall tau is a well-known ranking metric measures how the ranking sequence is similar to the ground truth sequence. The value is bounded in 1 (fully identical sequence) and -1 (fully different sequence). sequence similarity, but real applications pay more attention on higher ranking similarity than whole ranking. Therefore, we define a novel frame-level ranking metric called Ranking IoU. Given a percentage of frame P f to retrieve, we retrieve frames depend on error G (P f ) = {g 1 , g 2 , ..., g m , ..., g M } and uncertainty U (P f ) = {u 1 , u 2 , ..., u m , ..., u M }. The ranking IoU is:</p><formula xml:id="formula_10">Ranking IoU = G (P f ) ? U (P f ) G (P f ) ? U (P f )<label>(11)</label></formula><p>Larger Ranking IoU means that those frames we choose are hard to predict, so they are valuable to be labeled. Left of the <ref type="table" target="#tab_7">Table 3</ref> shows the ranking IoU performance of SegNet backbone between different methods and uncertainty functions. We show performance in different P f . In column P f = 10%, TA in Variation Ratio has 52.2% which is larger than RTA's 47.8% about 4.4%; however, 10% of test data only contains 23 frames so that RTA and TA actually only have 1 frame difference. For P f = 30% which is a practical percentage for real applications, RTA outperforms other methods in all uncertainty functions. The best score of RTA 69.6% is larger than MC dropout's best score 66.7% about 3%, which means our uncertainty method can more retrieve 3% of hardest frames. For Entropy and Variation Ratio, RTA outperforms other methods in almost all percentage. The right of the <ref type="table" target="#tab_7">Table 3</ref> shows the Ranking IOU of tiramisu backbone. The results are similar to SegNet backbone that for Entropy and Variation Ratio, RTA outperforms other methods. <ref type="table" target="#tab_4">Table 2</ref> and <ref type="table" target="#tab_7">Table 3</ref> indicate that RTA can generate high-quality uncertainty. <ref type="figure" target="#fig_3">Fig. 7</ref> shows the visualization of prediction, uncertainty, and error. It shows that RTA's uncertainty quality is comparable to MC dropout and the large uncertainty pixels are correlated to the misclassified pixels.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we propose the region-based temporal aggregation (RTA) method to simulate the sampling procedure of Monte Carlo (MC) dropout for video segmentation. Our RTA method utilizes the temporal information from videos and only needs to sample one time to generate the prediction and the uncertainty for each frame. Compared to using general MC dropout, RTA can achieve similar performance on CamVid dataset with only 1.2% drop on mean IoU metric and incredibly speed up the inference process 10.97 times. Moreover, the uncertainty obtained by the RTA method is also comparable on pixel-level metric and even outperforms MC dropout on frame-level metric when using Entropy and Variation Ratio as the uncertainty estimation function. With our faster approach, we expect to extend our method on instance segmentation task in future work. In real-time applications, it's more important to obtain the instance-level uncertainty more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgment</head><p>We thank Umbo CV, MediaTek, MOST 107-2634-F-007-007 for their support. The top row is the input image, with the ground truth shown in the second row. The third row and fourth row show the segmentation prediction of MC dropout and RTA-MC respectively. Its corresponding uncertainty map is also shown in the fifth and sixth row where the more brighter space represents higher uncertainty. We even show the error in the last two rows where the red space represents the wrong prediction, and the tiffany-blue space represents correct prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Comparison of MC dropout and TA-MC/RTA-MC dropout. Left: MC dropout samples N times for every frame, which cause N times slower. Right: TA-MC/RTA-MC dropout aggregates previous output into final prediction and uncertainty. For every frame, TA-MC/RTA-MC dropout only needs to calculate segmentation model and optical flow once.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The process of our temporal aggregation MC dropout (TA-MC) for video segmentation. Every time step Bayesian segmentation model sample one output and calculate optical flow. To average the incoming output, we warp the previous prediction depend on optical flow. Final prediction is weighted sum with multiplying factor ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Pixel-level precision-recall curves. Left four figures are the results of SegNet backbone. Right four figures are the results of Tiramisu backbone. We use mean IoU as the precision metric. We show the comparison of MC dropout, TA-MC dropout and RTA-MC dropout on four different uncertainty estimation methods. Our methods achieve comparable results especially when using Entropy and Variation Ratio as the uncertainty estimation functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Results comparison on CamVid dataset(MC dropout v.s RTA-MC dropout).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>First, in Sec. 4.3, we compare the performance of video segmentation and the inference time. Second, in Sec. 4.4, we show that our methods can obtain comparable uncertainty estimation. The frames are labeled every 1 second, and each pixel is labeled into 11 classes such as sky, building, road, car, etc. There are total 701 labeled frames split into 367 training frames, 101 validation frames and 233 test frames. All frames are resized to 360x480 pixels in our experiments.</figDesc><table><row><cell>4.1 Dataset CamVid [3] is a road scene segmentation dataset which contains four 30Hz Building Tree Sky Car Sign-Symbol Road Pedestrian Fence Column-Pole Side-walk Bicyclist Class avg. Global avg. Mean IoU Inference Time (s) Speed Up Ratio SegNet MC (N=50) 88.9 88.7 95.0 89.0 49.4 95.8 73.5 51.4 43.3 93.2 57.0 75.0 90.6 63.7 2.00 1 SegNet TA-MC 88.6 89.7 94.5 88.2 48.1 95.3 70.7 44.7 36.4 94.1 52.7 73.0 90.3 62.1 0.18 10.97 SegNet RTA-MC 88.4 89.3 94.9 88.9 48.7 95.4 73.0 45.6 41.4 94.0 51.6 73.7 90.4 62.5 0.18 10.97 Tiramisu MC (N=50) 89.7 87.2 95.6 84.9 58.4 95.1 82.5 54.1 49.6 84.6 52.3 75.8 89.8 64.0 11.72 1 Tiramisu MC (N=5) 88.7 86.6 95.4 83.7 58.4 94.6 80.6 52.0 49.2 84.0 55.0 75.3 89.2 62.4 1.17 10.00 Tiramisu TA-MC 90.3 87.4 94.8 84.2 55.8 94.5 79.2 51.3 40.6 85.6 46.7 73.8 89.6 62.3 0.37 31.58 RTA-MC videos. Method Tiramisu 90.1 87.1 94.9 84.1 56.7 94.7 79.2 48.1 42.2 85.4 49.8 73.9 89.5 62.4 0.37 31.58</cell></row></table><note>.1 and our implementation details in Sec. 4.2. Next, we compare our TA-MC dropout and RTA-MC dropout with MC dropout in several different aspects.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance</figDesc><table /><note>test on CamVid dataset. Upper three rows are comparisons of SegNet backbone; Lower four rows are comparisons of Tiramisu backbone. Both com- parisons show that our methods can speed up more than 10x with only 1-2 percentage drop. For fairly comparison, we reduce the Tiramisu MC sample time to N=5 to get the same accuracy as our methods. In this situation, our methods are still 10x faster.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>inference time. The results are shown in Table 1. For SegNet,</figDesc><table><row><cell>Test dataset</cell></row><row><cell>...</cell></row><row><cell>Percentage of recall pixel</cell></row><row><cell>Uncertain</cell></row><row><cell>Pixel ranking by uncertainty</cell></row><row><cell>Certain</cell></row><row><cell>1</cell></row><row><cell>Fig. 4: Explanation of the Pixel-level metric Precision-Recall curve. First, calculate</cell></row></table><note>the uncertainty map of all test data. Then rank all pixels by uncertainty value. The horExplanizontal axis is the percentage of recall pixel which means we keep how many percentages of most certain pixels to calculate precision. The vertical axis is the mIoU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>shows the comparison of Kendall tau by SegNet and Tiramisu backbone.ThoughTable 2shows that the highest scores appears in Mean STD and BALD, RTA-MC outperforms MC in Entropy and Variation Ratio. It also shows that RTA-MC improves frame-level ranking compare to TA-MC. It is attributed to the decision function A(E) which reduces the uncertainty value of pixels with wrong flow estimation which harm the frame-level uncertainty. Kendall tau compares the whole Explanation of frame-level metric. First, calculate the error rate of each test data frame by looking at the ground truth to get the error ranking sequence. Second, calculate the frame uncertainty of all test data to get the uncertainty ranking sequence. Then measure the similarity between two sequences by Kendall tau and Ranking IOU.</figDesc><table><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>N-1</cell><cell>N</cell></row><row><cell>Error ranking</cell><cell></cell><cell>...</cell><cell></cell><cell></cell></row><row><cell>Correct</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Error</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Uncertainty ranking</cell><cell></cell><cell>...</cell><cell></cell><cell></cell></row><row><cell>Certain</cell><cell></cell><cell>Uncertain</cell><cell></cell><cell></cell></row><row><cell>Fig. 6:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Comparison of Kendall tau. For both backbone, the result shows that RTA-MC has the highest value in Entropy and Variation Ratio. For overall performance, RTA-MC is comparable to MC dropout. Mean STD MC 52.2 66.7 71.6 86.4 TA 43.5 65.2 65.5 74.1 RTA 43.5 69.6 69.8 82.7 BALD MC 47.8 66.7 71.6 87.7 TA 43.5 62.3 64.7 74.1 RTA 43.5 69.6 67.2 82.1 Mean STD MC 30.4 63.8 76.7 86.4 TA 47.8 75.4 74.1 82.0 RTA 43.4 68.1 73.3 82.1 BALD MC 30.4 62.3 72.4 86.4 TA 43.5 71.0 71.6 80.2 RTA 47.8 66.7 71.6 80.9</figDesc><table><row><cell>Metric</cell><cell>Method</cell><cell>Percentage 10% 30% 50% 70%</cell><cell>Metric</cell><cell>Method</cell><cell>Percentage 10% 30% 50% 70%</cell></row><row><cell></cell><cell cols="2">MC 47.8 59.4 72.4 85.2</cell><cell></cell><cell cols="2">MC 34.8 60.9 70.7 86.4</cell></row><row><cell>Entropy</cell><cell>TA</cell><cell>47.8 65.2 69.8 85.8</cell><cell>Entropy</cell><cell>TA</cell><cell>47.8 63.8 71.6 84.6</cell></row><row><cell></cell><cell cols="2">RTA 47.8 66.7 73.3 88.9</cell><cell></cell><cell cols="2">RTA 47.8 63.8 74.1 86.4</cell></row><row><cell></cell><cell cols="2">MC 47.8 62.3 74.1 85.2</cell><cell></cell><cell cols="2">MC 34.8 60.9 74.1 86.4</cell></row><row><cell>Variation Ratio</cell><cell>TA</cell><cell>52.2 63.8 70.7 85.2</cell><cell>Variation Ratio</cell><cell>TA</cell><cell>47.8 65.2 72.4 85.8</cell></row><row><cell></cell><cell cols="2">RTA 47.8 65.2 76.7 88.3</cell><cell></cell><cell cols="2">RTA 52.1 65.2 75.9 87.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ranking IoU. Left table is the result of SegNet backbone. Right table is the result of Tiramisu backbone. For retrieving 30%, 50% and 70% of frames RTA-MC have the highest score by using Variation ratio.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t -1 t -2 t time</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05424</idno>
		<title level="m">Weight uncertainty in neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A highdefinition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A video representation using temporal superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2051" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="686" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transforming neural-net output levels to probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="853" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Jumpcut: nonsuccessive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="195" to="196" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Elementary applied statistics: for students in behavioral science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International Conference on Machine Learning, ICML 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1651" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02158</idno>
		<title level="m">Bayesian convolutional neural networks with bernoulli approximate variational inference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2141" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bayesian active learning for classification and preference learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lengyel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1112.5745</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5849" to="5858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1175" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Salberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2016 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="680" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modelling uncertainty in deep learning for camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4762" to="4769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5580" to="5590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A practical bayesian framework for backpropagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="472" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3227" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Seamseg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOBILE Mobile Computing and Communications Review</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="55" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting. The Journal of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10025</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
