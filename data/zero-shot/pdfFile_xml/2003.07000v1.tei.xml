<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
							<email>zhiheng@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Amazon AWS AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
							<email>pengx@amazon.com</email>
							<affiliation key="aff1">
								<orgName type="department">Amazon AWS AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Liang</surname></persName>
							<email>liadavis@amazon.com</email>
							<affiliation key="aff2">
								<orgName type="department">Amazon AWS AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Mishra</surname></persName>
							<email>misaja@amazon.com</email>
							<affiliation key="aff3">
								<orgName type="department">Amazon AWS AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
							<email>bxiang@amazon.com</email>
							<affiliation key="aff4">
								<orgName type="department">Amazon AWS AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bidirectional Encoder Representations from Transformers (BERT) has recently achieved state-of-the-art performance on a broad range of NLP tasks including sentence classification, machine translation, and question answering. The BERT model architecture is derived primarily from the transformer. Prior to the transformer era, bidirectional Long Short-Term Memory (BLSTM) has been the dominant modeling architecture for neural machine translation and question answering. In this paper, we investigate how these two modeling techniques can be combined to create a more powerful model architecture. We propose a new architecture denoted as Transformer with BLSTM (TRANS-BLSTM) which has a BLSTM layer integrated to each transformer block, leading to a joint modeling framework for transformer and BLSTM. We show that TRANS-BLSTM models consistently lead to improvements in accuracy compared to BERT baselines in GLUE and SQuAD 1.1 experiments. Our TRANS-BLSTM model obtains an F1 score of 94.01% on the SQuAD 1.1 development dataset, which is comparable to the state-of-the-art result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning representations <ref type="bibr" target="#b14">(Mikolov et al., 2013)</ref> of natural language and language model pre-training <ref type="bibr" target="#b3">(Devlin et al., 2018;</ref><ref type="bibr" target="#b15">Radford et al., 2019)</ref> has shown promising results recently. These pretrained models serve as generic up-stream models and they can be used to improve down-stream applications such as natural language inference, paraphrasing, named entity recognition, and question answering. The innovation of BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> comes from the "masked language model" with a pre-training objective, inspired by the Cloze task <ref type="bibr" target="#b22">(Taylor, 1953)</ref>. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original token based only on its context. Follow-up work including RoBERTa <ref type="bibr" target="#b13">(Liu et al., 2019b)</ref> investigated hyper-parameter design choices and suggested longer model training time. In addition, XLNet <ref type="bibr" target="#b27">(Yang et al., 2019)</ref> has been proposed to address the BERT pre-training and fine-tuning discrepancy where masked tokens were found in the former but not in the latter. Nearly all existing work suggests that a large network is crucial to achieve the state-of-the-art performance. For example, <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> has shown that across natural language understanding tasks, using larger hidden layer size, more hidden layers, and more attention heads always leads to better performance. However, they stop at a hidden layer size of 1024. ALBERT <ref type="bibr" target="#b10">(Lan et al., 2019)</ref> showed that it is not the case that simply increasing the model size would lead to better accuracy. In fact, they observed that simply increasing the hidden layer size of a model such as BERT-large can lead to significantly worse performance. On the other hand, model distillation <ref type="bibr" target="#b6">(Hinton et al., 2015;</ref><ref type="bibr" target="#b21">Tang et al., 2019;</ref><ref type="bibr" target="#b20">Sun et al., 2019;</ref><ref type="bibr" target="#b19">Sanh et al., 2019)</ref> has been proposed to reduce the BERT model size while maintaining high performance.</p><p>In this paper, we attempt to improve the performance of BERT via architecture enhancement. BERT is based on the encoder of the transformer model <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref>, which has been proven to obtain state-of-the-art accuracy across a broad range of NLP applications <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>. Prior to BERT, bidirectional LSTM (BLSTM) has dominated sequential modeling for many tasks including machine translation <ref type="bibr" target="#b2">(Chiu and Nichols, 2016)</ref> and speech recognition <ref type="bibr" target="#b4">(Graves et al., 2013)</ref>. Given both models have demonstrated superior accuracy on various benchmarks, it is natural to raise the question whether a combination of the transformer and BLSTM can outperform each individual architecture. In this paper, we attempt to answer this question by proposing a transformer BLSTM joint modeling framework. Our major contribution in this paper is two fold: 1) We propose the TRANS-BLSTM model architectures, which combine the transformer and BLSTM into one single modeling framework, leveraging the modeling capability from both the transformer and BLSTM.</p><p>2) We show that the TRANS-BLSTM models can effectively boost the accuracy of BERT baseline models on SQuAD 1.1 and GLUE NLP benchmark datasets.</p><p>2 Related work 2.1 BERT Our work focuses on improving the transformer architecture <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref>, which motivated the recent breakthrough in language representation, BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>. Our work builds on top of the transformer architecture, integrating each transformer block with a bidirectional LSTM <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997)</ref>. Related to our work, XLNet <ref type="bibr" target="#b27">(Yang et al., 2019)</ref> proposes twostream self-attention as opposed to single-stream self-attention used in classic transformers. With two-stream attention, XLNet can be treated as a general language model that does not suffer from the pretrain-finetune discrepancy (the mask tokens are seen during pretraining but not during finetuning) thanks to its autoregressive formulation. Our method overcomes this limitation with a different approach, using single-stream self-attention with an integrated BLSTM layer for each transformer layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bidirectional LSTM</head><p>The LSTM network <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997)</ref> has demonstrated powerful modeling capability in sequential learning tasks including named entity tagging <ref type="bibr" target="#b8">(Huang et al., 2015;</ref><ref type="bibr" target="#b2">Chiu and Nichols, 2016)</ref>, machine translation <ref type="bibr" target="#b1">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b26">Wu et al., 2016)</ref> and speech recognition <ref type="bibr" target="#b4">(Graves et al., 2013;</ref><ref type="bibr" target="#b18">Sak et al., 2014)</ref>. The motivation of this paper is to integrate bidirectional LSTM layers to the transformer model to further improve transformer performance. The work of <ref type="bibr" target="#b21">(Tang et al., 2019)</ref> attempts to distill a BERT model to a single-layer bidirectional LSTM model. It is relevant to our work as both utilizing bidirectional LSTM. However, their work leads to inferior accuracy compared to BERT baseline models. Similar to their observation, we show that in our experiments, the use of BLSTM model alone (even with multiple stacked BLSTM layers) leads to significantly worse results compared to BERT models. However, our proposed joint modeling framework, TRANS-BLSTM, is able to boost the accuracy of the transformer BERT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combine Recurrent Network and Transformer</head><p>Previous work has explored the combination of the recurrent network and transformer. For example, <ref type="bibr" target="#b11">(Lei et al., 2018)</ref> has substituted the feedforward network in transformer with the simple recurrent unit (SRU) implementation and achieved better accuracy in machine translation. It is similar to one of the proposed models in this paper. However, the difference is that our paper investigates the gain of the combination in BERT pre-training context, while their paper focused on the parallelization speedup of SRU in machine translation encoder and decoder context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TRANS and Proposed TRANS-BLSTM Architectures</head><p>In this section, we first review the transformer architecture, then propose the transformer bidirectional LSTM network architectures (TRANS-BLSTM), which integrates the BLSTM to either the transformer encoder or decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformer architecture (TRANS)</head><p>The BERT model consists of a transformer encoder <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> as shown in <ref type="figure" target="#fig_0">Figure  1</ref>. The original transformer architecture uses multiple stacked self-attention layers and point-wise fully connected layers for both the encoder and decoder. However, BERT only leverages the encoder to generate hidden value representation and the original transformer decoder (for generating text in neural machine translation etc.) is replaced by a linear layer followed by a softmax layer, shown in <ref type="figure" target="#fig_0">Figure 1</ref>, both for sequential classification tasks (named entity tagging, question answering) and sentence classification tasks (sentiment classification etc.). The encoder is composed of a stack of N = 12 or N = 24 layers for the BERT-base and -large cases respectively. Each layer consists of two sub-layers. The first sub-layer is a multi-head selfattention mechanism, and the second sub-layer is a simple, position-wise fully connected feed-forward network. <ref type="bibr" target="#b23">(Vaswani et al., 2017</ref>) employs a residual connection <ref type="bibr" target="#b5">(He et al., 2016)</ref> around each of the two sub-layers, followed by layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref>. That is, the output of each sublayer is LayerN orm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension of 768 and 1024 for BERT-base and BERT-large, respectively. We used the same multi-head selfattention from the original paper <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref>. We used the same input and output representations, i.e., the embedding and positional encoding, and the same loss objective, i.e., masked LM prediction and next sentence prediction, from the BERT paper <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed transformer bidirectional LSTM (TRANS-BLSTM) architectures</head><p>Previous experiments indicated that a bidirectional LSTM model alone may not perform on par with a transformer. For example, the distillation from a transformer model to a single-layer bidirectional LSTM model <ref type="bibr" target="#b21">(Tang et al., 2019)</ref> resulted in significantly lower accuracy. We also confirmed this on our experiments in Section 4.3. In this paper, we hypothesize that the transformer and bidirectional LSTM may be complementary in sequence modeling. We are motivated to investigate how a bidirectional LSTM can further improve accuracy in down-stream tasks relative to a classic transformer model. <ref type="figure" target="#fig_1">Figure 2</ref> shows the two proposed Transformer with Bidirectional LSTM architectures (denoted as the TRANS-BLSTM-1 and TRANS-BLSTM-2) models respectively:</p><p>TRANS-BLSTM-1 For each BERT layer, we replace the feedforward layer with a bidirectional LSTM layer.</p><p>TRANS-BLSTM-2 We add a bidirectional LSTM layer which takes the same input as the original BERT layer. The output of the bidirectional LSTM layer is summed up with the original BERT layer output (before the Layer-Norm).</p><p>The motivation of adding BLSTM is to integrate the self-attention and bidirectional LSTM to produce a better joint model framework (as we will see in the experiments later). We found that these two architectures lead to similar accuracy in our experiments (see Section 4.5). We thus focus on the latter (TRANS-BLSTM-2) and refer to this model as TRANS-BLSTM henceforth for simplicity. For both architectures, if we use the same number of BLSTM hidden units as in the BERT model H, we obtain the BLSTM output with dimension of 2H, and we therefore need a linear layer to project the output of the BLSTM (with dimensionality 2H) to H in order to match the transformer output. Alternatively, if we set the number of BLSTM hidden units to H/2 (we denote this model as TRANS-BLSTM-SMALL), we need not include an additional projection layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adding bidirectional LSTM to transformer decoder</head><p>While the above method adds bidirectional LSTM layers to a transformer encoder, we can in addition replace the linear layer with bidirectional LSTM layers in decoder. The number of bidirectional LSTM layers is a hyper parameter to tune; we use 2 in this paper. While the bidirectional LSTM layers in encoder help the pre-training task for the masked language model and next sentence prediction task, the bidirectional LSTM in decoder may help in downstream sequential prediction tasks such as question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Objective functions</head><p>Following the BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, we use masked language model loss and next sentence prediction (NSP) loss to train the models. The masked LM (MLM) is often referred to as a Cloze task in the literature <ref type="bibr" target="#b22">(Taylor, 1953)</ref>. The encoder output, corresponding to the mask tokens, are fed into an output softmax over the vocabulary.</p><p>In our experiments, we randomly mask 15% of all whole word wordpiece tokens in each sequence <ref type="bibr" target="#b26">(Wu et al., 2016)</ref>.</p><p>We also use the next sentence prediction loss as introduced in <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> to train our models. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A, and 50% of the time it is a random sentence from the corpus. We note that recent work <ref type="bibr" target="#b27">(Yang et al., 2019;</ref><ref type="bibr" target="#b12">Liu et al., 2019a;</ref><ref type="bibr" target="#b10">Lan et al., 2019;</ref><ref type="bibr" target="#b16">Raffel et al., 2019)</ref> has argued that the NSP loss may not be useful in improving model accuracy. Nevertheless, we used the NSP loss in our experiments to have a fair comparison between the proposed models and the original BERT models. <ref type="table">Table 1</ref> shows the model parameter size and training speedup for TRANS/BERT (TRANS and BERT are exchangeable in this paper), TRANS-BLSTM-SMALL, and TRANS-BLSTM respectively. Here, the TRANS-BLSTM-SMALL and TRANS-BLSTM models are 50% and 100% larger than the TRANS model (base, large) respec-tively. Consequently, TRANS-BLSTM-SMALL and TRANS-BLSTM models require more computational resources and longer training times compared to the vanilla transformer model. The slowest-training model is the TRANS-BLSTM which is also our baseline. Models with fewer parameters can train faster. For example, the large TRANS/BERT model boasts a 2.8 fold speedup compared to the TRANS-BLSTM large model. We note that the focus of the paper is to investigate whether a joint transformer and BLSTM architecture can further improve the performance over a transformer baseline. This is important to keep in mind because simply increasing the number of hidden units in BERT-large is not enough to positively affect accuracy <ref type="bibr" target="#b10">(Lan et al., 2019)</ref> (also see Section 4.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>We use the same large-scale data which has been used for BERT model pre-training, the BooksCorpus (800M words) <ref type="bibr" target="#b28">(Zhu et al., 2015)</ref> and English Wikipedia (2.5B words) <ref type="bibr">(Wikipedia contributors, 2004;</ref><ref type="bibr" target="#b3">Devlin et al., 2018)</ref>. The two corpora consist of about 16GB of text. Following the original BERT setup <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, we format the inputs as "[CLS]  <ref type="table" target="#tab_1">TRANS/BERT  108M  12  768  768  12  6.0X  Base  TRANS-BLSTM-SMALL  152M  12  768  768  12  3.3X  TRANS-BLSTM  237M  12  768  768  12  2.5X  Large TRANS/BERT  334M  24  1024  1024  16  2.8X  TRANS-BLSTM-SMALL  487M  24  1024  1024  16  1.4X  TRANS-BLSTM  789M  24  1024  1024  16  1   Table 1</ref>: Parameter size and training speed for TRANS/BERT, TRANS-BLSTM-SMALL, and TRANS-BLSTM on base and large settings respectively.</p><p>x 1 = x 11 , x 12 . . . and x 2 = x 21 , x 22 . . . are two segments. To reduce the training memory consumption, we set the maximum input length to 256 (as opposed to 512 in the original BERT paper). We note that this setting may adversely affect the best accuracy we report in our paper 1 , but the relative accuracy gain by the proposed models are still valid. Similar to BERT, we use a vocabulary size of 30k with wordpiece tokenization.</p><p>We generate the masked input from the MLM targets using unigram masking, which is denoted as whole word masking. That is, each masking applies to a whole word at one time. We note that using n-gram masking (for example, with n = 3) <ref type="bibr" target="#b9">(Joshi et al., 2019;</ref><ref type="bibr" target="#b10">Lan et al., 2019)</ref> with the length of each n-gram mask selected randomly can further improve the downstream task accuracy (for example, 2% F1 score increase was observed on SQuAD 1.1 data set with n-gram masking and span boundary representation prediction <ref type="bibr" target="#b9">(Joshi et al., 2019)</ref>). However, in the whole word masking setting, we are able to fairly compare the proposed TRANS-BLSTM models to the original BERT models. Similar to <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, the training data generator chooses 15% of the token positions at random for making. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time.</p><p>The model updates use a batch size of 256 and Adam optimizer with learning rate starting from 1e-4. Training was done on a cluster of nodes, where each node consists of 8 Nvidia Tesla V100 GPUs. We vary the node size from 1 to 8 depending on the model size. Our TRANS-BLSTM is implemented on top of Pytorch transformer repository 2 .</p><p>1 Nevertheless, our implementation of baseline BERT model obtained higher accuracy than that reported by the original BERT paper <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>. 2 https://github.com/huggingface/pytorch-transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Downstream evaluation datasets</head><p>Following the previous work <ref type="bibr" target="#b3">(Devlin et al., 2018;</ref><ref type="bibr" target="#b27">Yang et al., 2019;</ref><ref type="bibr" target="#b12">Liu et al., 2019a;</ref><ref type="bibr" target="#b10">Lan et al., 2019)</ref>, we evaluate our models on the General Language Understanding Evaluation (GLUE) benchmark  and the Stanford Question Answering Dataset (SQuAD 1.1) <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref>. GLUE is the General Language Understanding Evaluation benchmark consisting of a diverse collection of natural language understanding tasks. GLUE is model-agnostic and the tasks are selected to incentivize the development of general and robust NLU systems. The tasks included in GLUE are (1) Multi-Genre Natural Language Inference (MNLI) for sentence entailment classification, (2) Quora Question Pairs (QQP) for semantic equivalence classification, (3) Question Natural Language Inference (QNLI) for predicting whether the sentence in a query-sentence pair contains a correct answer, (4) Stanford Sentiment Treebank (SST-2) for sentiment analysis of movie reviews, (5) Corpus of Linguistic Acceptability (CoLA) for determining whether an English sentence is linguistically acceptable, (6) Semantic Textual Similarity (STS-B). The Stanford Question Answering Dataset (SQuAD) is a corpus consisting of 100k question/answer pairs sourced from Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bidirectional LSTM model on SQuAD dataset</head><p>For the down-stream fine-tuning experiments on SQuAD 1.1 dataset, we have the following hyperparameters for training. We set the learning rate to be 3e-5, training batch size to be 12, and the number of training epochs to be 2. We first run the experiment by replacing the transformer in BERT base with a bidirectional LSTM model with the same number of layers. That is, we replace the 12 transformer layers with 12 BLSTM layers. <ref type="table" target="#tab_1">Table 2</ref> shows the BERT base models, including the original BERT-base model in <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> and our implementation, and the bidirectional LSTM model accuracy over SQuAD 1.1 development dataset. Our implementation results in a higher F1 score (90.05%) compared to the original BERT-base one (88.50%). This may be due to the fact that we use the whole word masking while BERT-base used partial word masking (an easier task, which may prevent from learning a better model). We found that the BLSTM model has F1 score of 83.43%, which is significantly worse than our TRANS/BERT baseline (90.05%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>EM F1 BERT-base <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Models pre-training</head><p>We run three pre-training experiments for base and large settings respectively. 1) BERT model training baseline (denoted as TRANS/BERT representing a transformer model or BERT), 2) TRANS-BLSTM-SMALL, with BLSTM having half of the hidden units of the transformer (768/2 = 384 on BERT base and 1024/2 = 512 on BERT large) for BLSTM, and 3) TRANS-BLSTM, with BLSTM having the same hidden units as the transformer (768 on BERT base and 1024 on BERT large). <ref type="figure" target="#fig_3">Figure 3</ref> shows the training loss for base TRANS/BERT, TRANS-BLSTM-SMALL, and TRANS-BLSTM models. As can be seen, TRANS-BLSTM-SMALL model has lower training loss than the TRANS/BERT model. TRANS-BLSTM can further decrease the training loss compared to TRANS-BLSTM-SMALL. This suggests that the proposed TRANS-BLSTM-SMALL and TRANS-BLSTM are capable of fitting the training data better than the original BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Compare two versions of TRANS-BLSTM models</head><p>We proposed two versions of TRANS-BLSTM models in section 3.2 (see <ref type="figure" target="#fig_1">Fig 2)</ref>, with TRANS-BLSTM-1 replacing the feedforward layer with a bidirectional LSTM layer, and TRANS-BLSTM-2 adding a parallel bidirectional LSTM layer. We trained these two models and list their performance on SQuAD 1.1 development dataset in <ref type="table" target="#tab_2">Table 3</ref>. We note that these two models lead to similar accuracy on this dataset. We will use TRANS-BLSTM-2 to report the accuracy in the rest of the experiments (denoted as TRANS-BLSTM for notational simplicity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>EM F1 TRANS-BLSTM-1 84.87 91.52 TRANS-BLSTM-2 84.75 91.53  <ref type="table" target="#tab_3">Table 4</ref> shows the results of SQuAD dataset for TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM models for base and large settings respectively. As can been see, the TRANS-BLSTM-SMALL can boost the baseline BERT model from F1 score of 90.05% to 90.76%, and from 92.34% to 92.86% on base and large cases respectively. In addition, the TRANS-BLSTM can further boost accuracy on top of TRANS-BLSTM-SMALL to 91.53% and 93.82% on base and large respectively. The accuracy boosts suggest that the bidirectional LSTM model can add additional accuracy gain on top of the transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Models evaluation on SQuAD dataset</head><p>Compared to adding bidirectional LSTM layers to the encoder, the addition of bidirectional LSTMs to the decoder (see +BLSTM experiments in <ref type="table" target="#tab_3">Table 4</ref>) offers additional improvements on five out of six cases. For example, it boosts the base TRANS/BERT model F1 score of 90.05% to 90.67%, and boosts the large TRANS-BLSTM model F1 score of 93.82% to 94.01%.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Model evaluation on GLUE datasets</head><p>Following <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, we use a batch size of 32 and 3-epoch fine-tuning over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the development set. Additionally similar to <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, for large BERT and TRANS-BLSTM models, we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the development set. <ref type="table" target="#tab_6">Table 5</ref> shows the results of GLUE datasets for original BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, ours TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base and large settings respectively. Following the BERT setting <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, we exclude the problematic WNLI set. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. Unlike the evaluation on SQuAD dataset, we do not apply the BLSTM layer to the decoder. This is because that the tasks on GLUE are classification tasks based on the [CLS] token, and are not sequential prediction tasks (for example the SQuAD dataset) which may benefit more from including a BLSTM layer.</p><p>We note again the accuracy discrepancy between the original BERT and our implementation of BERT, which may be due to the fact that the former uses partial word masking while the later uses whole word masking. Similar to the SQuAD results, the TRANS-BLSTM-SMALL and TRANS-BLSTM base models can improve the  TRANS/BERT base model from the average GLUE score of 84.63% to 84.77% and 85.35% respectively. In addition, the TRANS-BLSTM-SMALL and TRANS-BLSTM large models can improve the TRANS/BERT large model from the average GLUE score of 85.59% to 86.23% and 86.50% respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Transformer architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Two transformer with bidirectional LSTM architectures. The left one, TRANS-BLSTM-1, replaces the feedforward layer with BLSTM layer and the right , TRANS-BLSTM-2, adds a BLSTM layer in parallel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>x 1 [SEP] x 2 [SEP]", where Model Parameters (M) Layers Hidden Embedding Heads Speedup</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Training loss as a function of training steps for base TRANS/BERT, TRANS-BLSTM-SMALL, and TRANS-BLSTM models respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>SQuAD development results for BERT base and the bidirectional LSTM model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>SQuAD development results for two versions of base TRANS-BLSTM models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>also shows the accuracy of original</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>SQuAD development results for TRANS/BERT, TRANS-BLSTM-SMALL, and TRANS-BLSTM on base and large settings respectively. BERT large model. That is, the extra depth of 24 layers does not generate additional accuracy gain compared to BERT large model.Table 4also shows the the BERT xlarge model, which simply doubles the hidden units of BERT large model (ie, with 1024 * 2 = 2048 hidden units). It has F1 score of 86.3% which is significantly worse than BERT large. This suggests that simply increasing the BERT model size makes it hard to train the model, resulting in lower accuracy in this case.Finally, we list the current state-of-the-art AL-BERT model, which has F1 score of 94.1% on SQuAD 1.1 development dataset. Our TRANS-BLSTM model can obtain similar accuracy to this modeling approach.</figDesc><table><row><cell>BERT models (Devlin et al., 2018), which under-</cell></row><row><cell>perform our TRANS/BERT implementations, pos-</cell></row><row><cell>sibly due to whole word masking is used in our</cell></row><row><cell>model training. We also trained a TRANS/BERT-</cell></row><row><cell>48 model, which has 48 layers (instead of the 24</cell></row><row><cell>layers in BERT large config) and has 638M model</cell></row><row><cell>parameters (comparable to the model parameter</cell></row><row><cell>size of 789M for TRANS-BLSTM). We observe</cell></row><row><cell>that the TRANS/BERT-48 has similar accuracy as</cell></row><row><cell>in the TRANS/</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>GLUE development results for TRANS/BERT, TRANS-BLSTM-SMALL and TRANS-BLSTM on base and large settings respectively.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Previous research suggested that simply increasing the hidden layer size of BERT model cannot improve the model performance. In this paper, we proposed the TRANS-BLSTM model architectures, which combine the transformer and BLSTM into one single modeling framework, leveraging the modeling capability from both transformer and BLSTM. We showed that TRANS-BLSTM models consistently lead to accuracy boost compared to transformer baselines on GLUE and SQuAD.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<title level="m">Spanbert: Improving pre-training by representing and predicting spans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02755</idno>
		<title level="m">Simple recurrent units for highly parallelizable recurrence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11504</idno>
		<title level="m">Multi-task deep neural networks for natural language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francoise</forename><surname>Beaufays</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.1128</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09355</idno>
		<title level="m">Patient knowledge distillation for bert model compression</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12136</idno>
		<title level="m">Distilling taskspecific knowledge from bert into simple neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cloze procedure: A new tool for measuring readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Bulletin</title>
		<imprint>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Plagiarism -Wikipedia, the free encyclopedia</title>
	</analytic>
	<monogr>
		<title level="m">Wikipedia contributors</title>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
	<note>Online; accessed</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Googles neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
