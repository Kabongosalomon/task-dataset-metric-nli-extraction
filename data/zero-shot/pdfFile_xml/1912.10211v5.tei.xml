<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE, Turab Iqbal</roleName><forename type="first">Yin</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
						</author>
						<title level="a" type="main">PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Audio tagging</term>
					<term>pretrained audio neural net- works</term>
					<term>transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classification, music classification, speech emotion classification and sound event detection. Recently, neural networks have been applied to tackle audio pattern recognition problems. However, previous systems are built on specific datasets with limited durations. Recently, in computer vision and natural language processing, systems pretrained on large-scale datasets have generalized well to several tasks. However, there is limited research on pretraining systems on large-scale datasets for audio pattern recognition. In this paper, we propose pretrained audio neural networks (PANNs) trained on the large-scale AudioSet dataset. These PANNs are transferred to other audio related tasks. We investigate the performance and computational complexity of PANNs modeled by a variety of convolutional neural networks. We propose an architecture called Wavegram-Logmel-CNN using both log-mel spectrogram and waveform as input feature. Our best PANN system achieves a state-of-the-art mean average precision (mAP) of 0.439 on AudioSet tagging, outperforming the best previous system of 0.392. We transfer PANNs to six audio pattern recognition tasks, and demonstrate state-of-the-art performance in several of those tasks. We have released the source code and pretrained models of PANNs: https://github.com/qiuqiangkong/audioset_tagging_cnn. Index Terms-Audio tagging, pretrained audio neural networks, transfer learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Audio pattern recognition is an important research topic in the machine learning area, and plays an important role in our life. We are surrounded by sounds that contain rich information of where we are, and what events are happening around us. Audio pattern recognition contains several tasks such as audio tagging <ref type="bibr" target="#b0">[1]</ref>, acoustic scene classification <ref type="bibr" target="#b1">[2]</ref>, music classification <ref type="bibr" target="#b2">[3]</ref>, speech emotion classification and sound event detection <ref type="bibr" target="#b3">[4]</ref>.</p><p>Audio pattern recognition has attracted increasing research interest in recent years. Early audio pattern recognition work Q. <ref type="bibr">Kong</ref> focused on private datasets collected by individual researchers <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b5">[6]</ref>. For example, Woodard <ref type="bibr" target="#b4">[5]</ref> applied a hidden Markov model (HMM) to classify three types of sounds: wooden door open and shut, dropped metal and poured water. Recently, the Detection and Classification of Acoustic Scenes and Events (DCASE) challenge series <ref type="bibr" target="#b6">[7]</ref>[8] <ref type="bibr" target="#b8">[9]</ref> <ref type="bibr" target="#b1">[2]</ref> have provided publicly available datasets, such as acoustic scene classification and sound event detection datasets. The DCASE challenges have attracted increasing research interest in audio pattern recognition. For example, the recent DCASE 2019 challenge received 311 entries across five subtasks <ref type="bibr" target="#b9">[10]</ref>.</p><p>However, it is still an open question how well an audio pattern recognition system can perform when trained on largescale datasets. In computer vision, several image classification systems have been built with the large-scale ImageNet dataset <ref type="bibr" target="#b10">[11]</ref>. In natural language processing, several language models have been built with the large-scale text datasets such as Wikipedia <ref type="bibr" target="#b11">[12]</ref>. However, systems trained on large-scale audio datasets have been more limited <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b12">[13]</ref>[14] <ref type="bibr" target="#b14">[15]</ref>.</p><p>A milestone for audio pattern recognition was the release of AudioSet <ref type="bibr" target="#b0">[1]</ref>, a dataset containing over 5,000 hours of audio recordings with 527 sound classes. Instead of releasing the raw audio recordings, AudioSet released embedding features of audio clips extracted from a pretrained convolutional neural network <ref type="bibr" target="#b12">[13]</ref>. Several researchers have investigated building systems with those embedding features <ref type="bibr" target="#b12">[13]</ref> <ref type="bibr" target="#b15">[16]</ref> <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b18">[19]</ref> <ref type="bibr" target="#b19">[20]</ref>. However, the embedding features may not be an optimal representation for audio recordings, which may limit the performance of those systems. In this article, we propose pretrained audio neural networks (PANNs) trained on raw AudioSet recordings with a wide range of neural networks. We show that several PANN systems outperform previous stateof-the-art audio tagging systems. We also investigate the audio tagging performance and computation complexities of PANNs.</p><p>We propose that PANNs can be transferred to other audio pattern recognition tasks. Previous researchers have previously investigated transfer learning for audio tagging. For example, audio tagging systems were pretrained on the Million Song Dataset were proposed in <ref type="bibr" target="#b20">[21]</ref>, with embedding features extracted from pretrained convolutional neural networks (CNNs) are used as inputs to second-stage classifiers such as neural networks or support vector machines (SVMs) <ref type="bibr" target="#b13">[14]</ref> <ref type="bibr" target="#b21">[22]</ref>. Systems pretrained on MagnaTagATune <ref type="bibr" target="#b22">[23]</ref> and acoustic scene <ref type="bibr" target="#b23">[24]</ref> datasets were fine-tuned on other audio tagging tasks <ref type="bibr" target="#b24">[25]</ref> <ref type="bibr" target="#b25">[26]</ref>. These transfer learning systems were mainly trained with music datasets, and were limited to smaller datasets than AudioSet.</p><p>The contribution of this work includes: (1) We introduce arXiv:1912.10211v5 [cs.SD] 23 Aug 2020</p><p>PANNs trained on AudioSet with 1.9 million audio clips with an ontology of 527 sound classes; <ref type="bibr" target="#b1">(2)</ref> We investigate the trade-off between audio tagging performance and computation complexity of a wide range of PANNs; (3) We propose a system that we call Wavegram-Logmel-CNN that achieves a mean average precision (mAP) of 0.439 on AudioSet tagging, outperforming previous state-of-the-art system with an mAP 0.392 and Google's system with an mAP 0.314; <ref type="bibr" target="#b3">(4)</ref> We show that PANNs can be transferred to other audio pattern recognition tasks, outperforming several state-of-the-art systems; <ref type="bibr" target="#b4">(5)</ref> We have released the source code and pretrained PANN models. This paper is organized as follows: Section II introduces audio tagging with various convolutional neural networks; Section III introduces our proposed Wavegram-CNN systems; Section IV introduces our data processing techniques, including data balancing and data augmentation for AudioSet tagging; Section VI shows experimental results, and Section VII concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. AUDIO TAGGING SYSTEMS</head><p>Audio tagging is an essential task of audio pattern recognition, with the goal of predicting the presence or absence of audio tags in an audio clip. Early work on audio tagging included using manually-designed features as input, such as audio energy, zero-crossing rate, and mel-frequency cepstrum coefficients (MFCCs) <ref type="bibr" target="#b26">[27]</ref>. Generative models, including Gaussian mixture models (GMMs) <ref type="bibr" target="#b27">[28]</ref> <ref type="bibr" target="#b28">[29]</ref>, hidden Markov models (HMMs), and discriminative support vector machines (SVMs) <ref type="bibr" target="#b29">[30]</ref> have been used as classifiers. Recently, neural network based methods such as convolutional neural networks (CNNs) have been used <ref type="bibr" target="#b2">[3]</ref> to predict the tags of audio recordings. CNN-based systems have achieved stateof-the-art performance in several DCASE challenge tasks including acoustic scene classification <ref type="bibr" target="#b1">[2]</ref> and sound event detection <ref type="bibr" target="#b3">[4]</ref>. However, many of those works focused on particular tasks with a limited number of sound classes, and were not designed to recognize a wide range of sound classes. In this article, we focus on training large-scale PANNs on AudioSet <ref type="bibr" target="#b0">[1]</ref> to tackle the general audio tagging problem.</p><p>A. CNNs 1) Conventional CNNs: CNNs have been successfully applied to computer vision tasks such as image classification <ref type="bibr" target="#b30">[31]</ref> <ref type="bibr" target="#b31">[32]</ref>. A CNN consists of several convolutional layers. Each convolutional layer contains several kernels that are convolved with the input feature maps to capture their local patterns. CNNs adopted for audio tagging <ref type="bibr" target="#b2">[3]</ref>[20] often use log mel spectrograms as input <ref type="bibr" target="#b2">[3]</ref> <ref type="bibr" target="#b19">[20]</ref>. Short time Fourier transforms (STFTs) are applied to time-domain waveforms to calculate spectrograms. Then, mel filter banks are applied to the spectrograms, followed by a logarithmic operation to extract log mel spectrograms <ref type="bibr" target="#b2">[3]</ref> <ref type="bibr" target="#b19">[20]</ref>.</p><p>2) Adapting CNNs for AudioSet tagging: The PANNs we use are based on our previously-proposed cross-task CNN systems for the DCASE 2019 challenge <ref type="bibr" target="#b32">[33]</ref>, with an extra fully-connected layer added to the penultimate layer of CNNs to further increase the representation ability. We investigate 6-, 10-and 14-layer CNNs. The 6-layer CNN consists of 4 convolutional layers with a kernel size of 5 ? 5, based on AlexNet <ref type="bibr" target="#b33">[34]</ref>. The 10-and 14-layer CNNs consist of 4 and 6 convolutional layers, respectively, inspired by the VGG-like CNNs <ref type="bibr" target="#b34">[35]</ref>. Each convolutional block consists of 2 convolutional layers with a kernel size of 3 ? 3. Batch normalization <ref type="bibr" target="#b35">[36]</ref> is applied between each convolutional layer, and the ReLU nonlinearity <ref type="bibr" target="#b36">[37]</ref> is used to speed up and stabilize the training. We apply average pooling of size of 2 ? 2 to each convolutional block for downsampling, as 2 ? 2 average pooling has been shown to outperform 2 ? 2 max pooling <ref type="bibr" target="#b32">[33]</ref>. Global pooling is applied after the last convolutional layer to summarize the feature maps into a fixed-length vector. In <ref type="bibr" target="#b14">[15]</ref>, maximum and average operation were used for global pooling. To combine their advantages, we sum the averaged and maximized vectors. In our previous work <ref type="bibr" target="#b32">[33]</ref>, those fixedlength vectors were used as embedding features for audio clips. In this work, we add an extra fully-connected layer to the fixed length vectors to extract embedding features which can further increase their representation ability. For a particular audio pattern recognition task, a linear classifier is applied to the embedding features, followed by either a softmax nonlinearity for classification tasks or a sigmoid nonlinearity for tagging tasks. Dropout <ref type="bibr" target="#b37">[38]</ref> is applied after each downsampling operation and fully connected layers to prevent systems from overfitting. <ref type="table" target="#tab_1">Table I</ref> summarizes our proposed CNN systems. The number after the "@" symbol indicates the number of feature maps. The first column shows the VGGish network proposed by <ref type="bibr" target="#b12">[13]</ref>. MP is the abbreviation of max pooling. The "Pooling 2?2" in <ref type="table" target="#tab_1">Table I</ref> is average pooling with size of 2?2. In <ref type="bibr" target="#b12">[13]</ref>, an audio clip was split into 1-second segments, <ref type="bibr" target="#b12">[13]</ref> also assumed each segment inherits the label of the audio clip, which may lead to incorrect labels. In contrast, our systems from the second to the fourth columns in <ref type="table" target="#tab_1">Table I</ref> applies an entire audio clip for training without cutting the audio clip into segments. We denote the waveform of an audio clip as x n , where n is the index of audio clips, and f (x n ) ? [0, 1] K is the output of a PANN representing the presence probabilities of K sound classes. The label of x n is denoted as y n ? {0, 1} K . A binary cross-entropy loss function l is used to train a PANN: <ref type="bibr" target="#b0">(1)</ref> where N is the number of training clips in AudioSet. In training, the parameters of f (?) are optimized by using gradient descent methods to minimize the loss function l.</p><formula xml:id="formula_0">l = ? N n=1 (y n ? lnf (x n ) + (1 ? y n ) ? ln(1 ? f (x n )),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ResNets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Conventional residual networks (ResNets): Deeper</head><p>CNNs have been shown to achieve better performance than shallower CNNs for audio classification <ref type="bibr" target="#b30">[31]</ref>. One challenge of very deep conventional CNNs is that the gradients do not propagate properly from the top layers to the bottom layers <ref type="bibr" target="#b31">[32]</ref>. To address this problem, ResNets <ref type="bibr" target="#b31">[32]</ref> introduced shortcut connections between convolutional layers. In this way, the forward and backward signals can be propagated from one layer to any other layer directly. The shortcut connections only introduce a small number of extra parameters and a little additional computational complexity. A ResNet consists of several blocks, where each block consists of two convolutional layers with a kernel size of 3 ? 3, and a shortcut connection between input and output. Each bottleneck block consists of three convolutional layers with a network-in-network architecture <ref type="bibr" target="#b38">[39]</ref> that can be used instead of the basic blocks in a ResNet <ref type="bibr" target="#b31">[32]</ref>.</p><p>2) Adapting ResNets for AudioSet tagging: We adapt ResNet <ref type="bibr" target="#b31">[32]</ref> for AudioSet tagging as follows. To begin with, two convolutional layers and a downsampling layer are applied on the log mel spectrogram to reduce the input log mel spectrogram size. We implement three types of ResNets with different depths: a 22-layer ResNet with 8 basic blocks; a 38layer ResNet with 16 basic blocks, and a 54-layer ResNet with 16 residual blocks. <ref type="table" target="#tab_1">Table II</ref> shows the architecture of the ResNet systems adapted for AudioSet tagging. The BasicB and BottleneckB are abbreviations of basic block and bottleneck block, respectively.  <ref type="bibr" target="#b39">[40]</ref> by factorizing a standard convolution into a depthwise convolution and a 1 ? 1 pointwise convolution <ref type="bibr" target="#b39">[40]</ref>.</p><p>2) Adapting MobileNets for AudioSet tagging: We adapt MobileNetV1 <ref type="bibr" target="#b39">[40]</ref> and MobileNetV2 <ref type="bibr" target="#b40">[41]</ref> systems for Au-dioSet tagging shown in <ref type="table" target="#tab_1">Table III</ref>. The V1Blocks and V2Blocks are MobileNet convolutional blocks <ref type="bibr" target="#b39">[40]</ref> <ref type="bibr" target="#b40">[41]</ref>, each consisting of two and three convolutional layers, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. One-dimensional CNNs</head><p>Previous audio tagging systems were based on the log mel spectrogram, a hand-crafted feature. To improve performance, several researchers proposed to build one-dimensional CNNs which operate directly on the time-domain waveforms. For example, Dai et al. <ref type="bibr" target="#b30">[31]</ref> proposed a one-dimensional CNN for acoustic scene classification, and Lee et al. <ref type="bibr" target="#b41">[42]</ref> proposed a one-dimensional CNN that was later adopted by Pons et al. <ref type="bibr" target="#b14">[15]</ref> for music tagging.</p><p>1) DaiNet: DaiNet <ref type="bibr" target="#b30">[31]</ref> applied kernels of length 80 and stride 4 to the input waveform of audio recordings. The kernels are learnable during training. To begin with, a maximum operation is applied to the first convolutional layer, which is designed to make the system be robust to phase shift of the input signals. Then, several one-dimensional convolutional blocks with kernel size 3 and stride 4 were applied to extract high level features. An 18-layer DaiNet with four convolutional layers in each convolutional block achieved the best result in UrbanSound8K <ref type="bibr" target="#b42">[43]</ref> classification <ref type="bibr" target="#b30">[31]</ref>.</p><p>2) LeeNet: In contrast to DaiNet that applied large kernels in the first layer, LeeNet <ref type="bibr" target="#b41">[42]</ref> applied small kernels with length 3 on the waveforms, to replace the STFT for spectrogram extraction. LeeNet consists of several one dimensional convolutional layers, each followed by a downsampling layer of size 2. The original LeeNet consists of 11 layers.</p><p>3) Adapting one-dimensional CNNs for AudioSet tagging: We modify LeeNet by extending it to a deeper architecture with 24 layers, replacing each convolutional layer with a convolutional block that consists of two convolutional layers. To further increase the number of layers of the onedimensional CNNs, we propose a one-dimensional residual network (Res1dNet) with a small kernel size of 3. We replace the convolutional blocks in LeeNet with residual blocks, where each residual block consists of two convolutional layers with kernel size 3. The first and second convolutional layers of convolutional block have dilations of 1 and 2, respectively, to increase the receptive field of the corresponding residual block. Downsampling is applied after each residual block. By using 14 and 24 residual blocks, we obtain a Res1dNet31 and a Res1dNet51 with 31 and 51 layers, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. WAVEGRAM-CNN SYSTEMS</head><p>Previous one-dimensional CNN systems <ref type="bibr" target="#b30">[31]</ref>[42] <ref type="bibr" target="#b14">[15]</ref> have not outperformed the systems trained with log mel spectrograms as input. One characteristic of previous time-domain CNN systems <ref type="bibr" target="#b30">[31]</ref> <ref type="bibr" target="#b41">[42]</ref> is that they were not designed to capture frequency information, because there is no frequency axis in the one-dimensional CNN systems, so they can not capture frequency patterns of a sound event with different pitch shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Wavegram-CNN systems</head><p>In this section, we propose architectures which we call Wavegram-CNN and Wavegram-Logmel-CNN for AudioSet tagging. The Wavegram-CNN we propose is a time-domain audio tagging system. Wavegram is a feature we propose that is similar to log mel spectrogram, but is learnt using a neural network. A Wavegram is designed to learn a timefrequency representation that is a modification of the Fourier transform. A Wavegram has a time axis and a frequency axis. Frequency patterns are important for audio pattern recognition, for example, sounds with different pitch shifts belong to the same class. A Wavegram is designed to learn frequency information that may be lacking in one-dimensional CNN systems. Wavegrams may also improve over hand-crafted log mel spectrograms by learning a new kind of time-frequency transform from data. Wavegrams can then replace log mel spectrograms as input features resulting in our Wavegram-CNN system. We also combine the Wavegram and the log mel spectrogram as a new feature to build the Wavegram-Logmel-CNN system as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>To build a Wavegram, we first apply a one-dimensional CNN to time-domain waveform. The one-dimensional CNN begins with a convolutional layer with filter length 11 and stride 5 to reduce the size of the input. This immediately reduces the input lengths by a factor of 5 times to reduce memory usage. This is followed by three convolutional blocks, where each convolutional block consists of two convolutional layers with dilations of 1 and 2, respectively, which are designed to increase the receptive field of the convolutional layers. Each convolutional block is followed by a downsampling layer with stride 4. By using the stride and downsampling three times, we downsample a 32 kHz audio recording to 32, 000/5/4/4/4 = 100 frames of features per second. We denote the output size of the one-dimensional CNN layers as T ? C, where T is the number of frames and C is the number of channels. We reshape this output to a tensor with a size of T ? F ? C/F by splitting C channels into C/F groups, where each group has F frequency bins. We call this tensor a Wavegram. The Wavegram learns frequency information by introducing F frequency bins in each of C/F channels. We apply CNN14 described in Section II-A as a backbone architecture on the extracted Wavegram, so that we can fairly compare the Wavegram and log mel spectrogram based systems. Two dimensional CNNs such as CNN14 can capture time-frequency invariant patterns on the Wavegram, because kernels are convolved along both time and frequency axis in a Wavegram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Wavegram-Logmel-CNN</head><p>Furthermore, we can combine the Wavegram and log mel spectrogram into a new representation. In this way, we can utilize the information from both time-domain waveforms and log mel spectrograms. The combination is carried out along the channel dimension. The Wavegram provides extra information for audio tagging, complementing the log mel spectrogram. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the architecture of the Wavegram-Logmel-CNN.</p><p>IV. DATA PROCESSING In this section, we introduce data processing for AudioSet tagging, including data balancing and data augmentation. Data balancing is a technique used to train neural networks on a highly unbalanced dataset. Data augmentation is a technique used to augment the dataset, to prevent systems from overfitting during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data balancing</head><p>The number of audio clips available for training varies from sound class to sound class. For example, there are over 900,000 audio clips belonging to the categories "Speech" and "Music". On the other hand, there are only tens of audio clips belonging to the category "Toothbrush". The number of audio clips of different sound classes has a long tailed distribution. Training data are input to a PANN in mini-batches during training. Without a data balancing strategy, audio clips are uniformly sampled from AudioSet. Therefore, sound classes with more training clips such as "Speech" are more likely to be sampled during training. In an extreme case, all data in a mini-batch may belong to the same sound class. This will cause the PANN to overfit to sound classes with more training clips, and underfit to sound classes with fewer training clips. To solve this problem, we design a balanced sampling strategy to train PANNs. That is, audio clips are approximately equally sampled from all sound classes to constitute a minibatch. We use the term "approximately" because an audio clip may contain more than one tag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data augmentation</head><p>Data augmentation is a useful way to prevent a system from overfitting. Some sound classes in AudioSet contain only a small number (e.g., hundreds) of training clips which may limit the performance of PANNs. We apply mixup <ref type="bibr" target="#b43">[44]</ref> and SpecAugment <ref type="bibr" target="#b44">[45]</ref> to augment data during training.</p><p>1) Mixup: Mixup <ref type="bibr" target="#b43">[44]</ref> is a way to augment a dataset by interpolating both the input and target of two audio clips from a dataset. For example, we denote the input of two audio clips as x 1 , x 2 , and their targets as y 1 , y 2 , respectively. Then, the augmented input and target can be obtained by x = ?x 1 +(1? ?)x 2 and y = ?y 1 +(1??)y 2 respectively, where ? is sampled from a Beta distribution <ref type="bibr" target="#b43">[44]</ref>. By default, we apply mixup on the log mel spectrogram. We will compare the performance of mixup augmentation on the log mel spectrogram and on the time-domain waveform in Section VI-C4.</p><p>2) SpecAugment: SpecAugment <ref type="bibr" target="#b44">[45]</ref> was proposed for augmenting speech data for speech recognition. SpecAugment operates on the log mel spectrogram of an audio clip using frequency masking and time masking. Frequency masking is applied such that f consecutive mel frequency bins [f 0 , f 0 +f ] are masked, where f is chosen from a uniform distribution from 0 to a frequency mask parameter f , and f 0 is chosen from [0, F ? f ], where F is the number of mel frequency bins <ref type="bibr" target="#b44">[45]</ref>. There can be more than one frequency mask in each log mel spectrogram. The frequency mask can improve the robustness of PANNs to frequency distortion of audio clips <ref type="bibr" target="#b44">[45]</ref>. Time masking is similar to frequency masking, but is applied in the time domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. TRANSFER TO OTHER TASKS</head><p>To investigate the generalization ability of PANNs, we transfer PANNs to a range of audio pattern recognition tasks. Previous works on audio transfer learning <ref type="bibr" target="#b20">[21]</ref>[14] <ref type="bibr" target="#b21">[22]</ref>[25] <ref type="bibr" target="#b22">[23]</ref> mainly focused on music tagging, and were limited to smaller datasets than AudioSet. To begin with, we demonstrate the training of a PANN in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>. Here, D AudioSet is the AudioSet dataset, and x 0 , y 0 are training input and target, respectively. FC AudioSet is the fully connected layer for AudioSet tagging. In this article, we propose to compare the following transfer learning strategies.</p><p>1) Train a system from scratch. All parameters are randomly initialized. Systems are similar to PANNs, except for the final fully-connected layer which depends on the task dependent number of outputs. This system is used as a baseline system to be compared with other transfer learning systems.</p><p>2) Use a PANN as a feature extractor. For new tasks, the embedding features of audio clips are calculated by using the PANN. Then, the embedding features are used as input to a classifier, such as a fully-connected neural network. When training on new tasks, the parameters of the PANN are frozen and not trained. Only the parameters of the classifier built on the embedding features are trained. <ref type="figure" target="#fig_1">Fig. 2</ref>(b) shows this strategy, where D NewTask is a new task dataset, and FC NewTask is the fully connected layer of a new task. The PANN is used as a feature extractor. A classifier is built on the extracted embedding features. The shaded rectangle indicates the parameters which are frozen and not trained.</p><p>(3) Fine-tune a PANN. A PANN is used for a new task, except the final fully-connected layer. All parameters are initialized from the PANN, except the final fully-connected layer which is randomly initialized. All parameters are finetuned on D NewTask . <ref type="figure" target="#fig_1">Fig. 2</ref>(c) demonstrates the fine-tuning of a PANN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>First, we evaluate the performance of PANNs on AudioSet tagging. Then, the PANNs are transferred to several audio pattern recognition tasks, including acoustic scene classification, general audio tagging, music classification and speech emotion classification.  <ref type="figure">Fig. 3</ref>. Class-wise AP of sound events with the CNN14 system. The number inside parentheses indicates the number of training clips. The left, middle, right columns show the AP of sound classes with the number of training clips ranked the 1st to 10th, 250th to 260th and 517th to 527th in the training set of AudioSet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. AudioSet dataset</head><p>AudioSet is a large-scale audio dataset with an ontology of 527 sound classes <ref type="bibr" target="#b0">[1]</ref>. The audio clips from AudioSet are extracted from YouTube videos. The training set consists of 2,063,839 audio clips, including a "balanced subset" of 22,160 audio clips, where there are at least 50 audio clips for each sound class. The evaluation set consists of 20,371 audio clips. Instead of using the embedding features provided by <ref type="bibr" target="#b0">[1]</ref>, we downloaded raw audio waveforms of AudioSet in Dec. 2018 using the links provided by <ref type="bibr" target="#b0">[1]</ref>, and ignored the audio clips that are no longer downloadable. We successfully download 1,934,187 (94%) of the audio clips of the full training set, including 20,550 (93%) of the audio clips of the balanced training set. We successfully download 18,887 audio clips of the evaluation dataset. We pad the audio clips to 10 seconds with silence if they are shorter than 10 seconds. Considering the fact that a large number of audio clips from YouTube are monophonic and have a low sampling rate, we convert all audio clips to monophonic and resample them to 32 kHz.</p><p>For the CNN systems based on log mel spectrograms, STFT is applied on the waveforms with a Hamming window of size 1024 <ref type="bibr" target="#b32">[33]</ref> and a hop size of 320 samples. This configuration leads to 100 frames per second. Following <ref type="bibr" target="#b32">[33]</ref>, we apply 64 mel filter banks to calculate the log mel spectrogram. The lower and upper cut-off frequencies of the mel banks are set to 50 Hz and 14 kHz to remove low frequency noise and the aliasing effects. We use the torchlibrosa 1 , a PyTorch implementation of functions of librosa <ref type="bibr" target="#b45">[46]</ref> to build log mel spectrogram extraction into PANNs. The log mel spectrogram of a 10-second audio clip has a shape of 1001 ? 64. The extra one frame is caused by applying the "centre" argument when calculating STFT. A batch size of 32, and an Adam <ref type="bibr" target="#b46">[47]</ref> optimizer with a learning rate of 0.001 is used for training. Systems are trained on a single card Tesla-V100-PCIE-32GB. Each system takes around 3 days to train from scratch for 600 k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation metrics</head><p>Mean average precision (mAP), mean area under the curve (mAUC) and d-prime are used as official evaluation metrics for AudioSet tagging <ref type="bibr" target="#b19">[20]</ref> <ref type="bibr" target="#b0">[1]</ref>. Average precision (AP) is the 1 https://github.com/qiuqiangkong/torchlibrosa area under the recall and precision curve. AP does not depend on the number of true negatives, because neither precision nor recall depends on the number of true negatives. On the other hand, AUC is the area under the false positive rate and true positive rate (recall) which reflects the influence of the true negatives. The d-prime <ref type="bibr" target="#b0">[1]</ref> is also used as an metric and be calculated directly from AUC <ref type="bibr" target="#b0">[1]</ref>. All metrics are calculated on individual classes, and then averaged across all classes. Those metrics are also called macro metrics.</p><p>C. AudioSet tagging results 1) Comparison with previous methods: <ref type="table" target="#tab_1">Table IV</ref> shows the comparison of our proposed CNN14 system with previous Au-dioSet tagging systems. We choose CNN14 as a basic model to investigate a various of hyper-parameter configurations for AudioSet tagging, because CNN14 is a standard CNN that has a simple architecture, and can be compared with previous CNN systems [3] <ref type="bibr" target="#b32">[33]</ref>. As a baseline, random guess achieves an mAP of 0.005, an AUC of 0.500 and a d-prime of 0.000, respectively. The result released by Google <ref type="bibr" target="#b0">[1]</ref> trained with embedding features from <ref type="bibr" target="#b12">[13]</ref> achieved an mAP of 0.314 and an AUC of 0.959, respectively. The single-level attention and multi-level attention systems <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> achieved mAPs of 0.337 and 0.360, which were later improved by a featurelevel attention neural network that achieved an mAP of 0.369. Wang et al. <ref type="bibr" target="#b18">[19]</ref> investigated five different types of attention functions and achieved an mAP of 0.362. All the above systems were built on the embedding features released with AudioSet <ref type="bibr" target="#b0">[1]</ref>. The recent DeepRes system <ref type="bibr" target="#b47">[48]</ref> was built on waveforms downloaded from YouTube, and achieved an mAP of 0.392. The bottom rows of <ref type="table" target="#tab_1">Table IV shows</ref>   CNN14 system achieves an mAP of 0.431, outperforming the best of previous systems. We use CNN14 as a backbone to build Wavegram-Logmel-CNN for fair comparison with the CNN14 system. <ref type="figure" target="#fig_3">Fig. 4(a)</ref> shows that Wavegram-Logmel-CNN outperforms the CNN14 system, and the MobileNetV1 system. Detailed results are shown in later this section in <ref type="table" target="#tab_1">Table XI.</ref> 2) Class-wise performance: <ref type="figure">Fig. 3</ref> shows the class-wise AP of different sound classes with the CNN14 system. The left, middle, right columns show the AP of sound classes with the number of training clips ranked the 1st to 10th, 250th to 260th and 517th to 527th in the training set of AudioSet. The performance of different sound classes can be very different. For example, "Music" and "Speech" achieve APs of over 0.80. On the other hand, some sound classes such as "Inside, small" achieve an AP of only 0.19. <ref type="figure">Fig. 3</ref> shows that APs are usually not correlated with the number of training clips. For example, the left column shows that "Inside, small" contains 70,159 training clips, while its AP is low. In contrast, the right column shows that "Hoot" only has 106 training clips, but achieves an AP of 0.86, and is larger than many other sound classes with more training clips. In the end of this article, we plot the mAP of all 527 sound classes in <ref type="figure" target="#fig_0">Fig. 12</ref>, which shows the class-wise comparison of the CNN14, MobileNetV1 and Wavegram-Logmel-CNN systems with previous state-of-the-art audio tagging system <ref type="bibr" target="#b19">[20]</ref> built with embedding features released by <ref type="bibr" target="#b0">[1]</ref>. The blue bars in <ref type="figure" target="#fig_0">Fig.  12</ref> show the number of training clips in logarithmic scale. The "+" symbol indicates label qualities between 0 and 1, which are measured by the percentage of correctly labelled audio clips verified by an expert <ref type="bibr" target="#b0">[1]</ref>. The label quality vary from sound class to sound class. The "?" symbol indicates the sound classes whose label quality are not available. <ref type="figure" target="#fig_0">Fig. 12</ref> shows that the average precisions of some classes are higher than others. For example, sound classes such as "bagpipes" achieve an average precision of 0.90, while sound classes such as "mouse" achieve an average precision less than 0.2. One explanation is that the audio tagging difficulty is different between sound class to sound class. In addition, audio tagging performance is not always correlated with the number of training clips and label qualities <ref type="bibr" target="#b19">[20]</ref>. <ref type="figure" target="#fig_0">Fig. 12</ref> shows that our proposed systems outperform previous state-of-the-art systems <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> across a wide range of sound classes. 3) Data balancing: Section IV-A introduces the data balancing technique that we use to train AudioSet tagging systems. <ref type="figure" target="#fig_3">Fig. 4(b)</ref> shows the performance of the CNN14 system with and without data balancing. The blue curve shows that it takes a long time to train PANNs without data balancing. The green curve shows that with data balancing, a system converges faster within limited training iterations. In addition, the systems trained with full 1.9 million training clips perform better than the systems trained with the balanced subset of 20k training clips. <ref type="table" target="#tab_6">Table V</ref> shows that the CNN14 system achieves an mAP of 0.416 with data balancing, higher than that without data balancing (0.375).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Data augmentation:</head><p>We show that mixup data augmentation plays an important role in training PANNs. By default, we apply mixup on the log mel spectrogram. <ref type="figure" target="#fig_3">Fig. 4(b)</ref> and <ref type="table" target="#tab_6">Table V</ref> shows that the CNN14 system trained with mixup data augmentation achieves an mAP of 0.431, outperforming that trained without mixup data augmentation (0.416). Mixup is especially useful when training with the balanced subset containing only 20k training clips, yielding an mAP of 0.278, compared to training without mixup (0.221). In addition, we show that mixup on the log mel spectrogram achieves an mAP of 0.431, outperforming mixup in the time-domain waveform of 0.425, when training with full training data. This suggests that mixup is more effective when used with the log mel spectrogram than with the time-domain waveform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Hop sizes:</head><p>The hop size is the number of samples between adjacent frames. A small hop size leads to high resolution in the time domain. We investigate the influence of different hop sizes on AudioSet tagging with the CNN14 system. We investigate hop sizes of 1000, 640, 500 and 320: these correspond to time domain resolutions of 31.25 ms, 20.00 ms, 15.63 ms and 10.00 ms between adjacent frames, respectively. <ref type="table" target="#tab_1">Table VI</ref> shows that the mAP score increases as hop sizes decrease. With a hop size of 320, the CNN14 system achieves an mAP of 0.431, outperforming the larger hop sizes of 500, 640 and 1000. 6) Embedding dimensions: Embedding features are fixedlength vectors that summarize audio clips. By default, the CNN14 has an embedding feature dimension of 2048. We investigate a rage of CNN14 systems with embedding dimen-  <ref type="figure" target="#fig_3">Fig. 4(c)</ref> and <ref type="table" target="#tab_1">Table VII</ref> show that mAP performance increases as embedding dimension increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7)</head><p>Training with partial data: The audio clips of AudioSet are sourced from YouTube. Same audio clips are no longer downloadable, and others may be removed in the future. For better reproducibility of our work in future, we investigate the performance of systems trained with randomly chosen partial data ranging from 50% to 100% of our downloaded data. <ref type="figure" target="#fig_3">Fig.  4(d)</ref> and <ref type="table" target="#tab_1">Table VIII</ref> show that the mAP decreases slightly from 0.431 to 0.426 (a 1.2% drop) when the CNN14 system is trained with 80% of full data, and decreases to 0.406 (a 5.8% drop) when trained with 50% of full data. This result shows that the amount of training data is important for training PANNs.</p><p>8) Sample rate: <ref type="figure" target="#fig_3">Fig. 4</ref>(e) and <ref type="table" target="#tab_1">Table IX</ref> show the performance of the CNN14 system trained with different sample rate. The CNN14 system trained with 16 kHz audio recordings achieves an mAP of 0.427, similar (within 1.0%) to the CNN14 system trained with a sample rate of 32 kHz. On the other hand, the CNN14 system trained with 8 kHz audio recordings achieves a lower mAP of 0.406 (5.8% lower). This indicates that information in the 4 kHz -8 kHz range is useful for audio tagging. 9) Mel bins: <ref type="figure" target="#fig_3">Fig. 4(f)</ref> and <ref type="table">Table X</ref> show the performance of the CNN14 system trained with different number of mel bins. The system achieves an mAP of 0.413 with 32 mel bins, compared to 0.431 with 64 mel bins and 0.442 with 128 mel bins. This result suggests that PANNs achieve better performance with more mel bins, although the computation complexity increases linearly with the number of mel bins. Throughout this paper, we adopt 64 mel bins for extracting the log mel spectrogram, as a trade-off between computational complexity and system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10) Number of CNN layers:</head><p>We investigate the performance of CNN systems with 6, 10 and 14 layers, as described in Section II-A. <ref type="table" target="#tab_1">Table XI</ref> shows that the 6-, 10-and 14-layer CNNs achieve mAPs of 0.343, 0.380 and 0.431, respectively. This result suggests that PANNs with deeper CNN architectures achieve better performance than shallower CNN architectures. This result is in contrast to previous audio tagging systems trained on smaller datasets where shallower CNNs such as 9-layer CNN performed better than deeper CNNs <ref type="bibr" target="#b32">[33]</ref>. One possible explanation is that smaller datasets may suffer from overfitting, while AudioSet is large enough to train deeper CNNs, at least up to the 14 layers CNNs that we investigate. 11) ResNets: We apply ResNets to investigate the performance of deeper PANNs. <ref type="table" target="#tab_1">Table XI</ref> shows that the ResNet22 system achieves an mAP of 0.430 similar to the CNN14 system. ResNet38 achieves an mAP of 0.434, slightly outperforming other systems. ResNet54 achieves an mAP of 0.429, which does not further improve the performance. 12) MobileNets: The systems mentioned above show that PANNs achieve good performance in AudioSet tagging. However, those systems do not consider computational efficiency when implemented on portable devices. We investigate building PANNs with light weight MobileNets described in Section II-C. <ref type="table" target="#tab_1">Table XI</ref> shows that MobileNetV1 achieves an mAP of 0.389, 9.7% lower to the CNN14 system of 0.431. The number of multiplication and addition (multi-adds) and parameters of the MobileNetV1 system are only 8.6% and 5.9% of the CNN14 system, respectively. The MobileNetV2 system achieves an mAP of 0.383, 11.1% lower than CNN14, and is more computationally efficient than MobileNetV1, where the number of multi-adds and parameters are only 6.7% and 5.0% of the CNN14 system. 13) One-dimensional CNNs: <ref type="table" target="#tab_1">Table XI</ref> shows the performance of one-dimensional CNNs. The DaiNet with 18 layers <ref type="bibr" target="#b30">[31]</ref> achieves an mAP of 0.295. The LeeNet11 with 11 layers <ref type="bibr" target="#b41">[42]</ref> achieves an mAP of 0.266. Our improved LeeNet with 24 layers improves the mAP of LeeNet11 to 0.336. Our proposed Res1dNet31 and Res1dNet51 described in Section II-D3 achieve mAPs of 0.365 and 0.355 respectively, and achieve state-of-the-art performance among one-dimensional CNN systems. <ref type="table" target="#tab_1">Table  XI</ref> show the result of our proposed Wavegram-CNN and Wavegram-Logmel-CNN systems. The Wavegram-CNN system achieves an mAP of 0.389, outperforming the best previous one-dimensional CNN system (Res1dNet31). This  indicates that the Wavegram is an effective learnt feature. Furthermore, our proposed Wavegram-Logmel-CNN system achieves a state-of-the-art mAP of 0.439 among all PANNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14) Wavegram-Logmel-CNN: The bottom rows of</head><p>15) Complexity analysis: We analyze the computational complexity of PANNs for inference. The number of multiadds and parameters are two important factors for complexity analysis. The middle column of <ref type="table" target="#tab_1">Table XII</ref> shows the number of multi-adds to infer a 10-second audio clip. The right column of <ref type="table" target="#tab_1">Table XII</ref> shows the number of parameters of different systems. The number of multi-adds and parameters of the CNN14 system are 42.2 ? 10 9 and 80.8 million, respectively, which are larger than the CNN6 and CNN10 systems. The number of multi-adds for the ResNets22 and ResNet38 systems are slightly less than for the CNN14 system. The ResNet54 system contains the most multi-adds at 54.6 ? 10 9 . Onedimensional CNNs have a similar computational cost to the two-dimensional CNNs. The best performing one-dimensional system, Res1dNet31, contains 32.7 ? 10 9 multi-adds and 80.5 million parameters. Our proposed Wavegram-CNN system contains 44.2 ? 10 9 multi-adds and 81.0 million parameters, which is similar to CNN14. The Wavegram-Logmel-CNN system slightly increases the multi-adds to 53.5 ? 10 9 , and the number of parameters is to 81.1 million, which is similar to CNN14. To reduce the number of multi-adds and parameters,    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Transfer to other tasks</head><p>In this section, we investigate the application of PANNs to a range of other pattern recognition tasks. PANNs can useful for few-shot learning, for the tasks where only a limited number of training clips are provided. Few-shot learning is an important research topic in audio pattern recognition, as collecting labelled data can be time consuming. We transfer PANNs to other audio pattern recognition tasks using the methods described in Section V. To begin with, we resample all audio recordings to 32 kHz, and convert them to monophonic to be consistent with the PANNs trained on AudioSet. We perform the following strategies described in Section V for each task: 1) Train a system from scratch; 2) Use a PANN as a feature extractor; 3) Fine-tune a PANN. When using a PANN as the feature extractor, we build classifiers on the embedding features with one and three fully-connected layers, and call them "Freeze + 1 layers" (Freeze_L1) and "Freeze + 3 layers" (Freeze_L3), respectively. We adopt the CNN14 system for transfer learning to provide a fair comparison with other CNN based systems for audio pattern recognition. We also investigate the performance of PANNs trained with different number of shots when training other audio pattern recognition tasks.</p><p>1) ESC-50: ESC-50 is an environmental sound dataset <ref type="bibr" target="#b49">[50]</ref> consisting of 50 sound events, such as "Dog" and "Rain". There are 2,000 5-second audio clips in the dataset, with      <ref type="table" target="#tab_1">Table XIII</ref> shows the 5-fold cross validation <ref type="bibr" target="#b49">[50]</ref> accuracy of the CNN14 system. Sailor et al. <ref type="bibr" target="#b48">[49]</ref> proposed a state-of-the-art system for ESC-50, achieved an accuracy of 0.865 using unsupervised filterbank learning with a convolutional restricted Boltzmann machine. Our fine-tuned system achieves an accuracy of 0.947, outperforming previous state-of-the-art system by a large margin. The Freeze_L1 and Freeze_L3 systems achieve accuracies of 0.918 and 0.908, respectively. Training the CNN14 system from scratch achieves an accuracy of 0.833. <ref type="figure" target="#fig_6">Fig. 6</ref> shows the accuracy of ESC-50 with different numbers of training clips of each sound class. Using a PANN as a feature extractor achieves the best performance when fewer than 10 clips per sound class are available for training. With more training clips, the finetuned systems achieve better performance. Both the fine-tuned system and the system using the PANN as a feature extractor outperform the systems trained from scratch.</p><p>2) DCASE 2019 Task 1: DCASE 2019 Task 1 is an acoustic scene classification task <ref type="bibr" target="#b1">[2]</ref>, with a dataset consisting of over   40 hours of stereo recordings collected from various acoustic scenes in 12 European cities. We focus on Subtask A, where each audio recording has two channels with a sampling rate of 48 kHz. In the development set, there are 9185 and 4185 audio clips for training and validation respectively. We convert the stereo recordings to monophonic by averaging the stereo channels. CNN14 trained from scratch achieves an accuracy of 0.691, while the fine-tuned system achieves an accuracy of 0.764. Freeze_L1 and Freeze_L3 achieve accuracies of 0.689 and 0.607 respectively, and do not outperform the CNN14 trained from scratch. One possible explanation for this underperformance is that the audio recordings of acoustic scene classification have different distribution of AudioSet. So using PANN as a feature extractor does not outperform finetune or train a system from scratch. The fine-tuned system achieves better performance than the system trained from scratch. <ref type="figure" target="#fig_9">Fig. 7</ref> shows the classification accuracy of systems with various numbers of training clips per class. <ref type="table" target="#tab_1">Table XIV</ref> shows the overall performance. The state-of-the-art system of Chen et al <ref type="bibr" target="#b50">[51]</ref>. achieves an accuracy of 0.851 using the combination of various classifiers and stereo recordings as input, while we do not use any ensemble methods and stereo recordings.</p><p>3) DCASE 2018 Task 2: DCASE 2018 Task 2 is a generalpurpose automatic audio tagging task <ref type="bibr" target="#b53">[54]</ref> using a dataset of audio recordings from Freesound annotated with a vocabulary of 41 labels from the AudioSet ontology. The development set consists of 9,473 audio recordings with durations from 300 ms to 30 s. The mAP@3 is used for evaluating system performance <ref type="bibr" target="#b53">[54]</ref>. In training, we break or pad audio recordings into 4-second audio segments. In inference, we average the predictions of those segments to obtain the prediction of an audio recording. <ref type="table" target="#tab_6">Table XV</ref> shows that the best previous method proposed by <ref type="bibr">Jeong and Lim [52]</ref> achieves an mAP@3 of 0.954 using an ensemble of several systems. In comparison, our CNN14 system trained from scratch achieves an accuracy of 0.902. The fine-tuned CNN14 system achieves an mAP@3 of 0.941. The Freeze_L1 and Freeze_L3 systems achieve accuracies of 0.717 and 0.768 respectively. <ref type="figure" target="#fig_10">Fig. 8</ref> shows the    <ref type="bibr" target="#b54">[55]</ref> is a task to predict an audio recording to one of five categories: "Nature", "Music", "Human", "Effects" and "Urban". The dataset consists of a development set of 1,500 audio clips and an evaluation set of 500 audio clips. All audio clips have a duration of 4 seconds. The state-of-the-art system proposed by Chen and Gupta <ref type="bibr" target="#b52">[53]</ref> achieves an accuracy of 0.930. Our fine-tuned CNN14 achieves an accuracy of 0.960, outperforming previous state-of-the-art system. CNN14 trained from scratch achieves an accuracy of 0.760. <ref type="figure" target="#fig_12">Fig. 9</ref> shows the accuracy of the systems with different number of training clips. The fine-tuned CNN14 system and the system using CNN14 as a feature extractor outperforms CNN14 trained from scratch. 5) GTZAN: The GTZAN dataset <ref type="bibr" target="#b56">[57]</ref> is a music genre classification dataset containing 1,000 30-second music clips of 10 genres of music, such as "Classical" and "Country". All music clips have a duration of 30 seconds and a sampling rate of 22,050 Hz. In development, 10-fold cross validation is used to evaluate the performance of systems. <ref type="table" target="#tab_1">Table XVII</ref> shows that previous state-of-the-art system proposed by Liu et al. <ref type="bibr" target="#b55">[56]</ref> achieves an accuracy of 0.939 using a bottom-up broadcast neural network. The fine-tuned CNN14 system achieves an accuracy of 0.915, outperforming CNN14 trained from scratch with an accuracy of 0.758 and the Freeze_L1 and Freeze_L3 systems with accuracies of 0.827 and 0.858 respectively. <ref type="figure" target="#fig_0">Fig.  10</ref> shows the accuracy of systems with different numbers of training clips. The Freeze_L1 and Freeze_L3 systems achieve better performance than other systems trained with less than 10 clips per class. With more training clips, the fine-tuned CNN14 system performs better than other systems. 6) RAVDESS: The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) is a human speech emotion dataset <ref type="bibr" target="#b58">[59]</ref>. The database consists of sounds from  24 professional actors including 12 female and 12 male simulating 8 emotions, such as "Happy" and "Sad". The task is to classify each sound clip into an emotion. There are 1,440 audio clips in the development set. We evaluate our systems with 4-fold cross validation. <ref type="table" target="#tab_1">Table XVIII</ref> shows that previous state-of-the-art system proposed by Zeng et al. <ref type="bibr" target="#b57">[58]</ref> achieves an accuracy of 0.645. Our CNN14 system trained from scratch achieves an accuracy of 0.692. The fine-tuned CNN14 system achieves a state-of-the-art accuracy of 0.721. The Freeze_L1 and Freeze_L3 systems achieve much lower accuracies of 0.397 and 0.401 respectively. <ref type="figure" target="#fig_0">Fig. 11</ref> shows the accuracy of systems with respect to a range of training clips. The fine-tuned systems and the system trained from scratch outperform the system using PANN as a feature extractor. This suggests that audio recordings of the RAVDESS dataset may have different distributions of the AudioSet dataset. Therefore, the parameters of a PANN need be fine-tuned to achieve good performance on the RAVDESS classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussion</head><p>In this article, we have investigated a wide range of PANNs for AudioSet tagging. Several of our proposed PANNs have outperformed previous state-of-the-art AudioSet tagging systems, including CNN14 achieves an mAP of 0.431, and ResNet38 achieves an mAP of 0.434, outperforming Google's baseline of 0.314. MobileNets are light-weight systems that have fewer multi-adds and numbers of parameters. MobileNetV1 achieves an mAP of 0.389. Our adapted one-dimensional system Res1dNet31 achieves an mAP of 0.365, outperforming previous one-dimensional CNNs including DaiNet <ref type="bibr" target="#b30">[31]</ref> of 0.295 and LeeNet11 <ref type="bibr" target="#b41">[42]</ref> of 0.266. Our proposed Wavegram-Logmel-CNN system achieves the highest mAP of 0.439 among all PANNs. PANNs can be used as a pretrained model for new audio pattern recognition tasks.</p><p>PANNs trained on the AudioSet dataset were transferred to six audio pattern recognition tasks. We show that fine-tuned PANNs achieve state-of-the-art performance in the ESC-50, MSOS and RAVDESS classification tasks, and approach the state-of-the-art performance in the DCASE 2018 Task 2 and the GTZAN classification task. Of the PANN systems, the finetuned PANNs always outperform PANNs trained from scratch on new tasks. The experiments show that PANNs have been successful in generalizing to other audio pattern recognition tasks with limited number of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We have presented pretrained audio neural networks (PANNs) trained on the AudioSet for audio pattern recognition. A wide range of neural networks are investigated to build PANNs. We propose a Wavegram feature learnt from waveform, and a Wavegram-Logmel-CNN that achieves stateof-the-art performance in AudioSet tagging, archiving an mAP of 0.439. We also investigate the computational complexity of PANNs. We show that PANNs can be transferred to a wide range of audio pattern recognition tasks and outperform several previous state-of-the-art systems. PANNs can be useful when fine-tuned on a small amount of data on new tasks. In the future, we will extend PANNs to more audio pattern recognition tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bagpipes</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Architecture of Wavegram-Logmel-CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) A PANN is pretrained with the AudioSet dataset. (b) For a new task, the PANN is used as a feature extractor. A classifier is built on the extracted embedding features. The shaded rectangle indicates the parameters are frozen and not trained. (c) For a new task, the parameters of a neural network are initialized with a PANN. Then, all parameters are fine-tuned on the new task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Comparison of training data and augmentation CNN14,bal,mixup (1.9m) CNN14,bal,mixup-wav (1.9m) CNN14,bal,no-mixup (1.9m) CNN14,no-bal,no-mixup (1.9m) CNN14,bal,mixup (20k) CNN14,bal,no-mixup (20k)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Results of PANNs on AudioSet tagging. The transparent and solid lines are training mAP and evaluation mAP, respectively. The six plots show the results with different: (a) architectures; (b) data balancing and data augmentation; (c) embedding size; (d) amount of training data; (e) sampling rate; (f) number of mel bins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Multi-adds versus mAP of AudioSet tagging systems. The same types of architectures are grouped in the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Accuracy of ESC-50 with various number of training clips per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5</head><label>5</label><figDesc>summarizes the mAP versus multi-adds of different PANNs. The same type of systems are linked by lines of the same color. The mAP increases from bottom to top. On the top-right is our proposed Wavegram-Logmel-CNN system that achieves the best mAP. On the top-left are MobileNetV1 and MobileNetV2 that are the most computationally efficient systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Accuracy of DCASE 2019 Task 1 with various number of training clips per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Accuracy of DCASE 2018 Task 2 with various number of training clips per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 .</head><label>9</label><figDesc>Accuracy of MSoS with various number of training clips per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 10 .</head><label>10</label><figDesc>Accuracy of GTZAN with various number of training clips per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 11 .</head><label>11</label><figDesc>Accuracy of RAVDESS with various number of training clips per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 12 .</head><label>12</label><figDesc>Class-wise performance of AudioSet tagging systems. Red, blue and black curves are APs of CNN14, MobileNetV1 and the audio tagging system<ref type="bibr" target="#b19">[20]</ref>. The blue bars show the number of training clips in logarithmic scale.Qiuqiang Kong (S'17) received the B.Sc. degree in 2012, and the M.E. degree in 2015 from South China University of Technology, Guangzhou, China. He received the Ph.D. degree from University of Surrey, Guildford, UK in 2020. Following his PhD, he joined ByteDance AI Lab as a research scientist. His research topic includes the classification, detection and separation of general sounds and music. He is known for developing attention neural networks for audio tagging, and winning the audio tagging task in the detection and classification of acoustic scenes and events (DCASE) challenge in 2017. He was nominated as the postgraduate research student of the year in University of Surrey, 2019. He is a frequent reviewer for journals and conferences in the area including IEEE/ACM Transactions on Audio, Speech, and Language Processing.Yin Cao (M'18) received the B.Sc. degree in Electronic Science and Engineering from Nanjing University, China in 2008, and Ph.D. degree from Institute of Acoustics, Chinese Academy of Sciences, China in 2013. He then worked in Acoustics group at Brigham Young University, US, and at Institute of Acoustics, Chinese Academy of Sciences, China. In 2018, he joined Centre for Vision, Speech and Signal Processing at the University of Surrey. His research topic includes active noise control, air acoustics and signal processing, detection, classification and separation of audio. He is known for his work on decentralized active noise control, weighted spatial gradients control metric, and polyphonic sound event detection and localization. He was the winner of urban sound tagging in detection and classification of acoustic scenes and events (DCASE) 2020 challenge and achieved second-best of sound event detection and localization tasks in DCASE 2019 challenge. He has served as an Associate Editor for Noise Control Engineering Journal since 2020. He is also a frequent reviewer for IEEE/ACM Transactions on Audio, Speech, and Language Processing. Turab Iqbal received the B.Eng. degree in Electronic Engineering from the University of Surrey, U.K., in 2017. Currently, he is working towards a Ph.D. degree from the Centre for Vision, Speech and Signal Processing (CVSSP) in the University of Surrey. During his time as a Ph.D. student, he has worked on a number of projects in the area of audio classification and localization using machine learning methods. His research is mainly focused on learning with weakly-labeled or noisy training data. Wenwu Wang (M'02-SM'11) was born in Anhui, China. He received the B.Sc. degree in 1997, the M.E. degree in 2000, and the Ph.D. degree in 2002, all from Harbin Engineering University, China. He then worked in King's College London, Cardiff University, Tao Group Ltd. (now Antix Labs Ltd.), and Creative Labs, before joining University of Surrey, UK, in May 2007, where he is currently a professor in signal processing and machine learning, and a Co-Director of the Machine Audition Lab within the Centre for Vision Speech and Signal Processing. He has been a Guest Professor at Qingdao University of Science and Technology, China, since 2018. His current research interests include blind signal processing, sparse signal processing, audio-visual signal processing, machine learning and perception, machine audition (listening), and statistical anomaly detection. He has (co)-authored over 200 publications in these areas. He served as an Associate Editor for IEEE Transactions on Signal Processing from 2014 to 2018. He is also Publication Co-Chair for ICASSP 2019, Brighton, UK. He currently serves as Senior Area Editor for IEEE Transactions on Signal Processing and an Associate Editor for IEEE/ACM Transactions on Audio Speech and Language Processing.Mark D. Plumbley (S'88-M'90-SM'12-F'15) received the B.A.(Hons.) degree in electrical sciences and the Ph.D. degree in neural networks from University of Cambridge, Cambridge, U.K., in 1984 and 1991, respectively. Following his PhD, he became a Lecturer at King's College London, before moving to Queen Mary University of London in 2002. He subsequently became Professor and Director of the Centre for Digital Music, before joining the University of Surrey in 2015 as Professor of Signal Processing. He is known for his work on analysis and processing of audio and music, using a wide range of signal processing techniques, including matrix factorization, sparse representations, and deep learning. He is a co-editor of the recent book on Computational Analysis of Sound Scenes and Events, and Co-Chair of the recent DCASE 2018 Workshop on Detection and Classifications of Acoustic Scenes and Events. He is a Member of the IEEE Signal Processing Society Technical Committee on Signal Processing Theory and Methods, and a Fellow of the IET and IEEE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, Y. Cao, T. Iqbal, and M. D. Plumbley are with the Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford GU2 7XH, U.K. (e-mail: q.kong@surrey.ac.uk; yin.cao@surrey.ac.uk; t.iqbal@surrey.ac.uk; m.plumbley@surrey.ac.uk). This work was supported in part by the EPSRC Grant EP/N014111/1 "Making Sense of Sounds", in part by the Research Scholarship from the China Scholarship Council 201406150082, and in part by a studentship (Reference: 1976218) from the EPSRC Doctoral Training Partnership under Grant EP/N509772/1. This work was supported by National Natural Science Foundation of China (Grant No. 11804365). (Qiuqiang Kong is first author.) (Yin Cao is corresponding author.) Y. Wang is with the ByteDance AI Lab, Mountain View, CA, USA (e-mail: wangyuxuan.11@bytedance.com). W. Wang is with the Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford GU2 7XH, U.K., and also with Qingdao University of Science and Technology, Qingdao 266071, China (e-mail: w.wang@surrey.ac.uk).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I CNNS</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">FOR AUDIOSET TAGGING</cell><cell></cell></row><row><cell cols="2">VGGish [1]</cell><cell></cell><cell>CNN6</cell><cell>CNN10</cell><cell></cell><cell>CNN14</cell></row><row><cell cols="3">Log-mel spectrogram</cell><cell></cell><cell cols="2">Log-mel spectrogram</cell><cell></cell></row><row><cell cols="3">96 frames ? 64 mel bins</cell><cell></cell><cell cols="3">1000 frames ? 64 mel bins</cell></row><row><cell cols="2">3 ? 3 @ 64 ReLU</cell><cell></cell><cell>5 ? 5 @ 64 BN, ReLU</cell><cell>3 ? 3 @ 64 BN, ReLU</cell><cell>? 2</cell><cell>3 ? 3 @ 64 BN, ReLU</cell><cell>? 2</cell></row><row><cell cols="2">MP 2 ? 2</cell><cell></cell><cell></cell><cell cols="2">Pooling 2 ? 2</cell><cell></cell></row><row><cell cols="2">3 ? 3 @ 128 ReLU</cell><cell></cell><cell>5 ? 5 @ 128 BN, ReLU</cell><cell>3 ? 3 @ 128 BN, ReLU</cell><cell>? 2</cell><cell>3 ? 3 @ 128 BN, ReLU</cell><cell>? 2</cell></row><row><cell cols="2">MP 2 ? 2</cell><cell></cell><cell></cell><cell cols="2">Pooling 2 ? 2</cell><cell></cell></row><row><cell cols="2">3 ? 3 @ 256 ReLU</cell><cell>? 2</cell><cell>5 ? 5 @ 256 BN, ReLU</cell><cell>3 ? 3 @ 256 BN, ReLU</cell><cell>? 2</cell><cell>3 ? 3 @ 256 BN, ReLU</cell><cell>? 2</cell></row><row><cell cols="2">MP 2 ? 2</cell><cell></cell><cell></cell><cell cols="2">Pooling 2 ? 2</cell><cell></cell></row><row><cell cols="2">3 ? 3 @ 512 ReLU</cell><cell>? 2</cell><cell>5 ? 5 @ 512 BN, ReLU</cell><cell>3 ? 3 @ 512 BN, ReLU</cell><cell>? 2</cell><cell>3 ? 3 @ 512 BN, ReLU</cell><cell>? 2</cell></row><row><cell cols="2">MP 2 ? 2 Flatten</cell><cell></cell><cell cols="2">Global pooling</cell><cell></cell><cell cols="2">Pooling 2 ? 2</cell></row><row><cell>FC 4096 ReLU</cell><cell>? 2</cell><cell></cell><cell cols="2">FC 512, ReLU</cell><cell></cell><cell>3 ? 3 @ 1024 BN, ReLU</cell><cell>? 2</cell></row><row><cell cols="3">FC 527, Sigmoid</cell><cell cols="2">FC 527, Sigmoid</cell><cell></cell><cell cols="2">Pooling 2 ? 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3 ? 3 @ 2048 BN, ReLU</cell><cell>? 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Global pooling</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FC 2048, ReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FC 527, Sigmoid</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II RESNETS</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">FOR AUDIOSET TAGGING</cell></row><row><cell>ResNet22</cell><cell>ResNet38</cell><cell>ResNet54</cell></row><row><cell cols="3">Log mel spectrogram 1000 frames ?64 mel bins</cell></row><row><cell></cell><cell cols="2">3 ? 3 @ 512, BN, ReLU ? 2</cell></row><row><cell></cell><cell>Pooling 2 ? 2</cell><cell></cell></row><row><cell>BasicB @ 64 ? 2</cell><cell>BasicB @ 64 ? 3</cell><cell>BottleneckB @ 64 ? 3</cell></row><row><cell></cell><cell>Pooling 2 ? 2</cell><cell></cell></row><row><cell>BasicB @ 128 ? 2</cell><cell>BasicB @ 128 ? 4</cell><cell>BottleneckB @ 128 ? 4</cell></row><row><cell></cell><cell>Pooling 2 ? 2</cell><cell></cell></row><row><cell>BasicB @ 256 ? 2</cell><cell>BasicB @ 256 ? 6</cell><cell>BottleneckB @ 256 ? 6</cell></row><row><cell></cell><cell>Pooling 2 ? 2</cell><cell></cell></row><row><cell>BasicB @ 512 ? 2</cell><cell>BasicB @ 512 ? 3</cell><cell>BottleneckB @ 512 ? 3</cell></row><row><cell></cell><cell>Pooling 2 ? 2</cell><cell></cell></row><row><cell></cell><cell cols="2">3 ? 3 @ 2048, BN, ReLU ? 2</cell></row><row><cell></cell><cell>Global pooling</cell><cell></cell></row><row><cell></cell><cell>FC 2048, ReLU</cell><cell></cell></row><row><cell></cell><cell>FC 527, Sigmoid</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="2">MOBILENETS FOR AUDIOSET TAGGING</cell></row><row><cell>MobileNetV1</cell><cell>MobileNetV2</cell></row><row><cell cols="2">3 ? 3 @ 32, BN, ReLU</cell></row><row><cell cols="2">Pooling 2 ? 2</cell></row><row><cell>V1Block @ 64</cell><cell>V2Block, t=1 @ 16</cell></row><row><cell>V1Block @ 128</cell><cell>(V2Block, t=6 @ 24) ? 2</cell></row><row><cell cols="2">Pooling 2 ? 2</cell></row><row><cell>V1Block @ 128 V1Block @ 256</cell><cell>(V2Block, t=6 @ 32) ? 3</cell></row><row><cell cols="2">Pooling 2 ? 2</cell></row><row><cell>V1Block @ 256 V1Block @ 512</cell><cell>(V2Block, t=6 @ 64) ? 4</cell></row><row><cell cols="2">Pooling 2 ? 2</cell></row><row><cell>(V1Block @ 512) ? 5 V1Block @ 1024</cell><cell>(V2Block, t=6 @ 96) ? 3</cell></row><row><cell cols="2">Pooling 2 ? 2</cell></row><row><cell>V1Block @ 1024</cell><cell>(V2Block, t=6 @ 160) ? 3 (V2Block, t=6 @ 320) ? 1</cell></row><row><cell cols="2">Global pooling</cell></row><row><cell cols="2">FC, 1024, ReLU</cell></row><row><cell cols="2">FC, 527, Sigmoid</cell></row><row><cell>C. MobileNets</cell><cell></cell></row><row><cell cols="2">1) Conventional MobileNets: Computational complexity is</cell></row><row><cell cols="2">an important issue when systems are implemented on portable</cell></row><row><cell cols="2">devices. Compared to CNNs and ResNets, MobileNets were</cell></row><row><cell cols="2">intended to reduce the number of parameters and multiply-add</cell></row><row><cell cols="2">operations in a CNN. MobileNets were based on depthwise</cell></row><row><cell>separable convolutions</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc></figDesc><table><row><cell cols="3">WITH PREVIOUS METHODS</cell></row><row><cell></cell><cell>mAP</cell><cell>AUC</cell><cell>d-prime</cell></row><row><cell>Random guess</cell><cell cols="2">0.005 0.500</cell><cell>0.000</cell></row><row><cell>Google CNN [1]</cell><cell cols="2">0.314 0.959</cell><cell>2.452</cell></row><row><cell>Single-level attention [16]</cell><cell cols="2">0.337 0.968</cell><cell>2.612</cell></row><row><cell>Multi-level attention [17]</cell><cell cols="2">0.360 0.970</cell><cell>2.660</cell></row><row><cell cols="3">Large feature-level attention [20] 0.369 0.969</cell><cell>2.640</cell></row><row><cell>TAL Net [19]</cell><cell cols="2">0.362 0.965</cell><cell>2.554</cell></row><row><cell>DeepRes [48]</cell><cell cols="2">0.392 0.971</cell><cell>2.682</cell></row><row><cell>Our proposed CNN14</cell><cell cols="2">0.431 0.973</cell><cell>2.732</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V RESULTS</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">WITH DATA BALANCING AND AUGMENTATION</cell></row><row><cell cols="2">Augmentation</cell><cell>mAP</cell><cell>AUC</cell><cell>d-prime</cell></row><row><cell cols="2">no-bal,no-mixup (20k)</cell><cell cols="2">0.224 0.894</cell><cell>1.763</cell></row><row><cell cols="2">bal,no-mixup (20k)</cell><cell cols="2">0.221 0.879</cell><cell>1.652</cell></row><row><cell cols="2">bal,mixup (20k)</cell><cell cols="2">0.278 0.905</cell><cell>1.850</cell></row><row><cell cols="4">no-bal,no-mixup (1.9m) 0.375 0.971</cell><cell>2.690</cell></row><row><cell cols="2">bal,no-mixup (1.9m)</cell><cell cols="2">0.416 0.968</cell><cell>2.613</cell></row><row><cell cols="2">bal,mixup (1.9m)</cell><cell cols="2">0.431 0.973</cell><cell>2.732</cell></row><row><cell cols="2">bal,mixup-wav (1.9m)</cell><cell cols="2">0.425 0.973</cell><cell>2.720</cell></row><row><cell></cell><cell cols="2">TABLE VI</cell><cell></cell></row><row><cell></cell><cell cols="3">RESULTS OF DIFFERENT HOP SIZES</cell></row><row><cell cols="2">Hop size Time resolution</cell><cell>mAP</cell><cell>AUC</cell><cell>d-prime</cell></row><row><cell>1000</cell><cell>31.25 ms</cell><cell cols="2">0.400 0.969</cell><cell>2.645</cell></row><row><cell>640</cell><cell>20.00 ms</cell><cell cols="2">0.417 0.972</cell><cell>2.711</cell></row><row><cell>500</cell><cell>15.63 ms</cell><cell cols="2">0.417 0.971</cell><cell>2.682</cell></row><row><cell>320</cell><cell>10.00 ms</cell><cell cols="2">0.431 0.973</cell><cell>2.732</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII</head><label>VII</label><figDesc></figDesc><table><row><cell cols="4">RESULTS OF DIFFERENT EMBEDDING DIMENSIONS</cell></row><row><cell>Embedding</cell><cell>mAP</cell><cell>AUC</cell><cell>d-prime</cell></row><row><cell>32</cell><cell cols="2">0.364 0.958</cell><cell>2.437</cell></row><row><cell>128</cell><cell cols="2">0.412 0.969</cell><cell>2.634</cell></row><row><cell>512</cell><cell cols="2">0.420 0.971</cell><cell>2.689</cell></row><row><cell>2048</cell><cell cols="2">0.431 0.973</cell><cell>2.732</cell></row><row><cell></cell><cell cols="2">TABLE VIII</cell><cell></cell></row><row><cell cols="4">RESULTS OF PARTIAL TRAINING DATA</cell></row><row><cell>Training data</cell><cell>mAP</cell><cell>AUC</cell><cell>d-prime</cell></row><row><cell>50% of full</cell><cell cols="2">0.406 0.964</cell><cell>2.543</cell></row><row><cell>80% of full</cell><cell cols="2">0.426 0.971</cell><cell>2.677</cell></row><row><cell>100% of full</cell><cell cols="2">0.431 0.973</cell><cell>2.732</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX RESULTS</head><label>IX</label><figDesc></figDesc><table><row><cell cols="4">OF DIFFERENT SAMPLE RATES</cell></row><row><cell>Sample rate</cell><cell>mAP</cell><cell>AUC</cell><cell>d-prime</cell></row><row><cell>8 kHz</cell><cell cols="2">0.406 0.970</cell><cell>2.654</cell></row><row><cell>16 kHz</cell><cell cols="2">0.427 0.973</cell><cell>2.719</cell></row><row><cell>32 kHz</cell><cell cols="2">0.431 0.973</cell><cell>2.732</cell></row><row><cell></cell><cell cols="2">TABLE X</cell><cell></cell></row><row><cell cols="4">RESULTS OF DIFFERENT MEL BINS</cell></row><row><cell>Mel bins</cell><cell>mAP</cell><cell>AUC</cell><cell>d-prime</cell></row><row><cell>32 bins</cell><cell cols="2">0.413 0.971</cell><cell>2.691</cell></row><row><cell>64 bins</cell><cell cols="2">0.431 0.973</cell><cell>2.732</cell></row><row><cell>128 bins</cell><cell cols="2">0.442 0.973</cell><cell>2.735</cell></row><row><cell cols="2">sions of 32, 128, 512 and 2048.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE XI RESULTS</head><label>XI</label><figDesc></figDesc><table><row><cell cols="3">OF DIFFERENT SYSTEMS</cell><cell></cell></row><row><cell>Architecture</cell><cell>mAP</cell><cell>AUC</cell><cell>d-prime</cell></row><row><cell>CNN6</cell><cell cols="2">0.343 0.965</cell><cell>2.568</cell></row><row><cell>CNN10</cell><cell cols="2">0.380 0.971</cell><cell>2.678</cell></row><row><cell>CNN14</cell><cell cols="2">0.431 0.973</cell><cell>2.732</cell></row><row><cell>ResNet22</cell><cell cols="2">0.430 0.973</cell><cell>0.270</cell></row><row><cell>ResNet38</cell><cell cols="2">0.434 0.974</cell><cell>2.737</cell></row><row><cell>ResNet54</cell><cell cols="2">0.429 0.971</cell><cell>2.675</cell></row><row><cell>MobileNetV1</cell><cell cols="2">0.389 0.970</cell><cell>2.653</cell></row><row><cell>MobileNetV2</cell><cell cols="2">0.383 0.968</cell><cell>2.624</cell></row><row><cell>DaiNet [31]</cell><cell cols="2">0.295 0.958</cell><cell>2.437</cell></row><row><cell>LeeNet11 [42]</cell><cell cols="2">0.266 0.953</cell><cell>2.371</cell></row><row><cell>LeeNet24</cell><cell cols="2">0.336 0.963</cell><cell>2.525</cell></row><row><cell>Res1dNet31</cell><cell cols="2">0.365 0.958</cell><cell>2.444</cell></row><row><cell>Res1dNet51</cell><cell cols="2">0.355 0.948</cell><cell>2.295</cell></row><row><cell>Wavegram-CNN</cell><cell cols="2">0.389 0.968</cell><cell>2.612</cell></row><row><cell>Wavegram-Logmel-CNN</cell><cell cols="2">0.439 0.973</cell><cell>2.720</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE XII NUMBER</head><label>XII</label><figDesc>OF MULTI-ADDS AND PARAMETERS OF DIFFERENT SYSTEMS</figDesc><table><row><cell cols="4">Architecture</cell><cell></cell><cell>Multi-Adds</cell><cell></cell><cell cols="2">Parameters</cell></row><row><cell cols="3">CNN6</cell><cell></cell><cell></cell><cell>21.986 ?10 9</cell><cell></cell><cell cols="2">4,837,455</cell></row><row><cell cols="3">CNN10</cell><cell></cell><cell></cell><cell>28.166 ?10 9</cell><cell></cell><cell cols="2">5,219,279</cell></row><row><cell cols="3">CNN14</cell><cell></cell><cell></cell><cell>42.220 ?10 9</cell><cell></cell><cell cols="2">80,753,615</cell></row><row><cell cols="3">ResNet22</cell><cell></cell><cell></cell><cell>30.081 ?10 9</cell><cell></cell><cell cols="2">63,675,087</cell></row><row><cell cols="3">ResNet38</cell><cell></cell><cell></cell><cell>48.962 ?10 9</cell><cell></cell><cell cols="2">73,783,247</cell></row><row><cell cols="3">ResNet54</cell><cell></cell><cell></cell><cell>54.563 ?10 9</cell><cell></cell><cell cols="2">104,318,159</cell></row><row><cell cols="4">MobileNetV1</cell><cell></cell><cell>3.614 ?10 9</cell><cell></cell><cell cols="2">4,796,303</cell></row><row><cell cols="4">MobileNetV2</cell><cell></cell><cell>2.810 ?10 9</cell><cell></cell><cell cols="2">4,075,343</cell></row><row><cell cols="3">DaiNet</cell><cell></cell><cell></cell><cell>30.395 ?10 9</cell><cell></cell><cell cols="2">4,385,807</cell></row><row><cell cols="3">LeeNet11</cell><cell></cell><cell></cell><cell>4.741 ?10 9</cell><cell></cell><cell cols="2">748,367</cell></row><row><cell cols="3">LeeNet24</cell><cell></cell><cell></cell><cell>26.369 ?10 9</cell><cell></cell><cell cols="2">10,003,791</cell></row><row><cell cols="4">Res1dNet31</cell><cell></cell><cell>32.688 ?10 9</cell><cell></cell><cell cols="2">80,464,463</cell></row><row><cell cols="4">Res1dNet51</cell><cell></cell><cell>61.833 ?10 9</cell><cell></cell><cell cols="2">106,538,063</cell></row><row><cell cols="4">Wavegram-CNN</cell><cell></cell><cell>44.234 ?10 9</cell><cell></cell><cell cols="2">80,991,759</cell></row><row><cell cols="6">Wavegram-Logmel-CNN 53.510 ?10 9</cell><cell></cell><cell cols="2">81,065,487</cell></row><row><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mAP</cell><cell>0.35 0.40 0.45</cell><cell cols="2">MobileNetV1 MobileNetV2</cell><cell>Cnn6</cell><cell cols="3">LeeNet18 Cnn10 Res1dNet30 Cnn14 Wavegram-CNN Res1dNet44 ResNet54 ResNet22 ResNet38 Wavegram-Logmel-CNN</cell></row><row><cell></cell><cell>0.30</cell><cell></cell><cell></cell><cell></cell><cell>DaiNet</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.25</cell><cell cols="2">LeeNet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.20</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30 Multi-adds (million) 40</cell><cell>50</cell><cell>60</cell><cell>70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XIII</head><label>XIII</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">ACCURACY OF ESC-50</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">STOA [49] Scratch fine-tune Freeze_L1 Freeze_L3</cell></row><row><cell>Acc.</cell><cell>0.865</cell><cell>0.833</cell><cell>0.947</cell><cell>0.908</cell><cell>0.918</cell></row></table><note>MobileNets are applied. The MobileNetV1 and MobileNetV2 systems are light weight CNNs, with only 3.6 ? 10 9 and 2.8 ? 10 9 multi-adds and around 4.8 million and 4.1 million parameters, respectively. MobileNets reduce both the com- putational cost and system size.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XIV</head><label>XIV</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="4">ACCURACY OF DCASE 2019 TASK 1</cell></row><row><cell></cell><cell></cell><cell cols="5">STOA [51] Scratch Fine-tune Freeze_L1 Freeze_L3</cell></row><row><cell cols="2">Acc.</cell><cell>0.851</cell><cell>0.691</cell><cell>0.764</cell><cell>0.589</cell><cell>0.607</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Map@3</cell><cell>0.0 0.2 0.4 0.6</cell><cell>2</cell><cell cols="4">5 Training samples per class 10 Finetune Scratch Freeze + 1layer 32 (all) Freeze + 3 layers</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XV</head><label>XV</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">ACCURACY OF DCASE 2018 TASK 2</cell><cell></cell></row><row><cell></cell><cell cols="5">STOA [52] Scratch Fine-tune Freeze_L1 Freeze_L3</cell></row><row><cell>mAP@3</cell><cell>0.954</cell><cell>0.902</cell><cell>0.941</cell><cell>0.717</cell><cell>0.768</cell></row><row><cell cols="2">40 clips per class.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XVI</head><label>XVI</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">ACCURACY OF MSOS</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">STOA [53] Scratch Fine-tune Freeze_L1 Freeze_L3</cell></row><row><cell>Acc.</cell><cell>0.930</cell><cell>0.760</cell><cell>0.960</cell><cell>0.886</cell><cell>0.930</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE XVII</head><label>XVII</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">ACCURACY OF GTZAN</cell><cell></cell><cell></cell></row><row><cell cols="6">STOA [56] Scratch Fine-tune Freeze_L1 Freeze_L3</cell></row><row><cell>Acc.</cell><cell>0.939</cell><cell>0.758</cell><cell>0.915</cell><cell>0.827</cell><cell>0.858</cell></row><row><cell cols="6">mAP@3 with different numbers of training clips. The fine-</cell></row><row><cell cols="6">tuned CNN14 system outperforms the systems trained from</cell></row><row><cell cols="6">scratch and the systems using PANN as a feature extractor.</cell></row><row><cell cols="6">The fine-tuned CNN14 system achieves comparable results to</cell></row><row><cell cols="3">the state-of-the-art system.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">4) MSoS: The Making Sense of Sounds (MSoS) data</cell></row><row><cell>challenge</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE XVIII</head><label>XVIII</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">ACCURACY OF RAVDESS</cell><cell></cell></row><row><cell></cell><cell cols="5">STOA [58] Scratch Fine-tune Freeze_L1 Freeze_L3</cell></row><row><cell>Acc.</cell><cell>0.645</cell><cell>0.692</cell><cell>0.721</cell><cell>0.397</cell><cell>0.401</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Audio Set: An ontology and humanlabeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A multi-device dataset for urban acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic tagging using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Society for Music Information Retrieval</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="805" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Polyphonic sound event detection using multi label deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cakir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling and classification of natural sounds by product code hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Woodard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1833" to="1835" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Detecting alarm sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<idno type="DOI">https:/academiccommons.columbia.edu/doi/10.7916/D8F19821/download</idno>
		<ptr target="https://academiccommons.columbia.edu/doi/10.7916/D8F19821/download" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detection and classification of acoustic scenes and events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giannoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1733" to="1746" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detection and classification of acoustic scenes and events: Outcome of the DCASE 2016 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="379" to="393" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DCASE 2017 challenge setup: Tasks, datasets and baseline system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diment</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Detection and Classification of Acoustic Scenes and Events</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="85" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<ptr target="http://dcase.community/challenge2019" />
	</analytic>
	<monogr>
		<title level="j">DCASE Challenge</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CNN architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transfer learning for music classification and regression tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Society of Music Information Retrieval</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="141" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end learning for music audio tagging at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prockup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Society for Music Information Retrieval</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="637" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Audio Set classification with attention model: A probabilistic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="316" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-level attention model for weakly supervised audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Barsim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Detection and Classification of Acoustic Scenes and Events (DCASE)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="188" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to recognize transient sound events using attentional supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><forename type="middle">R</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3336" to="3342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comparison of five multiple instance learning pooling functions for sound event detection with weak labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly labelled audioset tagging with attention neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1791" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transfer learning by supervised pre-training for audio-based music classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Society for Music Information Retrieval</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="29" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Polyphonic sound event detection with weak labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Input-agreement: a new mechanism for collecting data using human computation games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Von Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1197" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TUT database for acoustic scene classification and sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on European Signal Processing Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1128" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">MUSICNN: Pre-trained convolutional neural networks for music audio tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06654</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transfer learning of weakly labelled audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diment</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Classification of general audio data for content-based retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">K</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dimitrova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mcgee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="533" to="544" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An MFCC-GMM approach for event detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Broeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karsmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vanrumste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Acoustic event detection in real life recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eronen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1267" to="1271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Non-speech environmental sound classification using SVMs with a new set of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Barkana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cevikalp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Innovative Computing, Information and Control</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="3511" to="3524" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Very deep convolutional neural networks for raw waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="421" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cross-task learning for audio tagging, sound event detection and spatial localization: DCASE 2019 baseline systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03476</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>MobileNetV2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sample-level deep convolutional neural networks for music auto-tagging using raw waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sound and Music Computing Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="220" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A dataset and taxonomy for urban sound research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1041" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python in Science Conference</title>
		<meeting>the Python in Science Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A deep residual network for large-scale acoustic scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INTERSPEECH</title>
		<imprint>
			<biblScope unit="page" from="2568" to="2572" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Unsupervised filterbank learning using convolutional restricted Boltzmann machine for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Sailor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3107" to="3111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">ESC: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Integrating the data augmentation scheme with various classifiers for acoustic scene modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2019 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Audio tagging system for DCASE 2018: focusing on label noise data augmentation and its efficient learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE Challenge Tech. Rep</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Attention-based convolutional neural network for audio event classification with feature transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<ptr target="https://cvssp.org/projects/making_sense_of_sounds/site/assets/challenge_abstracts_and_figures/Tianxiang_Chen.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">General-purpose tagging of freesound audio with audioset labels: Task description, dataset, and baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)</title>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="69" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Generalisation in environmental sound classification: The &apos;Making Sense of Sounds&apos; data set and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kroos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8082" to="8086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Bottom-up broadcast neural network for music genre classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08928</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Musical genre classification of audio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="293" to="302" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Spectrogram based multi-task audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="3705" to="3722" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">RAVDESS: The Ryerson audio-visual database of emotional speech and song</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Canadian Society for Brain, Behaviour and Cognitive Science (CSBBCS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
