<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Points to Patches: Enabling the Use of Self-Attention for 3D Shape Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Berg</surname></persName>
							<email>axel.berg@arm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Arm ML Research Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centre for Mathematical Sciences</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Oskarsson</surname></persName>
							<email>magnus.oskarsson@math.lth.se</email>
							<affiliation key="aff1">
								<orgName type="department">Centre for Mathematical Sciences</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>O&amp;apos;connor</surname></persName>
							<email>mark.oconnor@arm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Arm ML Research Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Points to Patches: Enabling the Use of Self-Attention for 3D Shape Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While the Transformer architecture has become ubiquitous in the machine learning field, its adaptation to 3D shape recognition is non-trivial. Due to its quadratic computational complexity, the self-attention operator quickly becomes inefficient as the set of input points grows larger. Furthermore, we find that the attention mechanism struggles to find useful connections between individual points on a global scale. In order to alleviate these problems, we propose a two-stage Point Transformer-in-Transformer (Point-TnT) approach which combines local and global attention mechanisms, enabling both individual points and patches of points to attend to each other effectively. Experiments on shape classification show that such an approach provides more useful features for downstream tasks than the baseline Transformer, while also being more computationally efficient. In addition, we also extend our method to feature matching for scene reconstruction, showing that it can be used in conjunction with existing scene reconstruction pipelines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Due to the unordered nature of 3D point clouds, applying neural networks for shape recognition requires the use of permutation-equivariant architectures. The Transformer, first introduced by Vaswani et al. <ref type="bibr" target="#b0">[1]</ref> for the task of natural language processing, is an example of such an architecture and given its recent success in the fields of image classification <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, object detection <ref type="bibr" target="#b3">[4]</ref>, video analysis <ref type="bibr" target="#b4">[5]</ref>, speech recognition <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and more, its application to 3D point clouds is a natural step of exploration. While some attempts to adopt the Transformer architecture for this task have been made <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, they all suffer from different weaknesses, such as a reduced receptive field and high computational cost. Inspired by the Transformer-in-Transformer architecture for image processing <ref type="bibr" target="#b12">[13]</ref>, we propose a method that addresses both of these problems by using a two-stage attention mechanism which is able to learn more descriptive features while also lowering the number of required computations.</p><p>In summary, our main contributions are 1) We propose a two-stage Transformer architecture that combines attention mechanisms on a local and global scale. By sampling a sparse set of anchor points we create patches of local features. Self-attention can then be applied both on points within the patches and on the patches themselves. 2) Our experiments show that this approach gives significant uplifts compared to applying self-attention on the  <ref type="bibr" target="#b13">[14]</ref>. Our proposed Point-TnT networks provides the best trade-off between accuracy and fast inference when compared to previous methods, and significantly improves performance compared to a Transformer baseline. entire set of points, while also reducing the computational complexity. <ref type="bibr" target="#b2">3)</ref> We show that our proposed architecture achieves competitive results on 3D shape classification benchmarks, while requiring less floating point operations (FLOPs) compared to other methods, and that it can be used for improving 3D feature matching. The volumetric approach, as proposed in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, aims to map the points onto a three-dimensional occupancy grid of voxels, which allows for the use of CNNs for feature extraction. However, this approach suffers from poor scaling behaviour with the grid-resolution, making it intractable for processing of large point cloud scenes. This problem can be alleviated to some extent by using sparse convolutions <ref type="bibr" target="#b16">[17]</ref>, but will inevitably suffer from loss of geometric detail when points are mapped to an occupancy grid. In contrast, the multi-view approach instead maps the point clouds onto multiple twodimensional grids, each capturing a different view of the scene. This method, first proposed in <ref type="bibr" target="#b17">[18]</ref>, has later been refined to use adaptive views that are learned at training time <ref type="bibr" target="#b18">[19]</ref>. Others have proposed combining the multi-view approach with graph convolutional networks (GCN) in order to aggregate views over nearby view positions <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>A family of neural networks termed Deep Sets, which allows for feature extraction on unordered sets without mapping them to any grid-like structure, was introduced by Zaheer et al. <ref type="bibr" target="#b20">[21]</ref>. Such networks can be realized by weight sharing across all input points and the use of a symmetric aggregation function, e.g. the max or mean values of the sets, which enables extraction of permutation-equivariant point features. Concurrent work by Qi et al. <ref type="bibr" target="#b21">[22]</ref> introduced PointNet, where this architecture was combined with a Spatial Transformer network <ref type="bibr" target="#b22">[23]</ref> (not to be confused with the attention-based Transformer), that can learn to align point clouds in a common frame of reference <ref type="bibr" target="#b21">[22]</ref>. This approach has later been enhanced by exploiting geometric properties of the point cloud, such as the nearest neighbour graph <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, which enables aggregation not only over the entire point cloud, but also over local neighbourhoods of points.</p><p>Self-Attention and the Transformer The Transformer architecture <ref type="bibr" target="#b0">[1]</ref> belongs to the family of permutation-equivariant neural networks and is therefore a natural extension of the Deep Sets architecture. By allowing all elements of the input set to attend to each other, the Transformer is able to learn interactions between elements, which greatly enhances the network ability to learn complex interactions. It was first applied to point clouds by Lee et al. <ref type="bibr" target="#b9">[10]</ref>, who noted that this quickly becomes infeasible as the computational requirement grows quadratically with number of input points. They addressed this problem by introducing the concept of induced set attention by allowing the points to attend to a set of learnable parameters instead of themselves.</p><p>Zhao et al. <ref type="bibr" target="#b10">[11]</ref> proposed a modified Transformer architecture, which only computes local attention between a point and its nearest neighbours. However, this approach removes the main benefit of self-attention, namely that it enables a global receptive field in early layers of the network. In contrast, Guo et al. <ref type="bibr" target="#b11">[12]</ref> introduced offset attention with a neighbour embedding module in order to perform attention between groups of points, where each group is represented by a local feature. This method omits local attention entirely and for segmentation tasks it also suffers from quadratic complexity in the number of input points. Finally, a third approach has been explored in <ref type="bibr" target="#b25">[26]</ref>, where self-attention is computed both between points and channels within points, but only at a late stage in the network and only for the particular task of areal LiDAR image segmentation.</p><p>Recently, Transformers have also shown great success in the domain of computer vision in the form of the Vision Transformer <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, where an image is regarded as a set of local patches, which can then be processed using a Transformer with minimal modifications. Han et al. <ref type="bibr" target="#b12">[13]</ref> proposed the Transformer-in-Transformer architecture, which extends the baseline vision Transformer with pixel-wise attention within patches. Our approach is inspired by this work, in the sense that we use two branches of Transformers, one for local and one for global attention. This enables a global receptive field early in the network, while reducing the computational requirement of the self-attention operation compared to a baseline Transformer implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Preliminaries Let X ? R N ?d be a matrix representation of a set, containing N features in d-dimensional space. Furthermore, following the definitions introduced in [1], let Q = X l W Q , K = X l W K and V = X l W V denote the queries, keys and values respectively, where W Q , W K , W V ? R d?d h are learnable parameters and d h is the attention-head dimension. We then define the self-attention (SA) operator as</p><formula xml:id="formula_0">SA(X) = Softmax QK T ? d h V,<label>(1)</label></formula><p>where the softmax function is applied on each row individually. We note that the SA operation is permutation-equivariant, since for any permutation ? over the rows of X, we have that SA(?X) = ?SA(X). This property is especially useful if X represents a collection of unordered points, which is common in many 3D learning scenarios. By performing multiple SA operations, where each operator has its own learnable set of weights, in parallel and concatenating the results column-wise, we can define the multi-headed SA operator (MSA) as <ref type="formula">(2)</ref> where ; denotes column-wise concatenation, W P ? R hd h ?d is another learnable parameter and h is the number of attention heads. Using the above notation, we can define the Transformer layer T ? asX</p><formula xml:id="formula_1">MSA(X) = [SA 1 (X); SA 2 (X); ...; SA h (X)]W P ,</formula><formula xml:id="formula_2">= MSA(LN(X)) + X,<label>(3)</label></formula><formula xml:id="formula_3">T ? (X) = MLP(LN(X)) +X,<label>(4)</label></formula><p>where MLP denotes a multi-layer perceptron with a single hidden layer and GELU activation <ref type="bibr" target="#b26">[27]</ref>, LN is the LayerNorm <ref type="bibr" target="#b27">[28]</ref> operation and ? is the collection of parameters for the particular layer. Here we use the pre-norm <ref type="bibr" target="#b28">[29]</ref> variant of the Transformer, where LayerNorm is applied before the MSA and MLP operations. In order to aggregate features in a permutation-invariant fashion, we follow <ref type="bibr" target="#b24">[25]</ref> and compute the maximum and average over all features and concatenate the results columnwise. This can be compactly expressed as</p><formula xml:id="formula_4">?(X) = [max i X i ; 1 N i X i ], i = 1, ..., N,<label>(5)</label></formula><p>where X i are the rows of X.</p><p>Point Transformer-in-Transformer Now we are ready to define our main architecture. Let X = {x i } N i=1 be a set of N points in three-dimensional space. A naive application of the Transformer architecture would be to apply self-attention between all points, such that each point in the set can attend to every other point. Here we instead investigate how the attention mechanism can be applied within and between local patches of points, effectively splitting the self-attention operator into two different branches. An overview of our method can be seen in <ref type="figure">Figure 2</ref>.</p><formula xml:id="formula_5">First, we compute the k-nearest neighbour (k-NN) graph G = (V, E), with vertices V = {i} N i=1 and directed edges E ? V ? V from vertex i to j if x j is one of x i 's k-nearest neighbours.</formula><p>In order to create patches, we then sample a sparse set of M anchor points and aggregate features from their neighbours. We do this by considering the subgraph H =</p><formula xml:id="formula_6">(V, E ) of G, which has edges from i to j if i ? V ?(i, j) ? E, where V ? V and |V | = M &lt; N .</formula><p>In practice, we create H by sampling a subset of points using farthest point sampling <ref type="bibr" target="#b29">[30]</ref>, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>, which guarantees that the vertices in V are evenly distributed across the whole point cloud. From now on, we will refer to the set Y = {x m } m?V as the anchor points.</p><p>Since local geometry is best represented in a local frame of reference <ref type="bibr" target="#b24">[25]</ref>, we then extract edge features for each anchor point as E m = [e m1 , ..., e mk ] T , where</p><formula xml:id="formula_7">e mj = x j ? x m , (m, j) ? E .<label>(6)</label></formula><p>Without loss of generality, we can stack the anchor points into a matrix Y = [x 1 , ..., x M ] T ? R M ?3 , where the order of the anchor points is not important. These are then, together with their corresponding edge features, projected to higher</p><formula xml:id="formula_8">dimensions as Y 0 = Y W Y and E m 0 = E m W E , where W Y ? R 3?d Y and W E ? R 3?d E respectively.</formula><p>The anchor and edge features are then processed using two branches of L sequential Transformer blocks. The local branch computes self-attention between edge features, which enables interaction between edges within the neighbourhoods of the anchor points:</p><formula xml:id="formula_9">E m l = T local ? l (E m l?1 ), l = 1, .., L.<label>(7)</label></formula><p>After each local Transformer layer, the neighbourhood features are aggregated and concatenated into a new matrix as</p><formula xml:id="formula_10">E l = [?(E 1 l ), ..., ?(E M l )] ? R M ?2d E ,</formula><p>which is then added to each anchor point using another linear projection: </p><formula xml:id="formula_11">Y l?1 = Y l?1 + E l W l , l = 1, ..., L,<label>(8)</label></formula><formula xml:id="formula_12">Y l = T global ? l (? l?1 ), l = 1, ..., L.<label>(9)</label></formula><p>In order to exploit intermediate feature representations, the anchor point features from each layer are concatenated, combined using a single-layer MLP and aggregated in order to form a single global feature for the entire point cloud:</p><formula xml:id="formula_13">Z = ?(MLP([Y 1 ; ...; Y L ])),<label>(10)</label></formula><p>such that Z ? R 2d f , where d f is the embedding dimension of the global feature. The global feature is then used for downstream tasks, using e.g. another MLP for classification.</p><p>Alluding to the Transformer-in-Transformer for images <ref type="bibr" target="#b12">[13]</ref>, we refer to our proposed method as Point Transformer-in-Transformer (Point-TnT).</p><p>Computational Analysis Whereas a naive transformer implementation requires O(N 2 ) computations for the SA operator, the complexity is reduced significantly by splitting the attention into local and global branches. For our method, the complexity of the local and global transformers are O(M k 2 ) and O(M 2 ) respectively, resulting in a reduced number of self-attention operations compared to a naive Transformer implementation as long as M k 2 + M 2 &lt; N 2 , a condition which can easily be satisfied by limiting the number of neighbours and anchor points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shape Classification</head><p>We train and evaluate our model on the ScanObjectNN dataset <ref type="bibr" target="#b13">[14]</ref>, which contains 14,510 real-world 3D objects in 15 categories, obtained from scans of indoor environments. This dataset is particularly challenging, since the point clouds contain cluttered backgrounds and partial occlusions. We use the hardest version of the dataset (PB T50 RS), which has been altered using random perturbations, and the official 80/20 train/test split. This dataset also comes with per-point labels that can be used to segment the point cloud into the object and background categories. While some methods use the segmentation masks during training in order to learn to discard points belonging to the background, we do not exploit this information, since it requires additional computational overhead.</p><p>In the default setting, we use M = 192 anchor points and k = 20 neighbours. The embedding dimensions are chosen as d Y = 192 and d E = 32 for the anchor points and edges respectively, and we use a global feature embedding dimension of d f = 1024 and a total of L = 4 sequential Transformer blocks. Following <ref type="bibr" target="#b2">[3]</ref>, we let d Y /h = 64 and consequently use h = 3 attention heads for both Transformer branches. As in <ref type="bibr" target="#b24">[25]</ref>, the final MLP used for classification contains two hidden layers of size 512 and 256 respectively, with batch normalization and dropout applied in each layer. We train our network using the standard cross-entropy loss function for 500 epochs, with a batch size of 32 using the AdamW optimizer <ref type="bibr" target="#b35">[36]</ref> with a weight decay of 0.1 and a cosine learning rate schedule starting at 0.001. For data augmentation, we use RSMix <ref type="bibr" target="#b36">[37]</ref> in addition to random anisotropic scaling and shifting. For each experiment, we train our model three times and report a 95 % confidence interval for the mean accuracy 1 .</p><p>We compare our method to a naive approach that applies self-attention directly on all points, which corresponds to using all points as anchors and no neighbours, thereby discarding the local branch of the network, and refer to this method simply as Baseline. When comparing to previously published methods, we use two different evaluation protocols. Protocol 1 uses the model at the last training epoch for evaluation on the test set, whereas Protocol 2 evaluates the model on the test set each training epoch and reports the best obtained test accuracy. Although Protocol 2 clearly exploits information from the test set, we present results using both protocols for better side-byside comparisons with other methods. For further discussion of different evaluation protocols, we refer to <ref type="bibr" target="#b33">[34]</ref>.</p><p>We compare the overall accuracy and average per-class accuracy to previously published methods in <ref type="table" target="#tab_1">Table I</ref>. Our method achieves an accuracy on par with state-of-the-art results, and the best result among the methods that do not exploit the background segmentation masks during training, outperforming all multi-view methods. Furthermore, it can be observed that using a sparse set of anchor points in conjunction with aggregation over neighbouring points performs significantly better than the baseline approach, which is not competitive on this task. This shows that global point-wise attention is not enough to capture semantic properties of objects, and that it is necessary to consider local geometric properties in order to efficiently utilize the attention mechanism. <ref type="figure">Fig. 4</ref>. Visualization of the learned attention patterns in the final Transformer layer. The blue cross indicates the point for which the attention map is computed and the red, green and yellow dots highlight the attention for the three different heads, where the size of the points are scaled proportionally to the magnitude of the attention weights. Top: our proposed method using a sparse set of anchor points. Bottom: baseline method, using all points as anchors. For completeness, we also evaluate our method on the Mod-elNet40 dataset <ref type="bibr" target="#b14">[15]</ref>, which contains 9,843 synthetic shapes for training and 2,468 for testing, in 40 different categories. We obtain 92.6 ? 0.2 % and 93.2 ? 0.2 % accuracy using Protocol 1 and 2 respectively, which is competitive with other methods.</p><p>We visualize the learned attention patterns from the different attention heads, as shown in <ref type="figure">Figure 4</ref>, using our proposed method and the naive baseline. It can be seen that when using a sparse set of anchor points, using their neighbours to form patches, the attention mechanism effectively learns to segment the point cloud into semantically meaningful parts. This allows the network to connect different parts of the point cloud when computing the global feature. However, when using the baseline approach, the attention map is not semantically meaningful. This illustrates the importance of neighbourhood context when computing self-attention, which is not used in the naive baseline approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study</head><p>Model scaling In order to highlight the trade-off between model size, computational complexity and classification accuracy, we ablate the number of attention heads (and scale the embedding dimension d Y accordingly) for our method. In <ref type="table" target="#tab_1">Table II</ref> it can be seen that accuracy increases with the model size and that the number of FLOPs is low compared to the baseline, even for similar model sizes. As shown in Section III, splitting the self-attention operator into two branches reduces the number of computations significantly, which in practice leads to a reduced number of FLOPs per forward-pass through the network.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, we illustrate the trade-off between FLOPs and classification accuracy for different methods, highlighting the competitive trade-off our proposed method. This result is in agreement with recent trends in computer vision, where Transformers are shown to be more computationally efficient compared to other architectures <ref type="bibr" target="#b12">[13]</ref>.</p><p>Attention mechanisms In order to demonstrate the effectiveness of the self-attention mechanisms, we ablate both the global and local attention modules in the Transformer blocks of the network by disabling the corresponding MSA operations. For the baseline method, there is only a single (global) attention mechanism which can be ablated. The results, shown in <ref type="table" target="#tab_1">Table III</ref>, verify that for our proposed method, both modules are indeed useful and increase the overall accuracy. Furthermore, they are additive in the sense that they can be used interchangeably or in combination, in order to trade-off accuracy for computation and model size.</p><p>The tighter bound on the accuracy when using global attention also suggests that it helps stabilize training. The baseline method also benefits from attention to some extent, but adding it to the network does not yield the same improvement as simply splitting the network into two branches.</p><p>Number of anchors and neighbours Finally, we investigate the effect of varying the number of anchor points and neighbours, as shown in <ref type="table" target="#tab_1">Table IV</ref>. As expected, a relatively large number of anchors is required in order to accurately represent the global shape of the point cloud. However, concerning the number of nearest neighbours, it seems that 10 is sufficient for good accuracy, but increasing it further reduces variance between different training runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Matching on 3DMatch</head><p>In order to demonstrate how Point-TnT can be applied to a real-world scene resonstruction scenario, we consider the problem of feature matching on the 3DMatch dataset <ref type="bibr" target="#b37">[38]</ref>, which consists of 62 indoor scenes collected from RGB-D measurements. The goal of the feature matching task is to generate descriptors of local patches of the scenes which can be used for matching scans that have significant overlap. More specifically, given a pair of point cloud scenes (X , X ) with at least 30 % overlap, we find the corresponding points by extracting local features and matching them using nearest neighbour search. It then becomes possible to register the two scenes by estimating a rigid transformation using e.g. RANSAC.</p><p>We use the official split, with 54 scenes for training and 8 for testing, and the same pre-processing setup as in DIP <ref type="bibr" target="#b38">[39]</ref>, with the exception of the feature extraction network. Whereas the original implementation uses a Spatial Transformer for initial alignment of the point clouds and then a simple PointNet for feature extraction, we remove the Spatial Transformer entirely and replace the PointNet with our Point-TnT model. During training, we sample local patches consisting of N = 256 points from the overlapping regions, and train by minimizing the hardest contrastive loss <ref type="bibr" target="#b40">[40]</ref>. We train our network for 10 epochs using the AdamW optimizer <ref type="bibr" target="#b35">[36]</ref> with a weight decay of 0.1 and a cosine learning rate schedule starting at 0.0001, without any data augmentation. Since each local patch processed by the network consists of only 256 points, we modify the parameters of our method to use M = 48 and k = 10 anchor points and neighbours respectively. For fair comparison to DIP, we use the same feature dimension <ref type="bibr" target="#b31">(32)</ref> at the final layer as the original implementation. During evaluation, we randomly sample 5000 points from each fragment and report the feature matching recall (FMR), i.e. the fraction of successful alignments with an inlier ratio of at least 5 %, where an inlier pair is defined by being less than 10 cm apart. In order to match corresponding patches, we use the global feature pairs (Z, Z ), where Z and Z are calculated using (10) for the two patches, and perform mutual nearest neighbour search in feature space. We refer to the original DIP authors' publication for a complete description of the experimental settings and evaluation protocol <ref type="bibr" target="#b38">[39]</ref>.</p><p>The results shown in <ref type="table" target="#tab_5">Table V</ref> suggest that Point-TnT finds more descriptive features than the original DIP implementation and reduces the number of unsuccessful matches by 38 %, without requiring any initial alignment, which shows that our method can be used for improving real-world scene reconstruction pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this work, we have explored the limitations and advantages of the Transformer architecture for 3D shape recognition. We have shown that naively applying self-attention to all points in a point cloud is both computationally inefficient and not very useful for learning descriptive feature representations. However, when applied within and between local patches of points, representation ability is drastically improved. This result is in agreement with recent works in image classification <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, where the Transformer architecture has shown to work better on local image patches rather than individual pixels. It also makes feature extraction more computationally tractable, which creates new opportunities for applying attention mechanisms to edge use cases that rely on unstructured 3D data, such as simultaneous localization and mapping (SLAM), where computational resources are limited. In future work, we will consider integrating and extending our method for mobile SLAM pipelines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Classification accuracy and GFLOPs on the ScanObjectNN dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>An example of the sparse subgraph H, using M = 5 anchor points (shown in orange) and k = 2 nearest neighbours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I CLASSIFICATION</head><label>I</label><figDesc>RESULTS ON SCANOBJECTNN<ref type="bibr" target="#b13">[14]</ref>. RESULTS OBTAINED USING PROTOCOL 2 ARE MARKED BY * .</figDesc><table><row><cell>Method</cell><cell>input</cell><cell>overall acc.</cell><cell>mean acc.</cell></row><row><cell>PointNet [22]</cell><cell>1024p</cell><cell>68.2</cell><cell>63.4</cell></row><row><cell>SpiderCNN [31]</cell><cell>1024p</cell><cell>73.7</cell><cell>69.8</cell></row><row><cell>PointNet++ [22]</cell><cell>1024p</cell><cell>77.9</cell><cell>75.4</cell></row><row><cell>DGCNN [25]</cell><cell>1024p</cell><cell>78.1</cell><cell>73.6</cell></row><row><cell>PointCNN [32]</cell><cell>1024p</cell><cell>78.5</cell><cell>75.1</cell></row><row><cell>BGA-PointNet++ [14]</cell><cell>mask, 1024p</cell><cell>80.2</cell><cell>77.5</cell></row><row><cell>BGA-DGCNN [14]</cell><cell>mask, 1024p,</cell><cell>79.7</cell><cell>75.7</cell></row><row><cell cols="2">Cloud Transformer [33] mask, 2048p</cell><cell>85.5  *</cell><cell>83.1  *</cell></row><row><cell>SimpleView [34]</cell><cell>6 views</cell><cell>79.5</cell><cell>-</cell></row><row><cell>GBNet [35]</cell><cell>1024p</cell><cell>80.5  *</cell><cell>77.8  *</cell></row><row><cell>MVTN [19]</cell><cell>12 views</cell><cell>82.8  *</cell><cell>-</cell></row><row><cell>Baseline (ours)</cell><cell>1024p</cell><cell>74.4 ? 1.3 75.4 ? 0.4  *</cell><cell>69.6 ? 1.5 71.3 ? 1.1  *</cell></row><row><cell>Point-TnT (ours)</cell><cell>1024p</cell><cell>83.5 ? 0.1 84.6 ? 0.5  *</cell><cell>81.0 ? 1.3 82.6 ? 1.2  *</cell></row><row><cell>Point-TnT (ours)</cell><cell>2048p</cell><cell>84.2 ? 0.9 85.0 ? 0.9  *</cell><cell>81.8 ? 0.9 83.0 ? 0.8</cell></row></table><note>* where W l ? R 2d E ?d Y . Now, Y l contains feature descriptors for the local patches around each anchor point. The global branch then computes attention between the patches, which enables interaction on a global scale:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II MODEL</head><label>II</label><figDesc>SIZE, COMPUTATIONAL COMPLEXITY AND ACCURACY ON SCANOBJECTNN FOR THE BASELINE AND OUR PROPOSED METHOD WITH DIFFERENT NUMBER OF ATTENTION HEADS.</figDesc><table><row><cell>Method</cell><cell cols="2">#params GFLOPs</cell><cell>acc.</cell></row><row><cell>Baseline</cell><cell>3.8M</cell><cell>4.33</cell><cell>74.4 ? 1.3</cell></row><row><cell>Point-TnT-1</cell><cell>1.7M</cell><cell>0.80</cell><cell>82.6 ? 0.4</cell></row><row><cell>Point-TnT-2</cell><cell>2.6M</cell><cell>0.97</cell><cell>83.3 ? 0.1</cell></row><row><cell>Point-TnT-3</cell><cell>3.9M</cell><cell>1.19</cell><cell>83.5 ? 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III ACCURACY</head><label>III</label><figDesc>ON SCANOBJECTNN WITH AND WITHOUT DIFFERENT ATTENTION MECHANISMS FOR THE BASELINE AND OUR PROPOSED METHOD.</figDesc><table><row><cell>Method</cell><cell>local att. global att.</cell><cell>acc.</cell></row><row><cell>Baseline</cell><cell></cell><cell>67.1 ? 0.7 74.4 ? 1.3</cell></row><row><cell></cell><cell></cell><cell>79.8 ? 0.8</cell></row><row><cell>Point-TnT</cell><cell></cell><cell>80.4 ? 1.8 82.9 ? 0.1</cell></row><row><cell></cell><cell></cell><cell>83.5 ? 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV ACCURACY</head><label>IV</label><figDesc>ON SCANOBJECTNN FOR DIFFERENT NUMBER OF ANCHOR POINTS M AND NEAREST NEIGHBOURS k. ? 0.7 82.5 ? 0.8 83.5 ? 0.1 79.7 ? 1.6 83.5 ? 1.5 83.5 ? 0.1</figDesc><table><row><cell></cell><cell></cell><cell>anchors</cell><cell></cell><cell></cell><cell>neighbours</cell><cell></cell></row><row><cell>M/k</cell><cell>12</cell><cell>48</cell><cell>192</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>acc.</cell><cell>74.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V FEATURE</head><label>V</label><figDesc>MATCHING RECALL AND STANDARD DEVIATION ON 3DMATCH.</figDesc><table><row><cell>Method</cell><cell>FMR</cell><cell>std</cell></row><row><cell>DIP [39]</cell><cell cols="2">0.948 0.046</cell></row><row><cell cols="3">DIP + Point-TnT 0.968 0.031</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at https://github.com/axeber01/point-tnt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP), funded by the Knut and Alice Wallenberg Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2021</title>
		<imprint>
			<biblScope unit="page" from="10" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Asselmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3163" to="3172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conformer: Convolution-augmented Transformer for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2020</title>
		<meeting>Interspeech 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Developing realtime streaming transformer transducer for speech recognition on largescale dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5904" to="5908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tera: Self-supervised learning of transformer encoder representation for speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2351" to="2366" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Keyword Transformer: A Self-Attention Model for Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4249" to="4253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pct: Point cloud transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="199" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1588" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mvtn: Multi-view transformation network for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">View-gcn: View-based graph convolutional network for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1850" to="1859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbhakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3394" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dapnet: A double self-attention convolutional network for segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08596</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (GELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The farthest point strategy for progressive image sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1305" to="1315" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="820" to="830" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cloud transformers: A universal approach to point cloud processing tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mazur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revisiting point cloud shape classification with a simple and effective baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geometric back-projection network for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regularization strategy for point cloud via rigidly mixed sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="900" to="915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3dmatch: Learning local geometric descriptors from rgb-d reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1802" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distinctive 3d local deep descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poiesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="5720" to="5727" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fully convolutional geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8958" to="8966" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
