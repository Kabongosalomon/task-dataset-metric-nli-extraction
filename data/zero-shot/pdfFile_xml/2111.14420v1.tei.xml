<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IB-MVS: An Iterative Algorithm for Deep Multi-View Stereo based on Binary Decisions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sormann</surname></persName>
							<email>christian.sormann@icg.tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Graphics and Vision</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Rossi</surname></persName>
							<email>mattia.rossi@sony.com</email>
							<affiliation key="aff1">
								<orgName type="department">Sony Europe B.V. R&amp;D Center -Stuttgart Laboratory 1</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuhn</surname></persName>
							<email>andreas.kuhn@sony.com</email>
							<affiliation key="aff1">
								<orgName type="department">Sony Europe B.V. R&amp;D Center -Stuttgart Laboratory 1</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Fraundorfer</surname></persName>
							<email>fraundorfer@icg.tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Graphics and Vision</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IB-MVS: An Iterative Algorithm for Deep Multi-View Stereo based on Binary Decisions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>SORMANN, ROSSI, KUHN, FRAUNDORFER: IB-MVS 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel deep-learning-based method for Multi-View Stereo. Our method estimates high resolution and highly precise depth maps iteratively, by traversing the continuous space of feasible depth values at each pixel in a binary decision fashion. The decision process leverages a deep-network architecture: this computes a pixelwise binary mask that establishes whether each pixel actual depth is in front or behind its current iteration individual depth hypothesis. Moreover, in order to handle occluded regions, at each iteration the results from different source images are fused using pixelwise weights estimated by a second network. Thanks to the adopted binary decision strategy, which permits an efficient exploration of the depth space, our method can handle high resolution images without trading resolution and precision. This sets it apart from most alternative learning-based Multi-View Stereo methods, where the explicit discretization of the depth space requires the processing of large cost volumes. We compare our method with state-of-the-art Multi-View Stereo methods on the DTU, Tanks and Temples and the challenging ETH3D benchmarks and show competitive results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The objective of a Multi-View Stereo (MVS) system is the estimation of a dense depth map for a reference image, given one or multiple source images and all the camera poses. This involves computing dense matching costs between the reference image and one or more source images. In recent years, learning-based methods have shown promising results using learned input representations in the form of feature maps and learned similarity measures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> for computing the matching costs. However, most learning-based methods discretize the depth space and compute the matching cost at each selected depth for each reference image pixel. The result is a cost volume whose size increases quadratically with respect to the image resolution for a given number of discretization steps. As a result, cost volume methods are subject to computational and memory bottlenecks. Newly proposed cascaded cost volume approaches <ref type="bibr" target="#b6">[7]</ref> mitigate these disadvantages, but they discretize the depth space using a pre-determined heuristic, which typically needs to be adapted for different datasets.</p><p>We propose a novel learning-based MVS method that explores the continuous depth space iteratively, without relying on an explicit cost-volume. At each iteration, our method computes a pixelwise binary decision mask that estimates whether a given pixel actual depth is in front or behind its current depth hypothesis. The binary decision permits to compute a new depth hypothesis at each pixel and the hypothesis is refined further at the next iteration. Our work is inspired by the two-view stereo method in <ref type="bibr" target="#b1">[2]</ref>. However, <ref type="bibr" target="#b1">[2]</ref> assumes the same depth hypothesis for each pixel, estimating the depth by means of binary masks computed for a predefined set of depth values, thus building a cost volume. Differently from <ref type="bibr" target="#b1">[2]</ref>, we do not construct a cost volume and rather propose a novel iterative architecture capable of estimating a binary decision mask for arbitrary depth hypotheses at individual pixels. Moreover, our method targets the MVS scenario and takes advantage of the multiple source images available. In particular, at each iteration, a binary decision mask is generated for each source image and the new resulting depth map hypotheses are fused with a learned weighting scheme inspired by <ref type="bibr" target="#b33">[34]</ref>. However, differently from <ref type="bibr" target="#b33">[34]</ref>, we do not fuse cost-volumes. Instead, we employ the weights to fuse 2D maps within an iterative depth estimation scheme.</p><p>We describe the proposed algorithm in detail in Section 3. As our core contributions, we 1.) design a network architecture to estimate pixelwise depth dependent binary decision masks in the MVS setting, 2.) introduce a pixelwise depth inference algorithm based on the prediction from the previous network, 3.) implement a learning-based fusion strategy, inspired by <ref type="bibr" target="#b33">[34]</ref>, for the depth maps predicted from different source images, 4.) verify our results on the popular benchmarks DTU <ref type="bibr" target="#b0">[1]</ref>, Tanks and Temples <ref type="bibr" target="#b12">[13]</ref> and ETH3D <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In this section, we discuss the previously published related work in the MVS field and compare it with our proposed method. Traditional MVS methods rely on hand-crafted similarity measures such as normalized cross-correlation <ref type="bibr" target="#b19">[20]</ref>. The depth hypotheses space is typically explored via a plane-sweeping cost volume <ref type="bibr" target="#b5">[6]</ref> or the PatchMatch algorithm <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>. These methods also introduce techniques for pixelwise source view selection, in order to suppress the influence of matching results from occluded source images <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref>. The main limitation of these methods is represented by their hand-crafted similarity measures. On the other hand, the PatchMatch exploration strategy for the depth hypothesis space and the employed pixelwise view-selection techniques elevate them above learning-based methods.</p><p>In recent years, deep-learning-based methods for MVS have received significant attention from the research community. Earlier works focus on learning a feature representation and combine this with a learned similarity measure in the form of a 3D convolutional neural network <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>. In order to reduce the computational cost of the regularization, recurrent <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref> and cascaded <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref> approaches were utilized. The addition of CRF-based cost volume regularization was also explored in several works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref>. Other methods rely on a voxel-based representation of the input <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> or refine an initial coarse estimate of a point cloud <ref type="bibr" target="#b2">[3]</ref>. The attention <ref type="bibr" target="#b23">[24]</ref> mechanism has been incorporated by several works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref> as well. More recent methods try to combine the benefits of traditional methods with the advantages of learned representation and similarity measures, e.g., by avoiding cost volumes and rather resorting to PatchMatch-based depth exploration strategies <ref type="bibr" target="#b24">[25]</ref> or by leveraging pixelwise source view selection <ref type="bibr" target="#b33">[34]</ref>.</p><p>Similarly, our proposed method does not employ a cost volume for depth estimation, but still incorporates beneficial concepts from learning-based methods. In fact, it benefits from learned input representations and similarity measures, like learning-based approaches, and it implements an efficient strategy for the exploration of the depth hypothesis space, similarly to traditional methods. Moreover, it employs a learning-based strategy to fuse estimates from different source images resembling the pixelwise view selection of traditional methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section we present our MVS method, named IB-MVS due its Iterative approach and its relying on Binary decisions. Below, first we provide a system overview of IB-MVS, then we elaborate on the details of its novel depth inference algorithm and its network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System overview</head><p>The goal of our system is to estimate a dense depth map d ? R M?N of a reference image I r ? R M?N given S source images I s ? R M?N with s = 0, 1, . . . , S ? 1. Hereafter (i, j) denotes a pixel location. Our MVS method is iterative and traverses the continuous space of feasible depth values at each pixel in a binary decision fashion. Inspired by <ref type="bibr" target="#b1">[2]</ref>, at each iteration t, and for each source image I s , our method predicts a binary mask with the property:</p><formula xml:id="formula_0">B t s (i, j) GT = 1 d GT (i, j) &lt; h t (i, j), 0 otherwise (1)</formula><p>where h t ? R M?N is the current depth map hypothesis for the reference image and d GT ? R M?N its ground truth depth map. The pixelwise mask B t s is predicted using a convolutional neural network named Decision Network (D-Net). In practice, we predict soft binary decision masks, hence B t s (i, j) takes values in [0, 1]. Each entry B t s (i, j) permits to establish whether d GT (i, j) is in front or behind the current depth hypothesis h t (i, j). Our method offsets the current depth hypothesis h t (i, j) and produces a new hypothesis h t+1 s (i, j) compliant with B t s (i, j). The S new depth map hypothesis h t+1 s , one for each source image, are then fused using learned pixelwise weights from a network named Weight Network (W-Net), in order to produce the next iteration depth map hypothesis h t+1 . We depict IB-MVS in <ref type="figure" target="#fig_1">Figure 1</ref> and provide a visual overview of the entire system in <ref type="figure" target="#fig_3">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Depth inference algorithm</head><p>The depth inference algorithm of IB-MVS assumes a feasible depth range [d min , d max ] ? R 2 as input and computes the reference image depth map d ? R M?N iteratively. The algorithm operates in the inverse depth domain, as this yields improved results in scenes with large depth ranges. To this end, we introduce the inverse depth map hypothesis H = 1/h ? R M?N and define the inverse depth range bounds D min = 1/d min and D max = 1/d max .</p><p>The algorithm starts at iteration t = 0 by setting H 0 (i, j) = D max +D min 2 at each pixel. It then uses D-Net with H 0 as inverse depth hypothesis in order to compute the first set of binary decision masks B 0 s (i, j), one for each source image. The next pixelwise depth map hypotheses at iteration t + 1 is then calculated as follows: where R = D max ?D min 2 and ?R t = R 2 t+1 is referred as the step size. The update in Eq. (2) is guided by the binary decision mask B t s , estimated from the current inverse depth hypothesis H t . For B t s (i, j) = 1, the sought ground truth depth is in front of the current hypothesis, therefore we step backwards. For B t s (i, j) = 0, we step forward instead. Since B t s (i, j) takes values in [0, 1], the step size performs a smooth update of the current depth hypothesis. The step size ?R t = R 2 t+1 decreases at each iteration, which represents the halving of the search space. However, it is noteworthy that the magnitude of the hypothesis update is adaptive (in both directions) thanks to its dependence on B t s (i, j), as modeled in Eq. (2). The update is sketched in <ref type="figure" target="#fig_1">Figure 1</ref>, where for the sake of simplicity one source image is assumed, hence omitting subscript s. After computing the new inverse depth map hypothesis H t+1 s (i, j) for each source image s, the fused inverse depth map hypothesis H t+1 ? R M?N is calculated as:</p><formula xml:id="formula_1">H t+1 s (i, j) = H t (i, j) ? R 2 t+1 (2B t s (i, j) ? 1), s = 0, 1, . . . , S ? 1 (2) D-Net H ! D min D max H 0 = D max + D min ! H 1 = H 0 ? R ! (2B 0 ? 1) B 0 &gt; 0.5 B 0 &lt; 0.5 H 2 = H 1 ? R " (2B 1 ? 1) B 1 &gt; 0.5 B 1 &gt; 0.5 B 1 &lt; 0.5 B 1 &lt; 0.5 B 0 B 1 D-Net</formula><formula xml:id="formula_2">H t+1 (i, j) = 1 W t (i, j) S?1 ? s=0 W t s (i, j)H t+1 s (i, j) (3) where W t (i, j) = ? S?1 s=0 W t s (i, j)</formula><p>is the sum of the weights estimated by W-Net for each source image. In <ref type="bibr" target="#b33">[34]</ref> this normalization was argued to be more beneficial than thresholding. We perform T iterations and set the final depth map estimate d = h T = 1/H T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Binary decision network</head><p>The decision network D-Net exhibits a U-Net <ref type="bibr" target="#b18">[19]</ref>-like encoder-decoder structure and it is depicted in <ref type="figure">Figure 3</ref>. At the top level, the decoder is fed both with the feature maps F r and F s ? R F?M?N , obtained by applying a Feature Pyramid Network <ref type="bibr" target="#b6">[7]</ref> to the reference and source images I r and I s , respectively, as well as with a depth map hypothesis h ? R M?N . At each level of the encoder, deformable convolutions <ref type="bibr" target="#b35">[36]</ref> are used to convolve the source image feature maps along locations on the epipolar line determined by the depth hypothesis. Specifically, we deform a k ? k kernel such that each of the k 2 sampling locations of the kernel correspond to locations on the epipolar line. We use kernel size k = 5 and can thus  center the deformed kernel on the epipolar line at the location predicted by the current depth hypothesis h t (i, j) and distribute the samples on either side spaced with a unit vector in pixel coordinates. The resulting sampled features from the source feature map are then concatenated with the reference feature map, as suggested in <ref type="bibr" target="#b1">[2]</ref>. This procedure is repeated at each resolution level, the resulting feature maps are further processed with standard convolutional layers and passed both to the next lower resolution level of the encoder and to the decoder, as depicted in <ref type="figure">Figure 3</ref>. At the decoder side, the feature maps are upsampled and further concatenated with the features from the encoder at the next higher resolution level. The output binary mask B t s is generated using a sigmoid activation function. We include a detailed specification of the convolutional hyper-parameters in Section A of the supplementary material. In practice, in order to predict the binary decision mask B t s , we employ three D-Nets on full, half and quarter resolution inputs. In particular, as depicted in <ref type="figure">Figure 3</ref>, the output features from the previous scale D-Net are employed on the next scale. We observed that employing three resolution levels leads to higher quality binary decision masks. In fact, this choice increases the overall architecture receptive field and permits a coarse to fine refinement of the estimated masks, as each level employs the previous level output.</p><formula xml:id="formula_3">I 0 Feature-Net (FPN [7]) M N F N M D-Net D-Net D-Net B ! " H ! "#$ I S-1 Feature-Net (FPN [7]) M N F N M H " D-Net D-Net D-Net W %&amp;$ " H %&amp;$ "#$ W ! " B %&amp;$ " H "#$ W-Net W-Net</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fusion weights network</head><p>In the proposed architecture, each D-Net is followed by a W-Net, a network whose objective is to assign a confidence, in the form of a weight map, to the predicted binary decision mask. <ref type="figure">Figure 3</ref> depicts W-Net in gray. Inspired by <ref type="bibr" target="#b33">[34]</ref>, W-Net operates on the pixelwise entropy of the predicted binary mask</p><formula xml:id="formula_4">E t s = ?(B t s log(B t s ) + (1 ? B t s ) log(1 ? B t s )</formula><p>). This design strategy, coupled with the choice to use the depth hypothesis to sample the source image, rather than as a network input, makes our overall algorithm scale independent. As suggested in <ref type="bibr" target="#b33">[34]</ref>, the final weight at the pixel (i, j) is predicted as W t </p><formula xml:id="formula_5">s (i, j) = exp(?w t s (i, j)) where w t s (i, j) is the network output.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, firstly we describe the training procedure adopted for IB-MVS, then we compare it to state-of-the-art works on popular MVS benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Network training</head><p>First, the multi-level D-Net is pre-trained on random uniform inverse depth map hypotheses with H(i, j) = D rand for every (i, j) and D rand ? [D min , D max ], as proposed in <ref type="bibr" target="#b1">[2]</ref>. The ground truth binary decision mask B GT ? R M?N for a given depth map hypothesis h = 1/H is computed from the ground truth depth map d GT using Equation <ref type="bibr" target="#b0">(1)</ref>. We employ a loss L k at each level k = 0, 1, 2 of the multi-level D-Net, with 2 being the full resolution. In particular, L k is defined as the average of the Binary Cross Entropy (BCE) at the valid pixels (i, j):</p><formula xml:id="formula_6">L k = 1 V ? i, j V (i, j) BCE(B(i, j), B GT (i, j))<label>(4)</label></formula><p>where V = ? i, j V (i, j) with V (i, j) equal to 1 if the pixel has a valid ground truth depth, 0 otherwise. The BCE loss in Eq. (4) is defined as follows:</p><formula xml:id="formula_7">BCE(b, b GT ) = ?(b GT log(b) + (1 ? b GT ) log(1 ? b)).<label>(5)</label></formula><p>The overall loss is the weighted sum L = ? 2 k=0 ? k L k with ? 0,1,2 = (0.25, 0.5, 1.0). After the pre-training, we train the multi-level D-Net using the depth inference algorithm described in Section 3.2 with T = 8 iterations. However, we do not consider the fusion step at this stage and work with a single randomly selected source image over the T iterations. The random selection of source images was shown to be beneficial by <ref type="bibr" target="#b24">[25]</ref>. We employ the previously introduced loss L on each one of the binary decision masks B t s , with t = 0, 1, ..., T ? 1, generated alongside the depth inference procedure. We refer to the loss at the iteration t as L t . At iteration t, the inverse depth hypothesis H t is used both by the multi-level D-Net to generate B t s and in Eq. (1) to generate its ground truth. Now the loss L t can be computed and the next hypothesis H t+1 generated. The procedure is iterated and the final loss is the sum over the T losses L t . We do not back-propagate across iterations. Finally, in the third training stage, we train W-Net jointly with D-Net. At this stage we fuse B t s from 4 randomly selected source images using the weights from W-Net with the same approach used to fuse the inverse hypotheses in Section 3.2 to generate B t . We compute the loss L t of one iteration, where t is randomly chosen in {0, . . . , T ? 1}, as the sum of losses on the S individual B t s and fused B t . We implemented IB-MVS in PyTorch <ref type="bibr" target="#b17">[18]</ref> and trained with batch size 1 using ADAM <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation metrics</head><p>In our evaluation, we present experimental results on three popular MVS benchmarks, namely DTU <ref type="bibr" target="#b0">[1]</ref>, Tanks and Temples <ref type="bibr" target="#b12">[13]</ref> and ETH3D high and low-res <ref type="bibr" target="#b20">[21]</ref>. These benchmarks compare the reconstructed point cloud against a dense ground truth and extract completeness and accuracy metrics (recall and precision, respectively, for Tanks and Temples <ref type="bibr" target="#b12">[13]</ref>). Completeness and accuracy are aggregated into a single metric: their average for DTU <ref type="bibr" target="#b0">[1]</ref> and harmonic mean, denoted F-score, for Tanks and Temples <ref type="bibr" target="#b12">[13]</ref> and ETH3D <ref type="bibr" target="#b20">[21]</ref>. For DTU <ref type="bibr" target="#b0">[1]</ref>, accuracy and completeness are measured in mm, hence lower is better. For ETH3D <ref type="bibr" target="#b20">[21]</ref> and Tanks and Temples <ref type="bibr" target="#b12">[13]</ref>, these metrics are percentages, hence higher is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study</head><p>We first investigate the influence of the iterations T on the point cloud results for DTU <ref type="bibr" target="#b0">[1]</ref>. In <ref type="table">Table 1</ref>, we show that increasing the number of iterations leads to improved results, which is coherent with our iterative depth refinement. The larger performance difference between 6 ? 7 iterations, compared to 7 ? 8 and 8 ? 9, is explained by the progressively shrinking step size of IB-MVS. Furthermore, we investigate the benefits of using our W-Net in the fusion step. In particular, we compare it to a naive strategy that simply averages the depth map hypotheses from the different source views. The results in <ref type="table">Table 1</ref> show that, for the same number of iterations T = 8, W-Net leads to a better completeness and an overall better quality (avg. metric) than the naive fusion strategy, while exhibiting a competitive accuracy. The improved accuracy of the naive fusion strategy is obtained at the cost of worse completeness, as the absence of W-Net leads to more inconsistent estimates in occluded regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Benchmark results</head><p>We use Tanks and Temples <ref type="bibr" target="#b12">[13]</ref> and ETH3D <ref type="bibr" target="#b20">[21]</ref> high and low-res in order to evaluate the generalization capabilities of IB-MVS, since we do not train on their respective training sets.</p><p>The feasible depth range [d min , d max ] is inferred from the SfM model using the method in <ref type="bibr" target="#b30">[31]</ref>. The computed depth maps are fused into a single point cloud with the proposed method of <ref type="bibr" target="#b30">[31]</ref>; we denote its parameters representing the number of consistent views and the geometric re-projection error threshold as S g and g, respectively. We denote the used image resolution as M ? N, the runtime per image as RT and the memory consumption as MEM. Finally, we recall that S is number of used source views.</p><p>For the evaluation on the DTU benchmark, we train the network on DTU for (8, 12, 2) epochs and learning rates (10 ?4 , 10 ?4 , 10 ?5 ) in the stages (1 2, 3) described in Section 4.1, respectively. For the evaluations on Tanks and Temples and ETH3D, we train on DTU at  <ref type="bibr" target="#b30">[31]</ref>. Finally, our method is run for T = 8 iterations on DTU and for T = 9 iterations on Tanks and Temples <ref type="bibr" target="#b12">[13]</ref> and ETH3D <ref type="bibr" target="#b20">[21]</ref>. All the experiments were performed using an Nvidia RTX 2080Ti graphics card.</p><p>ETH3D benchmark <ref type="bibr" target="#b20">[21]</ref> This dataset is the most challenging one for learning-based MVS methods, especially the high-res subset. On the one hand, the high resolution of its images represents a memory bottleneck for learning-based cost-volume methods. On the other, it is characterized by images with wide baselines and with a significantly lower overlap than in Tanks and Temples <ref type="bibr" target="#b12">[13]</ref> and DTU <ref type="bibr" target="#b0">[1]</ref>, which can make matching without pixelwise source view selection difficult. For the high-res dataset we set M ? N = 1984 ? 1312, S = 4, S g = 1, g = 1.0 and obtain RT = 7.7s, MEM = 7.9GB. For the low-res dataset we set M ? N = 928 ? 512, S = 4, S g = 3, g = 0.1 and obtain RT = 1.5s, MEM = 2.3GB. We provide quantitative results for both the datasets in <ref type="table">Table 2</ref>. Although this benchmark had been dominated by traditional methods such as ACMM <ref type="bibr" target="#b25">[26]</ref> and DeepC-MVS <ref type="bibr" target="#b14">[15]</ref> in the  <ref type="table">Table 2</ref>: ETH3D <ref type="bibr" target="#b20">[21]</ref> results: F-score, accuracy and completeness as percentage (2cm). Higher is better, overall best underlined, best among learning based in bold. For the test sets, we report the average per-scene runtime RT in seconds (for methods that provide this). past, the recently published PatchMatchNet <ref type="bibr" target="#b24">[25]</ref> was able to achieve competitive results: IB-MVS outperforms <ref type="bibr" target="#b24">[25]</ref> on both the training and the test datasets. Learning-based approaches relying on cost-volumes are limited on this benchmark, however our iterative approach allows IB-MVS to infer accurate results, even on high resolution images. Further, since large viewpoint changes are present in this benchmark, the implemented fusion scheme allows IB-MVS to deal with occluded regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DTU benchmark [1]</head><p>This dataset contains close-range images of various objects. For this dataset, we set M ? N = 1152 ? 864, S = 4, S g = 3, g = 0.25 and obtain RT = 2.7s, MEM = 3.8GB. In <ref type="table">Table 3</ref> we compare IB-MVS to the recent learning-based state-ofthe-art methods. It can be observed that IB-MVS provides very competitive accuracy and completeness values: in particular it achieves a good trade-off between the two, which results in the best average score. Finally, we achieve a complete reconstruction, even with a strict filtering parameter g. This is due to IB-MVS ability to yield very precise results without the need to use large cost-volumes. This is especially important in the case of the DTU benchmark, where we target highly precise reconstructions of single objects.</p><p>Tanks and Temples benchmark <ref type="bibr" target="#b12">[13]</ref> The intermediate subset focuses on reconstructing small and large single objects, while the advanced subset consists of large scale indoor and outdoor scenes. In our experiments, we set M ? N = 1920 ? 1056, S = 4, S g = 4, g = 0.5 (interm.), S g = 3, g = 0.5 (adv.) and obtain RT = 6s, MEM = 6.4GB. On the advanced subset, IB-MVS is competitive with other state-of-the-art learning-based methods such as Cas-MVSNet <ref type="bibr" target="#b6">[7]</ref> and PatchMatchNet <ref type="bibr" target="#b24">[25]</ref>, as shown in <ref type="table">Table 3</ref>. Our iterative exploration of the hypothesis space, along with our pixelwise source view fusion, allows IB-MVS to achieve competitive results among learning based methods on the challenging advanced subset. A possible direction for improving the intermediate set results could be to employ a confidence measure for the final predicted depth map, to filter out inaccurate points ahead of the point cloud fusion, similarly to <ref type="bibr" target="#b14">[15]</ref>. Another direction could consider the improvement of the core architecture, for instance, by incorporating a regularization stage. In particular, the binary decision mask prediction could be regularized by a differentiable CRF, such as <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DTU [1]</head><p>Tanks  <ref type="table">Table 3</ref>: DTU <ref type="bibr" target="#b0">[1]</ref> results: accuracy, completeness and their average are in mm, lower is better. Tanks and Temples <ref type="bibr" target="#b12">[13]</ref> results: precision, recall and F-score are percentages, higher is better. Overall best results are underlined, best among learning based methods are bold.</p><p>Finally, we discuss IB-MVS runtime performance on ETH3D, measured as the per-scene runtime of the complete reconstruction, in seconds, including the point cloud fusion step. IB-MVS is faster than traditional methods on both ETH3D high-res and low-res, as shown in <ref type="table">Table 2</ref>. It is noteworthy that the low-res scenes contain more images. While the learningbased method PM-Net <ref type="bibr" target="#b24">[25]</ref> yields a better runtime than IB-MVS, a direct comparison is difficult. In fact, the PatchMatch algorithm utilized within PM-Net <ref type="bibr" target="#b24">[25]</ref> operates at halfresolution and the full resolution is obtained via a subsequent up-sampling and refinement. Instead, IB-MVS operates at full resolution and does not perform an additional refinement. Furthermore, IB-MVS achieves a better F-score than PM-Net <ref type="bibr" target="#b24">[25]</ref> on ETH3D high-res. On low-res, IB-MVS offers a competitive runtime compared to learning-based methods. In terms of F-score, IB-MVS outperforms the fastest learning-based method P-MVSNet <ref type="bibr" target="#b15">[16]</ref>, which does not participate on high-res.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented IB-MVS, a novel learning-based approach for MVS that explores the depth space iteratively in a binary decision fashion. IB-MVS couples the advantages of learningbased methods, such as learned input representations, with an efficient exploration strategy of the hypothesis space. In fact, IB-MVS can handle high resolution images, as it does not require a cost volume. In addition, IB-MVS benefits from a pixelwise source view fusion strategy. Extensive results show that IB-MVS achieves competitive results compared to state of the art methods on popular MVS benchmarks. Acknowledgement: This work has been supported by the FFG, Contract No. 881844: "Pro 2 Future".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material A Network architecture hyper-parameters</head><p>We provide the network hyper-parameters of D-Net and W-Net in <ref type="table">Table 4</ref> and <ref type="table">Table 5</ref>, respectively. As specified in the main paper, we use three resolution levels l = (0, 1, 2) at quarter, half and full resolution. For generating the image features Feat r l and Feat s l , we utilize the FPN architecture of <ref type="bibr" target="#b6">[7]</ref> and set the number of feature channels for each level F l = <ref type="bibr" target="#b31">(32,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8)</ref>. Further, we replace the batch normalization <ref type="bibr" target="#b8">[9]</ref> in the FPN <ref type="bibr" target="#b6">[7]</ref> with instance normalization <ref type="bibr" target="#b22">[23]</ref>. We denote the entropy calculated from the output mask B s l according to Section 3.4 of the main paper as E s l . In <ref type="table">Tables 4 and 5</ref>, we denote 2D convolutions as 2D conv, deformable 2D convolutions <ref type="bibr" target="#b35">[36]</ref> as 2D def. conv and transposed 2D convolutions as 2D tran. conv. Further, we denote the leaky ReLU activation function as LReLU, the number of input and output channels with #C in and #C out , the stride with str. and kernel size with k (we add \b when no bias is used, the padding is set to k?1 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Supplementary qualitative results</head><p>In <ref type="figure">Figure 6</ref>, we provide qualitative point cloud results of our method IB-MVS for the DTU <ref type="bibr" target="#b0">[1]</ref>, Tanks and Temples <ref type="bibr" target="#b12">[13]</ref> and ETH3D <ref type="bibr" target="#b20">[21]</ref> high and low-res datasets. Additionally, in <ref type="figure" target="#fig_5">Figure 8</ref>, we provide supplementary qualitative IB-MVS depth map results from the DTU <ref type="bibr" target="#b0">[1]</ref> dataset.</p><p>In order to provide further insights into IB-MVS, in <ref type="figure" target="#fig_4">Figure 7</ref> we provide a visualization of its intermediate outputs at different iterations t = 0, 1, 4, 8. For each source image I s , at iteration t the hypothesis h t is used to compute the binary decision mask B t s and the weight mask W t s via D-Net and W-Net, respectively; this permits to compute the new reference depth map hypothesis h t+1 s using Eq. (2) of the main paper. The rows 2-5 of <ref type="figure" target="#fig_4">Figure 7</ref> show B t s , W t s and h t+1 s for the 4 source images and different values of t. The new reference image depth hypothesis h t+1 s are then fused into a single depth map h t+1 using the weights W t s . The first row of <ref type="figure" target="#fig_4">Figure 7</ref> shows the reference image along with h t+1 for different values of t. We conclude by observing that, as desired, the weight masks in <ref type="figure" target="#fig_4">Figure 7</ref> assign a low confidence to those areas of the reference image that are occluded in the source image, as these areas cannot be matched. This can be appreciated in the weight masks W t s depicted in rows 2 and 3, where the area below the sofa and the left-most region (highlighted in yellow) of the reference image are dark because they are occluded in the respective source images. <ref type="figure">Figure 6</ref>: Qualitative point cloud results for DTU <ref type="bibr" target="#b0">[1]</ref> (first row), Tanks and Temples <ref type="bibr" target="#b12">[13]</ref> (second row) and ETH3D <ref type="bibr" target="#b20">[21]</ref> high and low-res (third and fourth row). Fo l 6F l 4F l 2D conv., k=3, str.=1, act.=LReLU Fo l B s l 4F l 1 2D conv., k=3\b, str.=1, act.=sigmoid <ref type="table">Table 4</ref>: D-Net architecture hyper-parameters, specifying the convolution type, number of input and output channels, kernel size, stride and activation function. from different source images according to W t s . We also show the binary decision masks B t s . We color code low to high depth values from blue to red. For B t s and W t s black represents the value 0 and white represents the value 1.</p><p>in name out name #C in #C out operation E s l Conv1 1 2F l for l = 0 2D conv., k=3, str.=1, act.=LReLU E s l Conv0 1 F l for l &gt; 0 2D conv., k=3, str.=1, act.=LReLU Fo l?1 Fo Up F l?1 2 F l?1 2 for l &gt; 0 bilinear interp. upscale to double res. Fo Up ConvPr F l?1 2 F l for l &gt; 0 2D conv., k=3, str.=1, act.=LReLU Conv0 | ConvPr Conc1 2F l 2F l for l &gt; 0 concatenate along channel dim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conc1</head><p>Conv1 2F l 2F l for l &gt; 0 2D conv., k=3, str.=1, act.=LReLU Conv1</p><p>Conv2 2F l 2F l 2D conv., k=3, str.=1, act.=LReLU Conv2</p><p>Conv3 2F l F l 2D conv., k=3, str.=1, act.=LReLU Conv3</p><p>Fo l 2F l F l 2 2D conv., k=3, str.=1, act.=LReLU Fo l w s l F l 2 1 2D conv., k=3\b, str.=1, act.=identity <ref type="table">Table 5</ref>: W-Net architecture hyper-parameters, specifying the convolution type, number of input and output channels, kernel size, stride and activation function. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of IB-MVS depth inference algorithm for a single source image. The inverse depth range [D min , D max ] is explored iteratively by updating the inverse depth map hypothesis H t by means of the binary decision mask B t with t = 0, 1, . . . , T ? 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of the overall system. Deep features extracted from the reference and source images using a FPN architecture<ref type="bibr" target="#b6">[7]</ref> are used as the input for three hierarchy levels of D-Nets, which predict the decision masks B t s . They are then used to compute the new inverse depth hypothesis H t+1 s for s. These are fused using weights W t s estimated by W-Net and the fused result H t+1 is the new inverse depth hypothesis for the next iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>We visualize intermediate results of IB-MVS for a view of the ETH3D [21] highres living room scene. The top row shows the depth hypothesis h t+1 predicted at iteration t by fusing the depth hypothesis h t+1 s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative depth map results for DTU<ref type="bibr" target="#b0">[1]</ref>. For each column, the reference image is at the top and the corresponding IB-MVS depth map at the bottom. Low to high depth values are color coded from blue to red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The pixelwise weights are used by our depth inference algorithm during the fusion stage in Eq. (3). The ideally predicted weights are small in those areas where the binary decision mask is not reliable, such as in occluded regions, and large otherwise, Net architecture overview showing feature channel slices. The network uses standard 2D convolutions in combination with deformable 2D convolutions<ref type="bibr" target="#b35">[36]</ref> which utilize depth hypothesis h t to sample the epipolar line of the source image.such that inverse depth hypotheses from different source images can complement each other. This approach mitigates the negative effect of potentially erroneous estimates H t+1 s from the source views, when fusing them into the new depth hypothesis H t+1 . This is crucial, as H t+1 represents the starting point for the estimation of the next iteration binary decision masks.</figDesc><table><row><cell>reference feature source feature</cell><cell>ref feat.</cell><cell>half</cell><cell></cell><cell cols="3">previous scale D-Net output features</cell><cell>current D-Net output features standard convolution + leaky RELU upscaled previous scale W-Net output features output decision mask ? current W-Net output output weights features</cell></row><row><cell>h t</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>deformable convolution on epipolar line</cell></row><row><cell></cell><cell>src feat.</cell><cell>half</cell><cell>h t</cell><cell>ref feat.</cell><cell>quar.</cell><cell>(depending on h t ) + leaky RELU concatenate ( up/downscale if necessary) standard convolution + no activation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>src feat.</cell><cell>quar.</cell><cell>h t</cell><cell>?</cell><cell>sigmoid activation entropy calculation exp(-w t ) s</cell></row><row><cell cols="6">Figure 3: D-Net and W-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, 10 ?4 , 10 ?5 ). Every training on DTU employs the ground truth depth maps and train-test split of</figDesc><table><row><cell></cell><cell cols="2">W-Net T avg. acc. cmp. RT</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>6 0.717 0.769 0.664 2.0s</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>7 0.371 0.402 0.340 2.3s</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>8 0.321 0.334 0.309 2.7s</cell><cell></cell><cell></cell></row><row><cell></cell><cell>-</cell><cell>8 0.342 0.326 0.359 2.3s</cell><cell></cell><cell></cell></row><row><cell></cell><cell>-</cell><cell>9 0.343 0.324 0.362 2.6s</cell><cell></cell><cell></cell></row><row><cell cols="3">Table 1: Ablation study on DTU test-set</cell><cell></cell><cell></cell></row><row><cell cols="3">using accuracy, completeness and their</cell><cell></cell><cell></cell></row><row><cell cols="3">average in mm (lower is better). We ob-</cell><cell></cell><cell></cell></row><row><cell cols="3">serve improvements with increasing it-</cell><cell></cell><cell></cell></row><row><cell cols="3">erations T. After T=8 the subdivision of</cell><cell></cell><cell></cell></row><row><cell cols="3">the search space is sufficient, thus the</cell><cell></cell><cell></cell></row><row><cell cols="3">result for T=9 is very close. Further, the</cell><cell cols="3">Figure 4: Qualitative depth map results from Tanks</cell></row><row><cell cols="3">inclusion of W-Net improves the results</cell><cell cols="3">and Temples [13] and ETH3D [21].</cell></row><row><cell cols="3">for the same number of iterations.</cell><cell></cell><cell></cell></row><row><cell>PatchMatchNet [25]</cell><cell>F=68.08</cell><cell>A=58.24</cell><cell></cell><cell>C=81.91</cell></row><row><cell></cell><cell>F=87.33</cell><cell>A=82.36</cell><cell></cell><cell>C=92.93</cell></row><row><cell>IB-MVS (ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>accurate</cell><cell>inaccurate</cell><cell>unobserved</cell><cell>complete</cell><cell>incomplete</cell></row><row><cell cols="6">Figure 5: Point cloud comparison with PatchMatchNet [25] on the ETH3D [21] high-res</cell></row><row><cell cols="6">terrains dataset. We show the output point cloud, accuracy and completeness.</cell></row><row><cell cols="6">stage 1 and on Blended MVS [33] at stages 2 and 3, for (8, 21, 4) epochs using learning</cell></row><row><cell cols="2">rates (10 ?4</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>MVS [15] 62.37 65.89 59.42 5746 61.99 65.98 59.27 87.08 89.15 85.52 3155 84.81 90.37 80.30 COLMAP [20] 52.32 61.51 45.89 3312 49.91 69.58 40.86 73.01 91.97 62.98 1658 67.66 91.85 55.13 ACMM [26] 55.01 52.37 58.27 1662 55.12 54.69 57.01 80.78 90.65 74.34 1165 78.86 90.67 70.42 PCF-MVS [14] 57.06 56.56 58.42 9289 57.32 57.03 58.17 80.38 82.15 79.29 2272 79.42 84.11 75.73 R-MVSNet [32] 36.87 37.45 37.MVS (ours) 49.19 39.31 67.29 1487 55.84 61.06 52.66 75.85 71.64 82.18 616 71.21 75.21 69.02</figDesc><table><row><cell></cell><cell></cell><cell cols="2">low-res-test</cell><cell></cell><cell cols="3">low-res-train</cell><cell></cell><cell cols="2">high-res-test</cell><cell></cell><cell>high-res-train</cell></row><row><cell>publication</cell><cell>F</cell><cell cols="3">acc. cmp. RT</cell><cell>F</cell><cell cols="3">acc. cmp. F</cell><cell cols="3">acc. cmp. RT</cell><cell>F</cell><cell>acc. cmp.</cell></row><row><cell cols="6">DeepC-16 2413 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="9">CasMVSNet [7] 44.49 55.44 38.80 -49.00 62.06 41.86 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MVSCRF [28]</cell><cell cols="4">28.32 34.84 24.97 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">P-MVSNet [16] 44.46 54.95 38.28 627</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="9">BP-MVSNet [22] 43.22 32.65 64.34 -50.87 49.12 55.29 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Att-MVS [17]</cell><cell cols="4">45.85 64.84 37.07 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PVSNet [27]</cell><cell cols="5">45.78 38.39 57.76 2116 -</cell><cell>-</cell><cell cols="6">-72.08 66.41 80.05 830 67.48 -</cell><cell>-</cell></row><row><cell>PMNet [25]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">-73.12 69.71 77.46 493 64.21 64.81 65.43</cell></row><row><cell>IB-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>4F l + 4F l?1 for l &gt; 0 concatenate along channel dim.</figDesc><table><row><cell>in name</cell><cell>out name</cell><cell>#C in</cell><cell>#C out</cell><cell>operation</cell></row><row><cell>Feat r l</cell><cell>Conv1</cell><cell>F l</cell><cell>F l</cell><cell>2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell>Feat s l</cell><cell>DConv1</cell><cell>F l</cell><cell>F l</cell><cell>2D def. conv., k=5, str.=1, act.=LReLU</cell></row><row><cell cols="2">Conv1 | DConv1 Conc1</cell><cell>2F l</cell><cell>2F l</cell><cell>concatenate along channel dim.</cell></row><row><cell>Conc1</cell><cell>Conv2</cell><cell>2F l</cell><cell>2F l</cell><cell>2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell>Conv2</cell><cell>Sc1</cell><cell>2F l</cell><cell>2F l</cell><cell>2D conv., k=3, str.=2, act.=LReLU</cell></row><row><cell>Feat r l</cell><cell>Feat r l half</cell><cell>F l</cell><cell>F l</cell><cell>bilinear interp. downscale to half</cell></row><row><cell>Feat sl</cell><cell>Feat s l half</cell><cell>F l</cell><cell>F l</cell><cell>bilinear interp. downscale to half</cell></row><row><cell>Feat r l half</cell><cell>Conv3</cell><cell>F l</cell><cell>F l</cell><cell>2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell>Feat s l half</cell><cell>DConv2</cell><cell>F l</cell><cell>F l</cell><cell>2D def. conv., k=5, str.=1, act.=LReLU</cell></row><row><cell cols="2">Conv3 | DConv2 Conc2</cell><cell>2F l</cell><cell>2F l</cell><cell>concatenate along channel dim.</cell></row><row><cell>Conc2</cell><cell>Conv4</cell><cell>2F l</cell><cell>2F l</cell><cell>2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell>Sc1 | Conv4</cell><cell>Conc3</cell><cell>4F l</cell><cell>4F l</cell><cell>for l = 0 concatenate along channel dim.</cell></row><row><cell>Conc3</cell><cell>Conv5</cell><cell>4F l</cell><cell>4F l</cell><cell>for l = 0 2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell cols="3">Fo l?1 | Sc1 | Conv4 Conc3 4F Conc3 ConvPr 4F l + 4F l?1</cell><cell>4F l</cell><cell>for l &gt; 0 2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell>ConvPr</cell><cell>Conv5</cell><cell>4F l</cell><cell>4F l</cell><cell>for l &gt; 0 2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell>Conv5</cell><cell>Sc2</cell><cell>4F l</cell><cell>4F l</cell><cell>2D conv., k=3, str.=2, act.=LReLU</cell></row><row><cell>Feat r l</cell><cell>Feat r l quar.</cell><cell>F l</cell><cell>F l</cell><cell>bilinear interp. downscale to quarter</cell></row><row><cell>Feat sl</cell><cell>Feat s l quar.</cell><cell>F l</cell><cell>F l</cell><cell>bilinear interp. downscale to quarter</cell></row><row><cell>Feat r l quar.</cell><cell>Conv6</cell><cell>F l</cell><cell>F l</cell><cell>2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell>Feat s l quar.</cell><cell>DConv3</cell><cell>F l</cell><cell>F l</cell><cell>2D def. conv., k=5, str.=1, act.=LReLU</cell></row><row><cell cols="2">Conv6 | DConv3 Conc4</cell><cell>2F l</cell><cell>2F l</cell><cell>concatenate along channel dim.</cell></row><row><cell>Conc4</cell><cell>Conv7</cell><cell>2F l</cell><cell>2F l</cell><cell>2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell>Sc2 | Conv7</cell><cell>Conc5</cell><cell>6F l</cell><cell>6F l</cell><cell>concatenate along channel dim.</cell></row><row><cell>Conc5</cell><cell>Conv8</cell><cell>6F l</cell><cell>6F l</cell><cell>2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell>Conv8</cell><cell>Conv9</cell><cell>6F l</cell><cell>6F l</cell><cell>2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell>Conv9</cell><cell>Conv10</cell><cell>6F l</cell><cell>6F l</cell><cell>2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell>Conv10</cell><cell>UConv1</cell><cell>6F l</cell><cell>6F l</cell><cell>2D tran. conv.,k=4\b, str.=2, act.=LReLU</cell></row><row><cell cols="2">Conv5 | UConv1 Conc6</cell><cell>10F l</cell><cell>10F l</cell><cell>concatenate along channel dim.</cell></row><row><cell>Conc6</cell><cell>Conv11</cell><cell>10F l</cell><cell>4F l</cell><cell>2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell>Conv11</cell><cell>Conv12</cell><cell>4F l</cell><cell>4F l</cell><cell>2D conv., k=3, str.=1, act.=LReLU</cell></row><row><cell>Conv12</cell><cell>UConv2</cell><cell>4F l</cell><cell>4F l</cell><cell>2D tran. conv.,k=4\b, str.=2, act.=LReLU</cell></row><row><cell cols="2">Conv2 | UConv2 Conc7</cell><cell>6F l</cell><cell>6F l</cell><cell>concatenate along channel dim.</cell></row><row><cell>Conc7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>l + 4F l?1</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2021. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. International Journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Rasmus Ramsb?l Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vogiatzis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bi3d: Stereo depth estimation via binary classifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Badki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Troccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Point-based multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep stereo using adaptive thin volume representation with uncertainty awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Massively parallel multiview stereopsis by surface normal diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time plane-sweeping stereo with multiple sweeping directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippos</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deepmvs: Learning multi-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Han</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Surfacenet: An end-to-end 3d neural network for multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Surfacenet+: An end-to-end 3d neural network for very sparse multi-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tanks and temples: Benchmarking large-scale scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Plane completion and filtering for multiview stereo reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Erdler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the German Conference on Pattern Recognition (GCPR)</title>
		<meeting>the German Conference on Pattern Recognition (GCPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepc-mvs: Deep confidence prediction for multi-view stereo reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sormann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Erdler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Fraundorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attentionaware multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuesong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lutz Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A multi-view stereo benchmark with high-resolution images and multi-camera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sch?ps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bp-mvsnet: Belief-propagation-layers for multi-viewstereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sormann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Knobelreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Fraundorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>arxiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learned multi-view patchmatch stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangjinhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Speciale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patchmatchnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-scale geometric consistency guided multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pvsnet: Pixelwise visibility-aware multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07714</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mvscrf: Learning multi-view stereo with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youze</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiansheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weitao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dense hybrid recurrent multi-view stereo net with dynamic consistency checking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhuang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cost volume pyramid based depth inference for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent mvsnet for high-resolution multi-view stereo depth inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Blendedmvs: A large-scale dataset for generalized multi-view stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visibility-aware multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Longrange attention network for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
