<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">P-KDGAN: Progressive Knowledge Distillation with GANs for One-class Novelty Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Electronics</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Chen</surname></persName>
							<email>shifeng.chen@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
							<email>sunlei@bit.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Electronics</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">P-KDGAN: Progressive Knowledge Distillation with GANs for One-class Novelty Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One-class novelty detection is to identify anomalous instances that do not conform to the expected normal instances. In this paper, the Generative Adversarial Networks (GANs) based on encoderdecoder-encoder pipeline are used for detection and achieve state-of-the-art performance. However, deep neural networks are too over-parameterized to deploy on resource-limited devices. Therefore, Progressive Knowledge Distillation with GANs (P-KDGAN) is proposed to learn compact and fast novelty detection networks. The P-KDGAN is a novel attempt to connect two standard GANs by the designed distillation loss for transferring knowledge from the teacher to the student. The progressive learning of knowledge distillation is a twostep approach that continuously improves the performance of the student GAN and achieves better performance than single step methods. In the first step, the student GAN learns the basic knowledge totally from the teacher via guiding of the pretrained teacher GAN with fixed weights. In the second step, joint fine-training is adopted for the knowledgeable teacher and student GANs to further improve the performance and stability. The experimental results on CIFAR-10, MNIST, and FM-NIST show that our method improves the performance of the student GAN by 2.44%, 1.77%, and 1.73% when compressing the computation at ratios of 24.45:1, 311.11:1, and 700:1, respectively.</p><p>We briefly review the related works in term of one-class novelty detection and knowledge distillation, as well as the architecture of Ganomaly <ref type="bibr" target="#b0">[Ak?ay et al., 2018]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One-class novelty detection aims to identify patterns that do not belong to the normal data distribution <ref type="bibr" target="#b0">[Chandola et al., 2009]</ref>. Unlike traditional classification problem, novelty detection is usually trained in an unsupervised setting where novelty data is absent. Novelty detection has a wide variety of applications such as network intrusion <ref type="bibr">[Garc?a-Teodoro et al., 2009]</ref> 2008], medical diagnoses <ref type="bibr">[Schlegl et al., 2017]</ref> and many more. With the advantage of deep learning, novelty detection based on generative adversarial networks (GANs) has shown state-of-the-art performance by learning the representative latent space of high-dimensional data <ref type="bibr">[Schlegl et al., 2017;</ref><ref type="bibr" target="#b5">Zenati et al., 2018;</ref><ref type="bibr">Perera et al., 2019]</ref>. However, deep neural networks with high computational costs and large storage prohibit their deployment to computation and memory resource limited systems. For tackling the above issue, neural network compression has been widely applied in recent years <ref type="bibr" target="#b0">[Cheng et al., 2017]</ref>. As one of the mainstream compression methods, Knowledge Distillation (KD) following a teacher-student paradigm transfers knowledge from a teacher network with higher performance to a student network. The early contributions used the outputs of the softmax layers or intermediate layers in teacher networks to improve the performance of student networks <ref type="bibr" target="#b1">[Hinton et al., 2015;</ref><ref type="bibr">Romero et al., 2015]</ref>. In the later researches, the discriminator losses were proposed to evaluate the distinction between the distribution spaces of teacher and student networks <ref type="bibr" target="#b4">[Wang et al., 2018a;</ref><ref type="bibr" target="#b4">Wang et al., 2018b;</ref><ref type="bibr" target="#b1">Liu et al., 2018]</ref>. To our knowledge, there is no related works on two standard GANs <ref type="bibr" target="#b1">[Goodfellow et al., 2014]</ref> including two generators and two discriminators to design distillation loss for knowledge distillation. Additionally, there are rare works investigating the initialization of student networks and always random initialization is used. Our experiments demonstrate that student networks without "knowledge" reserve (with random initialization) do not mimic the outputs of teacher networks well.</p><p>In this paper, we apply GANs in the encoder-decoderencoder structure <ref type="bibr" target="#b0">[Ak?ay et al., 2018]</ref> for one-class novelty detection, which outperforms the state-of-the-art approaches. In order to deploy the deep neural networks in computation resources limited mobile devices, we propose the Progressive Knowledge Distillation with GANs (P-KDGAN) method to train the lightweight student network. The P-KDGAN approach improves the performance of student GAN by solving the following three problems. 1) How to design a distillation loss to measure the similarity of intermediate representations learned from the teacher GAN and the student GAN? As is shown in the student GAN of <ref type="figure" target="#fig_0">Figure 1(a)</ref>, the generator based on encoder-decoder-encoder pipeline can generate two latent vectors z 1 , z 2 and a reconstructed imagex. The generator is trained by minimizing the weighted sum of S con , S enc and S adv , which have defined in <ref type="bibr">Eq.2-4 [Ak?ay et al., 2018]</ref>. Therefore, the distillation loss K l described in Eq.6 is designed as the weighted sum of the losses K 1 , K x and K 2 , where K 1 , K 2 represent the difference between the two latent vectors (z 1 and? 1 , z 2 and? 2 ) in the teacher GAN and student GAN, and K x is the difference of two reconstructed images (x, x). 2) How to combine the distillation loss K l with existing generator losses L S g , L T g and discriminator losses L S d , L T d from the student and teacher GANs to improve the performance of the student GAN? As is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(b), we design four distillation structures (KDGAN-1 , KDGAN-2 , KDGAN-3 and KDGAN-4 ) based on different combinations of the above five losses. The difference between them consists of two aspects: on the one hand, whether the weights of the teacher GAN are fixed; on the other hand, whether the distillation loss K l is combined with the losses L S g , L S d of the student GAN for knowledge transfer. 3) Whether the designed four distillation structures can make the performance of the student GAN like that of the teacher GAN? If not, how to fix it? Our experimental results demonstrate that the performance of student GANs trained from scratch (or with random initialization) by the above four distillation structures is incomparable with the teacher GAN. Just as learning is a gradual cumulative process, a two-step progressive learning of KDGAN is proposed to continuously improve the performance of the student GAN. In the first step, the student GAN imitates the representation of the pre-trained teacher GAN with fixed weights to make itself have a certain "knowledge" reserve. Such a "teaching by teacher" step make the student learn the basic knowledge totally from the teacher. In the second step, the student GAN with basic "knowledge" reserve is fine-trained together with the teacher GAN. The second step of "fine-learning with teacher" can further improve the performance and stability by jointly utilizing the basic knowledge of the teacher and student.</p><p>The performance of proposed progressive knowledge distillation with GANs for one-class novelty detection is evaluated on <ref type="bibr">CIFAR-10 [Krizhevsky, 2009]</ref>, <ref type="bibr">MNIST [LeCun and Cortes, 2005]</ref> and FMNIST <ref type="bibr" target="#b5">[Xiao et al., 2017]</ref> datasets. Our contributions are summarized as follows.</p><p>? We utilize the encoder-decoder-encoder based GAN for one-class novelty detection, which outperforms all the state-of-the-art methods.</p><p>? We propose new distillation losses on latent vectors and reconstructed images of GANs that allow the student to better learn from the teacher.</p><p>? We regard the distillation process as a knowledgeable teacher to improve the performance and stability of student networks through two-step progressive learning, which includes basic knowledge learning and finelearning.</p><p>? Progressive Knowledge Distillation with GANs is proposed for one-class novelty detection. Our experiments demonstrate that the P-KDAGN can improve the performance of the student GAN on the three datasets CIFAR-10, MNIST and FMNIST by 2.44%, 1.77%, and 1.73%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">One-class Novelty Detection</head><p>In the unsupervised one-class novelty detection, only the normal samples with one class are used for training the model. Conventionally, novelty detection methods can be divided into two categories <ref type="bibr" target="#b0">[Chandola et al., 2009]</ref>. One is the traditional methods, such as One-Class SVM (OC-SVM) <ref type="bibr">[Sch?lkopf et al., 2001]</ref>, Kernel Density Estimation (KDE) <ref type="bibr">[Parzen, 1962]</ref> and Principal Component Analysis (PCA) <ref type="bibr" target="#b5">[Wold et al., 1987]</ref>. The disadvantage of such approaches is that they are not suitable for high  <ref type="figure" target="#fig_0">Figure 1</ref>(a) constructs a novel architecture for multi-class anomaly detection. In our method, the Ganomaly framework is used for one-class novelty detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Distillation</head><p>To reduce the large computation and storage cost of deep convolutional neural networks, knowledge distillation can transfer the generalization ability of a large network (or an ensemble of networks) to a light-weight network. Hinton et al. <ref type="bibr" target="#b1">[Hinton et al., 2015]</ref> used the outputs of the softmax layer of a teacher network as the target function to train the student network. <ref type="bibr">Romero et al. [Romero et al., 2015]</ref> proposed that a student network with random initialization can imitate the intermediate representations of the teacher network to improve its own performance. In order to ensure the student network to learn the true data distribution from the teacher network, knowledge distillation with a discriminator was used for distinguishing features extracted from the teacher and student networks <ref type="bibr" target="#b4">[Wang et al., 2018a;</ref><ref type="bibr" target="#b4">Wang et al., 2018b;</ref><ref type="bibr" target="#b1">Liu et al., 2018]</ref>. In our method, knowledge distillation is considered as a progressive learning process, which can continuously improve the performance of student networks.</p><p>GANs <ref type="bibr" target="#b1">[Goodfellow et al., 2014]</ref> have been applied to many real world applications such as domain transfer, image gen-eration, and novelty detection. However, to our knowledge, there is no related works that deploy the knowledge distillation on two standard GANs. Therefore, this paper designs a distillation loss to transfer knowledge from the teacher GAN to the student GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Architecture of Ganomaly</head><p>Akcay et al. <ref type="bibr" target="#b0">[Ak?ay et al., 2018]</ref> proposed Ganomaly for multi-class anomaly detection, in which multiple class of samples as normal data and one class of samples as abnormal data. In this paper, we utilize Ganomaly architecture for one-class novelty detection. One-class means that only the instances in one category are regarded as normal data, and the remaining categories are abnormal data.</p><p>GAN consists of two adversial modules, a generator G and a discriminator D. As is shown in the student GAN of <ref type="figure" target="#fig_0">Figure 1(a)</ref>, the Ganomaly [Ak?ay et al., 2018] framework is composed of two modules: 1) an encoder-decoder-encoder</p><formula xml:id="formula_0">(G E ? G D ? G R ) pipeline based generator G that learns the distribution of input image x, where x ? R w?h?c , from latent spaces z 1 , z 2 , where z 1 , z 2 ? R d ;</formula><p>2) a discriminator D that decides whether the reconstructed imagex is real or fake. D and G are simultaneously optimized by playing the following minmax game as:</p><formula xml:id="formula_1">min G max D V (D, G) =E x?X [logD(x)]+ +E x?X [log(1 ? G D (G E (x)))],<label>(1)</label></formula><p>where training dataset X comprises N normal images, X = [x 1 , x 2 , ..., x N ] ? R N ?w?h?c , and E x?X is the expected value of x obeying distribution of normal images X.</p><p>During training, the generator loss L S g and the crossentropy loss S ce are minimized to train the student G and student D, respectively. The model is trained from normal samples, therefore the reconstruction error is large on abnormal samples. In the previous methods <ref type="bibr">[Schlegl et al., 2017;</ref><ref type="bibr" target="#b5">Zenati et al., 2018;</ref><ref type="bibr">Perera et al., 2019]</ref>, the reconstruction errors of the images or latent vectors are used for anomaly detection. In Ganomaly <ref type="bibr" target="#b0">[Ak?ay et al., 2018]</ref>, the difference S enc between two latent vectors z 1 , z 2 is used as novelty score, which is defined in Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>In this work, we adopt the Ganomaly [Ak?ay et al., 2018] framework for one-class novelty detection and achieve stateof-the-art performance. To compress deep neural networks and deploy them to embedded devices with limited resources, we propose the Progressive Knowledge Distillation with GANs (P-KDGAN) to learn a lightweight student GAN from a pre-trained teacher GAN. The P-KDGAN method is composed of three modules. 1) Knowledge distillation with GANs (KDGAN), in which the distillation losses based on Ganomaly framework are proposed for transferring knowledge from the teacher GAN to the student GAN. 2) Four distillation structures are designed for KDGAN. 3) The two-step progressive learning of KDGAN can continuously improve the performance of the student GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">KDGAN</head><p>In the KDGAN, both the teacher GAN and the student GAN follow the network architecture of Ganomaly <ref type="bibr" target="#b0">[Ak?ay et al., 2018]</ref> and have the same network layers. The difference between them is the number of channels in each layer. Therefore, the generator loss L T g and the discriminator loss L T d in the teacher GAN have the same form as L S g and L S d . The generator loss L S g of student GAN includes reconstructed image loss S con , latent space loss S enc and adversarial loss S adv :</p><formula xml:id="formula_2">S con = E x?X x ?x 1 ,<label>(2)</label></formula><formula xml:id="formula_3">S enc = E x?X z 1 ? z 2 2 ,<label>(3)</label></formula><formula xml:id="formula_4">S adv = E x?X f (x) ? f (x) 2 ,<label>(4)</label></formula><formula xml:id="formula_5">L S g = w con S con + w enc S enc + w adv S adv ,<label>(5)</label></formula><p>where f (?) outputs the intermediate representations of discriminator D. S con , S enc and S adv denote the reconstruction errors of the images, latent vectors and feature maps, respectively. The weighted sum of S con , S enc and S adv constitutes the generator loss L S g and is minimized to train the student generator G. The discriminator loss L T d consists of S ce . The designed distillation loss K l is a novel attempt for knowledge distillation on two standard GANs. As is shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, the teacher GAN and the student GAN transfer knowledge through the intermediate layers of the generators, which includes two latent vectors and one reconstructed images. In the KDGAN, we design three losses K 1 , K x and K 2 to measure the similarity of the intermediate layers. K 1 and K 2 are the L2 distance of latent vectors (z 1 and? 1 , z 2 and z 2 ) from the teacher GAN and student GAN. K x is the L1 distance of reconstructed images (x, x). Based on the above three losses, we propose distillation loss K l as an objective function for knowledge distillation which is the weighted sum of K 1 , K x and K 2 :</p><formula xml:id="formula_6">K l = w 1 K 1 + w x K x + w 2 K 2 ,<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distillation Structures</head><p>As is shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, the designed distillation loss K l builds a "bridge" between the teacher and student GANs for knowledge transfer. The losses in the KDGAN consist of three parts: teacher GAN losses L T g , L T d , student GAN losses L S g , L S d , and distillation loss K l . We define the above five loss functions as the elements of set L:</p><formula xml:id="formula_7">L = ?L T g , ?L T d , ?L S g , ?L S d , ?K l ,<label>(7)</label></formula><p>where ?, ?, ?, ?, and ? ? {0, 1} indicates whether the corresponding loss is used to train the networks. The elements in L can be combined into four subsets (L 1 , L 2 , L 3 , L 4 ) to form different distillation structures according to the following two rules. The first rule is whether the teacher GAN has fixed weights; the second rule is whether the distillation loss K l is combined with the losses L S g , L S d to train the student GAN. Before the KDGAN, a teacher GAN for i=1 to N do 5:</p><p>Update student GAN when teacher GAN with fixed weights 6: end for 7: end for 8: Second step 9: Download the weights of the teacher and student GANs from the previous step 10: for m=1 to M do 11:</p><p>for i=1 to N do 12:</p><p>Update student GAN and teacher GAN together 13: end for 14: end for is trained by its own generator loss L T g and discriminator loss L T d . The designed four distillation structures are introduced as follows:</p><p>? KDGAN-1 : L 1 ={K l }. Without the use of real labels, the training of student network only depends on the distillation loss K l , which results in poor detection performance. There is no adversarial networks, and the teacher network is not updated, so its training speed is the fastest. ? KDGAN-2 : L 2 = L S g , L S d , K l . The student GAN is trained by minimizing its own losses L S g , L S d and distillation loss K l , while the teacher GAN is not updated. The adversarial network in student GAN causes its training speed to be slightly slower than KDGAN-1 . ? KDGAN-3 : L 2 = L T g , L T d , K l . The teacher GAN uses its own losses L T g , L T d to train to maintain its performance, when the training of the student GAN follows KDGAN-1 . Its training speed is almost the same as that of  </p><formula xml:id="formula_8">. ? KDGAN-4 : L 2 = L T g , L T d , L S g , L S d , K l .</formula><p>The trainings of the teacher and student GANs follow KDGAN-3 and KDGAN-2 , respectively. There are two adversarial networks that need to be trained simultaneously, so the training speed is the slowest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Progressive Learning of KDGAN</head><p>The progressive learning of KDGAN, shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>, is a two-step approach that continuously improves the performance of the student GAN and achieves better performance than the single step methods. The two-step P-KDGAN is described as follows. P-KDGAN-I. In the first step, four distillation structures are utilized to train student network. The experimental results shown in Section 4.4 demonstrate that the performance of student network with random initialization has a large gap compared with teacher network. Therefore, considering the detection accuracy and training time of the four distillation structures, KDGAN-2 is used as the first step of P-KDGAN to enable student network to learn the basics knowledge from teacher network. In the KDGAN-2 , the pre-trained teacher has already converged, so the teacher network with fixed weights is used to train the student network relying on real labels and distillation knowledge. P-KDGAN-II. In the second step, KDGAN-3 and KDGAN-4 continue to train the teacher networks, while the student networks with basic knowledge rely on distilling knowledge to fine-training, thereby further improving accuracy and stability. The fine-learning processes in this step are named as P-KDGAN-II-2 3 and P-KDGAN-II-2 4 . The experimental results prove that the performance of student network even exceeds the teacher network in some categories of one-class novelty detection.</p><p>The above process is illustrated in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, the proposed P-KDGAN is evaluated on the well-known CIFAR- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>For the three experimental datasets, the training and testing partitions remain as default. In the setup, one of the classes from training dataset is considered as normal samples for training. During testing, the remaining classes are used to represent novelty samples. For example, every experiment on the CIFAR-10 dataset is trained with 5000 samples and tested with 10,000 samples. The above experiment is repeated for all the ten categories. In addition, in order to compatible with the network architectures, all the images are resized to 32?32 by Bilinear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Network Architectures</head><p>The Ganomaly <ref type="bibr" target="#b0">[Ak?ay et al., 2018]</ref> framework based on encoder-decoder-encoder (G E ? G D ? G R ) pipeline is used in our method. G E , G R in the generator and discriminator D are encoders, G D is decoder. The encoder E(x) and decoder D(x) follow the <ref type="bibr">DCGAN [Radford et al., 2016]</ref> architecture, which have three basic layers in our model. As is shown in <ref type="table" target="#tab_4">Table 1</ref>, the basic layers consist of: convolutional layers (deconvolutional layers), batch normalization and activation. In contrast, LeakyReLU and ReLU activations are   used in encoders and decoders, except for the last layer in decoder, which uses Tanh. All the convolution filters are set to 4 ? 4.</p><p>The difference between a teacher network and a student network is the number of channels in the intermediate representations. For the three experimental datasets, the intermediate layers in the teacher networks are set to 64-128-256 channels following the <ref type="bibr">OCGAN [Perera et al., 2019]</ref>. The student networks in each dataset utilize intermediate representations with 8-16-64 channels, 2-4-8 channels and 1-2-4 channels respectively. The encoder E(x) and decoder D(x) architecture of the teacher GAN is illustrated in <ref type="table" target="#tab_4">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on One-class Novelty Detection</head><p>In this section, we compare our Ganomaly <ref type="bibr" target="#b0">[Ak?ay et al., 2018]</ref> based Teacher GAN with several traditional and deep learning based methods on CIFAR-10 and MNIST datasets , including one-class SVM (OC-SVM) <ref type="bibr">[Sch?lkopf et al., 2001]</ref>, kernel density estimation (KDE) <ref type="bibr">[Parzen, 1962]</ref>  <ref type="bibr" target="#b2">[Ruff et al., 2018]</ref> and <ref type="bibr">OCGAN [Perera et al., 2019]</ref>. In light of massive experiments, the parameters of w con , w enc and w adv in Eq. 5 are mannually configured as 10, 1 and 1. The parameters of w 1 , w x and w 2 in Eq. 6 are set as 1. We take the average AUC of the last epoch from multiple trials, but not the manually selected result, as the detection performance, which is more convictive.   Comparisons on CIFAR-10 and MNIST. The performance of one-class novelty detection on CIFAR-10 dataset, our method shown in <ref type="table" target="#tab_5">Table 2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of P-KDGAN Method</head><p>In this section, the progressive knowledge distillation with GANs is evaluated on CIFAR-10, MNIST and FMNIST datasets. In each experiment, the weights of the last epoch are served as the teacher network.</p><p>KDGAN vs. P-KDGAN. As is shown in <ref type="table" target="#tab_8">Table 3</ref>, P-KDGAN-II-2 3 achieves the best performance on three datasets, which illustrates the effectiveness of our progressive learning of KDGAN. Although KDGAN-3 achieves the second-best results on MNIST and FMNIST, it shows the worst performance on CIFAR-10 dataset. KDGAN-4 obtain the second-best results on CIFAR-10, but it was about 0.5% lower than the best result. In addition, KDGAN-d (KDGAN-4 ) iillustrated in <ref type="figure" target="#fig_2">Figure 2</ref>(c) is inferior in accuracy and training stability compared to P-KDGAN-II. The training curves of the AUC illustrated in <ref type="figure" target="#fig_2">Figure 2</ref> clearly shows that proposed P-KDGAN-II can improve the accuracy of the student network and even surpass the teacher network, and reduce shock. Therefore, the above analysis concludes that student networks with random initialization can only learn the basic knowledge of the teacher networks, and the fine-training in the second step of P-KDGAN can further improve performance.  Results on P-KDGAN. As is illustrated in <ref type="table" target="#tab_11">Table 4</ref>, the performance of the student GAN obtained by two-step P-KDGAN is only 0.71%, 0.55% and 0.18% lower than that of the teacher GAN when compressing the computation at ratios of 24.45:1, 311.11:1, and 700:1, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we use the encoder-decoder-encoder pipeline based GANs for one-class novelty detection and achieve state-of-the-art performance. To compress the model, the progressive knowledge distillation with GANs is proposed, which is a novel exploration that applies the knowledge distillation on two standard GANs. The two-step progressive learning can continuously improve the performance and reduce shock of the student network, in which the designed distillation loss plays an important role. Experiments on three datasets validate the effectiveness of our proposed method. Moreover, our proposed method can be used to compress other GANs-based applications, such as image generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The flowchart of Knowledge Distillation with GANs for One-class Novelty Detection. (a) The Knowledge Distillation with Generative Adversarial Networks (KDGAN), in which the distillation losses K l (K l = w1K1 +wxKx+ w2K2) is designed for training the student GAN. (b) The two-step progressive learning of KDGAN is used to continuously improve the performance of the student GAN. KDGAN-1 , KDGAN-2 , KDGAN-3 and KDGAN-4 are four different distillation structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>-dimensional image data. The other methods based on deep learning include Deep Belief Networks (DBN)<ref type="bibr" target="#b0">[Erfani et al., 2016]</ref>, Autoencoders (AE)<ref type="bibr" target="#b3">[Vincent et al., 2008]</ref> and generative adversarial networks (GANs)[Schlegl et al., 2017;<ref type="bibr" target="#b5">Zenati et al., 2018;</ref>  Perera et al., 2019].GANs have shown state-of-the-art performance in modeling complex high-dimensional image distributions<ref type="bibr" target="#b1">[Goodfellow et al., 2014]</ref>. Therefore, a lot of GANs based methods have been used for novelty detection[Schlegl et al., 2017;<ref type="bibr" target="#b5">Zenati et al., 2018;</ref> Perera et al., 2019]. The reconstruction errors of images or latent vectors are utilized as novelty score, which means that the learned model only reconstructs normal samples well, and shows very low tolerance for novel samples. Schlegl et al.[Schlegl et al., 2017]  proposed the first GANs based work, AnoGAN, for novelty detection. In training, the combination of the residual loss on images and discrimination loss on feature maps is minimized to iteratively search the best latent vector. The EfficientGAN [Zenati et  al., 2018]  based on BiGAN [Donahue et al., 2017] network was proposed for jointly training the map from the image to the latent space simultaneously. Perera et al. [Perera et al., 2019] proposed the OCGAN in which two discriminators were used in the latent space and the input space for making the learned network better model the input images. Recently, Ganomaly [Ak?ay et al., 2018] shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Training curves of the AUC on three datasets. The normal classes are: (a) AutoMobile on CIFAR-10. (b) 5 on MNIST. (c) Bag on FMNIST. P-KDGAN-I represents KDGAN-2 . P-KDGAN-II represents P-KDGAN-II-2 3 . KDGAN-d represents KDGAN-4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, credit card fraud [Srivastava et al., This work was done when Zhiwei Zhang was a research intern at Multimedia Laboratory of SIAT.</figDesc><table /><note>*? Corresponding author.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Progressive Knowledge Distillation with GANs Input: Pre-trained teacher G and D, training dataset with normal instances (x i , y i )</figDesc><table><row><cell>N i=1 , epoch M Output: Improved student G</cell></row><row><cell>1: First step</cell></row><row><cell>2: Student G and D with random initialization</cell></row><row><cell>3: for m=1 to M do</cell></row><row><cell>4:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>The encoder and decoder architecture for our teacher GAN, layer by layer. Units refer to number of filters in the case of convolution layers, and BN is Batch Normalization abbreviated.</figDesc><table><row><cell cols="2">NORMAL CLASS OCSVM</cell><cell>KDE</cell><cell>VAE</cell><cell cols="4">AND AnoGAN DSVDD OCGAN</cell><cell>Ours</cell></row><row><cell>AIRPLANE</cell><cell>0.630</cell><cell>0.658</cell><cell>0.700</cell><cell>0.717</cell><cell>0.671</cell><cell>0.617</cell><cell>0.757</cell><cell>0.825</cell></row><row><cell>AUTOMOBILE</cell><cell>0.440</cell><cell>0.520</cell><cell>0.386</cell><cell>0.494</cell><cell>0.547</cell><cell>0.659</cell><cell>0.531</cell><cell>0.744</cell></row><row><cell>BIRD</cell><cell>0.649</cell><cell>0.657</cell><cell>0.679</cell><cell>0.662</cell><cell>0.529</cell><cell>0.508</cell><cell>0.640</cell><cell>0.703</cell></row><row><cell>CAT</cell><cell>0.487</cell><cell>0.497</cell><cell>0.535</cell><cell>0.527</cell><cell>0.545</cell><cell>0.591</cell><cell>0.620</cell><cell>0.605</cell></row><row><cell>DEER</cell><cell>0.735</cell><cell>0.727</cell><cell>0.748</cell><cell>0.736</cell><cell>0.651</cell><cell>0.609</cell><cell>0.723</cell><cell>0.765</cell></row><row><cell>DOG</cell><cell>0.500</cell><cell>0.496</cell><cell>0.523</cell><cell>0.504</cell><cell>0.603</cell><cell>0.657</cell><cell>0.620</cell><cell>0.652</cell></row><row><cell>FROG</cell><cell>0.725</cell><cell>0.758</cell><cell>0.687</cell><cell>0.726</cell><cell>0.585</cell><cell>0.677</cell><cell>0.723</cell><cell>0.797</cell></row><row><cell>HORSE</cell><cell>0.533</cell><cell>0.564</cell><cell>0.493</cell><cell>0.560</cell><cell>0.625</cell><cell>0.673</cell><cell>0.575</cell><cell>0.723</cell></row><row><cell>SHIP</cell><cell>0.649</cell><cell>0.680</cell><cell>0.696</cell><cell>0.680</cell><cell>0.758</cell><cell>0.759</cell><cell>0.820</cell><cell>0.827</cell></row><row><cell>TRUCK</cell><cell>0.508</cell><cell>0.540</cell><cell>0.386</cell><cell>0.566</cell><cell>0.665</cell><cell>0.731</cell><cell>0.554</cell><cell>0.735</cell></row><row><cell>MEAN</cell><cell>0.5856</cell><cell cols="3">0.6097 0.5833 0.6172</cell><cell>0.6179</cell><cell>0.6481</cell><cell>0.6566</cell><cell>0.7376</cell></row><row><cell>0</cell><cell>0.988</cell><cell>0.885</cell><cell>0.997</cell><cell>0.984</cell><cell>0.966</cell><cell>0.980</cell><cell>0.998</cell><cell>0.996</cell></row><row><cell>1</cell><cell>0.999</cell><cell>0.996</cell><cell>0.999</cell><cell>0.995</cell><cell>0.992</cell><cell>0.997</cell><cell>0.999</cell><cell>0.999</cell></row><row><cell>2</cell><cell>0.902</cell><cell>0.710</cell><cell>0.936</cell><cell>0.947</cell><cell>0.850</cell><cell>0.917</cell><cell>0.942</cell><cell>0.969</cell></row><row><cell>3</cell><cell>0.950</cell><cell>0.693</cell><cell>0.959</cell><cell>0.952</cell><cell>0.887</cell><cell>0.919</cell><cell>0.963</cell><cell>0.969</cell></row><row><cell>4</cell><cell>0.955</cell><cell>0.844</cell><cell>0.973</cell><cell>0.960</cell><cell>0.894</cell><cell>0.949</cell><cell>0.975</cell><cell>0.970</cell></row><row><cell>5</cell><cell>0.968</cell><cell>0.776</cell><cell>0.964</cell><cell>0.971</cell><cell>0.883</cell><cell>0.885</cell><cell>0.980</cell><cell>0.951</cell></row><row><cell>6</cell><cell>0.978</cell><cell>0.861</cell><cell>0.993</cell><cell>0.991</cell><cell>0.947</cell><cell>0.983</cell><cell>0.991</cell><cell>0.992</cell></row><row><cell>7</cell><cell>0.965</cell><cell>0.884</cell><cell>0.976</cell><cell>0.970</cell><cell>0.935</cell><cell>0.946</cell><cell>0.981</cell><cell>0.982</cell></row><row><cell>8</cell><cell>0.853</cell><cell>0.669</cell><cell>0.923</cell><cell>0.922</cell><cell>0.849</cell><cell>0.939</cell><cell>0.939</cell><cell>0.965</cell></row><row><cell>9</cell><cell>0.955</cell><cell>0.825</cell><cell>0.976</cell><cell>0.979</cell><cell>0.924</cell><cell>0.965</cell><cell>0.981</cell><cell>0.987</cell></row><row><cell>MEAN</cell><cell>0.9513</cell><cell cols="3">0.8143 0.9696 0.9671</cell><cell>9127</cell><cell>0.9480</cell><cell>0.9750</cell><cell>0.9780</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>One-class novelty detection results on CIFAR-10 and MNIST dataset. The average AUC of three repeated experiments was used as detection performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Abati et al., 2019], AnoGAN [Schlegl et al., 2017], DSVDD</figDesc><table><row><cell>, deep</cell></row><row><cell>variational autoencoder (VAE) [Kingma and Welling, 2014],</cell></row><row><cell>AND [</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Compare the performance of KDGAN and P-KDGAN. We highlight the best results in red and the second-best results in blue color.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of our P-KDGAN method on CIFAR-10, MNIST and FMNIST datasets. (M means million, # means the compression ratio of parameter numbers and FLOPs compared to the teacher GAN.)</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High-dimensional and large-scale anomaly detection using a linear one-class svm with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abati</surname></persName>
		</author>
		<idno>abs/1710.09282</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<editor>Jeff Donahue, Philipp Kr?henb?hl, and Trevor Darrell</editor>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="121" to="134" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>ACM Comput. Surv.. Pedro Garc?a-Teodoro</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jes?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>D?az-Verdejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Maci?-Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
	</analytic>
	<monogr>
		<title level="m">Pramuditha Perera, Ramesh Nallapati, and Bing Xiang. Ocgan: One-class novelty detection using gans with constrained latent representations. CVPR</title>
		<editor>Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer</editor>
		<meeting><address><addrLine>Ba; Nicolas Ballas</addrLine></address></meeting>
		<imprint>
			<publisher>Antoine Chassang</publisher>
			<date type="published" when="1962" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2893" to="2901" />
		</imprint>
	</monogr>
	<note>Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
	</analytic>
	<monogr>
		<title level="m">Philipp Seeb?ck, Sebastian M. Waldstein, Ursula Schmidt-Erfurth, and Georg Langs</title>
		<editor>Bernhard Sch?lkopf, John C. Platt, John Shawe-Taylor, Alexander J. Smola, and Robert C</editor>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Deep one-class classification. ICML</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extracting and composing robust features with denoising autoencoders. ICML</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
	<note>Neural Computation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kdgan: knowledge distillation with generative adversarial networks</title>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
	<note>Adversarial learning of portable student networks</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fashion-mnist: A novel image dataset for benchmarking machine learning. ArXiv, abs/1708.07747</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
		<respStmt>
			<orgName>Chemometrics and Intelligent Laboratory Systems</orgName>
		</respStmt>
	</monogr>
	<note>Principal component analysis. Efficient gan-based anomaly detection. ArXiv, abs/1802.06222</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
