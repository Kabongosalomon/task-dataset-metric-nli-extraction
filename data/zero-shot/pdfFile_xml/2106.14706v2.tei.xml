<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Motion Projection Consistency Based 3D Human Pose Estimation with Virtual Bones from Monocular Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Motion Projection Consistency Based 3D Human Pose Estimation with Virtual Bones from Monocular Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep learning</term>
					<term>3D human pose estimation</term>
					<term>virtual bones</term>
					<term>motion constraints</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-time 3D human pose estimation is crucial for human-computer interaction. It is cheap and practical to estimate 3D human pose only from monocular video. However, recent bone splicing based 3D human pose estimation method brings about the problem of cumulative error. In this paper, the concept of virtual bones is proposed to solve such a challenge. The virtual bones are imaginary bones between non-adjacent joints. They do not exist in reality, but they bring new loop constraints for the estimation of 3D human joints. The proposed network in this paper predicts real bones and virtual bones, simultaneously. The final length of real bones is constrained and learned by the loop constructed by the predicted real bones and virtual bones. Besides, the motion constraints of joints in consecutive frames are considered. The consistency between the 2D projected position displacement predicted by the network and the captured real 2D displacement by the camera is proposed as a new projection consistency loss for the learning of 3D human pose. The experiments on the Human3.6M dataset demonstrate the good performance of the proposed method. Ablation studies demonstrate the effectiveness of the proposed inter-frame projection consistency constraints and intra-frame loop constraints.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, increasing attention is attracted to 3D human pose estimation in videos, due to its wide application in the field of action recognition <ref type="bibr" target="#b0">[1]</ref>, robot learning <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, and robot control <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>. The state-of-the-art approaches <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref> are mostly in two steps, 2D joint detection, and 3D pose estimation from the 2D joints.</p><p>The core of these methods is 3D pose estimation based on 2D joints, whose recognized difficulty is depth ambiguity. The information input is limited to the 2D plane while the goal is to predict the joint coordinates in 3D space, so the depth information needs to be compensated with other information, which nowadays is time and spatial information. Some approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> utilize the information of the adjacent frames and <ref type="bibr" target="#b9">[10]</ref> utilizes the information of the multiview. Specifically, Pavllo et al. <ref type="bibr" target="#b8">[9]</ref> propose an efficient approach for 3D human pose estimation in video based on dilated temporal convolutions on 2D keypoint trajectories. Chen et al. <ref type="bibr" target="#b9">[10]</ref> first use image-skeleton mapping module to obtain 2D skeleton maps from images, and then a view synthesis module is used to predict 3D pose. Lin et al. <ref type="bibr" target="#b10">[11]</ref> propose a deep learning-based framework that utilizes matrix factorization for sequential 3D human pose estimation with the input 2D joint position. Luvizon et al. <ref type="bibr" target="#b11">[12]</ref> propose a multitask framework that can estimate the 2D and 3D pose from images and recognize the action from video sequence. Lee et al. <ref type="bibr" target="#b12">[13]</ref> propose a new long short-term memory (LSTM)-based deep learning architecture named propagating LSTM networks (p-LSTMs) to infer depth. Compared with the method that directly predict 3D human pose from images, these two-stage work add the intermediate variables in the process of the prediction, which means introduce more constraints. However, joints still have a large of degree of freedom, which is most pronounced at the end of the human body. Chen et al. <ref type="bibr" target="#b13">[14]</ref> uses consecutive frames to estimate the middle frames 3D pose, which is decomposed into the length and direction prediction of bones. However, the method of obtaining joints by the accumulation of bones will accumulate errors.</p><p>To alleviate the accumulated errors, the first contribution in this paper is to improve the bone prediction network by adding the prediction of virtual bones between non-adjacent joints to the bone prediction network. The loop constraint is constructed by real bones and virtual bones to reduce cumulative error and increase the prediction accuracy. Specifically, to avoid the overfitting caused by the limited number of actors in training datasets, each sampled frame is used to predict the corresponding 3D joint position. The bone lengths are calculated from the estimated 3D joint positions. To obtain the real bone lengths in the current frame, a self-attention module is incorporated to weigh the real bone lengths from sampled frames. The virtual bone lengths are calculated from the 3D joint positions in the current frame directly. The ground truth of bone length is used to optimize the self-attention module. The temporal convolutional network in <ref type="bibr" target="#b8">[9]</ref> is used to predict the direction of all bones. The final positions of joints are derived from the bone length and direction of all bones through a fully connected network. The motivation to add the bones between arXiv:2106.14706v2 [cs.CV] 14 Sep 2022 <ref type="figure">Figure 1</ref>: The overview of the proposed framework. It uses consecutive frames to predict the direction of real and virtual bones, randomly sampled frames to predict the length of real bones, and current frames to predict the length of virtual bones. The 3D joint position is predicted by a fully connected network with predicted bones. Meanwhile, a projection consistency loss is used to constrain the learning of bone prediction. non-adjacent joints is based on the idea that adding a proper amount of input virtual bones can increase the accuracy of the final joint prediction and reduce the overfitting to a certain extent.</p><p>The other contribution is proposing a new projection consistency loss. According to <ref type="bibr" target="#b14">[15]</ref>, even if the l 1 mean distance between the ground truth and estimated positions is the same, there will be different distribution for joint positions in time dimension. Some researchers <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> make kinematics analysis and propose motion loss. Their ideas are all based on the continuity of the displacement of the joint, which is constrained by the real displacement in 3D. A new projection consistency loss is proposed in this paper, comparing the 2D projection of the 3D displacement of the estimated joints between adjacent frames and the 2D displacement derived from the input 2D keypoints. This loss can reduce the fluctuation of joint position estimation results in the continuous video, with no need of 3D ground truth of joints.</p><p>In summary, the approach proposed in this paper makes the following contributions:</p><p>? A bone length prediction network with additional bones among non-adjacent joints is presented to avoid overfitting and predict joints more accurately. ? A new projection loss based on the 2D displacement of 3D joints is proposed, not only smoothing the error between adjacent frames but also improving the accuracy of 3D joint position prediction. ? A variety of virtual bone combination modes are validated. Ablation studies demonstrate the effectiveness of the proposed method. The combination of virtual bones and projection loss lead to a good performance on Human3.6M dataset <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The development of the field of 3D human pose estimation has undergone a variety of method changes. One of them uses neural networks to estimate the 3D pose of the human body directly from the input image, including <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>. Mitra et al. <ref type="bibr" target="#b17">[18]</ref> first train the neural network in an unsupervised fashion with multi-view data to get the human pose representation.</p><p>Then, ground truth is used to map this representation to the 3D human joints. It belongs to a semi-supervised method. Gabeur et al. <ref type="bibr" target="#b18">[19]</ref> use the structure of encoder-decoder to estimate the depth of the front and rear surfaces of the human body as the representation of 3D pose. Considering the time information, Kocabas et al. <ref type="bibr" target="#b19">[20]</ref> use the video as the input, obtaining 82 parameters of the Skinned Multi-Person Linear model (SMPL) <ref type="bibr" target="#b20">[21]</ref> to represent the shape and pose of the human body.</p><p>Recently, the task of 3D human joint estimation is divided into two parts. 2D joint positions are first obtained from images, and then 3D joint positions are predicted from the 2D position. Methods for detecting 2D joint positions from images have become mature <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Zhang et al. <ref type="bibr" target="#b21">[22]</ref> optimize the probability distribution obtained from the heat map, which leads to better 2D joint estimation results. Nie et al. <ref type="bibr" target="#b22">[23]</ref> propose a chain method to represent joint coordinates. Each joint coordinate is represented by the root joint coordinate and the bone vector between adjacent joints.</p><p>With the help of mature technology for estimating 2D keypoints, researchers estimate 3D poses on this basis, such as <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b29">[30]</ref>. Lee et al. <ref type="bibr" target="#b12">[13]</ref> firstly use convolutional neural network to extract a 2D pose from RGB image, and then they propose the p-LSTMs model to infer depth to obtain 3D human pose. Wang et al. <ref type="bibr" target="#b23">[24]</ref> notice the importance of the occlusion relationship of the joints, which is used as part of the loss for the network training. Iqbal et al. <ref type="bibr" target="#b24">[25]</ref> first extract 2D joint positions from the input multi-view images, and then estimate 3D positions from the 2D positions and the camera external parameters through an optimization method. An unsupervised loss is constructed by the consistency of the poses from different viewing angles. Different from other works, Li et al. <ref type="bibr" target="#b25">[26]</ref> estimate the length and direction angle of each bone and propose a dataset augmentation method that improves the accuracy of the algorithm in unusual poses.</p><p>Recent studies <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b29">[30]</ref> take into account the time information and use video as input instead of a separate frame. Pavllo et al. <ref type="bibr" target="#b8">[9]</ref> regard the process of estimating 3D position from 2D as the encoding part. The process of projecting 3D back to 2D is regarded as the decoding part. They train with the labeled data and use unlabeled data to calculate the consistency of input and decoded output as an unsupervised loss. Liu et al. <ref type="bibr" target="#b26">[27]</ref> introduce an attention mechanism. Different frames are weighed and different convolution kernels are used to improve network structure and algorithm performance. Cheng et al. <ref type="bibr" target="#b27">[28]</ref> apply the method of multi-scale analysis in the time and space dimensions to deal with the problems of different sizes and different speeds of humans. Chen et al. <ref type="bibr" target="#b13">[14]</ref> decompose the 3D pose into the bone length and the bone direction. Considering the length invariance of bones and the visibility of the joints on the image, they use the whole video combined with the attention mechanism to obtain a more accurate bone length estimation. At the same time, a new layered bone direction prediction network is proposed to get better results. Wu et al. <ref type="bibr" target="#b28">[29]</ref> establish a depth map of joints to calculate the limb depth map. Then, the hidden information extracted from the picture is combined to directly obtain 3D poses. Jiang et al. <ref type="bibr" target="#b29">[30]</ref> pretrain a series of 3D poses using input pictures labeled with 2D joint information. They take the weights of these poses from the model to get a rough pose and use the residual compensation to obtain the final predicted pose. To increase the accuracy, some researchers use the motion of the human joints in the video to improve the method of supervision. For example, Xu et al. <ref type="bibr" target="#b15">[16]</ref> take the kinematics analysis for monocular 3D human pose estimation between multiple frames to correct the limbs at the end of the human body. Meanwhile, some researchers use distance matrix to measure the relationship between different joints, such as [31]- <ref type="bibr" target="#b32">[33]</ref>. Noguer <ref type="bibr" target="#b30">[31]</ref> proposes the Euclidean Distance Matrice (EDM) between 2D keypoints to estimate the EDM between 3D human joints, finally leading to the human pose. Guo et al. <ref type="bibr" target="#b31">[32]</ref> based on <ref type="bibr" target="#b30">[31]</ref>, recover the occluded joints before estimating 3D EDM. Gao et al. <ref type="bibr" target="#b32">[33]</ref> noticed the distance relationship between non-adjacent joints when determining the distance matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MOTION PROJECTION CONSISTENCY BASED 3D HUMAN POSE ESTIMATION WITH VIRTUAL BONES</head><p>The overview of the proposed method is illustrated in <ref type="figure">Figure  1</ref>. In this section, we elaborate on the details of our method. In Section III-A, we introduce the architecture of the bone prediction network with the virtual bone output. In Section III-B, we introduce the detail of obtaining 3D joint position from the bones. In Section III-C, we present the detail of projection consistency loss. In Section III-D, the other used losses are introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Bone Length and Direction Prediction Networks</head><p>In recent years, the human skeleton structure commonly used by researchers is shown in <ref type="figure" target="#fig_0">Figure 2</ref>(a), in which the bones between adjacent joints are recorded as real bones. In this paper, we use the concept of virtual bones to represent the imaginary bones between non-adjacent joints, as in <ref type="figure" target="#fig_0">Figure  2</ref>(b). These two terms will be used frequently in the following sections. In this paper, the learning of real bones is optimized through the prediction of virtual bones and real bones simultaneously.</p><p>The structure of the bone length prediction network is shown in <ref type="figure">Figure 3</ref>. The bone length prediction network not only predicts the bone length of real bones but also predicts the bone length of virtual bones. To get global information more effectively, like <ref type="bibr" target="#b13">[14]</ref>, we input 2D joint location x of J joints from f frames sampled from a video to the network. The 2D joint locations are first used to predict coarse 3D locations of the J joints, which are utilized to calculate the length of bones.</p><p>Because of the invariance of the bone length of real bones, the length of real bones is predicted for each of the random frames and weighed by an attention mechanism to get the length of real bones in the current frame:</p><formula xml:id="formula_0">L real = f i=1 w i ? real (?(x i )),<label>(1)</label></formula><p>where w i represents the matrix composed of the weight of each bone in i-th frame. x i (i = 1, 2, . . . , f ) represents the input 2D joint location in random frames. ? real represents the calculation to obtain the length of the real bones from coarse 3D joint location in frames. ? represents the network that predicts the coarse 3D joint location from random frames. However, the bone length of virtual bone varies from frame to frame, so only the current frame is used, skipping the attention mechanism, to predict the bone length of V virtual bones:</p><formula xml:id="formula_1">L virtual = ? virtual (?(x current )),<label>(2)</label></formula><p>where x current represents the 2D joint location in frame at the current timestamp. ? virtual represents the calculation to obtain the length of virtual bones from coarse 3D joint location in current frame. ? represents the network that predicts coarse 3D joint location from the current frame. The method for predicting bone direction is the same as <ref type="bibr" target="#b8">[9]</ref>, utilizing the temporal fully-convolutional network whose output is the unit vector of bone direction:</p><formula xml:id="formula_2">D o = ?(x 1 , x 2 , . . . , x f ),<label>(3)</label></formula><p>where x k (k = 1, 2, . . . , f ) represents the input 2D joint location in consecutive frames. ? represents the temporal fully-convolutional network.</p><p>For the standard 17-joint human skeleton structure, there is only one path from the root joint to the target joint. What our network predicts are the bone length and unit orientation of the bone, so with the bone vectors on the path from root joint to the target joint, we only get the unique target joint coordinates. After the introduction of virtual bones, for a target joint, we will get multiple joint coordinates along different paths. The final target joint coordinate is obtained by weighting multiple predicted values, which can reduce the accumulate error and increase the prediction accuracy of target joint.</p><p>The joints at the end of the human body are significantly more unstable than others <ref type="bibr" target="#b15">[16]</ref>, so the virtual bones input the network are selected. Moreover, considering the human skeleton is based on root joint (i.e. pelvis), we select the bones related to the four joints at the end of the human body (i.e. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3D Joint Prediction</head><p>Usually the k-th joint's position, P k is derived as:</p><formula xml:id="formula_3">P k = m?R k D o,m ? L m ,<label>(4)</label></formula><p>where D o,m ? D o and L m ? L = L real ? L virtual are the direction and length of bone m. R k is the collection of all bones along the path of the normal human skeleton from the root joint (i.e. pelvis) to the k-th joint.</p><p>However, the equation <ref type="formula" target="#formula_3">(4)</ref> only considers the path along with the real bones. If the virtual bones are added, there will be more than one path to a joint. Hence the equation changes to:</p><formula xml:id="formula_4">P k = R k,i ?? k ? k,i m?R k,i D o,m ? L m ,<label>(5)</label></formula><p>where ? k represents the set of all the paths from the root joint to the k-th joint. w k,i is the weight of the i-th path. R k,i is the collection of all the bones on the i-th path to the k-th joint. Thus, in the proposed approach, there are different ways to obtain the position of a joint. A fully connected network is used to calculate the position of joints with the direction and length of both real and virtual bones as input. The network will automatically adjust the weight distribution of every bone related to every predicted joint to obtain the 3D position. The way obtaining the joints is determined by the bones input, so the selected virtual bones are key of the proposed approach. At the beginning of the experiment, we try the all real bones and virtual bones that between every joints (e.g. 17-joint-skeleton has 136 bones). However, because the number of bones to be predicted is too large, the time required for each training <ref type="figure">Figure 5</ref>: Schematic diagram of the projection consistency loss. In the figure,P i,t represents the estimated projection position of the i-th joint in the t-th frame. P i,t is the ground truth projection of i-th joint position in the t-th frame. Similarly,P i,t+1 represents the estimated projection position of the i-th joint in the (t + 1)-th frame. P i,t+1 is the ground truth projection of the i-th joint position in the (t + 1)-th frame. ? i,t means the 2D displacement of the i-th joint's estimated projection at time t. ? i,t means the 2D displacement of the i-th joint's ground truth projection at time t. epoch is too long. In addition, the experimental effect is not good, so we gave up using all the bones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Projection Consistency Loss</head><p>In this section, a new loss function, projection consistency loss, is designed. As shown in <ref type="figure">Figure 5</ref>, considering the position of the 2D joints in two adjacent frames captured by the same camera, each joint has a certain displacement. Naturally, the 2D projection of the estimated 3D joint position will move in the same way as the 2D input. The calculation process is as follows.</p><p>First, according to the pinhole camera model, the estimated 3D position of each joint is projected back to the 2D plane:</p><formula xml:id="formula_5">Z c ? ?? v 1 ? ? = ? ? ? f dx 0 u 0 0 f dy v 0 0 0 1 ? ? ? ? ?X ? Y ? Z c ? ? ,<label>(6)</label></formula><p>where f represents the focal length of the camera.  Secondly, the 2D displacement of the estimated joint projection is calculated as:</p><formula xml:id="formula_6">? i,t = ? i,t v i,t ? ? i,t?1 v i,t?1 ,<label>(7)</label></formula><p>where (? i,t ,v i,t ) represents the i-th joint's estimated 2D projection at time t.? i,t means the estimated 2D displacement of the i-th joint's projection from time t ? 1 to time t. Similarly, the ground truth 2D displacement is calculated as:</p><formula xml:id="formula_7">? i,t = u i,t v i,t ? u i,t?1 v i,t?1 ,<label>(8)</label></formula><p>where (u i,t , v i,t ) represents the i-th joint's ground truth 2D projection at time t. ? i,t means the 2D displacement of the i-th joint's ground truth projection from time t ? 1 to time t. Finally, the projection consistency loss function is given as follows:</p><formula xml:id="formula_8">Loss proj = mean( ? i,t ? ? i,t<label>1 2</label></formula><p>).</p><p>The Loss proj can be used to train both length prediction network and direction prediction network using estimated joint position at different stages. With the estimated joint position in the bone length prediction stage, Loss proj-len is computed to train the length prediction network. Loss proj-dir is calculated by the final estimated joint position to train the direction prediction network.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 6</ref>, (a) and (b) have the same singleframe projection loss. However, the projection consistency loss of these two situations are very different. Therefore, on the one hand, adding the projection consistency loss can constrain the network and make estimated joint position more smooth between frames. On the other hand, by constraining the displacement of the joints, the positions of the joints in adjacent frames can be coupled. The joint position of each frame can be constrained in the motion direction of joints from    the previous frame to improve the accuracy of the single-frame joint position.</p><note type="other">Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Other Loss Functions</head><p>The loss function of the bone length prediction network is:</p><formula xml:id="formula_10">Loss length = 1 |J| j?J P j ?P j,L 2 ,<label>(10)</label></formula><p>where J represents the set of all joints.P j,L represents the 3D position of the j-th joint estimated by bone length estimated network. P j represents the ground truth of 3D position of the joint.</p><p>The loss function of the bone length attention network is:  The loss function of the bone direction prediction network is: Loss direction = D ?D 2 , whereD represents the direction of bones estimated by the direction prediction network. D represents the ground truth of bone directions.</p><formula xml:id="formula_11">Loss att = L ?L 2 ,</formula><p>The joint shift loss <ref type="bibr" target="#b13">[14]</ref> is calculated as follows:  <ref type="figure">Figure 7</ref>: Visualized comparison of the results of the baseline and the proposed method, both of which use 9-frame receptive field and are trained on the CPN [9] 2D inputs. As shown in the red mark in the picture, our prediction has higher accuracy in the end joints due to the application of virtual bones. In addition, our method also performs better under complex human pose and large motions due to the usage of motion constraints based on projection consistency loss.</p><formula xml:id="formula_12">Loss js = i?J,j?J,i =j (P i ? P j ) ? P i ?P j N (i, j),<label>(11)</label></formula><p>N (i, j) = 1, i, j are not connected by a real bone, 0, i, j are connected by a real bone,</p><p>whereP i represents the position of the i-th joint estimated by the final network. The loss function of the fully connected layers for the final joint prediction is:</p><formula xml:id="formula_14">Loss fc = 1 |J| j?J P j ?P j 2 ,<label>(13)</label></formula><p>whereP j represents the 3D position of the j-th joint estimated by the final fully connected layers.</p><p>In general, the complete loss function is as follows:</p><p>Loss total = w L Loss length + w att Loss att + w d Loss direction + w js Loss js + w pd Loss proj-dir + w pl Loss proj-len</p><formula xml:id="formula_15">+ w f c Loss fc ,<label>(14)</label></formula><p>where w L , w att , w d , w js , w pd , w pl and w f c are all hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Dataset and Evaluation</head><p>The proposed method is evaluated on Human3.6M dataset <ref type="bibr" target="#b16">[17]</ref>. Human3.6M provides annotated 2D and 3D joint posi-  <ref type="table" target="#tab_1">Table IV</ref>: Comparison of different models under Protocols on Human3.6M. "Baseline" represents the baseline 9-frame and 243-frame model we experiment based on <ref type="bibr" target="#b13">[14]</ref>. Other rows represent the loss or virtual bones we proposed. They are used respectively to test the impact on the baseline model. "5VB, 10VB, 13VB, 23VB" refer to the different numbers of virtual bones selected to be added to the bone prediction network. "PCL" refers to the projection consistency loss.</p><p>tions of 3.6 million video frames, which contain 4 camera views for 15 different activities of 11 subjects. Following previous works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b37">[38]</ref>, the training dataset is built on five subjects (S1, S5, S6, S7, S8). The test dataset is built on two subjects (S9, S11) with a 17-joint skeleton. Four protocols are used to evaluate the models: Protocol 1 (MPJPE, the mean per-joint position error) measures the mean Euclidean distance between the predicted and ground-truth joint positions. Protocol 2 (P-MPJPE) is the error between the aligned predicted 3D joints position and the ground truth. Protocol 3 (N-MPJPE) is the error between the estimated joint position and the ground truth at the same scale. Velocity errors (MPJVE), the errors of the derivative of the corresponding predicted 3D pose over time, are used to measure the smoothness of the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>The proposed method is tested on Human3.6M dataset, using the 2D coordinates of Cascaded Pyramid Network <ref type="bibr" target="#b8">[9]</ref> (CPN) or the 2D coordinates of ground truth as the model input. The visibility score used in the proposed method comes from AlphaPose <ref type="bibr" target="#b40">[41]</ref>. The results of the baseline method <ref type="bibr" target="#b13">[14]</ref> are experimented based on their open source codes.</p><p>The optimizer of the network is Adam <ref type="bibr" target="#b41">[42]</ref>. The batchsize b is 2048 for the 9-frame model (9-frame receptive field) and 1024 for the 243-frame model (243-frame receptive field). The number of training epochs are 60. Learning rate is set to 0.001 and the learning rate decays at the rate of 0.95 per epoch. We set w proj?dir = 1, w proj?len = 1 for the total loss function. The other hyper-parameters are the same as <ref type="bibr" target="#b13">[14]</ref>, such as w d = 0.02, w l = 1, w att = 0.05, w js = 0.1 and w f c = 1. In addition, because the bone direction prediction network can only give the bone direction estimation for the middle frame for each video, the middle frames of different videos under the same camera are used to replace the adjacent frames to calculate Loss proj-dir . Three NVIDIA 1080Ti GPUs are used to train the 243-frame model and one for the 9-frame model. <ref type="table" target="#tab_1">Table I and Table II</ref> show the quantitative comparison of the accuracy between the proposed method and other existing methods on the Human3.6M dataset. For using CPN <ref type="bibr" target="#b8">[9]</ref> 2D inputs in <ref type="table" target="#tab_1">Table I</ref>, our method achieves performance similar to state-of-the-art methods when using a large model with the 243-frame receptive fields. However, for using 2D truth values as input, our method outperforms the state-of-the-art method <ref type="bibr" target="#b13">[14]</ref> on all estimation protocols. Therefore, our slightly worse performance when using CPN <ref type="bibr" target="#b8">[9]</ref> 2D inputs is due to errors of the input information. When using accurate information as input, that is, 2D joint ground truth, our method shows high performance. Under the experimental condition of 9-frame receptive fields, the proposed method gets better results than <ref type="bibr" target="#b13">[14]</ref> both with CPN <ref type="bibr" target="#b8">[9]</ref> 2D inputs and the 2D ground truth inputs. The good performance of the proposed method using a smaller model means less computational resources and time consumption. <ref type="figure">Figure 7</ref> is the visualized results of the proposed method in the two actions, phoning and walking-dog. <ref type="table" target="#tab_1">Table III</ref> shows the parameter sensitivity of the proposed method. Only the parameters we introduced are tested. The proposed method is sensitive to the choice of super-parameters, so their values are set based on the test results in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Results</head><p>In addition, we also tested the time statistics of the proposed method. In our method, we have a lightweight model (9frame) and a highly accurate model (243-frame). In recent years, most videos for human pose estimation are recorded at a frequency of 25Hz, so in the real-time test, we believe that the prediction time of 25 frames is less than 1s to meet the real-time performance. For the 9-frame model, the time to predict 25 frames is 0.54s, less than 1s. For the 243-frame model, the time is 1.19s, more than 1s. The lightweight model meets real-time requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>The ablation experiments are performed on Human3.6M under Protocol 1, 2, 3, and Velocity. The 9-frame models and 243-frame models are used respectively for the comparisons between the baseline <ref type="bibr" target="#b13">[14]</ref> and the proposed method. Except for the conditions to be compared, other experimental settings are the same as Section IV-B.</p><p>1) Influence of Numbers of Virtual Bones: According to the comparison of the results in Table IV(a), when 5 virtual bones are added, most protocols are slightly improved. Too few virtual bones added to the joint prediction can only bring a slight improvement. When 10, 13, 23 virtual bones are added, all evaluation protocols have great improvement compared to the baseline. In a smaller receptive field, the proposed method has produced better results. Our method can have a greater improvement when there is less information input, which shows that our method is more effective when there is more room for improvement.</p><p>2) Influence of Projection Consistency Loss: It can be seen from the Table IV(b) that although increasing the projection consistency loss alone has little effect on the 243-frame model, it can effectively improve the performance of baseline under the 9-frame receptive field. Another point worth paying attention to is that increasing the projection consistency loss alone has a greater improvement on Protocol 1 and Protocol 3, but the impact on Protocol 2 is more limited. Protocol 2 is obtained by calculating the minimum error of the skeleton after the rigid body transformation, so the accuracy of the joint position after rotation is less improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, a novel 3D human pose prediction network and a novel projection consistency loss are proposed. Virtual bones between non-adjacent joints are proposed to optimize the estimation of bone length. Random frames are used to predict the real bone length combining with an attention mechanism, and the current frame is used to predict virtual bone length directly. The bone direction prediction network is implemented by a temporal convolutional network to predict direction. Moreover, a 2D projection consistency loss is presented to constrain the motion displacement of joints between adjacent frames. Experiments indicated that the improved framework performs well in 9-frame receptive field. The study of graphs composed of real bones and virtual bones based on graph networks will be our future direction. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) The common skeleton of human with joints and bones for representation. (b) Schematic diagram of virtual bones, such as left-elbow to neck, pelvis to right-elbow, and left-knee to right-knee.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>The detailed structure of bone length prediction network. The input of this network is the 2D joint location x in random f frames from a video. F 0 is intermediate features. n is the number of residual blocks. F 1 k and F 2 k are intermediate features in the k-th residual block. x current is the 2D joint location in the current frame. The 3D joint location of all frames ?(x) and 3D joint locations of current frame ?(x current ) are obtained through the coarse joint location prediction network ?. Then, real bone length ? real (?(x)) and virtual bone length ? virtual (?(x current )) are obtained by calculating. Final real bones L real in the current frame are obtained with attention module. b is the batchsize. c is the number of feature dimension.(a) 5 virtual bones (b) 10 virtual bones (c) 13 virtual bones (d) 23 virtual bones Schematic diagram of virtual bones added to the network. (a) 5 bones from pelvis to the joints at the end of the human body. (b) 10 bones between the joints at the end of the human body. (c) 13 bones from pelvis to every non-adjacent joint. (d) 23 bones, the combination of (b) and (c). Head, Left-wrist, Left-ankle, Right-wrist, Right-ankle) and the root joint of the human body.Finally, 4 options are determined as shown inFigure 4. Option one, as shown inFigure 4(a), contains 5 virtual bones between the root joint and joints at the end of the human body. Option two, as shown inFigure 4(b), contains 10 virtual bones among joints at the end of the human body. Option three, as shown inFigure 4(c), contains 13 virtual bones between the root joint and other non-adjacent joints. Option four, as shown inFigure 4(d), contains 23 virtual bones mentioned inFigure 4(b) andFigure 4(c). Experiments are conducted with these options separately to compare their performance. The experiment detail is in Section IV-D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(u 0 , v 0 ) represents the position of the optical center of the camera. (X c ,? c ,? c ) is the estimated joint coordinates in the camera coordinate system. (?,v) represents the coordinates of the estimated 2D projection of the joint. d x , d y represent zoom factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of projection consistency loss and projection loss, where (a) and (b) are two possible trajectories of the 2D projection of a certain joint's estimated position. The single frame projection loss is exactly the same in (a) and (b), while the projection consistency loss is completely different.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Comparison under the metric Protocol 1 Methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Guangming Wang (Graduate Student Member, IEEE) received the B.S. degree from Department of Automation from Central South University, Changsha, China, in 2018. He is currently pursuing the Ph.D. degree in Control Science and Engineering with Shanghai Jiao Tong University. His current research interests include computer vision and SLAM, in particular, 3D human pose estimation. Honghao Zeng is currently pursuing the B.S. degree with the Department of Automation, Shanghai Jiao Tong University. His current research interests include computer vision and SLAM, in particular, 3D human pose estimation. Ziliang Wang is currently pursuing the B.S. degree with the Department of Automation, Shanghai Jiao Tong University. His current research interests include computer vision and SLAM, in particular, 3D human pose estimation. Zhe Liu received his B.S. degree in Automation from Tianjin University, Tianjin, China, in 2010, and Ph.D. degree in Control Technology and Control Engineering from Shanghai Jiao Tong University, Shanghai, China, in 2016. From 2017 to 2020, he was a Post-Doctoral Fellow with the Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong. He is currently a Research Associate with the Department of Computer Science and Technology, University of Cambridge. His research interests include autonomous mobile robot, multirobot cooperation and autonomous driving system. Hesheng Wang (Senior Member, IEEE) received the B.Eng. degree in electrical engineering from the Harbin Institute of Technology, Harbin, China, in 2002, and the M.Phil. and Ph.D. degrees in automation and computer-aided engineering from The Chinese University of Hong Kong, Hong Kong, in 2004 and 2007, respectively. He is currently a Professor with the Department of Automation, Shanghai Jiao Tong University, Shanghai, China. His current research interests include visual servoing, service robot, computer vision, and autonomous driving. Dr. Wang is an Associate Editor of IEEE Transactions on Automation Science and Engineering, IEEE Robotics and Automation Letters, Assembly Automation and the International Journal of Humanoid Robotics, a Technical Editor of the IEEE/ASME Transactions on Mechatronics, an Editor of Conference Editorial Board of IEEE Robotics and Automation Society. He served as an Associate Editor of the IEEE Transactions on Robotics from 2015 to 2019. He was the General Chair of IEEE ROBIO 2022 and IEEE RCAR 2016, and the Program Chair of the IEEE ROBIO 2014 and IEEE/ASME AIM 2019. He will be the General Chair of IEEE/RSJ IROS 2025.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I :</head><label>I</label><figDesc>Comparisons of the proposed method with other existing methods in all actions of Human3.6M dataset under the metrics Protocol 1, Protocol 2, and MPJVE. The results are based on 2D joint input from CPN<ref type="bibr" target="#b8">[9]</ref>.</figDesc><table><row><cell></cell><cell>Protocol 1</cell><cell>Protocol 2</cell><cell>Protocol 3</cell><cell>MPJVE</cell></row><row><cell>Martinez et al. [34]</cell><cell>45.5</cell><cell>37.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Hossain &amp; Little [38]</cell><cell>41.6</cell><cell>31.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Lee et al. [13]</cell><cell>38.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Pavllo et al. [9] (243 frames)</cell><cell>37.2</cell><cell>27.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Chen et al. [14] (9 frames)</cell><cell>38.0</cell><cell>28.2</cell><cell>37.3</cell><cell>1.96</cell></row><row><cell>Ours (9 frames)</cell><cell>35.4</cell><cell>27.2</cell><cell>34.7</cell><cell>1.92</cell></row><row><cell>Chen et al. [14] (243 frames)</cell><cell>34.0</cell><cell>25.9</cell><cell>33.3</cell><cell>1.71</cell></row><row><cell>Ours (243 frames)</cell><cell>32.5</cell><cell>25.2</cell><cell>31.9</cell><cell>1.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table II :</head><label>II</label><figDesc>The results of 9-frame and 243-frame model on Human3.6M with the ground truth 2D input.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>whereL represents the length of bones</figDesc><table><row><cell>w pl</cell><cell>w pd</cell><cell cols="4">Protocol 1 Protocol 2 Protocol 3 MPJVE</cell></row><row><cell>0</cell><cell>0.01</cell><cell>36.6</cell><cell>27.9</cell><cell>35.7</cell><cell>1.94</cell></row><row><cell>0</cell><cell>0.1</cell><cell>36.6</cell><cell>27.9</cell><cell>35.9</cell><cell>1.93</cell></row><row><cell>0</cell><cell>1</cell><cell>35.9</cell><cell>27.6</cell><cell>35.5</cell><cell>1.94</cell></row><row><cell>0</cell><cell>10</cell><cell>36.5</cell><cell>27.9</cell><cell>36</cell><cell>1.95</cell></row><row><cell>0.01</cell><cell>0</cell><cell>36.5</cell><cell>27.8</cell><cell>36</cell><cell>1.95</cell></row><row><cell>0.1</cell><cell>0</cell><cell>37</cell><cell>28.5</cell><cell>36.3</cell><cell>1.95</cell></row><row><cell>1</cell><cell>0</cell><cell>36.2</cell><cell>27.7</cell><cell>35.5</cell><cell>1.93</cell></row><row><cell>10</cell><cell>0</cell><cell>37.2</cell><cell>28.4</cell><cell>36.1</cell><cell>1.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III :</head><label>III</label><figDesc>Parameter sensitivity test of 9-frame model on Human3.6M dataset, using ground truth 2D joint positions as inputs. In the table, w pl means w proj?len and w pd means w proj?dir . estimated by attention network. L represents the ground truth of bone lengths.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Features and classification schemes for view-invariant and realtime human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A W</forename><surname>Talha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hammouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ghorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fleury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ambellouis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="894" to="902" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning bodily expression of emotion for social robots through human interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T V</forename><surname>Tuyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elibol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Chong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="30" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spherical interpolated convolutional network with distance-feature density for 3-d semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Map-less long-term localization in complex industrial environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assembly Automation</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="714" to="724" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Design and evaluation of a unique social perception system for humanrobot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaraki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pieroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">De</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazzei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garofalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cominelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Dehkordi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="355" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual servoing of flexible-link manipulators by considering vibration suppression without deformation measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Integrated task allocation and path coordination for large-scale robot networks with uncertainties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image-based visual impedance force control for contact aerial manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="895" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Trajectory space factorization for deep videobased 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08289</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Anatomy-aware 3d human pose estimation with bone-based pose decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="198" to="209" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="764" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on computer vision and Pattern recognition</title>
		<meeting>the IEEE/CVF Conference on computer vision and Pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiviewconsistent semi-supervised learning for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6907" to="6916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Moulding humans: Non-parametric 3d human shape estimation from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2232" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7093" to="7102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6951" to="6960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kidzinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13517</idno>
		<title level="m">3d pose detection in videos: Focusing on occlusion</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly-supervised 3d human pose learning via multi-view images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5243" to="5252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6173" to="6183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d human pose estimation using spatio-temporal networks with explicit occlusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="631" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d human pose estimation via explicit compositional depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reweighted sparse representation with residual compensation for 3d human pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="page" from="332" to="343" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2823" to="2832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Occluded joints recovery in 3d human pose estimation based on distance matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimized skeleton-based action recognition via sparsified graph regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Chirality nets for human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
