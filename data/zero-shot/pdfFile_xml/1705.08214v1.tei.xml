<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
							<email>michael@gifs.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shot boundary detection (SBD)</head><p>is an important component of many video analysis tasks, such as action recognition, video indexing, summarization and editing. Previous work typically used a combination of low-level features like color histograms, in conjunction with simple models such as SVMs. Instead, we propose to learn shot detection end-to-end, from pixels to final shot boundaries. For training such a model, we rely on our insight that all shot boundaries are generated. Thus, we create a dataset with one million frames and automatically generated transitions such as cuts, dissolves and fades. In order to efficiently analyze hours of videos, we propose a Convolutional Neural Network (CNN) which is fully convolutional in time, thus allowing to use a large temporal context without the need to repeatedly processing frames. With this architecture our method obtains state-of-the-art results while running at an unprecedented speed of more than 120x real-time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A shot of a video consists of consecutive frames which show a continuous progression of video and which are thus interrelated. The goal of shot boundary detection is to predict when such a shot starts or ends. Thus, it needs to detect transitions such as cuts, dissolves and fades. Representing a video as a set of shots is useful for many tasks and has been an important pre-processing step for automatic video analysis such as action recognition <ref type="bibr" target="#b14">[15]</ref> and video summarization <ref type="bibr" target="#b6">[7]</ref>. It is also useful when manually re-editing videos.</p><p>Due to the broad use of SBD, it has been researched for more than 25 years <ref type="bibr" target="#b1">[2]</ref> and many methods have been proposed, e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Typical methods approach SBD with a set of low-level features, such as color and edge histogram difference or SURF <ref type="bibr" target="#b4">[5]</ref>, in combination with Support Vector Machines (SVMs) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3]</ref>. To improve the comparison of different methods and boost process, the TRECVid initia-(a) Two adjacent frames from different shots.</p><p>(b) Adjacent frames with strong variation due to a flash, but coming from the same shot.</p><p>(c) Frames of a dissolve transition which are 0.5 seconds apart, but visually very similar. tive hosted a shot detection challenge for several years <ref type="bibr" target="#b10">[11]</ref>. Nonetheless, shot detection is not solved yet. While it may appear to be a simple problem, as humans can easily spot most shot changes, it is challenging for an algorithm. This is due to several reasons. First, video editors often try to make shot transitions subtle, so that they are not distracting from the video content. In some cases the transitions are completely hidden, such as in the movie Rope by Alfred Hitchcock <ref type="bibr" target="#b8">[9]</ref>. Second, videos show strong variations in content and motion speed. In particular fast mo-tion, leading to motion blur, is often falsely considered a shot change. Third, transitions such as dissolves have small frame to frame changes, making it difficult them with traditional methods. <ref type="figure" target="#fig_0">Figure 1</ref> shows some challenging cases.</p><p>To tackle these challenges we propose to learn shot boundary detection end-to-end, by means of a fully convolutional neural network. In order to train this network we create a new dataset with one million frames and automatically generated labels: transition or none-transition frames. For our dataset we generated hard cuts, crop cuts, dissolves, wipes and fade transitions. Additionally we generated none-transition examples, that come from the same shot, including frames where we added an artificial flash. This allows to make the network invariant to flashes, which were previously often falsely detected as cuts and corrected in a post-processing step <ref type="bibr" target="#b10">[11]</ref>.</p><p>In summary, we make the following contributions:</p><p>1. A way to generate a large-scale dataset for training shot detection algorithm without the need to manually annotate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A novel and highly efficient CNN architecture by making it fully-convolutional in time, inspired by fully convolutional architectures for image segmentation <ref type="bibr" target="#b9">[10]</ref>. Our method runs at 121x real-time speed on a Nvidia K80 GPU.</p><p>3. An empirical comparison to previous state of the art in terms of accuracy and speed. We show that we improve upon existing methods in both regards.</p><p>Concurrently to our work, Hassanien et al. <ref type="bibr" target="#b7">[8]</ref> also have used synthetic data and a CNN for shot detection. Their architecture is however based on <ref type="bibr" target="#b13">[14]</ref> and processes 16 frames at a time, with a stride of 8. Thus, it processes each frame twice, while our fully convolutional architecture avoids that. This, and our more compact CNN, allow our model to run at 121x real-time speed on a Nvidia K80 GPU, while <ref type="bibr" target="#b7">[8]</ref> runs at 19x real-time, using faster Nvidia Titan X GPUs. In addition, because <ref type="bibr" target="#b7">[8]</ref> classifies 16 frames at once, it cannot accurately localize shot transitions and thus requires a postprocessing step. Our fully convolutional architecture, on the other hand, predicts frame-accurate labels directly from pixels. <ref type="figure">Figure 2</ref> illustrates the advantage of our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>We pose shot boundary detection as a binary classification problem. The objective is to correctly predict if a frame is part of the same shot as the previous frame or not. From this output, it is trivial to obtain the final shot boundaries:</p><p>We simply assign all frames that are labelled as "same shot" to the same shot as the previous frame 1 .</p><p>prediction prediction prediction prediction prediction prediction prediction prediction prediction prediction input: 20 frames output: 11 predictions prediction prediction <ref type="figure">Figure 2</ref>: Our network architecture. Each frame-prediction is based on a context of 10 frames. By using a model that is fully convolutional in time, we can increase the input size and thus make e.g. 11 predictions by analyzing 20 frames or 91 predictions by analyzing 100 frames, etc., thus minimizing redundant computation.</p><p>Network architecture. To solve the classification problem described above, we propose to use a Convolutional Neural Network. We use spatio-temporal convolutions to allow the network to analyze changes over time. The network takes 10 frames as input, runs them through four layers of 3D convolutions, each followed by a ReLU, and finally classifies if the two center frames come from the same shot or if there is an ongoing transition. Using 10 frames as input provides context around the frames of interest, something that is important to correctly detect slow transitions such as dissolves. We use a small input resolution of 64x64 RGB frames for efficiency and since such low resolution are often sufficient for scene understanding <ref type="bibr" target="#b12">[13]</ref>. Our network consists of only 48698 trainable parameters. In <ref type="table">Table 1</ref> we show its architecture in more detail.</p><p>Fully convolutional in time. Our architecture is inspired by C3D <ref type="bibr" target="#b13">[14]</ref>, but is more compact. More importantly, rather than using fully connected layers, our model consists of 3D convolutions only, thus making the network fully convolutional in time. The network is given an input of 10 frames, and trained to predict if frame 6 is part of the same shot as frame 5. Due to its fully convolutional architecture, however, it also accepts larger temporal inputs. E.g. by providing 20 frames, the network would predict labels for frames 6 to 16, thus making redundant computation unnecessary (also see <ref type="figure">Figure 2</ref>). This allows to obtain large speedups at inference, as we are showing in our experiments.  <ref type="table">Table 1</ref>: Our network architecture. All our layers are convolutional and each is followed by a ReLU non-linearity. By using a fully convolutional architecture, we are able to increase the input size by n, thus reusing the shared parts of the convolutional feature map and improving efficiency. Implementation details. To train our model we use a cross-entropy loss, which we minimize with vanilla stochastic gradient descent (SGD). Our model is implemented in TensorFlow <ref type="bibr" target="#b0">[1]</ref>. At inference, we process the video in snipets of 100 frames, with an overlap of 9 frames. If a frame is part of a transition such as a dissolve, it is labelled as not the same shot as the previous, as it is part of a transition, not a shot. We found the learning to be more stable when using these kind of labels compared to predicting if a frame is from a new shot or part of a transition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training Data</head><p>To obtain a dataset large enough to train an end-to-end model we create a new dataset automatically. The dataset consists of 79 videos with few or no shots transitions and has a total duration of 3.5 hours. From this data we sample snippets of 10 frames, which will serve as the input to our model. To generate training data we combine some of these snippets with a transition. Thus, we have two types of training examples: (i) snippets consisting of frames from a single shot, i.e. non-transitions and (ii) transition snippets, which have a transition from one shot to another. Generated transitions. We generate the following transitions: Hard cuts, Crop cuts (a hard cut to a zoomed in version of the same scene), dissolves, fade-ins, fade-outs and wipes. Dissolves linearly interpolate between shots,  F1 score Speed Apostolidis et al. <ref type="bibr" target="#b2">[3]</ref> 0.84 3.3x (GPU) Baraldi et al. <ref type="bibr" target="#b3">[4]</ref> 0.84 7.7x (CPU) Song et al. <ref type="bibr" target="#b11">[12]</ref> 0.68 33.2x (CPU) Ours 0.88 121x (GPU) w/o fully conv. inference 13.9x (GPU) <ref type="table">Table 3</ref>: Performance and speed comparison on the RAI dataset <ref type="bibr" target="#b3">[4]</ref>. As can be seen our method significant outperforms previous works, while being significantly faster. We note, however, that <ref type="bibr" target="#b3">[4]</ref> uses a single-threaded CPU implementation, while we run our method on a GPU.</p><p>while fades linearly interpolate from or to a frame of a single color. In wipes a shot is moved out while the next shot is moved in, typically in horizontal direction. We show the used transitions and their parameters in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Flashes. Videos often contain flashes (c.f . <ref type="figure" target="#fig_0">Figure 1b)</ref>, which result in heavy frame-to-frame changes. This has typically posed problems to SBD methods, which were required to remove these false positives in a post-processing step <ref type="bibr" target="#b10">[11]</ref>. Instead, we choose a different approach: We add artificial flashes to the non-transition snippets to make the network invariant to these kind of changes. For this purpose we transform random frames by converting them to the LUV color space and increasing the intensity by 70%. <ref type="figure" target="#fig_1">Figure 3</ref> shows an example result of this procedure, compared to real flash.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on the publicly available RAI dataset <ref type="bibr" target="#b3">[4]</ref>. Thereby we follow <ref type="bibr" target="#b3">[4]</ref> and report F1 scores. It is computed as the harmonic mean between transition recall and precision. i.e. it measures how many shot changes are correctly detected or missed and how many false positives a method has. <ref type="table">Table 3</ref> summarizes the results. We now discuss these results in detail.</p><p>Performance. As can be seen from <ref type="table">Table 3</ref>, our method outperforms the previous state of the art methods on this   <ref type="bibr" target="#b3">[4]</ref>. Our method makes most errors on partial scene changes, where labelling a shot change depends on what is fore-and background, and has problems with fast motion. We also note that some cases are ambiguous, e.g. transitions in (a) and (b).</p><p>dataset. Our method improves the mean F1 score from 84% to 88%, thus reducing the errors by 25%. It obtains an accuracy of more than 90% in recall and precision on most videos (c.f . <ref type="table" target="#tab_4">Table 4</ref>). Lower precision stems from custom transitions such as partial cuts as shown in <ref type="figure" target="#fig_3">Figure 4e</ref>). Indeed, more than 90% of the false positives on videos 25011 and 25012 are due to these effects. These are however special cases and extremely hard to detect. First, if such partial transitions are considered a shot change typically depends on whether the foreground or background changes. Second, such transitions were not included into training. On common transitions such as hard cuts, fades and dissolves our method is extremely accurate.</p><p>Speed comparison. For measuring speed, we use a machine with an single Nvidia K80 GPU and 32 Intel Xeron   <ref type="figure" target="#fig_3">Figure 4</ref> for examples.</p><p>CPUs with 2.30GHz. We measured the speed of <ref type="bibr" target="#b11">[12]</ref> and our method on this machine, using the video 25012 of the RAI dataset. For <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> we used the timings provided in the paper instead. Thus, these numbers don't allow for an exact comparison, but rather give a coarse indication of the relative speed of the different methods. From <ref type="table">Table 3</ref> we can see that our method is much faster than previous methods. We also show that making the architecture fully convolutional is crucial for obtaining fast inference speed. To the best of our knowledge, our model it is the fastest shot detection to date. Indeed, as our network uses a small input resolution of 64x64, the current bottleneck is not the network itself, but rather bi-linear resizing in ffmpeg. When we resize the video to 64x64 prior to running our model, it obtains 235x real-time or ?5895 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we have introduced a novel method for shot boundary detection. Our CNN architecture is fully convolutional in time, thus allowing for shot detection at more than 120x real-time. We have compared our model against the state-of-the art on the RAI dataset <ref type="bibr" target="#b3">[4]</ref> and have shown that it outperforms previous works, while requiring a fraction of time. We also note that our model was not using any realworld shot transitions for training. Currently, our model makes three main errors, which we visualize in <ref type="figure" target="#fig_3">Figure 4</ref>: (i) missing long dissolves, which it was not trained with, (ii) partial scene changes and (iii) fast scenes with motion blur. In the future, we would like to include real data into training, such that the network can learn about rare and unusual shot transitions and become more robust to rapid scene progression. Furthermore we will evaluate our method on the TRECVid that, in order to be able to compare to <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head><p>I would like to thank the gifs.com team for useful discussions and support, as well as Eirikur Agustsson, Anna Volokitin, Jordi Pont-Tuset, Santiago Manen.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The challenges of shot detection. Understanding if a scene shows strong variation or if a shot change occurs is often difficult.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Artificially generated flash compared a real flash. Left: Input frame. Middle: Frame with artificial flash. Right: The next frame which has a real flash.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Missed partial dissolve (annotated as a transition).(b) Falsely detected partial hard cut (annotated as no transition). (c) Missed slow dissolve (first and last frame are 1 second apart) (d) False positive due to fast motion with motion blur) (e) Falsely detected partial hard cut (annotated as no transition).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Error cases on the RAI dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The generated transitions and their parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Per video results on the RAI dataset. We obtain Recall and Precision &gt; 0.9 on most videos. Videos with lower precision or recall have custom transitions or animations. See</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This is in contrast to<ref type="bibr" target="#b7">[8]</ref>, which uses an additional SVM and false positive suppression based on color histogram difference.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video indexing using motion vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akutsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tonomura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ohba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications in Optical Science and Engineering</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast shot segmentation combining global and local visual descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Apostolidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shot and scene detection via hierarchical clustering for re-using broadcast video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CAIP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Speeded-up robust features (SURF)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>CVIU. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Information theorybased shot cut/fade detection and video summarization. Circuits and systems for video technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cernekova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nikou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video summarization by learning submodular mixtures of objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Large-scale, fast and accurate shot boundary detection through spatio-temporal convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassanien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hefeeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03281</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hitchcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laurents</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rope. Universal Studios</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Video shot boundary detection: Seven years of trecvid activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Doherty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">To click or not to click: Automatic selection of beautiful thumbnails from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Redi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vallmitjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
