<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRI: General Reinforced Imitation and its Application to Camera-Based Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rapha?l</forename><surname>Chekroun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mines Paris -PSL University</orgName>
								<address>
									<addrLine>2 Valeo DAR</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Toromanoff</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mines Paris -PSL University</orgName>
								<address>
									<addrLine>2 Valeo DAR</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Hornauer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mines Paris -PSL University</orgName>
								<address>
									<addrLine>2 Valeo DAR</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Moutarde</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mines Paris -PSL University</orgName>
								<address>
									<addrLine>2 Valeo DAR</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GRI: General Reinforced Imitation and its Application to Camera-Based Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Autonomous driving (AD) in urban areas is a convoluted task. Agents have to efficiently analyse a highly complex environment and make online decisions to follow driving rules whilst simultaneously interacting with other dynamic agents, such as drivers or pedestrians. That is why, literature in autonomous driving focuses on different learning methods rather than the design of general hand-crafted rules.</p><p>Imitation learning (IL) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, especially behavior cloning, aims at mimicking expert behavior for a given task. It requires a significant amount of annotated data, often recorded by human drivers. Even though this kind of data can be recorded easily on a large scale, practical safety concerns in real traffic lead to heavily biased observations showing predominantly safe driving examples, and underrepresents rare dangerous situations. Hence, IL agents suffer from distribution mismatch and will struggle to recover from its own mistakes.</p><p>Deep reinforcement learning (DRL) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> offers an alternative, more robust to distribution mismatch than IL, by letting the agent learn from its own mistakes through trial-and-error. In the RL framework, the agent explores its environment by itself and gathers rewards, a numerical value assessing how much a given action in a given state is good. The goal of the agent is to maximize its cumulative rewards. To do so, the agent needs to optimize sequences of actions rather than instantaneous ones. Nonetheless, DRL needs an order of magnitude more data than IL to converge due to this extensive, and often time-consuming, exploration of the environment during the training.</p><p>To overcome IL distribution mismatch and RL data inefficiency, we propose General Reinforced Imitation (GRI), a novel method which combines both exploration and prior knowledge from demonstrations. GRI is based on the simplifying hypothesis that expert data presents a perfect behavior and therefore, an expert's action should receive a constant, high reward. Straightforward to implement over any offpolicy algorithm, GRI introduces the notion of an offline demonstration agent. This offline agent sends expert data associated with a constant demonstration reward to the replay buffer of an RL online exploration agent. We note that those expert data are processed by the DRL algorithm concurrently and indistinguishably from the exploration data</p><p>The GRI method is applied to visual-based autonomous driving in an end-to-end pipeline on the CARLA simulator <ref type="bibr" target="#b8">[9]</ref>, an open-source simulator for research in autonomous driving. We call this algorithm GRI for Autonomous Driving (GRIAD). The whole pipeline is represented in <ref type="figure">Fig. 1</ref>. On the CARLA Leaderboard, an online benchmark ranking agents according to the quality of their driving, we achieved 17% better results than World on Rails <ref type="bibr" target="#b9">[10]</ref>, the prior top ranking entry. In addition, our method used only three cameras and no LiDAR, which is fewer sensors than the other top entries <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b2">[3]</ref>. At the time of writing (February 2022), GRIAD is the best camera-based agent on the CARLA Leaderboard according to the main metric, the driving score. We also conducted ablation studies to highlight the impact of GRIAD compared to standard RL training on the CARLA NoCrash Benchmark <ref type="bibr" target="#b10">[11]</ref>.</p><p>Finally, we conducted experiments on the Mujoco <ref type="bibr" target="#b12">[12]</ref> benchmark to investigate our method adaptability and generalizability. Tests were conducted on four different Mujoco environments, with two different DRL algorithm as backbones. Our experiments demonstrate that using the GRI framework systematically leads to better results, even when the expert data are noisy or not significantly better than the trained vanilla RL algorithm.</p><p>We summarize our main contributions below:</p><p>? Definition of the novel GRI method to combine offline demonstrations and online exploration. ? Presentation and ablation study of GRI for visual-based Autonomous Driving (GRIAD) algorithm. ? Further analysis of GRI-based algorithms on the Mujoco benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The GRI method aims at leveraging both offline expert demonstrations and online simulator exploration. Our main application is end-to-end camera-based autonomous driving on the CARLA simulator <ref type="bibr" target="#b8">[9]</ref>. Therefore, this section focuses both on end-to-end autonomous driving methods that achieved milestones on CARLA, and existing decisionmaking methods learning from demonstration and exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. End-to-end Autonomous Driving on CARLA</head><p>End-to-end autonomous driving, i.e. directly mapping sensor signals to control is a highly complex task on which training an agent with DRL is tedious. IL methods were the first to lead the CARLA Leaderboard. In particular, Learning by Cheating (LBC) <ref type="bibr" target="#b13">[13]</ref> presents an efficient method to train a behavior cloning agent in two steps: (i) train a privileged behavior cloning agent which has access to all the ground truth data, and (ii) train a behavior cloning agent to mimic the privileged one. An evaluation of several methods on the NoCrash benchmark, presented in Chen et al. <ref type="bibr" target="#b9">[10]</ref>, shows that LBC presents great results on the training conditions but generalizes poorly on unknown environments. DRL can also be used for end-to-end autonomous driving. However, camera-based DRL comes with some drawbacks. Indeed, image inputs are often of high dimensions thus requiring larger DRL networks which are usually difficult to train to convergence. Therefore, for camera-based DRL, one can encode the sensors' signal in a more compact and semantically rich representation to train the DRL network on this predefined latent space as in D. Gordon et al. <ref type="bibr" target="#b14">[14]</ref>. This latent space can be obtained by pretraining a visual encoder on some visual tasks, such as segmentation or classification.</p><p>Based on this principle, Toromanoff et al. <ref type="bibr" target="#b15">[15]</ref> introduced the Implicit Affordances (IAs) method. They design and train an efficient DRL agent on CARLA, winning the CARLA challenge two years in a row. To do so, they propose an endto-end pipeline composed of two subsystems trained successively. First, a visual encoder is trained on some auxiliary tasks. Those tasks are semantic segmentation, classification of the type of road, detection of traffic lights, and if there is a relevant traffic light, the state of, and distance to, the light. Then, the visual encoder is frozen and the DRL-based decision-making subsystem is trained on the encoder latent space.</p><p>Another top ranked camera-based agent on the CARLA Leaderboard is World on Rails <ref type="bibr" target="#b9">[10]</ref> which assumes the world to be on rails, meaning that the agent's actions affect only its own state and do not influence its environment. Based on that hypothesis, they transpose the driving problem into a simple, yet powerful, tabular RL setup.</p><p>Finally, Transfuser <ref type="bibr" target="#b2">[3]</ref> and Learning from All Vehicles, more recent top ranked agents on the CARLA Leaderboard, mainly focuses on LiDAR and camera fusion.</p><p>Other concurrent work combines an RL driving coach and an IL learner, mediated with a learned bird's map <ref type="bibr" target="#b16">[16]</ref> but are not currently in the CARLA Leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning from demonstration and exploration</head><p>The aforementioned IL and RL strengths and weaknesses are complementary. Indeed, IL suffers from distribution mismatch contrarily to online RL. Alternatively, as RL learns from scratch it is less data efficient than IL which incorporates prior demonstration knowledge during training.</p><p>To take the best of both worlds, some algorithms combine IL and RL to maximize efficacy by leveraging both expert data and exploration <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b19">[18]</ref>, <ref type="bibr" target="#b20">[19]</ref>, <ref type="bibr" target="#b21">[20]</ref>. In particular, demonstrations can be used to initialize policies by pretraining the network <ref type="bibr" target="#b22">[21]</ref>, <ref type="bibr" target="#b20">[19]</ref>, <ref type="bibr" target="#b17">[17]</ref> or leveraged with a specifically designed reward <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b19">[18]</ref>.</p><p>Soft-Q Imitation Learning (SQIL) <ref type="bibr" target="#b19">[18]</ref> and Deep Qlearning from Demonstrations (DQfD) <ref type="bibr" target="#b17">[17]</ref> are the two closest approaches to ours as both take advantage of demonstrations in a different way and can be applied to any offpolicy RL algorithms.</p><p>SQIL <ref type="bibr" target="#b19">[18]</ref> does IL using an RL agent. To do so, the replay buffer is initially filled with demonstrations, associated with a constant reward r demo = 1. A RL agent collects data from exploration into the replay buffer, associated with a constant reward r explo = 0. Thus, SQIL designed an RL agent that learns to imitate expert behavior, and has been mathematically demonstrated to be equivalent to regularized behavior cloning. However, SQIL does not efficiently leverage exploration as environment rewards are never used. Our method combines both the IL part from SQIL and the classical, rich RL online exploration.</p><p>DQfD <ref type="bibr" target="#b17">[17]</ref> is based on DQN <ref type="bibr" target="#b4">[5]</ref>, an off-policy RL algorithm with a replay buffer. DQfD first pretrains the agent on expert data with both IL and RL losses using the real reward given by the environment. After some steps of pretraining, the agent starts gathering data from the environment in the memory buffer. The network is then trained on batches composed of exploration data with a RL loss and expert data with both IL and RL losses. Nonetheless, DQfD uses simultaneously reinforcement and imitation which can have divergent losses and are difficult to jointly optimize <ref type="bibr" target="#b23">[22]</ref>. Our method leverages demonstrations and exploration exclusively with an RL loss, and thus cannot suffer from divergent losses issue. Moreover, DQfD, contrarily to GRI, relies on the true environment reward for the expert data, which cannot always be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GENERAL REINFORCED IMITATION (GRI)</head><p>Our pipeline for autonomous driving is an end-to-end system. Its decision-making subsystem uses the GRIAD algorithm, an adaptation of the GRI method to visual-based autonomous driving (AD) on CARLA. This section presents the GRI method and details the whole pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Method</head><p>GRI is a method which is straightforward to implement over any off-policy RL algorithm using a replay buffer, such as SAC <ref type="bibr" target="#b24">[23]</ref>, DDPG <ref type="bibr" target="#b25">[24]</ref>, DQN <ref type="bibr" target="#b4">[5]</ref> and its successive improvements <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b27">[26]</ref>. GRI is built upon the hypothesis that expert demonstrations can be seen as perfect data whose underlying policy gets a constant high reward. We denote this as demonstration reward, r demo . In our experiments we chose r demo to be the maximum of the reward. The idea of GRI is to distill expert knowledge from demonstrations into an RL agent during the training phase. To do so, we defined two types of agents: (i) the online exploration agent, which is the regular RL agent exploring its environment to gather experiences (s online t , a t , r t , s online t+1 ) into the memory buffer, and (ii) the offline demonstration agent which sends expert data associated with a constant demonstration reward (s offline t , a t , r demo , s offline t+1 ) to the memory buffer. s t is the state, a t the chosen action and r t the reward at time t. At any given training step, the next episode to add to the replay buffer comes from the demonstration agent with a probability of p demo , else from the exploration agent. GRI is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. GRI for Autonomous Driving</head><p>We applied GRI on a pipeline inspired by Toromanoff et al. Implicit Affordances method <ref type="bibr" target="#b15">[15]</ref>. As this method is trained in two phases, hence making it modular, we optimized both the visual and the decision-making subsystems independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design of the vision subsystem</head><p>We first train two visual encoders on segmentation and classifications tasks with different camera perspectives to extract compact semantic features.</p><p>We found that single camera setup is leading to more collisions on intersections, as sensors are not able to see close obstacles while turning. Thus, we mounted three RGB cameras on the hood of our agent vehicle, at the coordinates x = 2.5m, z = 1.2m and y ? {?0.8, 0, 0.8}m relatively to the center of the car. The side cameras are angled at 70?. All three cameras have a 100?field of view.</p><p>Our visual subsystem is composed of two highly specialized Efficientnet-b1 <ref type="bibr" target="#b28">[27]</ref> models, one for the segmentation and one for the classification and regression tasks, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. We concatenate the four outputs (three segmentations, one for each camera, and one classification from the front camera only) of both Efficientnet-b1 and use it similarly as Implicit Affordances (IAs) for the DRL training.  This architecture allows to keep the same accuracy on classification and segmentation metrics as if we were using a single bigger encoder for all the auxiliary tasks like in Toromanoff et al. <ref type="bibr" target="#b15">[15]</ref>, while reducing the encoder latent space dimension by a factor of ? 5.</p><p>The visual part for the CARLA Leaderboard has been trained on a dataset of 400,000 samples which corresponds to 44 hours of driving. This dataset has been generated with the CARLA autopilot on every town with random trajectories. Each sample of the dataset is composed of three images from the three cameras and the corresponding ground truth information, which are segmentation maps from CARLA, booleans indicating the presence of an intersection and the presence of a traffic light in front of the car. And if there is a traffic light, a class corresponding to its color, and the distance to it in meters. Trajectories have been augmented with random cameras translations and rotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design of the Decision Subsystem</head><p>The decision subsystem takes as input four consecutive encodings of the three camera images and outputs an action. Therefore, a state contains visual features from the last 300 milliseconds, as the simulator runs at 10 FPS. An action is defined by the combination of the desired steering of the wheel, and the throttle or brake to apply.</p><p>Generating data on the CARLA simulator is very computationally expensive. We used a Rainbow-IQN Ape-X <ref type="bibr" target="#b29">[28]</ref>, which is a distributed DRL backbone, to mitigate this issue.</p><p>Due to Rainbow-IQN Ape-X being based on DQN <ref type="bibr" target="#b4">[5]</ref> the action state has to be discrete. Therefore, we discretized the action state in 27 steering values, and 4 throttle or brake values. The discretized action space contains 27 ? 4 = 108 actions.</p><p>We called this setup GRI for Autonomous Driving (GRIAD). We diagram it in <ref type="figure" target="#fig_1">Fig. 3</ref>  The demonstration dataset contains 200,000 samples, which correspond to 22 hours of driving, generated using the autopilot from CARLA on predefined tracks published by the CARLA team 1 . Each sample from the demonstration dataset consists of three images from the three cameras and a discrete action obtained by mapping continuous actions of the expert to our discrete set of RL actions. We did not use any data augmentation. We note that the autopilot makes driving errors such as collisions, red light infractions, or the car getting stuck for hundreds of frames. As a result ? 10% of our demonstrations correspond to poor action choices. However, we decided to use this demonstration dataset as it is in order to assess the robustness of our method to noisy demonstrations.</p><p>In our experiments on CARLA, GRIAD had a total of 12 agents, including 3 demonstration agents, running in a distributed setup and sending data to the memory buffer. As demonstration agents have been constrained to send data at the same frequency as exploration agents, this is equivalent to having p demo = 25%.</p><p>The reward function used for the exploration agents is the same as in Toromanoff et al. <ref type="bibr" target="#b15">[15]</ref>. Since this reward has a range between 0 and 1, we set the demonstration reward to r demo = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>The GRI method was assessed on its primary application of visual-based autonomous driving on the CARLA Leaderboard and with an ablation study comparing it to vanilla RL. Further studies of the method have also been conducted on the Mujoco benchmark to analyze its behavior depending on the proportion of demonstration agents and highlight its generalizability to other DRL backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. GRIAD on CARLA</head><p>On the CARLA leaderboard. We trained GRIAD for 60M steps (?45M exploration steps + 200,000 expert data sampled ?15M times). Both visual and decision-making parts were trained on all available maps with all available weather. We compare to LBC <ref type="bibr" target="#b13">[13]</ref>, IAs <ref type="bibr" target="#b15">[15]</ref>, Transfuser+ <ref type="bibr" target="#b2">[3]</ref> and World on Rails <ref type="bibr" target="#b9">[10]</ref>. Our method outperforms World on Rails, the previous leading method on the CARLA leaderboard, by ? 17% on the main metric, the driving score, while using fewer sensors. However, more recent LiDAR-based methods give significantly better results, but cannot be compared directly as inputs are different. CARLA Leaderboard results are presented in <ref type="table" target="#tab_4">Table I</ref>  Ablation study on the NoCrash benchmark. We compared GRIAD to regular RL by using the same architecture with and without demonstration agents on the NoCrash benchmark <ref type="bibr" target="#b10">[11]</ref>. To do so, agents are trained on a single environment (Town01) under a specific set of training weather. Then, agents are evaluated on several scenarios with different traffic density on the training (Town01) and test (Town02) town with training and test sets of weather.  For these experiments, GRIAD was trained on 16M samples corresponding to 12M exploration steps + 25,000 expert data which have been sampled 4M times in total. We present an ablation study to show how GRIAD compares to RL without GRI i.e. without demonstration agents, using two vanilla RL models: one trained on 12M exploration steps and the other on 16M exploration steps. Each agent was trained using the exact same visual encoder trained on another demonstration dataset of 100,000 samples coming exclusively from Town01 under training weather. Results are presented in <ref type="table" target="#tab_4">Table II</ref>.</p><p>We first observe that GRIAD systematically gives better results than RL with 12M steps, while taking approximately the same time to train (+? 4%). Indeed, as demonstration agents do not require any interaction with the simulator, we can add them at a negligible cost and still improve results.</p><p>We also observe that while RL with 16M steps does better than GRIAD on train weather, GRIAD gives better results on the test weather while being ? 25% faster to train. We believe this is because RL tends to overfit on a given environment if it explores it too much. Hence, replacing 4M exploration data with 25,000 demonstration data sampled ? 160 times each appears to reduce the overfitting and allows a better generalization.</p><p>We also trained the same pipeline using the SQIL method during 20M steps, but the evaluation reward stayed particularly low during training. First test showed SQIL to be inefficient for autonomous driving on CARLA as it did not learn to drive at all, staying static or drifting off the road most of the time. It reached the score of 0 on every evaluated tasks. We believe that the reward signal as defined by SQIL is not rich enough to allow the network to converge on such a highly complex task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. GRI on the Mujoco benchmark</head><p>To further validate the GRI method we conducted experiments on selected Mujoco <ref type="bibr" target="#b12">[12]</ref> environments, shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Expert data were generated using chainerrl <ref type="bibr" target="#b30">[29]</ref> pretrained RL agent weights and contain 200,000 samples. For each environment, the value of r demo was chosen as the highest value chainerrl expert agent reached during the generation of the dataset. As we did not find real expert data on Mujoco environments, expert data is not always significantly better than our trained vanilla RL network. Hence, this study assesses the efficiency of GRI even with suboptimal expert data. Study on the proportion of demonstration agents. Experiments being faster on Mujoco environments than on CARLA we were able to investigate the impact of the proportion of demonstration agents. For these experiments we used a GRI-SAC i.e. a GRI algorithm using SAC <ref type="bibr" target="#b24">[23]</ref> as DRL backbone, and we vary the proportion of demonstration <ref type="figure">Fig. 5</ref>: Evolution of the evaluation reward on Mujoco environments with different proportions of demonstration agents with GRI-SAC. GRI-SAC with 0% demonstration agent is vanilla SAC. We observe that GRI-SAC always reaches the level of the expert even when the expert is significantly better than the trained vanilla SAC. The proportion of demonstration agent have a significant impact on the dynamic of the convergence. agents between 0% and 40%. Each experiment has been repeated three times, with different seeds. <ref type="figure">Fig. 5</ref> presents the results with the variances and the evaluation reward of the expert. Experiments were conducted with public code from GitHub 2 which has been adapted with GRI.</p><p>We observe three different dynamics.</p><p>? For HalfCheetah-v2, a difficult task on which the expert is significantly stronger than the trained SAC, we observe that the beginning of the training is slower using GRI-SAC; we call this a warm up phase which we will explain further in Section IV-C. However the rewards turns out to become significantly higher after some time. On this game, GRI-SAC is better than SAC with every proportion of demonstration agents. Best scores were reached with 10% and 20% of demonstration agents. ? For Humanoid-v2, a difficult task on which the expert is just a little stronger than the trained SAC, we observe that the higher the number of demonstration agents is, the longer the warm up phase is. Nonetheless, GRI-SAC models end up having higher rewards after their warm up phase. Best scores are reached with 10% and 20% of demonstration agents. ? Ant-v2 and Walker2d-v2 are the easiest tasks of the four evaluated. On Ant-v2 the SAC agent reaches the expert level, converging similarly as GRI-SAC regardless of the number of demonstration agents used. Nevertheless, <ref type="bibr" target="#b1">2</ref> Original code from https://github.com/dongminlee94/ deep_rl GRI-SAC converges faster with 10% and 20% demonstration agents. On Walker2d-v2 the final reward of GRI-SAC is significantly higher and reaches the expert level, while SAC remains below. More experiments were conducted with the proportion of demonstration agents varying between 50% and 90%. Results were significantly worse than using 20% demonstration agents. We therefore conclude that the proportion of demonstration agent should not exceed 50%. We discuss some qualitative insights in the limitation sections.</p><p>These experiments reveals, at least on the evaluated Mujoco environments, that 20% demonstration agents seems to be the best choice for GRI-SAC to reach the expert level.</p><p>1) GRI with DDPG as DRL backbone: We also investigated the contribution of the DRL backbone to assess the generalizability of the GRI method. To do so, we evaluated the same tasks with the Deep Deterministic Policy Gradient (DDPG) algorithm <ref type="bibr" target="#b25">[24]</ref> instead of SAC. For these experiments, we fixed the proportion of demonstration agents to 20%. Results are shown in <ref type="figure">Fig. 6</ref>.</p><p>We observe that, alike to GRI-SAC with a proportion of 20% demonstration agents, GRI-DDPG reaches better results than DDPG on all the tested environments. However, GRI-DDPG does not systematically reach the level of the expert. While final rewards are better with SAC and GRI-SAC, the dynamics of the rewards evolution is about the same with both backbones, cf. <ref type="figure">Fig. 5</ref>. We can conclude that GRI is easily adaptable and generalizes to locomotion tasks where <ref type="figure">Fig. 6</ref>: Comparaison of the evaluation reward evolution on Mujoco environments between GRI-DDPG with 20% of demonstration agents and vanilla DDPG. GRI-DDPG systematically leads to a better reward than vanilla DDPG. However, contrarily to GRI-SAC, GRI-DDPG with 20% demonstration agents does not systematically reach the expert level. it robustly outperforms two alternative methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Limitations</head><p>The main limitations of this method are consequences of our initial hypothesis that demonstration data can always be associated with a constant maximal reward r demo .</p><p>A first limitation occurs if the demonstration data is not constantly optimal, e.g. due to low expert performance on some aspect of a given task, as this introduces noise in the reward function. This is the case in our demonstration dataset on the CARLA simulator, as expert data have been generated using an imperfect autopilot containing ? 10% noisy demonstrations. Still, GRIAD showed to improve our model by a significant margin over vanilla RL. Therefore, we can consider the GRI setup to present some robustness to noisy demonstrations.</p><p>A second limitation of our approach is the warm-up phase on some difficult environments, as observed in <ref type="figure">Fig. 5</ref> on HalfCheetah-v2 and Humanoid-v2. This warm up phase can be seen as the consequence of a distribution shift. Indeed, GRI suffers from a sort of distribution shift when the training expert data mostly represent actions made in states not reached yet by the exploration agents. In particular, we observed this effect on HalfCheetah-v2: the expert agent does not walk but jumps as soon as it touches the ground, which is a complex yet highly efficient strategy. But to reach a state where it can successfully jump, it needs some warm up to gain the required speed and momentum by doing some low reward actions. Hence, our GRI-SAC agents learns to jump before it is able to walk, making it fall. Once the agent learned how to reach the jumping state, reward steadily increase until convergence. However, we observe that the lower the proportion of demonstration agents is, the faster the model is able to recover from this distribution shift. Indeed, collecting more exploration data following the current agent policy compensates the distribution shift between demonstration and exploration data.</p><p>Finally, a third limitation of our approach is the inconsistency of the rewards associated to some common actions collected by both the demonstration and exploration agents. Still on the HalfCheetah-v2 example, the demonstration agent will reward expert actions at the beginning of the agent run with the high demonstration reward, while the exploration agent will receive poor reward for the same exact actions. This induces a sort of discrepancy between data coming from the offline demonstration agent and experiences coming from the online RL exploration agent. It also implies an overestimation of demonstration actions. However, allocating high reward to demonstration data which are not correlated with the actual reward of the environment might encourage the agent to get to states closer to the expert's ones. Nonetheless, it is difficult to assess the impact on the training in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We present GRI, a method that efficiently leverages both expert demonstrations and environment exploration. GRI is straightforward to implement over any off-policy deep reinforcement learning algorithm. GRI-based algorithms improve data efficiency compared to vanilla reinforcement algorithms and do not suffer from distribution shift as much as imitation learning methods. This method also proved to be robust to noisy demonstrations in the expert dataset. We applied GRI to autonomous driving with the distributed GRIAD algorithm and outperformed the previous camera-based state-of-theart on the CARLA Leaderboard. Finally, we showed its generalizability using different DRL backbones on several Mujoco continuous control environments and highlighted its robustness. In future work, we plan on focusing on LiDAR and camera fusion for GRIAD, as it recently showed to significantly improve the driving quality on CARLA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Feature extraction from RGB camera images for the visual subsystem. Two encoder-decoder networks are pretrained on segmentation, classifications and regression tasks. Classifications and regression are only performed on the center image while all three images are segmented. After training, the visual encoders serve as fixed feature extractors with frozen weights. For the DRL backbone training, both encoder outputs are concatenated and sent to the memory buffer as input to DRL. Both encoders are Efficientnet-b1. The segmentation decoder is fully convolutional, and the classification decoder is an MLP with several outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Simplified representation of the distributed GRIAD setup with a Rainbow-IQN Ape-X backbone. A central computer receives data in a shared replay buffer from both exploration and demonstration agents running on other computers. Data are sampled from this replay buffer to make the backpropagation and update the weights of all the agents. Images from the agents are encoded using the network presented inFig. 2before being stored in the memory buffer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Mujoco environments used for our experiments. Respectively HalfCheetah-v2, Humanoid-v2, Ant-v2, and Walker2d-v2. Articulations are controlled to make them walk. Rewards depends on the covered distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>GRI: General Reinforced Imitation Input: r demo demonstration reward value, p demo probability to use demonstration agent; Initialize empty buffer B; while not converged do if len(B) ? min buffer then do a DRL network update; end if random.random() ? p demo then collect episode (s online</figDesc><table><row><cell></cell><cell>, a t , r t , s online t+1</cell><cell cols="2">) t in</cell></row><row><cell cols="2">buffer B with exploration agent</cell><cell></cell></row><row><cell>else</cell><cell></cell><cell></cell></row><row><cell>add episode (s t offline</cell><cell cols="2">, a t , r demo , s t+1 offline</cell><cell>) t in</cell></row><row><cell cols="2">buffer B with demonstration agent;</cell><cell></cell></row><row><cell>end</cell><cell></cell><cell></cell></row><row><cell>end</cell><cell></cell><cell></cell></row></table><note>t</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.</figDesc><table><row><cell>CARLA</cell><cell>CARLA</cell></row><row><cell>+</cell><cell>+</cell></row><row><cell>exploration</cell><cell>exploration</cell></row><row><cell>agents</cell><cell>agents</cell></row><row><cell>Memory</cell><cell>Rainbow-IQN Ape-X</cell></row><row><cell>buffer</cell><cell></cell></row><row><cell>Demonstration</cell><cell></cell></row><row><cell>data</cell><cell></cell></row><row><cell></cell><cell>CARLA</cell></row><row><cell>Demonstration</cell><cell>+</cell></row><row><cell>agents</cell><cell>exploration</cell></row><row><cell></cell><cell>agents</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Method</cell><cell>Cam. LiDAR</cell><cell>DS</cell><cell>RC</cell><cell>IS</cell></row><row><cell>GRIAD (ours)</cell><cell>3</cell><cell cols="3">36.79 61.85 0.60</cell></row><row><cell>Rails [10]</cell><cell>4</cell><cell cols="3">31.37 57.65 0.56</cell></row><row><cell>IAs [15]</cell><cell>1</cell><cell cols="3">24.98 46.97 0.52</cell></row><row><cell>LBC [13]</cell><cell>3</cell><cell>10.9</cell><cell>21.3</cell><cell>0.55</cell></row><row><cell>LAV</cell><cell>4</cell><cell>61.8</cell><cell>94.5</cell><cell>0.64</cell></row><row><cell>Transfuser+ [3]</cell><cell>4</cell><cell>50.5</cell><cell>73.8</cell><cell>0.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table /><note>Comparison of camera-based and LiDAR-based agents on CARLA Leaderboard's driving metrics: driving score (DS, main metric), route completion (RC), and infraction score (IS). Results from the public CARLA Leaderboard website on February 2022. Higher is better for all metrics. Our method improves the driving score by 17% relative to the prior camera-based state-of-the-art [10] while using fewer sensors than the two other best.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE II :</head><label>II</label><figDesc>Ablation study of GRIAD using the NoCrash benchmark. Mean and standard deviation over 3 evaluation seeds. Score is the percentage of road completed without any crash. GRIAD experimentally shows to generalize more on test weather than RL with 12M and 16M steps and globally gives the best agent. We note that, for computational reason, neither the RL nor GRIAD was trained until convergence. Hence, comparison to the state-of-the-art should rather be done on the CARLA Leaderboard with the fully trained GRIAD model, cfTable I.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/carla-simulator/scenario_ runner</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End to end learning for selfdriving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zieba</surname></persName>
		</author>
		<idno>abs/1604.07316</idno>
		<ptr target="http://arxiv.org/abs/1604.07316" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Osa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pajarinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-modal fusion transformer for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End to end vehicle lateral control using a single fisheye camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vejarano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3613" to="3619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<ptr target="http://www.nature.com/articles/nature14236" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno>abs/1707.06347</idno>
		<ptr target="http://dblp.uni-trier.de/db/journals/corr/corr1707.html#SchulmanWDRK17" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Addressing function approximation error in actor-critic methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v80/fujimoto18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, J. Dy and A. Krause</editor>
		<meeting>the 35th International Conference on Machine Learning, ser. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v48/mniha16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, M. F. Balcan and K. Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning, ser. Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016-06-22" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Annual Conference on Robot Learning</title>
		<meeting>the 1st Annual Conference on Robot Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note>license: CC-BY</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to drive from a world on rails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring the Limitations of Behavior Cloning for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korea</forename><surname>Seoul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-10" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="9328" to="9337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning by cheating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/9009082/" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Seoul, Korea (South</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="1022" to="1031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end model-free reinforcement learning for urban driving using implicit affordances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-toend urban driving by imitating a reinforcement learning coach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liniger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning from demonstrations for real world reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vecer?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sendonaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<idno>abs/1704.03732</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<ptr target="http://arxiv.org/abs/1704.03732" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SQIL: imitation learning via regularized behavioral cloning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.11108" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning complex dexterous manipulation with deep reinforcement learning and demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>abs/1709.10087</idno>
		<ptr target="http://arxiv.org/abs/1709.10087" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning from demonstrations with sacr2: Soft actor-critic with reward relabeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chekroun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14464</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural Task Programming: Learning to Generalize Across Hierarchical Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/8460689/" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting><address><addrLine>Brisbane, QLD</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-05" />
			<biblScope unit="page" from="3795" to="3802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reinforcement learning from imperfect demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1802.05313</idno>
		<ptr target="http://arxiv.org/abs/1802.05313" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJjvxl-Cb" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1509.02971" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<editor>Y. Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno>abs/1710.02298</idno>
		<ptr target="http://arxiv.org/abs/1710.02298" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Implicit quantile networks for distributional reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
		<editor>J. Dy and A. Krause</editor>
		<meeting>the 35th International Conference on Machine Learning, ser. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1096" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, K. Chaudhuri and R. Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ser. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Is Deep Reinforcement Learning Really Superhuman on Atari</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
		<ptr target="https://hal-mines-paristech.archives-ouvertes.fr/hal-02368263" />
	</analytic>
	<monogr>
		<title level="m">Deep Reinforcement Learning Workshop of 39th Conference on Neural Information Processing Systems (Neurips&apos;2019)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Chainerrl: A deep reinforcement learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ishikawa</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v22/20-376.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">77</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
