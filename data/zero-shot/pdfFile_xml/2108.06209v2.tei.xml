<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">W2V-BERT: COMBINING CONTRASTIVE LEARNING AND MASKED LANGUAGE MODELING FOR SELF-SUPERVISED SPEECH PRE-TRAINING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
							<email>weihan@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
							<email>chungchengc@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
							<email>jamesqin@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
							<email>rpang@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
							<email>yonghui@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">W2V-BERT: COMBINING CONTRASTIVE LEARNING AND MASKED LANGUAGE MODELING FOR SELF-SUPERVISED SPEECH PRE-TRAINING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Self-supervised learning</term>
					<term>representation learn- ing</term>
					<term>unsupervised pre-training</term>
					<term>BERT</term>
					<term>wav2vec 20</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two selfsupervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the Lib-riSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.</p><p>Index Terms-Self-supervised learning, representation learning, unsupervised pre-training, BERT, wav2vec 2.0</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>How to leverage large-scale unannotated speech to improve supervised automatic speech recognition (ASR) performance has been a longstanding research problem. To date, there have been two major streams for utilizing unlabeled speech data for tackling such a semi-supervised ASR task.</p><p>The first line of work is self-training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, also known as pseudo-labeling, where the system starts with training a teacher model using initially available labeled data. Next, the teacher model is used to label the unlabeled data. The combined labeled and pseudo-labeled data are then used to train a student model. The pseudo-labeling process can be repeated multiple times to improve the quality of the teacher model. Self-training has been a practically useful and extensively studied technique in ASR <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>The second direction of taking advantage of unlabeled speech data is unsupervised pre-training, or self-supervised pre-training. In unsupervised pre-training, a model is first trained to complete a * Work done during an internship at Google Brain. proxy task that is designed to consume only unlabeled data (hence unsupervised). Such proxy task is commonly believed to be capable of initializing the parameters of the model at a good starting point before it is being trained on the supervised data. Significant recent research effort has been made to develop proxy tasks that allow models to perform well when the models are fine-tuned on ASR tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. There have also been studies that show that the gains brought by self-training and unsupervised pre-training are additive in downstream ASR <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>In this work, we focus on improving the unsupervised pretraining aspect of semi-supervised ASR by proposing a novel pre-training framework. Our method, which we call w2v-BERT, combines the core methodologies from two recent frameworks for self-supervised pre-training of speech and language respectively: wav2vec 2.0 <ref type="bibr" target="#b22">[23]</ref> and BERT <ref type="bibr" target="#b23">[24]</ref>. The idea of w2v-BERT is to use the contrastive task defined in wav2vec 2.0 to obtain an inventory of a finite set of discriminative, discretized speech units, and then use them as target in a masked prediction task in a way that is similar to masked language modeling (MLM) proposed in BERT for learning contextualized speech representations. Although the masked prediction task requires to consume tokens that are to be learned by solving the contrastive task first, we show that in practice the two objectives can be optimized simultaneously. <ref type="figure">Figure 1</ref> illustrates the w2v-BERT pre-training framework.</p><p>In this paper, we make the following contributions:</p><p>? We propose w2v-BERT that directly optimizes a contrastive loss and a masked prediction loss simultaneously for end-toend self-supervised speech representation learning.</p><p>? We show that w2v-BERT yields state-of-the-art performance on the well-benchmarked LibriSpeech task.</p><p>? We show that w2v-BERT greatly improves a real-world recognition task (voice search) over conformer-based wav2vec 2.0.</p><p>? We provide an analysis that empirically confirms the necessity of contrastive learning for enabling masked prediction in our framework. We also show in our voice search experiments that mask prediction is very useful for alleviating the problem of "easy negative samples" in contrastive learning.</p><p>The rest of the paper is organized as follows. We begin with discussing the differences between w2v-BERT and some of the most relevant unsupervised speech pre-training frameworks from the literature in Section 2. Then, in Section 3 we present the w2v-BERT pre-training framework, including the model architecture and training objectives. Section 4 describes the experimental setup, followed by our results and analysis in Section 5 where we apply pre-trained w2v-BERT models to LibriSpeech and voice search ASR. Finally, we conclude in Section 6.  <ref type="figure">Fig. 1</ref>: Illustration of the w2v-BERT pre-training framework. w2v-BERT is composed of a feature encoder, a contrastive module, and a masked language modeling (MLM) module, where the latter two are both a stack of conformer blocks. N and M denote the number of conformer blocks in the two modules, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>We consider our work most related to HuBERT <ref type="bibr" target="#b24">[25]</ref>, vq-wav2vec <ref type="bibr" target="#b25">[26]</ref>, and DiscreteBERT <ref type="bibr" target="#b26">[27]</ref>: w2v-BERT and these methods all try to first transform continuous speech signals into discretized units so as to exploit masked language modeling (MLM) <ref type="bibr" target="#b23">[24]</ref> for learning contextualized speech representations. Despite sharing this same high-level philosophy for learning speech representations, there are two key differences between w2v-BERT and other methods. The most noticeable difference is that w2v-BERT's speech discretizing module and its main contextualized representation learning module can be trained end-to-end. This is in contrast to vq-wav2vec and DiscreteBERT, which involve a two-stage process where the speech discretizing module needs to be obtained in advance and is kept frozen during the training of the representation learning module. In vq-wav2vec and DiscreteBERT, a problematic token ID assignment would negatively affect the subsequent learning module and it is hard for the learning module to recover the errors made by the discretizer. Observing such drawback, HuBERT greatly improves vq-wav2vec and DiscreteBERT by allowing refinement on the ID assignment via iterating between k-means clustering and re-training its representation learning module. However, the fact that HuBERT iterates between the two stages also means it involves more heuristic design choices, for example, the gradually increasing number of clusters in different iterations. End-to-end methods such as w2v-BERT alleviate the need of coordinating multiple stages. One potential risk for end-to-end approaches compared to k-means clustering is codebook collapse. In w2v-BERT, we find the contrastive learning objective effectively avoids codebook collapse and thus enables masked prediction training.</p><p>In addition, unlike other methods that use transformer layers <ref type="bibr" target="#b27">[28]</ref> as building blocks, w2v-BERT adopts conformer layers <ref type="bibr" target="#b28">[29]</ref> for constructing the network. As demonstrated in <ref type="bibr" target="#b28">[29]</ref>, conformer layers, which combine convolution neural networks (CNNs) and transformers to model both local and global dependencies of audio sequences, are likely a better option for modeling speech than transformer layers and CNNs. That being said, using a potentially more powerful building block is not the only factor that makes w2v-BERT outperform other methods, as the effectiveness of the pre-training framework itself is also validated in our experiments where w2v-BERT outperforms w2v-Conformer <ref type="bibr" target="#b20">[21]</ref>, which is also built with conformer layers.</p><p>w2v-BERT is also related to wav2vec 2.0 <ref type="bibr" target="#b22">[23]</ref>. Same as w2v-BERT, wav2vec 2.0 is end-to-end where the discretizer is jointly trained with its representation learning module. However, wav2vec 2.0 only employs contrastive learning, whose resulting ASR performance lags behind that of combining contrastive learning and masked prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head><p>In this section we present each component in w2v-BERT, starting with its model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model architecture</head><p>Our model architecture for pre-training is composed of a feature encoder that extracts latent speech representations from raw acoustic inputs, a module for solving wav2vec 2.0's contrastive task <ref type="bibr" target="#b22">[23]</ref> to obtain a set of discretized speech tokens, and a module for solving a masked prediction task <ref type="bibr" target="#b23">[24]</ref> for learning contextualized speech representations.</p><p>Feature encoder The feature encoder acts as a convolutional subsampling block that consists of two 2D-convolution layers, both with strides (2, 2), resulting in a 4x reduction in the acoustic input's sequence length. Given, for example, a log-mel spectrogram as input, the feature encoder extracts latent speech representations that will be taken as input by the subsequent contrastive module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive module</head><p>The module contains a linear projection layer followed by a stack of conformer blocks <ref type="bibr" target="#b28">[29]</ref>, each of which is a series of multi-headed self attention <ref type="bibr" target="#b27">[28]</ref>, depth-wise convolution and feed-forward layers.</p><p>The goal of the contrastive module is to discretize the feature encoder output into a finite set of representative speech units. For this purpose, the contrastive module involves a quantization mechanism. The output of the feature encoder, on one hand, is fed into the linear projection layer followed by the stack of conformer blocks after masking to produce context vectors, and on the other hand, is passed to the quantizer without masking to yield quantized vectors and their assigned token IDs. The quantized vectors are used in conjunction with the context vectors that correspond to the masked positions to solve the contrastive task defined in wav2vec 2.0 to optimize the contrastive module; the assigned token IDs will be later used by the subsequent masked prediction module as prediction target.</p><p>Masked prediction module The masked prediction module is a stack of conformer blocks where each block has an identical configuration to those from the contrastive module. The module directly takes in the context vectors produced by the contrastive module and extracts high-level contextualized speech representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-training</head><p>During pre-training only unlabeled speech data is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive loss</head><p>The contrastive loss is used to train the contrastive module along with the quantizer, such that the former yields adequate context vectors that will be taken as input by the subsequent masked prediction module, and the latter produces discriminative discretized speech tokens that will be used by the masked prediction module as prediction target. We adopt the contrastive task defined in wav2vec 2.0 and follow its quantization mechanism. Once the feature encoder has transformed the raw acoustic input into latent speech representations, we randomly select some time steps to mask. Unlike wav2vec 2.0 where the masked positions' latent vectors are replaced with a shared learnable feature vector, we simply replace them with random vectors. The masked feature encoder output is fed into the contrastive module for producing context vectors. In parallel, the feature encoder output is also passed to the quantizer without masking to yield its quantized vectors. For a context vector ct corresponding to a masked time step t, the model is asked to identify its true quantized vector qt from a set of K distractors {q1,q2, ...,qK } that are also quantized vectors uniformly sampled from other masked time steps of the same utterance. We denote the loss as Lw, and further augment it with a codebook diversity loss L d to encourage a uniform usage of codes. Therefore, the final contrastive loss is defined as:</p><formula xml:id="formula_0">Lc = Lw + ? ? L d ,<label>(1)</label></formula><p>where ? = 0.1 following <ref type="bibr" target="#b22">[23]</ref>.</p><p>Masked prediction loss The context vectors produced by the contrastive module are directly passed to the masked prediction module for producing the final context vectors that are to be used to complete a masked prediction task. A softmax layer is appended on top of the module's last conformer block. If a context vector at the final layer corresponds to a masked position, the softmax layer will take the context vector as input and attempt to predict its corresponding token ID, which is assigned earlier in the contrastive module by the quantizer. We denote the cross-entropy loss for this masked prediction task as Lm. w2v-BERT is trained to solve the two self-supervised tasks at the same time. The final training loss to be minimized is:</p><formula xml:id="formula_1">Lp = ? ? Lc + ? ? Lm.<label>(2)</label></formula><p>In our experiments, we simply set both ? and ? to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fine-tuning</head><p>During fine-tuning we have access to labeled data. We apply our pre-trained w2v-BERT to two tasks: LibriSpeech and voice search. The ASR network is a sequence transducer <ref type="bibr" target="#b29">[30]</ref> that consists of a pre-trained w2v-BERT model and a LSTM <ref type="bibr" target="#b30">[31]</ref> decoder. We insert a linear layer with Swish activation <ref type="bibr" target="#b31">[32]</ref> and batch normalization <ref type="bibr" target="#b32">[33]</ref> between the pre-trained w2v-BERT model and the LSTM decoder as the projection block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL SETUP</head><p>Apart from the pre-training method, the rest of the experimental pipeline follows the exact same setup as in <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data</head><p>We use the Libri-Light unlab-60k subset <ref type="bibr" target="#b33">[34]</ref>, which contains about 60,000 hours of unannotated speech audio, for pre-training w2v-BERT models. For our main results, we use the LibriSpeech 960hr subset <ref type="bibr" target="#b34">[35]</ref> as the supervised data, and use the 100hr subset for ablation studies. We report word error rates (WERs) on the dev-clean, dev-other, test-clean, and test-other evaluation subsets. 80-dimensional log-mel filter bank coefficients are used as acoustic inputs to our model. For transcript tokenization, we use a 1024token WordPiece model <ref type="bibr" target="#b35">[36]</ref> that is constructed from the transcripts of the LibriSpeech training set (or the 100hr subset when the models are fine-tuned on it).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pre-training details</head><p>Masking For masking the feature encoder output, we randomly sample the starting positions to be masked with a probability of 0.065 and mask the subsequent 10 time steps (same as <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21]</ref>). The masked spans may overlap.</p><p>Optimization We pre-train two versions of w2v-BERT models, one has about 0.6 billion parameters and the other has about 1 billion parameters, denoted as w2v-BERT XL and w2v-BERT XXL, respectively. The two variants share the same model configuration that is summarized in <ref type="table" target="#tab_1">Table 1</ref>, and their only difference is the number of conformer blocks. Specifically, w2v-BERT XL's contrastive module consists of 12 conformer blocks and the masked prediction module is composed of another 12. w2v-BERT XXL, while having the same amount of conformer blocks in its contrastive module, enlarges its masked prediction module to 30 conformer blocks. For w2v-BERT XL, we train it with a batch size of 2048 using the Adam optimizer <ref type="bibr" target="#b36">[37]</ref> with a transformer learning rate schedule as described in section 5.3 of <ref type="bibr" target="#b27">[28]</ref>. The peak learning rate is 2e-3 and the warm-up steps are 25k. For w2v-BERT-XXL, we train it with the Adafactor optimizer <ref type="bibr" target="#b37">[38]</ref> with ?1 = 0.9 and ?2 = 0.98, with the learning rate schedule remaining the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fine-tuning details</head><p>Optimization For both w2v-BERT XL and w2v-BERT-XXL, we take their pre-trained checkpoints at 400k steps, and fine-tune them on the supervised data with a batch size of 256. The decoder for both models are a two-layer LSTM with a hidden dimension of 640. We employ separate optimizers and learning rate schedules for optimizing the pre-trained model and the decoder, given the fact that the former has been pre-trained while the latter needs to be trained from scratch. For w2v-BERT XL, both the pre-trained model and the decoder are optimized with an Adam optimizer with a transformer learning schedule. The difference is that for the pre-trained component we use a peak learning rate of 3e-4 with 5k warm-up steps, while for the decoder we use a peak learning rate of 1e-3 and 1.5k warm-up steps. For w2v-BERT-XXL, an Adafactor optimizer that <ref type="table">Table 2</ref>: WERs(%) when using the LibriSpeech 960hr subset as supervised data. We compare models trained without any unlabeled data (Trained from Scratch), trained using Noisy Student Training (NST) without any pre-training (Self-training Only), fine-tuned from a pre-trained model only using supervised data (Pre-training Only), and the models obtained by combining pre-training and self-training (Pretraining + Self-training). We also include the best results of several methods that we can find from the literature, and their corresponding references are where the numbers are quoted from. The lowest WER(s) under different settings are marked in bold. AM/LM Size denotes the number of parameters in the acoustic/language model. * The reason why we do not include Conformer XL and Conformer XXL is that, according to <ref type="bibr" target="#b20">[21]</ref>, simply enlarging Conformer L produces worse results when the model is trained from scratch. ? Calculated based on the LM configuration provided in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>; &gt; because some information such as the token embedding size is not given therefore not included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Unlabeled has the same configuration as in pre-training is used, and the learning rate schedules for the encoder and decoder are the same as the XL variant.</p><p>Self-training, data augmentation, and LM fusion In addition to self-supervised pre-training, in the fine-tuning stage we also employ a number of practical techniques that further improve models' performance on ASR. These techniques include SpecAugment <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> for data augmentation, Noisy Student Training <ref type="bibr" target="#b40">[41]</ref> for self-training, and language model fusion for decoding. When any of the techniques are used, we follow the exact same setup as in <ref type="bibr" target="#b20">[21]</ref>. We refer the readers to the paper for the details on these techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Main results</head><p>In <ref type="table">Table 2</ref>, we present our results on the four LibriSpeech evaluation sets using the 960hr subset as the supervised data. We compare w2v-BERT to a number of state-of-the-art self-supervised representation learning methods from the literature such as HuBERT <ref type="bibr" target="#b24">[25]</ref> and wav2vec 2.0 <ref type="bibr" target="#b22">[23]</ref> under different semi-supervised settings, including whether self-training is employed during the fine-tuning stage and whether a language model is incorporated during inference time. We also include the model size of the ASR network used by each method, denoted as acoustic model (AM) Size and language model (LM) Size. Results missing from the literature (e.g., results of HuBERT without self-training and LM) are indicated with a "?" in the table. From <ref type="table">Table 2</ref> we have the following two key conclusions.</p><p>Without self-training and LM, w2v-BERT already either outper-forms or matches other models with LM. We see that with just pretraining when neither self-training nor LM is used, w2v-BERT XL achieves a WER of 1.5/2.9 (test/test-other), which already either outperforms or matches other models with LM, and outperforms their counterparts without LM by a larger margin. w2v-BERT XXL further increases the gap on the more challenging dev-other and test-other subsets. Noticeably, compared to wav2vec 2.0, w2v-BERT-XXL shows a relative WER reduction of 28%, 42%, 32%, and 38% on the four evaluation subsets respectively without LM, and 13%/ 13%/ 17%/ 18% when LM is employed. We want to highlight that although w2v-BERT XL (0.6B) and w2v-BERT XXL (1.0B) have a larger pre-trained model size than wav2vec 2.0 (0.3B), the latter also incorporates a much larger LM (&gt; 0.4B) during self-training and decoding according to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. When considering the sum of the two components, wav2vec 2.0 (&gt; 0.7B) actually features a similar (if not bigger) model size as w2v-BERT XL (0.7B).</p><p>Contrastive learning combined with masked language modeling is more effective than contrastive learning alone.</p><p>w2v-Conformer <ref type="bibr" target="#b20">[21]</ref> and w2v-BERT only differ in their pre-training method and have all other aspects in common such as their model size and fine-tuning pipeline. This allows a truly apple-to-apple comparison between the two pre-training methods for their effectiveness in representation learning. Below we briefly describe their differences in pre-training.</p><p>w2v-Conformer adopted wav2vec 2.0's contrastive task as the sole pre-training objective, but replaced the quantization module with a linear layer as the authors did not find quantization helpful for improving downstream ASR performance. w2v-BERT, on the other hand, adopts wav2vec 2.0's contrastive task not just for learning contextualized speech representations, but mainly for the purpose of obtaining a codebook that can represent every segment of continuous speech as an discriminative discrete token, such that we can exploit MLM for learning powerful speech representations. As will be demonstrated in our analysis (Section 5.2), the contrastive loss is essential for making MLM to work. By comparing the Pre-training Only results of w2v-Conformer and w2v-BERT, we see that w2v-BERT XL, despite having fewer model parameters, already outperforms w2v-Conformer-XXL-the previous state of the art-especially on the dev-other and test-other subsets. When self-training is applied, w2v-BERT XL still either outperforms or matches w2v-Conformer-XXL's results. w2v-BERT-XXL, which is of the same model size as w2v-Conformer XXL, outperforms w2v-Conformer XXL on even more evaluation subsets. These results demonstrate the superiority of the proposed w2v-BERT over existing pre-training frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis and discussion</head><p>The goal of our analysis is to understand the roles of contrastive learning as well as its learned codebook in the w2v-BERT pretraining framework.</p><p>On the necessity of contrastive module The first natural question is whether the contrastive module is an essential component of the framework, or is that a masked prediction module alone can already derive a suitable codebook for its own MLM purpose.</p><p>Without the contrastive module, the feature encoder output is directly fed to the masked prediction module. Intuitively, the masked prediction module then gets a full control over the quantizer (which may originally be viewed as part of the contrastive module) and decide its own prediction target. In order to maximize the prediction performance, the masked prediction module can "cheat" by coming up with a trivial solution where it asks the quantizer to cooperate with it by quantizing all feature encoder's output frames that correspond to the masked positions to the same code vector, in other words, always assigning the same target ID for the masked prediction module to predict. The module thus perfectly solves the masked prediction task without learning any useful representation.</p><p>To verify our intuition, we train a series of w2v-BERT models without the contrastive module. These variants all have the same capacity as w2v-BERT XL, that is, they are all constructed with 24 conformer layers. We try different values of ? = 0.1, 0.3, 0.5, and 0.7 in Equation 1 for increasing encouragement of uniform usage of codes. Nevertheless, we find all models are untrainable and quickly collapse regardless of the value of ?. In <ref type="figure" target="#fig_0">Figure 2</ref> we show the training curves of a w2v-BERT model without contrastive module with ? set to 0.5. We include the curves of the successfully trained w2v-BERT XL for comparison. The plots include the models' masked prediction loss <ref type="figure" target="#fig_0">(Figure 2a</ref>), masked prediction accuracy <ref type="figure" target="#fig_0">(Figure 2b)</ref>, and diversity loss <ref type="figure" target="#fig_0">(Figure 2c</ref>) during pre-training.</p><p>We find that the training curves of w2v-BERT without contrastive module (in blue) strongly align with our intuition: the masked prediction loss quickly decreases to close to 0 at the early stage of training <ref type="figure" target="#fig_0">(Figure 2a)</ref> where the model reaches 100% prediction accuracy <ref type="figure" target="#fig_0">(Figure 2b)</ref>. Meanwhile, as shown in <ref type="figure" target="#fig_0">Figure 2c</ref>, the diversity loss quickly increases to close to 1, where in our implementation this indicates an extremely low entropy of softmax distribution over the codebook entries, suggesting code collapse. Comparing the curves of w2v-BERT models with and without contrastive module, we hypothesize that the contrastive loss guides the entries in the codebook to be discriminative, thus preventing the masked prediction module from deriving a trivial solution just to maximize masked prediction performance.</p><p>On the impact of contrastive module's capacity After confirming the necessity of contrastive module, next we are interested in investigating the impact of its capacity on downstream ASR performance.</p><p>We train a series w2v-BERT models with different numbers of conformer layers in their contrastive module. To rule out the factor of masked prediction module's capacity, we keep the total number of conformer layers in the two modules fixed at 24. We use Cn to denote each variant, where n is the number of conformer layers in the contrastive module. For instance, C4 has 4 conformer layers in its contrastive module and 20 in its masked prediction module. Here we consider n = 2, 4, 6, 8, 10, 12, and 24, where C24 is an extreme case where the two modules are completely overlapped with each other and hence the contrastive and MLM tasks will be both tackled at the last (24-th) layer. Note that C12 is essentially w2v-BERT XL.</p><p>We use the LibriSpeech 100hr subset as the supervised data for this experiment, and both self-training and LM fusion are not used when training the ASR network. Results are shown in <ref type="table" target="#tab_3">Table 3</ref>. We include the results of some pre-training methods from the literature that also do not incorporate self-training and LM.</p><p>From <ref type="table" target="#tab_3">Table 3</ref> we can roughly observe a performance sweet spot on all four evaluation subsets when we increase the number of layers in the contrastive module. From C2 to C8, the WERs are mostly decreasing, meaning that enlarging the contrastive module is helpful for learning better representations. The fact that the performance continues to improve while the masked prediction module shrinks (and hence becomes less expressive) as we deepen the contrastive module further suggests the importance of making the contrastive module sufficiently large. Starting from C8, however, the WERs stop decreasing as we deepen the contrastive module. We hypothesize that this is because the masked prediction module has now become too small to learn representations useful for the MLM task. Such reasoning is supported by the fact that enlarging the masked prediction module while keeping the contrastive module the same size can still improve the performance (w2v-BERT XL vs. w2v-BERT XXL).</p><p>Last but not least, we see that w2v-BERT always outperforms wav2vec 2.0 regardless of its layer configuration. It also either outperforms or matches w2v-Conformer XL's performance when its contrastive module has enough capacity (i.e., when n &gt; 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on Voice Search traffic</head><p>So far we have shown w2v-BERT pre-trained on read speech audio can achieve great performance on the well-benchmarked Lib-riSpeech task. To validate the effectiveness of w2v-BERT on realworld audio traffic, we apply it to Google's Voice Search traffic. Our train and test sets are derived from <ref type="bibr" target="#b41">[42]</ref>. We use 34.3k hours of English audio for pre-training, and randomly pick 1k hours as the finetuning data, which is anonymized and human-transcribed. The test set contains around 12k Voice Search utterances with duration less than 5.5s long. The testing utterances are anonymized and humantranscribed, and are representative of Google's Voice Search traffic.</p><p>The traffic data is more challenging to be used for pre-training than read speech audio in two folds: (1) It is noisier and contains more silences that make negative sampling for contrastive learning less effective. (2) The average length of the traffic audio (5 seconds) is much shorter than that of read speech audio. These factors make the context learned from the audio segments much less effective.</p><p>As shown in <ref type="table" target="#tab_4">Table 4</ref>, if we take the same training script as w2v-Conformer-XL, the model tends to cheat on negative samples due to the large portion of non-speech and shorter context. To make contrastive learning more effective, we have to use a less aggressive subsampling: instead of using a 4 times convolutional stride, we stack 3 frames as target to encourage the model to learn better context. However, by taking an identical architecture and using the same training receipt, our w2v-BERT XL significantly improves the tuned contrastive baseline by relative 30%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION AND FUTURE WORK</head><p>We proposed w2v-BERT for self-supervised speech representation learning. w2v-BERT is composed of a contrastive module for discretizing continuous speech and a masked prediction module that performs masked language modeling with the discretized speech. The two modules can be jointly optimized. We pre-train w2v-BERT on 60k hours of unlabeled speech data from the Libri-Light corpus, and show it either outperforms or matches state-of-the-art systems such as w2v-Conformer, HuBERT, and wav2vec 2.0. The gain also transfers to a more challenging internal dataset. We also provide an analysis on the importance of the contrastive module for enabling effective masked language modeling in w2v-BERT.</p><p>In our experiments, all the hyperparameter setups are directly taken from <ref type="bibr" target="#b20">[21]</ref> without any changes. For future work, we plan to first search for the best training configuration for w2v-BERT. We are also interested in evaluating w2v-BERT in low-resource data settings using the Libri-Light 10min, 1hr, and 10hr benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Training curves of w2v-BERT models with and without contrastive module. From left to right: MLM training loss, MLM training accuracy, training diversity loss. The blue curve represents the w2v-BERT model without contrastive module, and the orange curve represents w2v-BERT XL (with contrastive module). We show results for the first 300k steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2108.06209v2 [cs.LG] 13 Sep 2021</figDesc><table><row><cell>Context Vectors</cell><cell>MLM Stack</cell><cell></cell></row><row><cell></cell><cell>Contrastive</cell><cell></cell></row><row><cell>Conformer Blocks</cell><cell>Stack</cell><cell></cell></row><row><cell>Conformer Blocks</cell><cell cols="2">MLM Loss</cell></row><row><cell cols="2">Context Vectors</cell><cell></cell></row><row><cell>Conformer Blocks</cell><cell cols="2">Contrastive Loss</cell></row><row><cell>Conformer Blocks</cell><cell cols="2">Target Context Vectors</cell><cell>Discretized ids</cell></row><row><cell>Linear</cell><cell>Masking</cell><cell cols="2">Quantization</cell></row><row><cell>Convolutional Subsampling</cell><cell>Encoded Features</cell><cell></cell></row><row><cell></cell><cell cols="2">Feature Encoder</cell></row><row><cell>Input Features</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Parameters for w2v-BERT models. Dim. stands for dimension.</figDesc><table><row><cell>Model</cell><cell># Params (B)</cell><cell># Contrastive Layers</cell><cell># Masked Layers</cell><cell>Model Dim.</cell><cell>Attention Heads</cell><cell>Conv. Layer Kernel Size</cell><cell>Relative Attention</cell><cell>Codebook Size</cell><cell>Code Dim.</cell></row><row><cell>w2v-BERT XL</cell><cell>0.6</cell><cell>12</cell><cell>12</cell><cell>1024</cell><cell>8</cell><cell>5</cell><cell>N</cell><cell>1024</cell><cell>1024</cell></row><row><cell>w2v-BERT XXL</cell><cell>1.0</cell><cell>12</cell><cell>30</cell><cell>1024</cell><cell>8</cell><cell>5</cell><cell>N</cell><cell>1024</cell><cell>1024</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>WERs (%) when using the LibriSpeech 100hr subset as supervised data. For all methods, both self-training and LM fusion are not used. References are where the numbers are quoted from.</figDesc><table><row><cell>Method</cell><cell cols="4">dev dev-other test test-other</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>wav2vec 2.0 [23]</cell><cell>3.3</cell><cell>6.5</cell><cell>3.1</cell><cell>6.3</cell></row><row><cell cols="2">w2v-Conformer XL [21] 2.5</cell><cell>4.7</cell><cell>2.6</cell><cell>4.9</cell></row><row><cell>w2v-BERT XXL (Ours)</cell><cell>2.3</cell><cell>4.0</cell><cell>2.3</cell><cell>4.3</cell></row><row><cell>w2v-BERT w/ 24 layers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C2</cell><cell>2.4</cell><cell>5.1</cell><cell>2.5</cell><cell>5.1</cell></row><row><cell>C4</cell><cell>2.5</cell><cell>4.6</cell><cell>2.5</cell><cell>5.1</cell></row><row><cell>C6</cell><cell>2.5</cell><cell>4.2</cell><cell>2.4</cell><cell>4.7</cell></row><row><cell>C8</cell><cell>2.3</cell><cell>4.3</cell><cell>2.4</cell><cell>4.6</cell></row><row><cell>C10</cell><cell>2.4</cell><cell>4.5</cell><cell>2.5</cell><cell>4.8</cell></row><row><cell>C12 (w2v-BERT XL)</cell><cell>2.4</cell><cell>4.4</cell><cell>2.5</cell><cell>4.6</cell></row><row><cell>C24</cell><cell>2.4</cell><cell>4.9</cell><cell>2.5</cell><cell>5.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on voice search data. Baseline conformer model is 100M parameters. All the other models are 600M parameters, marked as XL.</figDesc><table><row><cell>Method</cell><cell cols="2">Unlabeled Data (Domain) Test (VS)</cell></row><row><cell>Conformer</cell><cell>N/A</cell><cell>10.7</cell></row><row><cell>w2v-Conformer-XL</cell><cell>34.3k (Voice search)</cell><cell>10.8</cell></row><row><cell>w2v-Conformer-XL-tuned</cell><cell>34.3k (Voice search)</cell><cell>8.9</cell></row><row><cell>w2v-BERT XL (Ours)</cell><cell>34.3k (Voice search)</cell><cell>6.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning extraction patterns for subjective expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive patternrecognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-training for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end ASR: from supervised to semi-supervised learning with modern architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML SAS Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semisupervised training for end-to-end models via weak distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lessons from building acoustic models with a million hours of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikko</forename><surname>Sree Hari Krishnan Parthasarathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Analysis of lowresource acoustic model self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Novotney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Utilizing untranscribed training data to improve performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zavaliagkos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Colthurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DARPA Broadcast News Transcription and Understanding Workshop</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An unsupervised autoregressive model for speech representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep contextualized acoustic representations for semisupervised speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshi</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative pre-training for speech with autoregressive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Han</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Chun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved speech representations with multi-target autoregressive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TERA: Selfsupervised learning of transformer encoder representation for speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2351" to="2366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of bidirectional speech encoders via masked reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">DeCoAR 2.0: Deep contextualized acoustic representations with vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshi</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06659</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Representation learning for sequence data with deep autoencoding predictive components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwen</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pushing the limits of semi-supervised learning for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10504</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-training and pre-training are complementary for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paden</forename><surname>Tomasello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">HuBERT: Self-supervised speech representation learning by masked prediction of hidden units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07447</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">vq-wav2vec: Self-supervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Effectiveness of self-supervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03912</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<title level="m">Sequence transduction with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Libri-light: A benchmark for ASR with limited or no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgane</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Abdelrahman Mohamed, and Emmanuel Dupoux</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LibriSpeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Japanese and Korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>SpecAugment on large scale datasets,&quot; in ICASSP</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Improved noisy student training for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>in Interspeech</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Scaling end-to-end models for large-scale multilingual ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14830</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
