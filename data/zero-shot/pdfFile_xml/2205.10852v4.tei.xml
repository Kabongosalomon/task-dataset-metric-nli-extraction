<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relational Graph Trans-former for Knowledge Graph Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-25">2022. April 25-29, 2022. April 25-29, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Bi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ZJU-Hangzhou Global Scientific and Technological Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ZJU-Hangzhou Global Scientific and Technological Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhuan</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ZJU-Hangzhou Global Scientific and Technological Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
							<email>zhangningyu@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ZJU-Hangzhou Global Scientific and Technological Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xiong</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ZJU-Hangzhou Global Scientific and Technological Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Bi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ZJU-Hangzhou Global Scientific and Technological Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ZJU-Hangzhou Global Scientific and Technological Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">ZJU-Hangzhou Global Scientific and Technological Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhuan</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ZJU-Hangzhou Global Scientific and Technological Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ZJU-Hangzhou Global Scientific and Technological Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ZJU-Hangzhou Global Scientific and Technological Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ZJU-Hangzhou Global Scientific and Technological Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Relational Graph Trans-former for Knowledge Graph Representations</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Virtual Event</title>
						<meeting>the ACM Web Conference 2022 (WWW &apos;22) <address><addrLine>Lyon, France</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2022-04-25">2022. April 25-29, 2022. April 25-29, 2022</date>
						</imprint>
					</monogr>
					<note>Event, Lyon, France. ACM, New York, NY, USA, 12 pages. https:// 1 Code is available in https://github.com/zjunlp/Relphormer. ACM ISBN 978-1-4503-9096-5/22/04. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Information systems ? Information extraction KEYWORDS Knowledge Graph</term>
					<term>Transformer</term>
					<term>Knowledge Graph Representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers have achieved remarkable performance in widespread fields, including natural language processing, computer vision and graph mining. However, vanilla Transformer architectures have not yielded promising improvements in the Knowledge Graph (KG) representations, where the translational distance paradigm dominates this area. Note that vanilla Transformer architectures struggle to capture the intrinsically heterogeneous semantic and structural information of knowledge graphs. To this end, we propose a new variant of Transformer for knowledge graph representations dubbed Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample contextualized sub-graph sequences as the input to alleviate the heterogeneity issue. We propose a novel structure-enhanced self-attention mechanism to encode the relational information and keep the globally semantic information among sub-graphs. Moreover, we propose masked knowledge modeling as a new paradigm for knowledge graph representation learning. We apply Relphormer to three tasks, namely, knowledge graph completion, KG-based question answering and KG-based recommendation for evaluation. Experimental results show that Relphormer can obtain better performance on benchmark datasets compared with baselines 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, Knowledge Graphs (KGs) have shown effectiveness in data mining and are widely applied to a variety of applications such as recommendation <ref type="bibr" target="#b22">[21]</ref>, time series prediction <ref type="bibr">[4]</ref>, natural language understanding <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b54">53]</ref>. As shown in <ref type="figure">Figure 1</ref>, KG representation learning aims to project the entities and relations into a continuous low-dimensional vector space, which can implicitly promote computations of the reasoning between entities and has been proved helpful for knowledge-intensive tasks <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b47">46]</ref>. Previous KG representation learning methods, such as TransE <ref type="bibr">[2]</ref>, ComplEx <ref type="bibr" target="#b37">[36]</ref> and RotatE <ref type="bibr" target="#b34">[33]</ref>, embed the relational knowledge into a vector space and then optimize the target object by leveraging a pre-defined scoring function to those vectors. However, it is rather challenging to encode all information about an entity into a single vector. To this end, several works leverage graph neural networks (GNN) <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b55">54]</ref> or attention-based approaches <ref type="bibr" target="#b43">[42]</ref> to learning representations based on both entities and their graph context. However, these methods are still restricted in expressiveness regarding the shallow network architectures.</p><p>The Transformer <ref type="bibr" target="#b39">[38]</ref>, acknowledged as the most powerful neural network in modeling sequential data, has achieved success in representing natural language <ref type="bibr" target="#b7">[7]</ref> and computer vision <ref type="bibr" target="#b8">[8]</ref>. Besides, there are many attempts to leverage the Transformer into the graph domain. The majority of these approaches <ref type="bibr" target="#b52">[51,</ref><ref type="bibr" target="#b56">55]</ref> apply Transformer on the entire graph and enhance the vanilla feature-based attention mechanism with topology-enhanced attention mechanism and structural encoding or leveraging Transformer through ego-graphs with proximity-enhanced attention. Note that different from the pure graph, KGs are heterogeneous graphs consisting of multiple types of nodes. Therefore, it is still an open question whether Transformer architecture is suitable to model arXiv:2205.10852v4 [cs.CL] 20 Oct 2022</p><p>KGs and how to make it work in KG representation learning. Concretely, there are two nontrivial challenges for fully applying Transformer architecture to KGs as follows:</p><p>? Heterogeneity for edges and nodes. On the one hand, KGs are relational graphs with semantic-enriched edges. Multiple edges have different relational information, resulting in distinct heterogeneous representations. On the other hand, KGs are text-rich networks. Generally, there are two types of knowledge for each node, that is, topological structure and textual descriptions. Therefore, different nodes have unique topological and textual features. For example, given an entityrelation pair (?, ), characterizing the structural context and textual descriptions will provide valuable information when inferring the entity node from ? and . However, vanilla KG Transformer architecture usually treats all entities and relations as plain tokens, missing the essential structural information. ? Task Optimization Universality. Note that most previous studies <ref type="bibr">[2]</ref> follow the paradigm with a pre-defined scoring function for knowledge embeddings. However, such a strategy requires optimizing different objects for entity/relation prediction and costly scoring of all possible triples in inference. Meanwhile, previous KG representation learning methods struggle to provide unified learned representations for various KG-based tasks, such as knowledge graph completion, question answering and recommendation. Therefore, it is intuitive to design a new technical solution for knowledge graph representation.</p><p>In this study, to address the above-mentioned issues, we explore the Transformer architecture for knowledge graph representation and propose Relational Graph Transformer (Relphormer).</p><p>First, we propose a relational transformer framework. To handle the heterogeneity for edges, we first use Triple2Seq to sample relational sub-graphs as input for our model. We regard relations as normal nodes in the sub-graphs and then feed the contextualized sub-graphs into the transformer module. We use a dynamic sampling strategy to maximally preserve the localized contextual information and semantics. Furthermore, we propose a structureenhanced mechanism, which is equipped with a novel extension of the self-attention mechanism, to effectively model structure bias within its building blocks and through all network layers bottom-toup. With our designed module, the model alleviates the node-level heterogeneity problem, which simultaneously encodes the textual features while preserving the structural information. Note that we not only focus on knowledge graph representation; such a mechanism is readily applicable to other kinds of Transformer-based approaches to incorporate any structural bias.</p><p>Second, we propose a masked knowledge modeling mechanism for unified knowledge graph representation learning. Inspired by Masked Language Modeling in natural language processing, we introduce a unified optimization object of predicting masked entities as well as relation tokens in the input sequence. In such a way, we can simply leverage a unified optimization object for entity and relation prediction in knowledge graph completion. Moreover, we can utilize learned KG representations for various KG-based tasks such as question answering and recommendation tasks.</p><p>We conduct comprehensive experiments to evaluate our Relphormer in various tasks. Specifically, for knowledge graph completion, we use FB15K-237 <ref type="bibr" target="#b36">[35]</ref>, WN18RR <ref type="bibr" target="#b5">[5]</ref> and UMLS <ref type="bibr" target="#b0">[1]</ref> for entity prediction and choose FB15K-237 and WN18RR for relation prediction. We also evaluate Relphormer in two KG-based tasks: FreebaseQA <ref type="bibr" target="#b16">[15]</ref> and WebQuestionSP <ref type="bibr" target="#b51">[50]</ref> for Question Answering (QA) and ML-20m <ref type="bibr" target="#b11">[11]</ref> for the recommendation task. Experimental results illustrate that the proposed Relphormer can yield better performance and require less inference time. The contributions of this study can be summarized as follows:</p><p>? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Knowledge Graph Representation Learning</head><p>To date, most existing KG completion methods utilize the embeddingbased paradigm, such as TransE <ref type="bibr">[2]</ref>, TransD <ref type="bibr" target="#b14">[13]</ref>, TransR <ref type="bibr" target="#b21">[20]</ref> and TransH <ref type="bibr" target="#b44">[43]</ref>. DistMult <ref type="bibr" target="#b48">[47]</ref> uses a simple bilinear formulation and introduces a novel approach that utilizes the learned relation embeddings to mine logical rules. ComplEx <ref type="bibr" target="#b37">[36]</ref> leverages the composition of complex embeddings that can handle a large variety of binary relations. DenseE <ref type="bibr" target="#b24">[23]</ref> also provides an improved modeling scheme for the complex composition patterns of relations. RotatE <ref type="bibr" target="#b34">[33]</ref> is a practical approach to model and infer various relation patterns for knowledge graph embedding. Apart from traditional translational distance methods, there has been considerable interest in using graph convolution networks <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b40">39]</ref>. RGCN <ref type="bibr" target="#b31">[30]</ref> deals with the highly multi-relational data characteristic of realistic KGs. CompGCN <ref type="bibr" target="#b38">[37]</ref> proposes a novel graph convolutional framework that jointly embeds both nodes and relations. <ref type="bibr" target="#b55">[54]</ref> further explores the real effect of GCNs in KGC and finds that the transformations for entity representations are responsible for the performance improvements.</p><p>KG representations can boost the performance of KG-based tasks. <ref type="bibr">[3]</ref> leverages their pre-trained HittER model and proves its performance on two QA datasets. <ref type="bibr" target="#b23">[22]</ref> formulates complex logical query answering as a masked prediction problem and validates it on the downstream reasoning datasets. KGT5 <ref type="bibr" target="#b29">[28]</ref> is a sequence-tosequence method for both KGC and QA tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer for Graphs</head><p>Recently, a great variety of Transformers has been proposed to encode graph-structured data. GraphTrans <ref type="bibr" target="#b46">[45]</ref> uses the Transformerbased self-attention to learn long-range pairwise relationships, where a novel readout mechanism is designed to obtain a global The contextualized sub-graph is sampled with Triple2Seq (Section 3.2.1), and then it will be converted into sequences while maintaining its sub-graph structure. Next, we conduct masked knowledge modeling (Section 3.3), which randomly masks the nodes (e.g., entity, relation) in the center triple in the contextualized subgraph sequences. For the transformer architecture, we design a novel structure-enhanced mechanism (Section 3.2.2) to preserve the structure feature. Finally, we utilize our pre-trained KG transformer for KG-based downstream tasks (Section 3.5).</p><p>graph embedding. To address the issues of insufficient labeled molecules samples and poor generalization capability on large-scale molecular data, Grover <ref type="bibr" target="#b28">[27]</ref> carefully designs self-supervised tasks in node-, edge-and graph-level, which can learn rich structural and semantic information of molecules from enormous unlabelled data. Graphormer <ref type="bibr" target="#b52">[51]</ref> proposes several simple yet effective structural encoding methods to better model graph-structured data and mathematically proves that many popular GNN variants could be covered as the special cases of Graphormer.</p><p>To overcome the limitation due to the independent modeling of textual features, GraphFormers <ref type="bibr" target="#b49">[48]</ref> designs a new architecture where layerwise GNN components are nested alongside the transformer blocks of language models. Gophormer <ref type="bibr" target="#b56">[55]</ref> applies transformers on ego-graphs instead of full graphs to alleviate severe scalability issues on the node classification task. GraphFormers <ref type="bibr" target="#b49">[48]</ref> proposes a novel architecture where layerwise GNN components are nested alongside the Transformer blocks of language models. Heterformer <ref type="bibr" target="#b17">[16]</ref> proposes a heterogeneous GNN-nested transformer that blends GNNs and PLMs into a unified model. GraphiT <ref type="bibr" target="#b26">[25]</ref> views graphs as sets of node features and incorporates structural and positional information into a transformer architecture, which outperforms representations learned with GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transformer for Knowledge Graph</head><p>Various efforts have been devoted to exploring Transformer in the knowledge graph. KG-BERT <ref type="bibr" target="#b50">[49]</ref> treats triples in knowledge graphs as textual sequences and uses pre-trained language models for knowledge graph completion. StAR <ref type="bibr" target="#b41">[40]</ref> designs a structureaugmented text representation learning framework for efficient knowledge graph completion. PKGC <ref type="bibr" target="#b25">[24]</ref> proposes to convert each triple and its information into prompt sentences, which are further fed into PLMs for classification tasks. HittER <ref type="bibr">[3]</ref> uses entity and contextualization and designs a hierarchical Transformer for knowledge graph embeddings. kgTransformer <ref type="bibr" target="#b23">[22]</ref> presents the Knowledge Graph Transformer with masked pre-training and finetuning strategies. Besides, there are also some recently proposed promising works such as LP-BERT <ref type="bibr" target="#b20">[19]</ref>, SimKGC <ref type="bibr" target="#b42">[41]</ref>.</p><p>Unlike those approaches, Relphormer focuses on addressing key issues of Transformer for KGs and paves a new step for architecture designing. We propose a novel framework for unified knowledge representation learning and utilize the pre-trained knowledge representations for KG-based tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY 3.1 Preliminaries</head><p>Knowledge Graph represents the fact that exists in the form of the triple (? , , ). In this paper, we target the task for knowledge graph representation learning, which includes knowledge graph completion (entity and relation prediction) tasks and knowledge-graph-enhanced downstream tasks. We detail the general notations in <ref type="table">Table 1</ref> and illustrate the model architecture of Relphormer in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Given a relational graph = (E, R) composed of an entity set</p><formula xml:id="formula_0">E = { 1 , 2 , 3 , ...} and a relation set R = { 1 , 2 , 3 , ...}.</formula><p>The node set consists of both entities and relations, that is, = E ? R. The graph structure can be represented as an adjacency matrix A ? {0, 1} | |? | | , where | | denotes the number of nodes in the graph. The element A[ , ] in the adjacency matrix equals to 1 if there exists a link between node and , otherwise A[ , ] = 0.</p><p>In the knowledge graph completion task, a triple is denoted as ( , , ) or ( , , ), T in short. For most knowledge graphs, both entities and relation in the triplet are also represented by their corresponding textual description , and . We define the contextualized sub-graph T as the sub-graph surrounding the center triple T (including the triple itself). Meanwhile, <ref type="table">Table 1</ref>: Notation used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbol Description</head><p>Relational graph</p><formula xml:id="formula_1">E Entity set R Relation set Nodes in the relational graph T triple ( , , ) T contextualized sub-graph of the triple T T masked contextualized sub-graph T A</formula><p>Adjacency matrix of T The order of the contextualized sub-graph T neighborhood nodes surrounding T the corresponding graph structure of the contextualized sub-graph sequence is preserved in adjacency matrix A . T denotes the masked contextualized sub-graph. Therefore, the goal of knowledge graph completion is to learn a mapping :</p><formula xml:id="formula_2">T , A ? Y, where Y ? R | E |+| R | are the label sets of the triple T .</formula><p>For the QA task, each question is labeled with an answering entity . Given the question, the goal of the QA task is to find the correct answer. Since parts of candidate entities are in the knowledge graph , the potential associations in the KG will help improve the performance of the QA task. For the recommendation, given a set of items ( 1 , 2 , ..) for the specific user , the goal is to predict the potential items in which the user might be interested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relational Transformer Framework</head><p>3.2.1 Triple2Seq. As stated previously, the large-scale KG contains massive relational facts; thus, the whole relational graph can not be directly fed into the Transformer.</p><p>To alleviate such limitations of full-graph-based Transformers, inspired by <ref type="bibr" target="#b56">[55]</ref>, we propose Triple2Seq, which utilizes contextualized sub-graphs as input sequences to encode the local structural information. The contextualized sub-graph T here is defined as a node-set containing entities and relations. T consists of a center triple T and its surrounding neighborhood node set T :</p><formula xml:id="formula_3">T = T ? T</formula><p>(1) Note that the sub-graph sampling process aims to generate contextualized triples, then we can obtain a sampled neighborhood node set :</p><formula xml:id="formula_4">T = { | = , ?( , , ) ? N }<label>(2)</label></formula><p>where , ? E, ? R, and N is the fixed-size neighborhood triple set of the center triple T . To better capture the local structural feature, we leverage a dynamic sampling strategy during training and randomly select multiple contextualized sub-graphs for the same center triple in each epoch 2 .</p><p>Remark 1. Note that with Triple2Seq, which dynamically samples contextualized sub-graphs to construct input sequences, Transformers can be easily applied to large knowledge graphs. However, <ref type="bibr">2</ref> Multiple contextualized sub-graphs can be viewed as data augmentation for a triple. our approach focuses on heterogeneous graphs and regards edges (relation) as special nodes in contextualized sub-graphs for sequential modeling. Besides, the sampling process can also be viewed as a data augmentation operator in <ref type="bibr" target="#b56">[55]</ref>, which boosts the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2</head><p>Structure-enhanced Self-attention. In this section, we introduce the internal details of Relphormer. We first introduce the construction of node features and semantic features, then illustrate the structure-enhanced self-attention to inject vital structure information. Finally, we introduce a contextual contrastive learning strategy during the dynamic sampling to reduce the instability.</p><p>After the procedure of Triple2Seq, we can obtain the contextualized sub-graph T . Meanwhile, the local structure of the contextualized sub-graph is preserved in adjacency matrix A . We notice that entity-relation pair information is quite an essential signal for KGs, which proves the effectiveness such as the hierarchical Transformer framework of HittER <ref type="bibr">[3]</ref>. Thus, we represent entityrelation pairs as plain text and regard relations as the special nodes in the contextualized sub-graph. For example, the contextualized sub-graph is converted to { 1 , 2 , ..., }, which includes the nodes of center triple ( , , ). In this way, we can obtain node-pair information, including entity-relation, entity-entity and relation-relation pairs interaction. Another advantage is that the relation node can be seen as a special node. Since the number of relations in the knowledge graph is much smaller than that of entities, so it can maintain globally semantic information among the contextualized sub-graphs sampled by Triple2Seq. Notably, we also add a global node to explicitly preserve the global information following <ref type="bibr" target="#b56">[55]</ref>. The global node plays a similar role as [CLS] token in natural language pre-training models. We link the global node with nodes in the contextualized sub-graphs via a learnable virtual distance or fixed distance. So we obtain the final input sequence as { , 1 , 2 , ..., } to construct node features.</p><p>Note that the structural information with sequential input may be lost due to the fully-connected nature of the attention mechanism; we propose structure-enhanced self-attention to preserve vital structure information in contextualized sub-graphs. We utilize attention bias to capture the structural information between node pairs in the contextualized sub-graph. The attention bias is denoted as ( , ), which is a bias term between node and node .</p><formula xml:id="formula_5">= ( W )( W ) ? + ( , ) ( , ) = ( A 1 , A 2 , ..., A )<label>(3)</label></formula><p>A refers to the normalized adjacency matrix. The structure encoder is a linear layer with the A as the input, where is a hyper-parameter. A ( A to the -th power) refers the reachable relevance by taking steps from one node to the other node.</p><p>For some dense KGs, too many contextualized sub-graphs for the same center triple may tend to inconsistency during training. Thus, we leverage contextual contrastive strategy during the dynamic sampling to overcome the instability. We use the contextualized sub-graphs of the same triple in different epochs to enforce the model to conduct similar predictions. We encode the input sequence of a contextualized sub-graph and take the hidden vector as contextual representation at current epoch , and ?1 at last epoch ?1. The goal is to minimize the differences between different sub-graphs; we get the contextual loss L as:</p><formula xml:id="formula_6">? ( ( , ?1 )/ ) ( ( , ?1 )/ ) + ( ( , )/ )<label>(4)</label></formula><p>Where denotes a temperature parameter and ( , ?1 ) is the cosine similarity ?1 ? ? ? ? ?1 ? . is the hidden state representation at epoch , which belongs to different center triples.</p><p>Remark 2. It should be noted that our structure-enhanced Transformer is model-agnostic and, therefore, orthogonal to existing approaches, which injects semantic and structural information into the Transformer architecture. In contrast to <ref type="bibr" target="#b39">[38]</ref> where attention operations are only performed between nodes with literal edges in the original graph, structure-enhanced Transformer offers the flexibility in leveraging the local contextualized sub-graph structure and influence from the semantic features, which is convenient for information exchange between similar nodes in the local graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Masked Knowledge Modeling</head><p>In this section, we introduce our proposed masked knowledge modeling for knowledge graph representation. KGC aims to learn a mapping : T , A ? . Inspired by masked language modeling, we randomly mask specific tokens of the input sequences and then predict those masked tokens.</p><p>Given an input contextualized sub-graph node sequence T , we randomly mask tokens in the center triple. Concretely, the masked center triple will be ( , , ) or ( , , ) for link prediction task, and ( , , ) for relation prediction task, respectively. We denote the candidates set as , and the task of masked knowledge modeling is to predict the missing parts of the triple T given the masked node sequence T M and contextualized sub-graph structure A as:</p><formula xml:id="formula_7">T = (T ) ? (T , A ) ?<label>(5)</label></formula><p>where ? R | E |+| R | . Specifically, we randomly mask only one token for a sequence to better integrate the contextual information due to the unique structure of the contextualized sub-graph.</p><p>Intuitively, masked knowledge modeling is different from previous translational distance methods, which can avoid the defects brought by score-function-based methods. However, such as strategy may cause a serious label leakage problem for link prediction if we simultaneously sample the head and tail entity's neighborhood nodes. Note that during training, the structure of the predicted mask token should be unknown. To handle the label leakage issue and bridge the gap between training and testing, we remove the context nodes of target entities to ensure fair comparison.</p><p>Remark 3. Note that our masked knowledge modeling is similar to masked language modeling, thus, making it possible for large-scale heterogeneity graph pre-training. On the other hand, the advancement of empirical results illustrates that masked knowledge modeling may be a parametric score function approximator, which can automatically find a suitable optimization target for better link prediction. </p><p>Then we choose one term ( , , ). We use (?) ? (?), which acts as a score function role. Therefore, we hypothesize that masked knowledge modeling may be the score function approximator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization and Inference</head><p>The overall Relphormer optimization procedure is illustrated in Algorithm 1. During training, we jointly optimize masked knowledge loss and contextual contrastive learning objects. The L can be viewed as a constraint term to the whole loss L as follows:</p><formula xml:id="formula_9">L = L + L (7)</formula><p>where is a hyper-parameter, L and L are mask knowledge and contextual loss.</p><p>During inference, we use a multi-sampling strategy for testing following <ref type="bibr" target="#b56">[55]</ref>, which can enhance the stability of prediction:</p><formula xml:id="formula_10">= 1 ??<label>(8)</label></formula><p>where ? R | |?1 refers to the predicted result of one contextualized sub-graph, and = | T | denotes the number of sampled sub-graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Fine-tuning for KG-based Tasks</head><p>We introduce the fine-tuning strategy of Relphormer for KG-based downstream tasks such as question answering and recommendation tasks in this section, which is formulated as a mapping:</p><formula xml:id="formula_11">: Q , M ( ) ? (9)</formula><p>where Q is the masked query and M ( ) is the pre-trained KG transformer. For different downstream tasks, the masked query Q can be defined in different forms. For example, the masked query for QA is defined by [ ; <ref type="figure">[MASK]</ref>], which predicts the answer entities in the KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Dataset</head><p>For the Knowledge Graph completion (KGC) task, we use three publicly available datasets for evaluation: WN18RR <ref type="bibr" target="#b5">[5]</ref>, FB15K-237 <ref type="bibr" target="#b36">[35]</ref> and UMLS <ref type="bibr" target="#b0">[1]</ref>. WN18RR is a subset of the WordNet and contains a lexical knowledge graph for English. FB15K-237 is a subset of the Freebase and is constructed by limiting the set of relations in FB15K. UMLS is a small dataset that contains medical semantic entities and relations.</p><p>For the QA task, we use two publicly available datasets for evaluation: FreeBaseQA <ref type="bibr" target="#b16">[15]</ref>, WebQuestionSP <ref type="bibr" target="#b51">[50]</ref>. FreebaseQA is a data set for open-domain QA over the Freebase knowledge graph. WebQuestionSP contains semantic parses for the questions from WebQuestions that are answerable using Freebase. For the recommendation task, we adopt the well-established versions ML-20m in MovieLens <ref type="bibr" target="#b11">[11]</ref> dataset and obtain textual descriptions movies from Freebase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>For the KGC task, We compare Relphormer with several baseline models to demonstrate the effectiveness of our proposed approach. Firstly, we choose the tranlational-based models, such as TransE <ref type="bibr">[2]</ref> and RotatE <ref type="bibr" target="#b34">[33]</ref>. We regard R-GCN <ref type="bibr" target="#b31">[30]</ref> as a translational distance model because R-GCN still uses the score function as its decoder. Further, we compare our Relphormer with models based on Transformer architecture such as StAR <ref type="bibr" target="#b41">[40]</ref>.</p><p>For the QA task, we use BERT as the baseline in two datasets. Then we compare our Relphormer with HittER <ref type="bibr">[3]</ref>, where two models are first pre-trained in the FB15K-237 dataset and then injected into the BERT module. For the recommendation task, we choose the baseline models including BERT4Rec <ref type="bibr" target="#b33">[32]</ref>, SASRec <ref type="bibr" target="#b18">[17]</ref>, Caser <ref type="bibr" target="#b35">[34]</ref>, GRU4Rec <ref type="bibr" target="#b12">[12]</ref> and FPMC <ref type="bibr" target="#b27">[26]</ref>.</p><p>We use MR (Mean Rando), MRR (Mean Reciprocal Rank) and Hit@1, 3, 10 (Hit Ratio values) as main evaluation metrics. Detailed statistics is shown in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Settings</head><p>We initialize Relphormer with BERT for the entity prediction task. For the relation prediction task, we use [CLS] as a special token for the global node and do not utilize textual descriptions for a fair comparison with baseline models. In the knowledge downstream task, we feed the pre-trained weights or learned representations into downstream tasks. For the QA task, each question is labeled with an answering entity. We use the pre-trained knowledge graph transformer as the backbone. The question is concatenated with [MASK] token, and then we fine-tune the pre-trained Transformer. Following the settings in <ref type="bibr">[3]</ref>, we test the performance of KG representation models under Filterand Fullsetting. For the recommendation task, we first pre-train our Relphormer in the item-item network and then use it as initialization for each user's item prediction. Detailed settings are shown in Appendix A.3 <ref type="table" target="#tab_1">Table 2</ref>, we observe that our proposed approach can achieve competitive performance on all datasets compared with baselines. Relphormer achieves the best performance on Hits@1 and MRR metrics and yields the second-best performance on Hits@10 in WN18RR. Compared to the previous SOTA translational distance models, such as QuatE, our method has improvements in all metrics. We notice that Relphormer is superior to SOTA Transformer-based model HittER in WN18RR. In the FB15K-237 dataset, we also find that Relphormer outperforms most translational distance models. Compared with transformerbased models, we find that Relphormer has the best performance on hits@1, which is better than KG-BERT, StAR and HittER. HittER advances the performance in FB15K-237 since they explicitly utilize more transformer architectures, and our proposed method still obtains comparable performance. Besides, we notice that Relphormer obtains the best performance in UMLS, especially on Hits@10. The excellent performance of Relphormer proves that our relational transformer framework is helpful for knowledge completion tasks. <ref type="table" target="#tab_2">Table 3</ref>, we observe that Relphormer obtain competitive performance compared with baselines. In the WN18RR dataset, Relphormer can already outperform all baselines, which demonstrates the excellent performance of our approach for relation prediction. Compared to TransE, our method improves 15.8% on Hits@1, 9.7%. In the FB15K-237 dataset, the improvement of Relphormer is significant on Hits@3. Relphormer performs better than DistMult but worse than RotatE. <ref type="table" target="#tab_3">Table 4</ref>, we find that Relphormer obtains the best performance on FreebaseQA and WebQuestionSP. Compared to HittER, Relphormer improves 6.8% in the Full setting in the FreebaseQA dataset. Meanwhile, Relphormer improves 2.9% and 1.4% in the Full and Filter settings on WebQuestionSP. Note that Relphormer can be easily initialized with BERT and is optimized with masked knowledge modeling, so it is efficient to inject pretrained textual representations with Relphormer for the QA task, leading to better performance. <ref type="table" target="#tab_4">Table 5</ref> shows that Relphormer performs better than all baseline models in the recommendation task. Compared to BERT4REC, Relphormer improves 2% on Hits@1 and 1% on MRR. Moreover, Relphormer outperforms traditional recommendation models like SASRec and GRU4Rec. Similar to the QA task, the superiority of Relphormer is actually due to mask knowledge modeling. For downstream tasks, we can utilize the [MASK] token to    build an optimization object similar to pre-training to fully leverage the knowledge during pre-training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Knowledge Graph Completion Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Entity Prediction. From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Relation Prediction. From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">KG-based Task Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Question Answering. From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Recommendation.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>The number of sampled contextualized sub-graph triples. We investigate the performance of our model with different numbers of sampled contextualized sub-graph triples. We change the number of triples from 4 to 64 and report the results of entity prediction in WN18RR and FB15K-237 in <ref type="figure" target="#fig_2">Figure 3</ref>. When the number is smaller, a few contextual nodes are sampled surrounding the center triple. It is evident to see that increasing contextualized sub-graphs give a boost in performance in both two datasets, which indicates that our proposed method can effectively encode contextual information. However, when the number is too large, the performance of the model will no longer increase. We argue that although neighborhood information is beneficial, too many unrelated nodes will bring out unnecessary noisy signals, thus, affecting the performance. Note that those low-quality or irrelevant nodes may lead to negative contextual information infusion, which is detrimental to the performance. It is intuitive to investigate selective contextual information integration, and we leave this for future works.</p><p>Structure-enhanced self-attention. To study the effects of structureenhanced attention in Relphormer, we conduct the ablation study as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. All models without structure-enhanced attention have a performance decay. We random an instance and visualize  <ref type="figure">Figure 5</ref>: Attention visualization study on FB15K-237. We choose a center triple (Z?rich, Travel, October) and its contextualized sub-graph triples. Left is the attention layer with structure-enhanced self-attention.</p><p>its attention matrix to analyze the impact of structure-enhanced self-attention. From <ref type="figure">Figure 5</ref>, given a center triple of (Z?rich, Travel, October) 3 , we observe that models with structure-enhanced selfattention have a vital impact to attentive weights. Concretely, injecting structural information with structure-enhanced self-attention can capture the semantic correlation of distance entities. For example, one entity can learn the structure correlation with other faraway entities in contextualized sub-graph.</p><p>Optimization object. In the knowledge graph, there are relationspecific patterns, and some approaches can not solve patterns like 1-N, N-1 and N-N relations. For example, given a specific entityrelation pair (?, ), there usually exists more than one tail entity as labels. It seems to be more significant to investigate whether different optimization objects have an impact on Relphormer. Specifically, we conduct an ablation study without masked knowledge modeling (w/o MKM) but using the negative log-likelihood loss instead. From <ref type="table" target="#tab_3">Table 4</ref>, we notice that the model with MKM can yield better performance on Hit@1 on both datasets; however, it fails to achieve advancement in MR on WN18RR. This may be because WN18RR lacks enough structural features.</p><p>Global node. We further conduct an ablation study to analyze the impact of the global node. w/o global node refers to the model without the global node. From <ref type="figure" target="#fig_3">Figure 4</ref>, we can see that the model without a global node achieves poorer performance than baselines. It demonstrates that the global node might be a more helpful solution to preserving global information.  Inference speed comparison. We further compare the inference speed between Relphormer and other Transformer models. From <ref type="table" target="#tab_5">Table 6</ref>, we notice that the approach with masked knowledge modeling obtains faster inference speed than KG-BERT. We note that KG-BERT uses the Transformer as its encoder and the translational distance score function as the decoder. Since KG-BERT has to repeatedly calculate the scores of candidates thus, it is timeconsuming during inference. However, Relphormer leverages the masked knowledge modeling strategy, and the model infers the targets by predicting the masked entities or relations. Although the Triple2Seq procedure costs some time, Relphormer is still much faster than KG-BERT in inference. From <ref type="table" target="#tab_6">Table 7</ref>, we report the comparisons for three KG transformer models. We initialize all models with BERT-base for fairness and notice that our approach is more efficient than StAR and KG-BERT, which is basically in line with the analysis result from <ref type="table" target="#tab_5">Table 6</ref> We further provide case analysis of different models in Appendix A.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose Relphormer and evaluate its effectiveness in knowledge graph completion, question answering and recommendation. Our approach illustrates that Transformer architecture can be suitable for modeling KGs. We point out three directions for future work. First, as we discussed in Section 3.2.1, Triple2Seq is a simple but effective way to consider context information to address the limitation of input length for the Transformer. It is worth studying what contexts play a more critical role in the triple and proposing efficient sampling strategies. Second, it is a promising direction to reduce the memory consumption of Relphormer for real-world KGs with an anchor-based approach like NodePiece <ref type="bibr" target="#b9">[9]</ref>. <ref type="table">Table 8</ref>: Ranking results of target entities for different approaches. The first column includes incomplete triples for inference, as well as their labels. And the others include the ranking position and whether the link prediction result is right (ranking is not more than 10).     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incomplete</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The model architecture of Relphormer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 : 2 :4 7 T , A ? 2 ( 10 Calculate</head><label>127210</label><figDesc>Relphormer Algorithm 1 M ( ): model of Relphormer ; learning rate, ? ? : initial parameters of Relphormer ; 3 ? ? : pre-trained semantic parameters; Initialize pre-trained parameters of ? ; 5 Initialize model M ; 6 while M ( ) not converged do // Obtain the contextualized sub-graph sequence loss of M and update ; 11 end Hypothesis 1. (score function approximator) Let T be the masked triplet, h ? R is the masked head derived from multi-head attention layers in Relphormer M ( ). The vocab token embedding is ? R ? , where = |E | + |R|. If the T is the triplet ( , , [MASK]), where the tail entity is masked. We define the (?) function as the multi-head attention layer modules and ? the candidate tail entities embeddings. The output logits are sigmoid( h), approximately equal to sigmoid(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Ablation study on WN18RR and FB15K-237. Results of the increasing number of contextualized sub-graph triples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Ablation study on WN18RR and FB15K-237. Left: results of different model variants on Hits@1. Right: results of different model variants on Mean Rank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of the link prediction on FB15K-237, WN18RR and UMLS. The bold numbers denote the best results, while the underlined ones are the second-best performance.</figDesc><table><row><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell><cell></cell><cell>FB15K-237</cell><cell></cell><cell>UMLS</cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">Hits@1 Hits@10 MRR Hits@1 Hits@10 MRR Hits@10 MR</cell></row><row><cell>Translational distance models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TransE [2]</cell><cell>0.061</cell><cell>0.522</cell><cell>0.232</cell><cell>0.218</cell><cell>0.495</cell><cell>0.310</cell><cell>0.989</cell><cell>1.84</cell></row><row><cell>R-GCN [30]</cell><cell>0.080</cell><cell>0.207</cell><cell>0.123</cell><cell>0.100</cell><cell>0.300</cell><cell>0.164</cell><cell>-</cell><cell>-</cell></row><row><cell>DistMult [47]</cell><cell>0.412</cell><cell>0.504</cell><cell>0.444</cell><cell>0.199</cell><cell>0.446</cell><cell>0.281</cell><cell>0.846</cell><cell>5.52</cell></row><row><cell>ConvE [6]</cell><cell>0.419</cell><cell>0.531</cell><cell>0.456</cell><cell>0.225</cell><cell>0.497</cell><cell>0.312</cell><cell>0.990</cell><cell>1.51</cell></row><row><cell>ComplEx [36]</cell><cell>0.409</cell><cell>0.530</cell><cell>0.449</cell><cell>0.194</cell><cell>0.450</cell><cell>0.278</cell><cell>0.967</cell><cell>2.59</cell></row><row><cell>RotatE [33]</cell><cell>0.428</cell><cell>0.571</cell><cell>0.476</cell><cell>0.241</cell><cell>0.533</cell><cell>0.338</cell><cell>-</cell><cell>-</cell></row><row><cell>QuatE [52]</cell><cell>0.436</cell><cell>0.564</cell><cell>0.481</cell><cell>0.221</cell><cell>0.495</cell><cell>0.311</cell><cell>-</cell><cell>-</cell></row><row><cell>Transformer-based models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HittER [3]</cell><cell>0.436</cell><cell>0.579</cell><cell>0.485</cell><cell>0.279</cell><cell>0.558</cell><cell>0.373</cell><cell>-</cell><cell>-</cell></row><row><cell>KG-BERT [49]</cell><cell>0.041</cell><cell>0.524</cell><cell>0.216</cell><cell>-</cell><cell>0.420</cell><cell>-</cell><cell>0.990</cell><cell>1.47</cell></row><row><cell>StAR [40]</cell><cell>0.243</cell><cell>0.709</cell><cell>0.401</cell><cell>0.205</cell><cell>0.482</cell><cell>0.296</cell><cell>0.991</cell><cell>1.49</cell></row><row><cell>Relphormer</cell><cell>0.448</cell><cell>0.591</cell><cell cols="2">0.495 0.314</cell><cell>0.481</cell><cell>0.371</cell><cell>0.992</cell><cell>1.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of relation prediction in FB15K-237, WIN18RR.</figDesc><table><row><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell><cell></cell><cell>FB15K-237</cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">MRR ? Hit@1 Hit@3 MRR ? Hit@1 Hit@3</cell></row><row><cell>TransE</cell><cell>0.784</cell><cell>0.669</cell><cell>0.870</cell><cell>0.966</cell><cell>0.946</cell><cell>0.984</cell></row><row><cell>ComplEx</cell><cell>0.840</cell><cell>0.777</cell><cell>0.880</cell><cell>0.924</cell><cell>0.879</cell><cell>0.970</cell></row><row><cell>DistMult</cell><cell>0.847</cell><cell>0.787</cell><cell>0.891</cell><cell>0.875</cell><cell>0.806</cell><cell>0.936</cell></row><row><cell>RotatE</cell><cell>0.799</cell><cell>0.735</cell><cell>0.823</cell><cell>0.970</cell><cell>0.951</cell><cell>0.980</cell></row><row><cell>DRUM</cell><cell>0.854</cell><cell>0.778</cell><cell>0.912</cell><cell>0.959</cell><cell>0.905</cell><cell>0.958</cell></row><row><cell cols="2">Relphormer 0.897</cell><cell cols="2">0.827 0.967</cell><cell>0.958</cell><cell>0.930</cell><cell>0.986</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of QA accuracy of combining HittER and BERT in two Freebase-based question answering datasets.</figDesc><table><row><cell></cell><cell cols="2">FreebaseQA</cell><cell cols="2">WebQuestionSP</cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Full Filtered Full</cell><cell>Filter</cell></row><row><cell>BERT</cell><cell>19.8</cell><cell>30.8</cell><cell>23.2</cell><cell>46.5</cell></row><row><cell>+HittER</cell><cell>21.2</cell><cell>37.1</cell><cell>27.1</cell><cell>51.0</cell></row><row><cell cols="2">+Relphormer 28.0</cell><cell>37.1</cell><cell>30.0</cell><cell>52.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of Recommendation on MovieLens.</figDesc><table><row><cell>Model</cell><cell cols="2">Hits@1 MRR</cell></row><row><cell>FPMC</cell><cell>0.11</cell><cell>0.23</cell></row><row><cell>GRU4Rec</cell><cell>0.20</cell><cell>0.35</cell></row><row><cell>Caser</cell><cell>0.12</cell><cell>0.25</cell></row><row><cell>SASRec</cell><cell>0.25</cell><cell>0.40</cell></row><row><cell>BERT4REC</cell><cell>0.34</cell><cell>0.48</cell></row><row><cell>Relphormer</cell><cell>0.36</cell><cell>0.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Inference efficiency comparison. |E |, |R| and |G| are numbers of entities, relations, inference samples in the graph respectively. is the length of input sequence. The time refers to the speed of inference time given a single (entity, relation) on FB15K-237.</figDesc><table><row><cell cols="2">Inference Method</cell><cell>Complexity</cell><cell>Speed up</cell></row><row><cell></cell><cell>KG-BERT</cell><cell>O ( (|E | + |R|))</cell><cell>? |E |</cell></row><row><cell>Triple</cell><cell cols="2">Relphormer O ( + 1)</cell></row><row><cell></cell><cell>KG-BERT</cell><cell cols="2">O ( (|E | + |R|) ? G) ? |E |</cell></row><row><cell>Graph</cell><cell cols="2">Relphormer O (( + 1) ? G)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparisons with KG-BERT and StAR on FB15K-237. "T/Ep" stands for time per training epoch and "Infer" denotes inference time on test set. The time was collected on one 1080-Ti GPU.</figDesc><table><row><cell></cell><cell cols="5">Hits@1 @3 @10 MRR T/Ep Infer</cell></row><row><cell>KG-BERT</cell><cell>-</cell><cell>-</cell><cell>.420</cell><cell>-</cell><cell>?10h ?1500h</cell></row><row><cell>StAR</cell><cell>.162</cell><cell cols="2">.255 .400</cell><cell>.240</cell><cell>?9.5h ?8h</cell></row><row><cell cols="2">Relphormer .314</cell><cell cols="3">.393 .481 .371</cell><cell>?1.5h ? 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Triple Positive entity ranking position &amp; Link prediction result is True or False Relphormer[Textual Encoding] StAR [Textual Encoding] RotatE [Graph Embedding] (? , /music/genre/artists, Danzig)</figDesc><table><row><cell>? Blues-rock</cell><cell>2, True</cell><cell>64, False</cell><cell>7, True</cell></row><row><cell>(Marquette University, school type, ?) ? Society of Jesus-GB</cell><cell>3, True</cell><cell>18, False</cell><cell>5, True</cell></row><row><cell>(? , /music/record label/artist, John 5) ? Interscope Records</cell><cell>2, True</cell><cell>40, False</cell><cell>10, True</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Statistics of KG completion datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="5">#Rel. #Ent. #Train #Dev #Test</cell></row><row><cell>FB15K-237</cell><cell>237</cell><cell cols="4">14,541 272,115 17,535 20,466</cell></row><row><cell>WN18RR</cell><cell>18</cell><cell cols="3">40,943 141,442 5,000</cell><cell>5,000</cell></row><row><cell>UMLS</cell><cell>46</cell><cell>135</cell><cell>5,216</cell><cell>652</cell><cell>661</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Statistics for FreebaseQA and WebQuestionSP datasets</figDesc><table><row><cell></cell><cell cols="2">FreebaseQA</cell><cell cols="2">WebQuestionSP</cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Full</cell><cell cols="2">Filtered Full</cell><cell>Filter</cell></row><row><cell>Train</cell><cell>20,358</cell><cell>3,713</cell><cell>3,098</cell><cell>850</cell></row><row><cell>Test</cell><cell>3,996</cell><cell>755</cell><cell>1,639</cell><cell>484</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Statistics for Knowledge Graph RecommendationDataset #users #items #actions #Avg. length #Density</figDesc><table><row><cell>ML-20m 138,493 26,744</cell><cell>20m</cell><cell>144.4</cell><cell>0.54%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>The difference between Relphormer and previous Transformer-based approaches for KGs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The length of input sequence is 12.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Dataset Details</head><p>Detailed statistics of dataset are shown in <ref type="table">Table 9</ref>, <ref type="table">Table 10</ref> and Table11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Case Details</head><p>As shown in <ref type="table">Table 8</ref>, we list several triples in the FB15k-237 dataset, where the prediction result obtains obvious improvements when applying our Relphormer. Relphormer can simultaneously handle the structural and textual features and solve the heterogeneity problem of both entity and relation levels. As a result, our approach still keeps excellent performance for some difficult samples with plenty of heterogeneous features. ideas may be introduced by different research teams. Here, we list the major difference between our model and other approaches as shown in <ref type="table">Table 12</ref>: To conclude, our approach mainly focuses on KGs, which are heterogeneity graphs. However, it can also be applied to pure graph representation for node classification or graph classification tasks. On the other hand, our approach is different from previous Transformer-based KG representation approaches (Hitter, KG-BERT). Note that Relphormer utilizes both semantic and structure feature for KG representation and leverage dynamic graph context with vital structure information. Thus, Relphormer can better represent KG representation to boost performance. Moreover, Relphormer can be applied to various KG-based tasks, such as QA and recommendation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Implementation Details</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bodenreider</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkh061</idno>
		<ptr target="https://doi.org/10.1093/nar/gkh061" />
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<editor>J. C. Burges, L?on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">HittER: Hierarchical Transformers for Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="7" to="11" />
		</imprint>
		<respStmt>
			<orgName>Virtual Event / Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<idno type="DOI">10.18653/v1/2021.emnlp-main.812</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.emnlp-main.812" />
		<title level="m">Association for Computational Linguistics</title>
		<editor>Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih</editor>
		<imprint>
			<date type="published" when="2021-11" />
			<biblScope unit="page" from="10395" to="10407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge-Driven Stock Trend Prediction and Explanation via Temporal Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308560.3317701</idno>
		<ptr target="https://doi.org/10.1145/3308560.3317701" />
	</analytic>
	<monogr>
		<title level="m">Companion of The 2019 World Wide Web Conference</title>
		<editor>Sihem Amer-Yahia, Mohammad Mahdian, Ashish Goel, Geert-Jan Houben, Kristina Lerman, Julian J. McAuley, Ricardo Baeza-Yates, and Leila Zia</editor>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05-13" />
			<biblScope unit="page" from="678" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><forename type="middle">G</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=xMJWUKJnFSw" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022-04-25" />
		</imprint>
	</monogr>
	<note>ICLR 2022, Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Survey on Knowledge Graph-Based Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2020.3028705</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2020.3028705" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3549" to="3568" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The MovieLens Datasets: History and Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2827872</idno>
		<ptr target="https://doi.org/10.1145/2827872" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Interact. Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent Neural Networks with Top-k Gains for Session-based Recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bal?zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018<address><addrLine>Torino, Italy; James Allan</addrLine></address></meeting>
		<imprint>
			<publisher>Norman W</publisher>
			<date type="published" when="2018-10-22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divesh</forename><surname>Paton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">Z</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sel?uk Candan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3269206.3271761</idno>
		<ptr target="https://doi.org/10.1145/3269206.3271761" />
		<editor>Alexandros Labrinidis, Assaf Schuster, and Haixun Wang</editor>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding via Dynamic Mapping Matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/p15-1067</idno>
		<ptr target="https://doi.org/10.3115/v1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15" to="1067" />
		</imprint>
	</monogr>
	<note>Long Papers. The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2021.3070843</idno>
		<ptr target="https://doi.org/10.1109/TNNLS.2021.3070843" />
	</analytic>
	<monogr>
		<title level="j">A Survey on Knowledge Graphs: Representation, Acquisition, and Applications. IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="494" to="514" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1028</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1028" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="323" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Heterformer: A Transformer Architecture for Node Representation Learning on Heterogeneous Text-Rich Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bowen Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.10282</idno>
		<idno type="arXiv">arXiv:2205.10282</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2205.10282" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-Attentive Sequential Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2018.00035</idno>
		<ptr target="https://doi.org/10.1109/ICDM.2018.00035" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining, ICDM 2018</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-11-17" />
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJU4ayYgl" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">LP-BERT: Multi-task Pre-training Knowledge Graph BERT for Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukai</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04843</idno>
		<ptr target="https://arxiv.org/abs/2201.04843" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<editor>Blai Bonet and Sven Koenig</editor>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015-01-25" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reinforced Anchor Knowledge Graph Generation for News Recommendation Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3467315</idno>
		<ptr target="https://doi.org/10.1145/3447548.3467315" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<editor>Feida Zhu, Beng Chin Ooi, and Chunyan Miao</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-08-14" />
			<biblScope unit="page" from="1055" to="1065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3534678.3539472</idno>
		<idno>1120-1130</idno>
		<ptr target="https://doi.org/10.1145/3534678.3539472" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<editor>Aidong Zhang and Huzefa Rangwala</editor>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-08-14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">DensE: An Enhanced Non-Abelian Group Representation for Knowledge Graph Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04548</idno>
		<ptr target="https://arxiv.org/abs/2008.04548" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.findings-acl.282" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<editor>Smaranda Muresan, Preslav Nakov, and Aline Villavicencio</editor>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-05-22" />
			<biblScope unit="page" from="3570" to="3581" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">GraphiT: Encoding Graph Structure in Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margot</forename><surname>Selosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05667</idno>
		<ptr target="https://arxiv.org/abs/2106.05667" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Factorizing personalized Markov chains for next-basket recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="DOI">10.1145/1772690.1772773</idno>
		<ptr target="https://doi.org/10.1145/1772690.1772773" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web, WWW 2010</title>
		<editor>Michael Rappa, Paul Jones, Juliana Freire, and Soumen Chakrabarti</editor>
		<meeting>the 19th International Conference on World Wide Web, WWW 2010<address><addrLine>Raleigh, North Carolina, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010-04-26" />
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-Supervised Graph Transformer on Large-Scale Molecular Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/94aef38441efa3380a3bed3faf1f9d5d-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<imprint>
			<date type="published" when="2020-12-06" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence-to-Sequence Knowledge Graph Completion and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Kochsiek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.201</idno>
		<ptr target="https://doi.org/10.18653/v1/2022.acl-long.201" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Smaranda Muresan, Preslav Nakov, and Aline Villavicencio</editor>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-22" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2814" to="2828" />
		</imprint>
	</monogr>
	<note>ACL 2022</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93417-4_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-93417-4_38" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference</title>
		<editor>Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler, Rapha?l Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam</editor>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-06-03" />
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
	<note>Proceedings (Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93417-4_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-93417-4_38" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference</title>
		<editor>Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler, Rapha?l Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam</editor>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-06-03" />
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
	<note>Proceedings (Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A Decade of Knowledge Graphs in Natural Language Processing: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Schopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juraj</forename><surname>Vladika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Matthes</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.00105</idno>
		<idno type="arXiv">arXiv:2210.00105</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2210.00105" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357384.3357895</idno>
		<ptr target="https://doi.org/10.1145/3357384.3357895" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019</title>
		<editor>Wenwu Zhu, Dacheng Tao, Xueqi Cheng, Peng Cui, Elke A. Rundensteiner, David Carmel, Qi He, and Jeffrey Xu Yu</editor>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkgEQnRqYQ" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3159652.3159656</idno>
		<ptr target="https://doi.org/10.1145/3159652.3159656" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018</title>
		<editor>Chang, Chengxiang Zhai, Yan Liu, and Yoelle Maarek</editor>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018<address><addrLine>Marina Del Rey, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-02-05" />
			<biblScope unit="page" from="565" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd workshop on continuous vector space models and their compositionality</title>
		<meeting>the 3rd workshop on continuous vector space models and their compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/trouillon16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<editor>Maria-Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Composition-based Multi-Relational Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BylA_C4tPr" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA, Isabelle Guyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442381.3450043</idno>
		<ptr target="https://doi.org/10.1145/3442381.3450043" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;21: The Web Conference 2021, Virtual Event / Ljubljana</title>
		<editor>Jure Leskovec, Marko Grobelnik, Marc Najork, Jie Tang, and Leila Zia</editor>
		<meeting><address><addrLine>Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-04-19" />
			<biblScope unit="page" from="1737" to="1748" />
		</imprint>
	</monogr>
	<note>ACM / IW3C2</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.295</idno>
		<ptr target="https://doi.org/10.18653/v1/2022.acl-long.295" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Smaranda Muresan, Preslav Nakov, and Aline Villavicencio</editor>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-22" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4281" to="4294" />
		</imprint>
	</monogr>
	<note>ACL 2022</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">KGAT: Knowledge Graph Attention Network for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330989</idno>
		<ptr target="https://doi.org/10.1145/3292500.3330989" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<editor>Ankur Teredesai, Vipin Kumar, Ying Li, R?mer Rosales, Evimaria Terzi, and George Karypis</editor>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-08-04" />
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8531" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<editor>Carla E. Brodley and Peter Stone</editor>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence<address><addrLine>Qu?bec City, Qu?bec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2014-07-27" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficiently embedding dynamic knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arijit</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2022.109124</idno>
		<ptr target="https://doi.org/10.1016/j.knosys.2022.109124" />
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">250</biblScope>
			<biblScope unit="page">109124</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Representing Long-Range Context for Graph Neural Networks with Global Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paras</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/6e67691b60ed3e4a55935261314dd534-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<editor>Marc&apos;Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan</editor>
		<imprint>
			<date type="published" when="2021-12-06" />
			<biblScope unit="page" from="13266" to="13279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">PromptKG: A Prompt Learning Framework for Knowledge Graph Representation Learning and Application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoubo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.00305</idno>
		<idno type="arXiv">arXiv:2210.00305</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2210.00305" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6575" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">GraphFormers: GNN-nested Language Models for Linked Text Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02605</idno>
		<ptr target="https://arxiv.org/abs/2105.02605" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">KG-BERT: BERT for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03193</idno>
		<ptr target="http://arxiv.org/abs/1909.03193" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The Value of Semantic Parse Labeling for Knowledge Base Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jina</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-2033</idno>
		<ptr target="https://doi.org/10.18653/v1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="16" to="2033" />
		</imprint>
	</monogr>
	<note>Short Papers. The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Do Transformers Really Perform Badly for Graph Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021</title>
		<editor>Marc&apos;Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan</editor>
		<imprint>
			<date type="published" when="2021-12-06" />
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="28877" to="28888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Quaternion Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/d961e9f236177d65d21100592edb0769-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="2731" to="2741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced Language Representation with Informative Entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1139</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1139" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<editor>Long Papers, Anna Korhonen, David R. Traum, and Llu?s M?rquez</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rethinking Graph Convolutional Networks in Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3485447.3511923</idno>
		<ptr target="https://doi.org/10.1145/3485447.3511923" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;22: The ACM Web Conference 2022, Virtual Event</title>
		<editor>Fr?d?rique Laforest, Rapha?l Troncy, Elena Simperl, Deepak Agarwal, Aristides Gionis, Ivan Herman, and Lionel M?dini</editor>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-04-25" />
			<biblScope unit="page" from="798" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Gophormer: Ego-Graph Transformer for Node Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianlong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13094</idno>
		<ptr target="https://arxiv.org/abs/2110.13094" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">All optimizations are performed with the AdamW optimizer with a linear warmup of the learning rate over the first 10% of gradient updates to a maximum value, then linear decay over the remainder of the training. Gradients are clipped if their norm exceeds 1.0, and weight decay on all non-bias parameters is set to 0.01. Early stopping is adopted to reduce over-fitting on the training set. Specifically, we use BERT-base to encode entity descriptions and relation descriptions for semantic node features. The overall architecture of Relphormer consists of 12 and 8 Transformer layers for entity and relation prediction, respectively. The number of contextualized sub-graph nodes is chosen from set {8, 16, 32, 64} by tuning on the development set. We detail the hyper-parameter as follows: (1) Entity prediction: WN18RR: ? learning rate 3e-5 ? max epoch</title>
	</analytic>
	<monogr>
		<title level="m">This section contains details about the training procedures and hyperparameters for each dataset. We utilize Pytorch to conduct experiments with 1 Nvidia 3090 GPU</title>
		<imprint/>
	</monogr>
	<note>True, False] FB15K237: ? learning rate: 3e-5 ? max epoch. 20, 25, 30] ? batch size: [32, 64, 128] ? max triplet: [0, 4, 8, 16, 32, 64] ? max seq length: [64, 128] ? bce: [True, False] UMLS: ? learning rate: 3e-5 ? max epoch: [20, 25, 30] ? batch size: [32, 64, 128] ? max triplet: [0, 4, 8, 16, 32, 64] ? max seq length: [64, 128] ? bce: [True, False</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<idno>16, 32] ? dropout rate: 0.08 ? input dropout rate: 0.08 ? attention dropout rate: 0.08</idno>
		<title level="m">WN18RR: ? learning rate</title>
		<imprint/>
	</monogr>
	<note>4, 8, 16] ? max seq length: [9, 17, 33] ? hidden dim: [256, 512] ? feed-forward dimension: [512, 1024] ? layers: [8, 16, 32] ? heads. 1e-3, 1e-4] ? max epoch: [25, 50, 100] ? batch size: [32, 64, 128] ? max triplet: [4, 8, 16] ? max seq length: [9, 17, 33] ? hidden dim: [256, 512] ? feed-forward dimension: [512, 1024] ? layers: [8, 16, 32] ? heads: [8, 16, 32] ? dropout rate: 0.08 ? input dropout rate: 0.08 ? attention dropout rate: 0.08</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qa Setting</surname></persName>
		</author>
		<idno>3e-5] ? weight decay: [0.01</idno>
		<title level="m">the QA task, we recode the AnswerEntity to fit the Relphormer if the answer has appeared in the training entities. The following are our hyper-parameter: FreebaseQA: ? learning rate</title>
		<imprint/>
	</monogr>
	<note>WebQuestionSP: ? learning rate. 3e-5] ? weight decay: [0.01</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<title level="m">Recommendation Setting: ML-20M: ? weight decay</title>
		<imprint/>
	</monogr>
	<note>0.01] ? label smoothing [0.2</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Comparison with Previous Studies Since Transformer-based architecture has become a general encoder for widespread areas</title>
		<imprint/>
	</monogr>
	<note>it appeals to many researchers. Due to the fast development of Transformer architecture, some similar</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
