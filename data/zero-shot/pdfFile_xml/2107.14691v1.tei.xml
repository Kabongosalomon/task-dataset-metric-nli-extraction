<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EMAILSUM: Abstractive Email Thread Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
							<email>shiyue@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill ? Facebook AI Research ? Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
							<email>aslic@fb.comjfgao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill ? Facebook AI Research ? Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill ? Facebook AI Research ? Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill ? Facebook AI Research ? Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EMAILSUM: Abstractive Email Thread Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent years have brought about an interest in the challenging task of summarizing conversation threads (meetings, online discussions,  etc.). Such summaries help analysis of the long text to quickly catch up with the decisions made and thus improve our work or communication efficiency. To spur research in thread summarization, we have developed an abstractive Email Thread Summarization (EMAILSUM) dataset, which contains humanannotated short (&lt;30 words) and long (&lt;100 words) summaries of 2,549 email threads (each containing 3 to 10 emails) over a wide variety of topics. We perform a comprehensive empirical study to explore different summarization techniques (including extractive and abstractive methods, single-document and hierarchical models, as well as transfer and semisupervised learning) and conduct human evaluations on both short and long summary generation tasks. Our results reveal the key challenges of current abstractive summarization models in this task, such as understanding the sender's intent and identifying the roles of sender and receiver. Furthermore, we find that widely used automatic evaluation metrics (ROUGE, BERTScore) are weakly correlated with human judgments on this email thread summarization task. Hence, we emphasize the importance of human evaluation and the development of better metrics by the community. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As one of the major natural language generation tasks, automatic summarization has been studied for decades. Most research efforts were focused on single-document summarization tasks, e.g., news document summarization <ref type="bibr">(Hermann et al., 2015;</ref><ref type="bibr" target="#b15">Narayan et al., 2018)</ref>. However, living in an information era, we are facing with diverse content Email Thread: Subject: lunch this week Susan: All, Regarding our lunch this week to celebrate the one year anniversaries for Michelle &amp; David, and Mark's birthday, I have a request to make it Wednesday instead of Tuesday. Does anyone have an objection to this? Susan David: I have another lunch engagement Wed, but I will skip it if everyone else wants to move our lunch. David Tamra: Susan, Wednesday works out better for me as well. I have a doctor's appointment tomorrow during lunch. Tamra</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short Summary:</head><p>Susan emails everyone about an anniversary and offers to change the date. David says he is busy but is willing to go with the majority. Tamra agrees with Susan's date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long Summary:</head><p>Susan emails everyone about a lunch to celebrate a one year anniversary as well as Mark's birthday. She says she would change the date to a different day. David says he is busy that day with his own appointment but is willing to go with the majority and cancel that appointment to make this one. Tamra agrees with Susan's date as she is busy Tuesday with an appointment. <ref type="table">Table 1</ref>: An email thread and human-written short and long summaries from our EMAILSUM Dataset. in different structures. The summarization need is varied along with different application scenarios. Recently, there is an increasing research interest in diverse summarization tasks <ref type="bibr">(Gao et al., 2020)</ref>, e.g., timeline <ref type="bibr" target="#b0">(Allan et al., 2001)</ref>, query-based <ref type="bibr">(Li and Li, 2014)</ref>, multi-modal <ref type="bibr" target="#b28">(Zhu et al., 2018)</ref>, meeting <ref type="bibr" target="#b4">(Carletta et al., 2006)</ref>, dialogue or discussion thread <ref type="bibr" target="#b14">(Misra et al., 2015;</ref><ref type="bibr">Gliwa et al., 2019;</ref><ref type="bibr" target="#b20">Rameshkumar and Bailey, 2020)</ref>, etc. Following the branch of dialogue or thread summarization, we introduce a new abstractive Email Thread Summarization (EMAILSUM) dataset.</p><p>Email threads are widely used at work. An email thread is a special type of dialogue that usually has a specific structure (sender, receiver, greeting line, main body, and the signature), contains technical information, and involves multiple speakers. Unlike a conversational dialog turn, an email in a thread is much longer with longer sentences, multiple action items or requests, and stylistically similar to written text. Studies have shown that on average a worker sends/receives 122 business emails <ref type="bibr" target="#b18">(Radicati, 2015)</ref> and spends more than 3 hours on those emails (Adobe, 2019) per day. One possible reason is that sometimes people have to read through the entire conversation before replying to the latest email. This happens when you forget the main points of previous discussions or you are newly included in a discussion thread. Therefore, automatically summarizing email threads can improve our work efficiency and provides practical benefits. Email Thread Summarization is not a new task. <ref type="bibr" target="#b3">Carenini et al. (2007)</ref> collected extractive summaries of 39 email threads from Enron email corpus <ref type="bibr">(Klimt and Yang, 2004)</ref> and proposed to use a fragment quotation graph and clue words to conduct summarization. <ref type="bibr" target="#b22">Ulrich et al. (2008)</ref> collected both extractive and abstractive summaries of 40 threads from W3C email corpus <ref type="bibr" target="#b7">(Craswell et al., 2006)</ref> plus speech acts, meta sentences, etc. However, this task has been much less studied compared to other summarization tasks, partially due to the lack of large labeled email thread datasets.</p><p>In this paper, we collect human-written short (&lt; 30 words) and long (&lt; 100 words) abstractive summaries of 2,549 email threads constructed from Avocado Research Email Collection <ref type="bibr" target="#b17">(Oard et al., 2015)</ref>, which is 64? the size of previously labeled email thread datasets <ref type="bibr" target="#b3">(Carenini et al., 2007;</ref><ref type="bibr" target="#b7">Craswell et al., 2006)</ref>. We limit each thread to a minimum of 3 and a maximum of 10 emails, an example is given in <ref type="table">Table 1</ref>. We also extract 8,594 unlabeled email threads from both Avocado and W3C to facilitate semi-supervised learning. 2 See Section 2 for details of data collection.</p><p>Next, we present comprehensive baselines from different learning paradigms as a benchmark for our new email summarization dataset. Specifically, we explore different summarization techniques, including extractive and abstractive summarization methods, single-document and hierarchical models, transfer learning, and semi-supervised learning for both short and long summary generation. Experiments demonstrate that utilizing pretrained language model (e.g., T5 <ref type="bibr" target="#b19">(Raffel et al., 2020)</ref>) is critical due to the small size of our data; taking the email thread as a single document sets up a good baseline; transferring from news or dialogue datasets barely improve the performance; using hierarchical encoders only marginally improves it; while semi-supervised learning by using unlabelled email threads significantly (p &lt; 0.01) improves ROUGE <ref type="bibr">(Lin, 2004)</ref> scores in some cases.</p><p>Lastly, to better understand how well the email thread summarization models perform and investigate the correlation between automatic metrics and human judgment, we ask humans to rate the "salience" (how well the model summarizes salient points) and "faithfulness" (how well the model stays true to the email thread) of model-generated summaries, as well as to perform a pairwise comparison between our best and base models. We find that even though semi-supervised learning improves ROUGE scores, human judges still favor the summary generated by the baseline model (T5 base ). Two frequent errors made by the model are (1) failing to understand the sender's intent and (2) failing to identify the roles of the sender and receiver. Relatedly, human correlation analysis reveals that automatic metrics <ref type="bibr">(ROUGE (Lin, 2004)</ref>, BERTScore <ref type="bibr" target="#b26">(Zhang et al., 2019)</ref>) are poorly correlated with human judgment, which stresses the importance of human evaluation in this task and the requirement for better metrics to be proposed. Overall, in this work, we propose the new EMAIL-SUM dataset that provides a larger resource for studying the email thread summarization task. We conduct a comprehensive empirical model study and human evaluation analysis, which will serve as an important starting point for future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">EMAILSUM Dataset</head><p>To collect email thread summarization data, we first need to obtain unlabeled email threads. We resort to existing email collections: Enron <ref type="bibr">(Klimt and Yang, 2004)</ref>, W3C <ref type="bibr" target="#b7">(Craswell et al., 2006)</ref>, and Avocado <ref type="bibr" target="#b17">(Oard et al., 2015)</ref>. However, none of them provides explicit thread structure. Therefore, in this section, we will introduce our email thread preprocessing and summary collection procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Email Thread Preprocessing</head><p>We extract email threads from the flat email collections in the following steps: (1) we give every email a "normalized subject" by removing the reply or forward tags (e.g., "Re:", "Fwd:", etc.) from its original subject; (2) we group emails by the normalized subjects and sort emails in the same group (i.e.,  (4) we traverse emails in every thread in temporal order and cut off the thread when none of the senders plus receivers of the current email appears in previous emails; (5) we filter out threads that only contain single repeated content.</p><p>To obtain a cleaner dataset, we remove threads that do not comply with the following constraints:</p><p>(1) 3 ? the number of emails ? 10; (2) 5 &lt; the number of words in each email &lt; 200; (3) 30 &lt; the total number of words &lt; 1000; (4) does not contain non-English (e.g., German) tokens; (5) does not contain reply or forward tags in the subject of the first email.</p><p>Emails often contain personal information such as full name, email/physical address, phone number, etc. To protect privacy, we anonymize all email threads before annotation: (1) only keep first names; (2) remove threads that have "password", "pwd", "confidential", etc.; (3) replace email address, physical address, phone number, URL, IP address, local path, and other sensitive numbers with USERNAME@DOMAIN.COM, ADDRESS, PHONENUMBER, HTTP://LINK, IPADDRESS, PATH, and NUMBER, respectively.</p><p>We conduct an extensive manual quality scan to make sure that the extracted threads are truly threads (instead of random emails grouped) and properly anonymized. Finally, we obtain 8,116 threads from Avocado and 3,478 threads from W3C. <ref type="bibr">3</ref> We randomly sample 3K Avocado threads for summary annotation, and the remaining threads are used as unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Thread Summary Collection</head><p>We collect summary annotations on Amazon Mechanical Turk. Since summarizing text is not an easy task, to get acceptable English summaries we use several quality control strategies: (1) We select annotators that are located in the US, have an approval rate greater than 97%, and have at least 10,000 approved HITs; (2) During annotation, we periodically sample summaries, manually check their quality, and reject or block poor-quality annotators; (3) After annotation, we randomly sample 2 examples per annotator and manually categorize annotators into "good", "fair", and "bad" groups, then filter examples written by bad annotators.</p><p>Email threads oftentimes contain technical information, we instruct annotators not to get stuck on technical details, instead, focus on the major concerns, decisions, and consensus. We collect both short (&lt; 30 words) and long (&lt; 100 words) abstractive summaries per thread. For the short summary, we instruct annotators to write a concise description of what the thread is mainly talking about; while for the long summary, we instruct them to write a narrative of what happens. We are intent to provide summaries with two different levels of abstractiveness, length, and concreteness. We show annotators an example written by an expert (a CS graduate student). More summary collection details can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Final Dataset Description</head><p>The summary collection and filtering process yield 2,549 email threads each with a long and a short summary. We randomly sample 500 examples from the "good" annotator group as our testing set and split the remaining examples into training (1,800 threads) and development (249 threads) sets. Table 2 shows the statistics of EMAILSUM. 4 . For ease of benchmarking, we also include statistics on other commonly used summarization datasets: CNN/DM <ref type="bibr">(Hermann et al., 2015)</ref> and XSum <ref type="bibr" target="#b15">(Narayan et al., 2018)</ref> are about news summarization; SAMSum <ref type="bibr">(Gliwa et al., 2019)</ref> is about chit-chat summarization; CRD3 <ref type="bibr" target="#b20">(Rameshkumar and Bailey, 2020</ref>) is a role-play dialogue summarization dataset; BC3 <ref type="bibr" target="#b22">(Ulrich et al., 2008)</ref> is another email thread summarization with 40 threads from W3C. Compared to the other datasets, the average document length in the EMAILSUM dataset is not very long, containing 233 words; long summaries are more than twice as longer than short summaries. "Ext-Oracle-R1" in <ref type="table" target="#tab_1">Table 2</ref> indicates how abstractive the summaries are. It computes the ROUGE-1 scores of an oracle extractive method (see Section 3.1 for details of the oracle extractive method). The lower it is, the more abstractive the dataset is. According to this score, the abstractiveness of the EMAILSUM summaries is lower than the XSum summaries, while higher than the CNNDM summaries. Furthermore, the short summaries of EMAILSUM dataset are more abstractive than its long summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>The summarization models we explore in this work take the email thread as input and generate the summary as output. We experiment on EMAIL-SUM short and EMAILSUM long tasks separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extractive</head><p>Oracle. This method maximize an evaluation metric w.r.t. the gold summary. "Ext-Oracle-R1" in <ref type="table" target="#tab_1">Table 2</ref> is computed from an oracle summary that maximizes ROUGE-1 <ref type="bibr">(Lin, 2004)</ref>.</p><p>Lead. This model simply picks the first sentence from the source document as the summary, which has surprisingly good performance on CNN/DM dataset <ref type="bibr" target="#b15">(Narayan et al., 2018)</ref>. We test two variants by selecting: (1) the first sentence of the email thread, which is usually the subject (see the example in <ref type="table">Table 1</ref>), referred as Lead-1; (2) the first sentence of the email thread (the subject) plus the first sentences of every email, named Lead-1-Email. 5</p><p>TextRank. This is a graph-based method <ref type="bibr" target="#b13">(Mihalcea and Tarau, 2004)</ref>. It first builds a graph between sentences by their embedding similarities; then the PageRank algorithm is applied to obtain the rank scores for each sentence, and top-rank sentences are selected as the summary.</p><p>BertSumExt. <ref type="bibr">Liu and Lapata (2019b)</ref> propose to build a sentence extractor upon BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> to perform extractive summarization, which achieves a good performance on CNN/DM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Abstractive</head><p>Fast Abs RL. As the simple non-pretrained abstractive baseline, we use <ref type="bibr" target="#b5">Chen and Bansal (2018)</ref>, which is a hybrid model that first extracts sentences from the source document, then rewrites the extracted sentences by an abstractive rewriter. They pair summary sentences with the extracted sentences to train the abstractive rewriter. Adapting their model to our email thread summarization task, we make two adjustments: (1) We extract emails instead of sentences, which is a natural unit for email thread; (2) Since summary sentences usually follow the temporal order of the emails, we enhance this pairing procedure by using the Neeleman-Wunsch algorithm <ref type="bibr" target="#b16">(Needleman and Wunsch, 1970;</ref><ref type="bibr" target="#b20">Rameshkumar and Bailey, 2020)</ref> to impose the order constraint to the alignment (see description and comparison in Appendix B).</p><p>T5. T5 <ref type="bibr" target="#b19">(Raffel et al., 2020</ref>) is a Transformer <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> based seq-to-seq model pretrained with large-scale English data. It achieves state-of-the-art performances on a lot of NLP tasks including the CNN/DM summarization task. As our main baseline, we take the email thread as a single document and finetune a T5 base to generate the summary (T5 base ). A similar setup is also used in transfer and semi-supervised learning. Since our training dataset is small, we find that using the pretrained knowledge transfer is crucial. Training a T5 model from scratch performs poorly (see the results in Appendix <ref type="table" target="#tab_3">Table 7)</ref>.</p><p>Transfer Learning. To analyze how information from other summarization datasets (listed in Table 2) can be transferred to this new task and its impact on the performance, we investigate two simple transfer learning methods: (1) Pre-finetuning, in which we first finetune T5 on a bigger summarization dataset (e.g., CNN/DM) then continue the finetuning on our dataset, referred as X pre (X is the bigger dataset's name, e.g., CNNDM pre ) in our result tables. This is analogous to the continual training method proposed for multilingual transfer learning of machine translation (Kocmi and Bojar,  2018).</p><p>(2) Joint-training, in which we upsample EMAILSUM data and mix it with another dataset, then use the combined data to finetune T5, similarly denoted as X joint . This is analogous to the multilingual joint training method used in machine translation <ref type="bibr">(Johnson et al., 2017)</ref>.</p><p>Semi-supervised learning. Since we only have 2.5K labeled email threads, another important technique to improve the performance is to utilize unlabeled data (i.e., email threads without labeled summaries). As introduced in Section 2.1, in addition to the 3K email threads used for summary collection, we have 8,594 unlabeled email threads (5,116 from Avocado; 3,478 from W3C). We explore semi-supervised learning via the simple self-training technique <ref type="bibr" target="#b21">(Scudder, 1965)</ref>. We use a trained model (a finetuned T5) to generate summaries for unlabeled threads, then mix the model-labeled and human-labeled data to finetune T5 again, referred as SemiSup x (x stands for the unlabeled data source we use, i.e., W3C, Avocado, or together).</p><p>Hierarchical T5. Hierarchical summarization models have been shown to improve the performance of multi-document summarization task <ref type="bibr">(Liu and Lapata, 2019a)</ref>. Although an email thread can be treated as a single document due to the temporal dependency between consecutive emails, it also has a clear turn structure that encourages using of the hierarchical encoders. Recently, <ref type="bibr" target="#b27">Zhu et al. (2020)</ref> proposed a hierarchical model (HMNet) for meeting summarization. Inspired by their work, we propose a hierarchical model that is similar to HMNet in structure but uses T5 as the backbone, therefore, it can take advantage of both the hierarchical structure and the pre-trained knowledge. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, this model contains two encoders: the token-level encodes the whole email thread (e.g., e 1 , e 2 , e 3 , e 4 ) while the email-level receives mean-pooled email-level representations as input. The decoder has two cross attentions that attend to the outputs of the email-level and the token-level encoders respectively. Both token-level and email-level encoders are sharing the weights of the T5 encoder. We add a small number of new parameters by adding new cross attention between the decoder and the email-level encoder.</p><p>4 Experiments 4.1 Evaluation Metrics ROUGE (Lin, 2004) is a commonly used automatic metric for summarization tasks. It has several variants: (1) ROUGE-1 (R1) measures the unigram overlap between the generated and reference summaries;</p><p>(2) ROUGE-2 (R2) measures the bi-gram overlap;</p><p>(2) ROUGE-L (RL) computes the longest common subsequence (LCS); (4) summary-level ROUGE-L (RLsum) computes LCS between each pair of reference and candidate sentences and returns the union-LCS. We use the rouge score package 7 and report F1 scores.</p><p>BERTScore <ref type="bibr" target="#b26">(Zhang et al., 2019)</ref> goes beyond n-gram overlap to provide contextualized semantic similarity. Specifically, it uses BERT <ref type="bibr" target="#b8">(Devlin et al., 2019</ref>) (or RoBERTa <ref type="bibr" target="#b11">(Liu et al., 2019)</ref>) representations to "softly" align the words in candidate and reference summaries and then computes a "soft" uni-gram F1 score. We use the bert score package 8 and report rescaled numbers with a baseline. <ref type="table" target="#tab_4">Table 3</ref> shows the evaluation results on the testing set of different models (the corresponding results on the development set can be found in Appendix   Taking the email thread as one single document and finetuning T5 (i.e., T5 base in <ref type="table" target="#tab_4">Table 3</ref>) sets up a strong baseline. Upon this baseline model, we test the transfer learning from four different summarization datasets (CNN/DM, XSum, SAMSum, and CRD3). However, as shown in <ref type="table" target="#tab_4">Table 3</ref>, transfer learning barely improves over baseline, and transferring by pre-finetuning always works better than joint-training. Since our EMAILSUM has a quite different domain as existing news or dialogue datasets, we conjecture that it is hard to transfer knowledge between them or better transferring techniques need to be applied. Similarly, we test the semi-supervised learning with unlabelled data from W3C, Avocado, and both of them (together). This method can mostly (or significantly in some cases) outperform the baseline's performance for both EMAILSUM short and EMAIL-SUM long . Lastly, the hierarchical T5 base model only marginally outperforms the non-hierarchical <ref type="figure">Figure 2</ref>: The impact of the number of emails in the thread on summarization performance (ROUGE-1). The results are on the testing set. short/long denotes EMAILSUM short /EMAILSUM long ; base/best denotes the baseline/best model. baseline for EMAILSUM long task. It is notable that overall EMAILSUM long has higher ROUGE scores but lower BERTScore than EMAILSUM short .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Since we focus on generating abstractive summaries for email threads and the human-written summaries are fairly abstractive (as shown in Table 2), we further investigate the abstractiveness of model-generated summaries. We take summaries generated by the baseline (T5 base ) and the best ROUGE-1 models (SemiSup together for EMAIL-SUM short , SemiSup w3c for EMAILSUM long ) as the pseudo ground-truth, respectively. Then, we evaluate the ROUGE-1 of extractive Oracle and Lead-1-Email models; higher scores means more extractive summaries. As shown in  <ref type="table" target="#tab_1">Tie  Salience  109  133  55  109  130  50  Faithfulness  116  123  58  126  122  41  Overall quality 120  138  39  125  140  24   Table 5</ref>: Pairwise comparison between summaries generated by the best ROUGE-1 models and T5 base .</p><p>to humans, models generate much more extractive summaries. Moreover, the semi-supervised models (R1-best) are even more extractive than the baseline, which is probably because the self-training procedure amplifies the extraction tendency. Lastly, for both base and best models as well as for both short and long summaries, the model performance (ROUGE-1) decreases as the number of emails in the thread increases (shown in <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Human Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Human Rating Collection</head><p>To better understand where the model still falls short and investigate if the automatic metrics correlate well with human judgments, we conduct a human evaluation on Amazon Mechanical Turk. Initially, by manually checking the quality of modelgenerated summaries, we find that models can mostly generate grammatical, relevant, and fluent summaries; however, they often fail to be salient and faithful, i.e., models tend to be overdetailed or do not stay true to the source thread. Therefore, we ask human annotators to rate the "salience" and "faithfulness" of model-generated summaries. We choose the best ROUGE-1 models, SemiSup together for EMAILSUM short , SemiSup w3c for EMAILSUM long , to evaluate, then we sample 100 examples, and collect 3 responses for each example. Human judges are asked to rate on a 5-point Likert scale for salience and faithfulness respectively and annotate which summary sentences are not salient or unfaithful. We explain the meaning of "salience" and "faithfulness" to annotators and instruct them how to rate from 1 to 5. Meanwhile, to verify the improvement obtained by best R1 models over T5 base , we ask them to compare the summaries generated by these models and those from T5 base , and judge which one is more salient, more faithful, and has overall higher quality. More collection details can be found in the Appendix D.</p><p>We check the average inter-rater agreement (Krippendorff's alpha (Krippendorff, 2011)) of "salience" and "faithfulness" ratings. It is around 0.09 to 0.23, i.e., slight to fair agreement <ref type="bibr">(Fleiss and Cohen, 1973)</ref>. However, when we convert the ratings to 3-point by taking {3}, {4 and 5}, {1 and 2} as 3 classes, the agreement increases to 0.36 to 0.63, i.e., fair to substantial agreement. This indicates that humans' subjectivity affects the ratings and people have a hard time distinguishing 'bad' from 'very bad' as well as 'good' from 'very good'. Meanwhile, the ratings for short summaries are always less agreed across raters (0.36-0.38) than that for long summaries (0.58-0.63). This indicates that there might be multiple different ways of summarizing an email thread into a short summary. The agreement of pairwise comparison is around 0.20 to 0.24 (fair agreement), which is because the baseline and the best models have non-distinguishable performance (shown in <ref type="table">Table 5</ref>). Finally, we take the 3-rater average as the final human rating for each example.</p><p>In addition, we evaluate the correlations (Pearson Correlation <ref type="bibr" target="#b1">(Benesty et al., 2009)</ref>) among different human ratings. The correlation between salience and faithfulness ratings is 0.36/0.45 for short/long summarization. And the correlations among salience, faithfulness, and overall quality pairwise preferences are around 0.53 to 0.79. Overall, moderate to large <ref type="bibr" target="#b6">(Cohen, 2013)</ref> correlations are observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Generated Summary's Quality Analysis</head><p>Surprisingly, human evaluators are mostly satisfied with the salience and faithfulness of modelgenerated summaries, ratings are around 4 out of 5. On average, humans rate 3.89 and 4.04 for the salience and faithfulness of SemiSup together generated short summaries, respectively; and they rate 4.22 and 4.29 for the salience and faithfulness of SemiSup w3c generated long summaries, respectively. Examples with low or high ratings are shown in <ref type="table" target="#tab_7">Table 6</ref> or Appendix <ref type="table">Table 8</ref>. Humans rate higher for model-generated long summaries, which is correlated to the trend of ROUGE, and they are more satisfied with faithfulness than salience. <ref type="table">Table 5</ref> presents the human pairwise compari-Fail to understand the sender's intent.</p><p>Thread: Subject: minutes of meeting: 3.5 plan ||| Om: 1. Nihar mentioned that we spent about 3 weeks in redefining the language, which was not originally planned. This is the major reason for moving the code freeze date from 8/24 to 9/21. 2. For phase-I code drop to QA on 8/28 The confidence in date is : 90% The confidence in statbility of build is : 80% 3. ... ||| Sharon: Hi Om -We also need to lock down the date for: 1 -service pack merge 2 -bug fix freeze and, Javascript library testing (Offline) resource thanks, sharon ||| Rajeev: Thanks for the meeting minutes. Nihar, Sharon can you list the Risks to the phase 1 &amp; Phase II schedules and what we are doing to manage the risk. Rajeev Generated Summary: Om tells Nihar that he spent 3 weeks redefining the language. Sharon tells Om that she needs to lock down the date for 1 -service pack merge 2 -bug fix freeze and Javascript library testing. <ref type="figure" target="#fig_1">(salience=4, faithfulness=3.</ref>3) Ground-truth: Om gives everyone minutes for a meeting. Sharon updates Om on some other plans and Rajeev asks Nihar/Sharon for some technical details.</p><p>Fail to identify the roles of the sender and receiver.</p><p>Thread: Subject: latest 4.0 ga palladium install for biogen ||| Nilesh: PATH/patchinstaller I tested this with build version 377 and it works fine. ||| Diana: This one looks good. I have verified that the 2 fixes in 382 are in the patch installer. Just to clarify, this is really a 382 patch installer that falls under the 377 directory? ... ||| Nilesh: Wilhan, I have deleted build 382 as there was no space to create patch installer. (as we discussed in the lab) And as we specified the build version to be 377 when creating the patch installer I thought we will need to put it under build 377 and use the jar files for that. Can you please clarify this. ... Generated Summary: Nilesh tells Diana that the 2 fixes in 382 are in the patch installer. Nileshe also asks Wilhan to clarify the definition of the build. <ref type="figure" target="#fig_1">(salience=3.3, faithfulness=3.</ref>3) Ground-truth: Nilesh says he tested something with a build. Diana thinks it looks good after verifying it but asks some questions. Nilesh updates Wilhan and has some questions. son between the best ROUGE-1 models and T5 base . Except for the faithfulness of EMAILSUM long , the best ROUGE-1 models mostly lose to the baseline (though the loss and win are mostly marginal). Together with <ref type="table" target="#tab_5">Table 4</ref>, we conjecture that the improvement obtained by semi-supervised learning exploits n-gram matching accuracy by making the summary more extractive, while humans prefer more abstractive summaries. Lastly, we analyze the non-salient and unfaithful sentences labeled by the human evaluators. We find that two errors are frequently made by the summarization model: (1) Failing to understand the sender's intent. Usually, when we send an email, there is a high-level intention behind the detailed content we write, e.g., start up a discussion, bring up a concern, broadcast a decision, etc. However, models are oftentimes unable to capture the intention and thus overly focus on details. As shown in the first example of <ref type="table" target="#tab_7">Table 6</ref>, Om intends to summarize the important points from a meeting, while the model only picks the first piece of detail in that email as the summary. This problem is also related to the over-extractive issue (shown in <ref type="table" target="#tab_5">Table 4</ref>). The model tends to extract details from the source thread and the extraction is biased to the first sentence of each email.</p><p>(2) Failing to identify the roles of the sender and receiver. An email thread is a special type of conversation with multiple speakers involved. One important task for the model is to identify the roles of different speakers and their relations, i.e., who does what to whom. As shown in the second example of <ref type="table" target="#tab_7">Table 6</ref>, the model wrongly takes "2 fixes in 382 are in the patch installer" as information provided by Nilesh, whereas it is supposed to be by Diana. The same issue can also be observed in the first example: Om is just summarizing what Nihar said instead of telling Nihar. This is considered as a type of unfaithfulness, which has been widely identified as a common issue of abstractive summarization models <ref type="bibr" target="#b24">(Wang et al., 2020;</ref><ref type="bibr" target="#b9">Durmus et al., 2020;</ref><ref type="bibr" target="#b12">Maynez et al., 2020)</ref>. <ref type="bibr">(Lin, 2004)</ref> measures n-gram overlap and BERTScore <ref type="bibr" target="#b26">(Zhang et al., 2019)</ref> is essentially based on "soft" uni-gram matching. However, according to our analysis presented above, the email thread summarization models mainly fail to be abstractive, salient, and faithful, which are hard to be evaluated by n-gram overlap. Furthermore, as pointed out by <ref type="bibr" target="#b2">Bhandari et al. (2020)</ref>, different datasets usually require different evaluation metrics. Therefore, here, we study the correlation between automatic metrics and human judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Correlation with Human Judgement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROUGE</head><p>Specifically, we evaluate the Pearson Correlation between human ratings and automatic metric scores on the 100 examples used in the human evaluation. Besides, as described above, we conduct a pairwise model comparison between the best ROUGE-1 models and T5 base for "salience", "faithfulness", and "overall quality". We convert them to a pairwise ranking score, i.e., -1 if T5 base is better; 1 if T5 base is worse; 0 if two models are non-distinguishable. In the same way, we convert different metric scores to ranking scores. Then, we also evaluate the Pearson Correlation between human and metric ranking scores. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the results. Overall, the correlations are fairly poor. The best correlation is between ROUGE-1 and human overall quality ranking for short summary generation (coefficient=0.14, p=0.16). There is little or negative correlation between metrics and human judgment for the long summary generation. Therefore, we emphasize the importance of human evaluation and better automatic proxies need to be proposed in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose an abstractive email thread summarization dataset, EMAILSUM, that contains 2,549 email threads with human-written short and long summaries. We explore different summarization paradigms and find that taking the email thread as a single document and finetuning T5 <ref type="bibr" target="#b19">(Raffel et al., 2020)</ref> sets up a good baseline. Transferring from other summarization datasets barely improves it. Using hierarchical structure also only marginally improves the performance. Semi-supervised learning by using unlabelled email threads improves automatic metrics (ROUGE) but still loses to the baseline in human evaluation. Finally, our human evaluation reveals that the model fails to understand the sender's main intention and the roles of different speakers. Automatic metrics are poorly correlated with human judgment, which emphasizes the importance of human evaluation and designing new metrics for this task in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Broader Impact Statement</head><p>We use two email collections in this work: Avocado <ref type="bibr" target="#b17">(Oard et al., 2015)</ref> and W3C <ref type="bibr" target="#b7">(Craswell et al., 2006)</ref>. W3C is derived from W3C Public Mailing List that is open-source available online. Avocado consists of emails and attachments taken from 279 accounts of a defunct information technology company referred to as "Avocado". Its copyright is protected by Linguistic Data Consortium. Based on the license agreement, we will only open-source our collected summaries and provide scripts to obtain email threads from the original Avocado email collection. To further protect copyright and the privacy of the persons involved in the emails, as introduced in Section 2, we carefully anonymize all the email threads we construct from both email collections. We fairly pay crowd-source workers $1.37 (for threads with 5 or fewer emails) or $2 (for threads with more than 5 emails) for writing the short and long summaries and $0.6 for human rating such that the pay rate is higher than the federal minimum wage requirement.</p><p>Joseph L Fleiss and Jacob <ref type="bibr">Cohen. 1973</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Fast Abs RL</head><p>The original Fast Abs RL method <ref type="bibr" target="#b5">(Chen and Bansal, 2018)</ref> uses ROUGE-L recall to align extracted source sentences and target summary sentences. In our case, we extract emails and align them with summary sentences. Since the emails and summary sentences usually follow the same temporal order, we enhance the alignment procedure by the Neeleman-Wunsch algorithm <ref type="bibr" target="#b16">(Needleman and Wunsch, 1970;</ref><ref type="bibr" target="#b20">Rameshkumar and Bailey, 2020)</ref> to imposing strict order constraints, e.g., there should not be "email i is aligned to sentence j while email i+1 is aligned to sentence j?1 " cases.</p><p>Meanwhile, we modify it to allow one email to be aligned with multiple summary sentences but avoid one summary sentence aligning with multiple emails. Specifically, we first obtain the similarity matrix M of size n e ? n s between each email and summary sentence by ROUGE-L recall (n e is the number of emails, n s is the number of summary sentences); then the alignment score matrix H of size (n e +1)?(n s +1) is initialized as all-zero then computed as follows for 1 ? x ? n e , 1 ? y ? n s : H Then we traceback from H ne,ns to H 0,0 to obtain the final alignment. As shown in <ref type="table" target="#tab_3">Table 7</ref>, the "Fast Abs RL (default)" model refers to this method with the default setting which works mostly worse than our enhanced Fast Abs RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Details &amp; Additional Results</head><p>We implement the TextRank <ref type="bibr" target="#b13">(Mihalcea and Tarau, 2004</ref>) model via the summa python package 9 and set the summarization ratio as the average summary length thread length ratio in the training set, which is 0.22 for short summary and 0.38 for long summary.</p><p>We test Fast Abs RL <ref type="bibr" target="#b5">(Chen and Bansal, 2018)</ref> via the author's open-source code. 10 Most of our models are built on T5 <ref type="bibr" target="#b19">(Raffel et al., 2020)</ref> and we use the base version that has 220 million parameters. Our hierarchical T5 shares the same T5 encoder parameters between the token-level and email-level encoders. The only new parameters added are from the first cross attention between decoder and emaillevel encoder. We use Transformers <ref type="bibr" target="#b25">(Wolf et al., 2020)</ref>  <ref type="bibr">11</ref> to run all the T5 based models. We run experiments on a single Tesla V100 GPU. We set the max input sequence length as 512 tokens and max output length as 56 tokens during training (200 tokens during evaluation). The total batch size (with gradient accumulation) is 128. The learning rate is 5e-4, except for training the T5 base from scratch, we use 1e-4 instead. Since our training set only contains 1.8K examples, it only takes 2-4 minutes per epoch. We train models for 70 epochs.</p><p>Our model selection is based on each of the five evaluation metrics, ROUGE-1/ROUGE-2/ROUGE-L/summary-level ROUGE-L/BERTScore. We select the best checkpoints for each of the five metrics on our development set, then test those checkpoints on the testing set to report the final numbers for each metric. <ref type="table" target="#tab_3">Table 7</ref> shows all the results on our development set. <ref type="table">Table 8</ref> shows two examples that have high-rating model-generated summaries. <ref type="figure">Figure 5</ref> shows the questions we asked to human judges to evaluate the quality of model-generated summaries. Before these questions, we instruct annotators how to rate on a 5-point Likert scale for "salience" and "faithfulness": (1) Rate salience from 1 to 5: 1 is the worst, none of the points in the summary is important enough to be summarized; 5 is the best, all of the points mentioned in the summary are important and worth to be summarized;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Human Evaluation</head><p>(2) Rate faithfulness from 1 to 5: 1 is the worst, all of the sentences in the summary are either wrong or not existing in the email thread; 5 is the best, all of the points mentioned in the summary are true to the thread. Plus, we also prompt examples of "non-salient" and "unfaithful" summaries on the webpage. We pay annotators $0.60 per HIT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of our hierarchical T5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Correlation between automatic metrics and human judgements. Short and Long refer to EMAIL-SUM short and EMAILSUM long tasks, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: The statistics of different summarization datasets. Ext-Oracle-R1s are the ROUGE-1 scores of the oracle</cell></row><row><cell>extractive method, which shows the abstractiveness of the summary (the lower the more abstractive).</cell></row><row><cell>thread) by timestamp; (3) we de-duplicate emails</cell></row><row><cell>in every thread by sender's email plus timestamp;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 )</head><label>7</label><figDesc>. It can be observed that the Oracle extractive model sets up a high upper bound on all metrics except for BERTScore (BertS). Among non-oracle extractive methods, the Lead-1-Email heuristic works best and even better than the deep extractive method, BertSumExt. The hybrid Fast Abs RL model outperforms purely extractive methods but works worse than purely abstractive methods with large-scale pretraining (e.g., T5). 44.56 * * 14.60 * * 31.38 * * 40.73 * * 32.81 * * SemiSup together 36.98 11.21 * 28.76 33.70 * * 33.91 44.08 14.06 31.17 * * 40.67 * * 32.30</figDesc><table><row><cell>Models</cell><cell></cell><cell cols="3">EMAILSUM short</cell><cell></cell><cell></cell><cell></cell><cell cols="2">EMAILSUM long</cell></row><row><cell></cell><cell>R1</cell><cell>R2</cell><cell>RL</cell><cell>RLsum</cell><cell cols="2">BertS R1</cell><cell>R2</cell><cell>RL</cell><cell>RLsum</cell><cell>BertS</cell></row><row><cell>Oracle</cell><cell cols="2">39.04 12.47</cell><cell cols="2">30.17 35.61</cell><cell cols="2">22.32 45.98</cell><cell>15.49</cell><cell>32.40</cell><cell>42.14</cell><cell>26.31</cell></row><row><cell>Lead-1</cell><cell cols="2">23.35 5.57</cell><cell cols="2">18.22 19.61</cell><cell cols="2">12.25 19.75</cell><cell>4.84</cell><cell>14.24</cell><cell>16.88</cell><cell>6.87</cell></row><row><cell>Lead-1-Email</cell><cell cols="2">26.62 5.60</cell><cell cols="2">19.72 23.77</cell><cell cols="2">13.00 35.71</cell><cell>8.69</cell><cell>24.70</cell><cell>32.13</cell><cell>16.93</cell></row><row><cell>TextRank</cell><cell cols="2">22.52 4.54</cell><cell cols="2">16.56 20.24</cell><cell>5.89</cell><cell>28.42</cell><cell>6.20</cell><cell>19.08</cell><cell>25.19</cell><cell>5.67</cell></row><row><cell>BertSumExt</cell><cell cols="2">24.84 5.15</cell><cell cols="2">17.81 21.81</cell><cell>7.51</cell><cell>30.23</cell><cell>7.08</cell><cell>19.59</cell><cell>26.68</cell><cell>7.78</cell></row><row><cell>Fast Abs RL</cell><cell cols="2">31.15 6.59</cell><cell cols="2">22.73 29.03</cell><cell>6.49</cell><cell>39.35</cell><cell>10.58</cell><cell>27.01</cell><cell>36.51</cell><cell>10.03</cell></row><row><cell>T5 base</cell><cell cols="2">36.57 10.56</cell><cell>28.3</cell><cell>32.76</cell><cell cols="2">33.90 43.81</cell><cell>14.08</cell><cell>30.47</cell><cell>39.88</cell><cell>32.09</cell></row><row><cell>CNNDMpre</cell><cell cols="2">35.43 10.75</cell><cell cols="2">27.49 32.15</cell><cell cols="2">33.61 44.15</cell><cell>14.20</cell><cell>30.84</cell><cell>40.21</cell><cell>32.53</cell></row><row><cell>XSumpre</cell><cell cols="2">36.14 10.26</cell><cell cols="2">28.66 33.47</cell><cell cols="2">33.97 43.48</cell><cell>13.82</cell><cell>30.14</cell><cell>39.80</cell><cell>31.60</cell></row><row><cell>SAMSumpre</cell><cell cols="2">34.68 10.56</cell><cell cols="2">26.62 31.22</cell><cell cols="2">33.25 42.83</cell><cell>13.54</cell><cell>30.00</cell><cell>39.13</cell><cell>31.82</cell></row><row><cell>CRD3pre</cell><cell cols="2">36.05 10.04</cell><cell cols="2">27.21 32.06</cell><cell cols="2">33.52 43.60</cell><cell>13.93</cell><cell>30.49</cell><cell>39.97</cell><cell>31.53</cell></row><row><cell>CNNDMjoint</cell><cell cols="2">34.38 9.27</cell><cell cols="2">27.20 31.30</cell><cell cols="2">32.70 43.28</cell><cell>12.37</cell><cell>28.84</cell><cell>39.39</cell><cell>29.95</cell></row><row><cell>XSumjoint</cell><cell cols="2">34.18 8.17</cell><cell cols="2">25.94 30.68</cell><cell cols="2">31.83 42.36</cell><cell>11.85</cell><cell>28.23</cell><cell>38.31</cell><cell>29.22</cell></row><row><cell>SAMSumjoint</cell><cell cols="2">35.57 10.07</cell><cell cols="2">27.95 32.57</cell><cell cols="2">33.55 42.96</cell><cell>13.44</cell><cell>29.99</cell><cell>39.54</cell><cell>31.82</cell></row><row><cell>CRD3joint</cell><cell cols="2">34.66 8.81</cell><cell cols="2">26.95 31.59</cell><cell cols="2">33.29 42.81</cell><cell>12.96</cell><cell>29.35</cell><cell>39.33</cell><cell>32.14</cell></row><row><cell cols="7">SemiSupw3c 33.61 SemiSup avocado 35.43 10.64 28.59 32.31 36.73 10.82 28.44 33.25 33.76 43.83</cell><cell cols="3">14.61  *  *  31.21  *  *  40.52  *</cell><cell>32.71  *  *</cell></row><row><cell>Hier. T5 base</cell><cell cols="2">36.17 10.37</cell><cell cols="2">28.44 33.34</cell><cell cols="2">33.39 44.50  *</cell><cell>14.53  *</cell><cell>30.89  *</cell><cell>40.22</cell><cell>32.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">EMAILSUM short</cell><cell cols="2">EMAILSUM long</cell></row><row><cell></cell><cell cols="4">EO-R1? LE-R1? EO-R1? LE-R1?</cell></row><row><cell>Human</cell><cell>39.0</cell><cell>26.62</cell><cell>46.0</cell><cell>35.71</cell></row><row><cell>T5 base</cell><cell>50.27</cell><cell>36.88</cell><cell>55.43</cell><cell>43.65</cell></row><row><cell>R1-best</cell><cell>52.50</cell><cell>39.22</cell><cell>60.04</cell><cell>49.14</cell></row></table><note>Summarization performance on the testing set of different models. We test the significance 6 of the improvement over T5 base ( * : p &lt; 0.05, * * : p &lt; 0.01).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>The extractive Oracle (EO) and Lead-1-Email (LE) models' ROUGE-1 by taking human summary, base or best model generated summary as the ground- truth. The lower the scores are, the more abstractive the summaries are (?).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>SemiSup together vs T5 base SemiSupw3c vs T5 base</figDesc><table><row><cell>, compared</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Error analysis examples. Emails are separated by '|||' and some content is omitted by '...'. (salience=xx, faithfulness=xx) gives the average human rating for that summary.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code and summary data have been made available at: https://github.com/ZhangShiyue/EmailSum</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We apply strict criteria for thread extraction (see Section 2). More threads can be extracted by relaxing those constraints.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We find that the extracted threads from Enron are usually short (fewer than 3 emails) and noisy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Since comparing the model-generated summary to only one human-written reference may not be fully informative, recently we have also collected one more reference for each email thread in our test set, i.e., each test example will have two gold references now in our final dataset. The results in the paper are all still based on the original one-reference setup but we will release the updated two-reference results for our best baselines on Github.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We also tested some other heuristics: e.g., the first sentence of the last email, the last 3-5 sentences of the email thread, etc. However, none of them perform better than Lead-1-Email.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The significance test is following the bootstrap test setup<ref type="bibr" target="#b10">(Efron and Tibshirani, 1994)</ref> and sample for 100k times. 7 https://github.com/google-research/ google-research/tree/master/rouge 8 https://github.com/Tiiiger/bert_score</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://github.com/summanlp/textrank</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/ChenRocks/fast_ abs_rl 11 https://github.com/huggingface/ transformers</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful comments and Xiang Zhou for useful discussions. We thank Saadia Gabriel, Yichen Jiang, Tom McCoy, and Yixin Nie for helping write summary examples (to show as initial examples to MTurk annotators) and estimate the workload for deciding the fair payment. This work was partially done while SZ was interning at MSR and later extended at UNC, where it was supported by NSF-CAREER Award 1846185, ONR Grant N00014-18-1-2871, and a Microsoft Investigator Fellowship. The views contained in this article are those of the authors and not of the funding agency.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Summary Collection</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Temporal summaries of new topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Khandelwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual international ACM SI-GIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SI-GIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pearson correlation coefficient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Noise reduction in speech processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>Yiteng Huang, and Israel Cohen</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reevaluating evaluation in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Narayan Gour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atabak</forename><surname>Ashfaq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.751</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9347" to="9359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Summarizing email conversations with clue words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The ami meeting corpus: A pre-announcement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Carletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Ashby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Bourban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mael</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaroslav</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasilis</forename><surname>Karaiskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Kronenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lathoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Multimodal Interaction</title>
		<meeting><address><addrLine>Mike Lincoln, Agnes Lisowska, Iain McCowan, Wilfried Post, Dennis Reidsma, and Pierre Wellner; Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast abstractive summarization with reinforce-selected sentence rewriting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="675" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Statistical power analysis for the behavioral sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overview of the trec-2005 enterprise track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arjen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">M</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5055" to="5070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On faithfulness and factuality in abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1906" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 conference on empirical methods in natural language processing</title>
		<meeting>the 2004 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using summarization to discover argument facets in online idealogical dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amita</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean E Fox</forename><surname>Tree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="430" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A general method applicable to the search for similarities in the amino acid sequence of two proteins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">D</forename><surname>Needleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Golitsynskiy</surname></persName>
		</author>
		<title level="m">Avocado Research Email Collection LDC2015T03. DVD. Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radicati</surname></persName>
		</author>
		<title level="m">Email statistics report</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2015" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Storytelling with dialogue: A critical role dungeons and dragons dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Revanth</forename><surname>Rameshkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5121" to="5134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive pattern-recognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A publicly available annotated corpus for supervised email summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of aaai email-2008 workshop</title>
		<meeting>of aaai email-2008 workshop<address><addrLine>chicago, usa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Asking and answering questions to evaluate the factual consistency of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5008" to="5020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A hierarchical network for abstractive meeting summarization with cross-domain pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="194" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Msmo: Multimodal summarization with multimodal output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 conference on empirical methods in natural language processing</title>
		<meeting>the 2018 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4154" to="4164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attached are some general ideas and issues around developing new demos for our new target markets. Please review and provide feedback. Also, please provide links where we can learn more about various FAA applications. Thanx, Dan. ||| Dan, Thanks for putting the high level descriptions together. My questions are: *Is it practical to do an EAI demo given the inherent complexity of application integration</title>
	</analytic>
	<monogr>
		<title level="m">Subject: faa demos ||| Dan: PM Team</title>
		<imprint/>
	</monogr>
	<note>*Should we delay looking at Outlook for now. What do you think that timelines are developing these demos</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Thanks for the feedback, please see my comments below: Generated Short Summary: Dan asks the PM team to review and provide feedback on FFA demos. Alex responds with questions. Dan thanks Alex and gives his feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>|||</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>salience=4.3, faithfulness=4.7)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ground-truth: Dan talks about general ideas about demos to his PM team. Alex provides some feedback and asks questions. Dan thanks Alex for the feedback and adds comments</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Please send me your comments. I am also attaching a list of features that would be good to have. Thanks, Mahesh ||| Amitabh: do we have a side-by-side comparison of solaris, hp-ux, and nt? also, a price-performance comparison might also be useful ||| Rajeev: Dan, Please consider Amitabh&apos;s suggestions for the sizing requirement document that you are prepaing</title>
	</analytic>
	<monogr>
		<title level="m">Subject: sun performance report ||| Mahesh: Hi, I am attaching the draft of the performance/sizing report for EMAS on Sun</title>
		<imprint/>
	</monogr>
	<note>||| Mahesh: we do not have comparison stats. It would be good to have them. ||| Dan: Good points, we should have side-by-side comparisons and also price/performance..</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mahesesh says there are no comparison stats, but it would be good to have them. Dan says there should be side-by-side comparies and also price/performance. (salience=4.3, faithfulness=5) Ground-truth: Mahesh shows everyone a performance report for a future meeting and attaches his feedback. Amitabh gives feedback which Rajeev asks Dan to consider in a different task. Mahesh and Dan make suggestions about comparisons. Table 8: Examples of high-quality summaries generated by model. Emails are separated by &apos;|||&apos; and some content are omit by &apos;...&apos;. (salience=xx, faithfulness=xx) gives the average human rating for that summary</title>
	</analytic>
	<monogr>
		<title level="m">Generated Long Summary: Mahesh is attaching a draft of the performance/sizing report for EMAS on Sun and asking for comments</title>
		<imprint/>
	</monogr>
	<note>Amitabh asks if there is a side-by-side comparison of solaris, hp-ux, and nt. Rajeev asks Dan to consider Amibh&apos;s suggestions for the sizing requirement document. Figure 5: A part of the Amazon Mechanical Turk webpage used for human evaluation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
