<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tenglong</forename><surname>Ao</surname></persName>
							<email>aubrey.tenglong.ao@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingzhe</forename><surname>Gao</surname></persName>
							<email>gaoqingzhe97@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Lou</surname></persName>
							<email>louyuke@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
							<email>baoquan@pku.edu.cn@sist&amp;klmp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><forename type="middle">2022</forename><surname>Liu</surname></persName>
							<email>libin.liu@pku.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">QINGZHE GAO</orgName>
								<orgName type="institution" key="instit1">TENGLONG AO</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">YUKE LOU</orgName>
								<orgName type="institution">Shandong University and Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">BAOQUAN CHEN</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">SIST &amp; KLMP (MOE)</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">LIBIN LIU *</orgName>
								<orgName type="institution" key="instit2">SIST &amp; KLMP (MOE)</orgName>
								<orgName type="institution" key="instit3">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Peking University</orgName>
								<address>
									<addrLine>No.5 Yiheyuan Road, Haidian District</addrLine>
									<postCode>100871</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Shandong University and Peking University</orgName>
								<address>
									<addrLine>China; Yuke Lou</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Peking University</orgName>
								<address>
									<addrLine>No.5 Yiheyuan Road, Haidian District, 100871; Baoquan Chen</addrLine>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Peking University</orgName>
								<address>
									<addrLine>No.5 Yiheyuan Road, Haidian District</addrLine>
									<postCode>100871</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">SIST &amp; KLMP (MOE)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<addrLine>No.5 Yiheyuan Road, Haidian District</addrLine>
									<postCode>100871</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3550454.3555435</idno>
					<note>This is the author&apos;s version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in ACM Transactions on Graphics, https:// Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings. ACM Trans. Graph. 41, 6, Article 209 (De-cember 2022), 19 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: ? Computing methodologies ? Animation</term>
					<term>Natural lan- guage processing</term>
					<term>Neural networks Additional Key Words and Phrases: non-verbal behavior, co-speech gesture synthesis, character animation, neural generative model, multi-modality, virtual agents ACM Reference Format:</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>hierarchical embeddings of the speech and the motion, resulting in rhythmand semantics-aware gesture synthesis. Evaluations with existing objective metrics, a newly proposed rhythmic metric, and human feedback show that our method outperforms state-of-the-art systems by a clear margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Well, for example, (pause) if you played an F and please do not play another F higher ? <ref type="figure">Fig. 1</ref>. Gesture results automatically synthesized by our system for a beat-rich TED talk clip. The red words represent beats, and the red arrows indicate the movements of corresponding beat gestures.</p><p>Automatic synthesis of realistic co-speech gestures is an increasingly important yet challenging task in artificial embodied agent creation. Previous systems mainly focus on generating gestures in an end-to-end manner, which leads to difficulties in mining the clear rhythm and semantics due to the complex yet subtle harmony between speech and gestures. We present a novel co-speech gesture synthesis method that achieves convincing results both on the rhythm and semantics. For the rhythm, our system contains a robust rhythm-based segmentation pipeline to ensure the temporal coherence between the vocalization and gestures explicitly. For the gesture semantics, we devise a mechanism to effectively disentangle both low-and high-level neural embeddings of speech and motion based on linguistic theory. The high-level embedding corresponds to semantics, while the low-level embedding relates to subtle variations. Lastly, we build correspondence between the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Gesturing is an important part of speaking. It adds emphasis and clarity to a speech and conveys essential non-verbal information that makes the speech lively and persuasive <ref type="bibr" target="#b9">[Burgoon et al. 1990</ref>]. There are rich demands for high-quality 3D gesture animation in many industries, such as games, films, and digital humans. However, the difficulties in reproducing the complex yet subtle harmony between vocalization and body movement make synthesizing natural-looking co-speech gestures remain a long-standing and challenging task.</p><p>Gestures are grouped into six categories by linguists <ref type="bibr" target="#b16">[Ekman and Friesen 1969;</ref><ref type="bibr" target="#b51">McNeill 1992</ref>]-adaptors, emblems, deictics, iconics, metaphorics, and beats. Among them, the beat gestures are rhythmic movements that bear no apparent relation to speech semantics <ref type="bibr" target="#b35">[Kipp 2004</ref>] but serve meta-narrative functions <ref type="bibr" target="#b51">[McNeill 1992</ref>] that are crucial to rhythmic harmony between speech and gestures. Generating realistic beat gestures requires modelling the relation between the gestural beats and the verbal stresses. However, it has been observed that these two modalities are not synchronized in a strict rhythmic sense <ref type="bibr" target="#b49">[McClave 1994</ref>], making it difficult to learn their temporal connection directly from data using an end-to-end method <ref type="bibr" target="#b6">[Bhattacharya et al. 2021a;</ref><ref type="bibr" target="#b71">Yoon et al. 2020</ref>].</p><p>Gestures are associated with different levels of speech information <ref type="bibr" target="#b51">[McNeill 1992]</ref>. For example, an emblem gesture such as thumbsup usually accompanies high-level semantics like good or great, while a beat gesture commonly comes with low-level acoustic emphasis. Many previous studies use only the features extracted at the last layer of an audio encoder to synthesize gestures <ref type="bibr" target="#b6">Bhattacharya et al. 2021a;</ref><ref type="bibr" target="#b54">Qian et al. 2021;</ref><ref type="bibr" target="#b71">Yoon et al. 2020</ref>]. This setup, however, may in effect encourage the encoder to mix the speech information from multiple levels into the same feature, causing ambiguity and increasing the difficulty in mining clear rhythmic and semantic cues.</p><p>In this paper, we focus on generating co-speech upper-body gestures that can accompany a broad range of speech content-from a single sentence to a public speech, aiming at achieving convincing results both on the rhythm and semantics. Our first observation is that gesturing can be considered as a special form of dancing under changing beats. We develop a rhythm-based canonicalization and generation framework to deal with the challenge of generating synchronized gestures to the speech, which segments the speech into short clips at audio beats, normalizes these clips into canonical blocks of the same length, generates gestures for every block, and aligns the generated motion to the rhythm of the speech. This framework, which is partially inspired by recent research in dance generation <ref type="bibr" target="#b2">[Aristidou et al. 2022]</ref>, provides the gesture model with an explicit hint of the rhythm, allowing the model to learn the pattern of gestural beats within a rhythmic block efficiently. Both the quantitative evaluation with a novel rhythmic metric and the qualitative evaluation with user studies show that the gestures generated by this pipeline exhibit natural synchronization to the speech.</p><p>As indicated in linguistics literature <ref type="bibr" target="#b35">[Kipp 2004;</ref><ref type="bibr" target="#b52">Neff et al. 2008;</ref><ref type="bibr" target="#b65">Webb 1996]</ref>, gestures used in everyday conversation can be broken down into a limited number of semantic units with different motion variations. We assume that these semantic units, usually referred to as lexemes, relate to the high-level features of speech audio, while the motion variations are determined by the low-level audio features. We thus disentangle high-and low-level features from different layers of an audio encoder and learn the mappings between them and the gesture lexemes and the motion variations, respectively. Experiments demonstrate that this mechanism successfully disentangles multi-level features of both the speech and motion and synthesizes semantics-matching and stylized gestures.</p><p>In summary, our main contributions in this paper are:</p><p>? We present a novel rhythm-and semantics-aware co-speech gesture synthesis system that generates natural-looking gestures. To the best of our knowledge, this is the first neural system that explicitly models both the rhythmic and semantic relations between speech and gestures. ? We develop a robust rhythm-based segmentation pipeline to ensure the temporal coherence between speech and gestures, which we find is crucial to achieving rhythmic gestures. ? We devise an effective mechanism to relate the disentangled multi-level features of both speech and motion, which enables generating gestures with convincing semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Data-driven Human Motion Synthesis</head><p>Traditional human motion synthesis frameworks often rely on concatenative approaches such as motion graph <ref type="bibr" target="#b37">[Kovar et al. 2002]</ref>.</p><p>Recently, learning-based methods with neural networks have been widely applied to this area to generate high-quality and interactive motions, using models ranging from feed-forward network <ref type="bibr" target="#b28">[Holden et al. 2017;</ref><ref type="bibr" target="#b60">Starke et al. 2022</ref>] to dedicated generative models <ref type="bibr" target="#b45">Ling et al. 2020]</ref>. Dealing with the one-to-many issue where a variety of motions can correspond to the same input or control signal is often a challenge for these learning-based approaches. Previous systems often employ additional conditions, such as contacts <ref type="bibr" target="#b61">[Starke et al. 2020]</ref> or phase indices <ref type="bibr" target="#b28">[Holden et al. 2017;</ref><ref type="bibr" target="#b60">Starke et al. 2022]</ref>, to deal with this problem. Closer to the gesture domain is the speech-driven head motion synthesis, where conditional GANs <ref type="bibr" target="#b57">[Sadoughi and Busso 2018]</ref>, and conditional VAEs <ref type="bibr" target="#b21">[Greenwood et al. 2017</ref>] have been used.</p><p>2.1.1 Music-driven Dance Synthesis. Among the general motion synthesis tasks, music-driven dance generation addresses a similar problem to the co-speech gesture synthesis, where the complex temporal relation between two different modalities needs to be modeled accurately. Both motion graph-based methods <ref type="bibr" target="#b34">Kim et al. 2006</ref>] and learning-based approaches <ref type="bibr" target="#b44">[Li et al. 2021b;</ref><ref type="bibr" target="#b58">Siyao et al. 2022;</ref><ref type="bibr" target="#b62">Valle-P?rez et al. 2021]</ref> have been adopted and successfully achieved impressive generation results. To deal with the synchronization between the dance and music,  develop a manually labeled rhythm signature to represent beat patterns and ensures the rhythm signatures of the generated dance match the music. <ref type="bibr" target="#b2">Aristidou et al. [2022]</ref> segment the dance into blocks at music onsets, convert each block into a motion motif <ref type="bibr" target="#b1">[Aristidou et al. 2018</ref>] that defines a specific cluster of motions, and use the motion motif to guide the synthesis of dance at the block level. <ref type="bibr" target="#b58">Siyao et al. [2022]</ref> employ a reinforcement learning scheme to improve the rhythmic performance of the generator using a reward function encouraging beat alignment. Our rhythm-based segmentation and canonicalization framework is partially inspired by <ref type="bibr" target="#b2">[Aristidou et al. 2022</ref>]. Similar to <ref type="bibr" target="#b2">[Aristidou et al. 2022]</ref>, we also segment the gestures into clips at audio beats but learn a high-level representation for each clip via the vector quantization scheme <ref type="bibr">[Oord et al. 2017]</ref> instead of the K-means clustering. Moreover, our framework generates gestures in blocks of motion and denormalizes the generated motion blocks to match the rhythm of the speech.</p><p>In contrast, <ref type="bibr" target="#b2">Aristidou et al. [2022]</ref> synthesize dance sequences in frames conditioned on the corresponding motion motifs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Co-speech Gesture Synthesis</head><p>The most primitive approach used to generate human non-verbal behaviors is to animate an artificial agent using the retargeted motion capture data. This kind of approach is widely used in commercial systems (e.g., films and games) because of its high-quality motion performance. However, it is not suitable for creating interactive content that cannot be prepared beforehand. Generating co-speech gestures according to an arbitrary input has been a long-standing research topic. Previous studies can be roughly categorized into two groups, i.e., rule-based and data-driven methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.1</head><p>Rule-based Method. The idea of the rule-based approach is to collect a set of gesture units and design specific rules that map a speech to a sequence of gesture units <ref type="bibr" target="#b10">[Cassell et al. 2004;</ref><ref type="bibr" target="#b29">Huang and Mutlu 2012;</ref><ref type="bibr" target="#b35">Kipp 2004;</ref><ref type="bibr" target="#b59">Softbank 2018]</ref>. <ref type="bibr" target="#b64">Wagner et al. [2014]</ref> have an excellent review of these methods. The results of the rule-based methods are generally highly explainable and controllable. However, the gesture units and rules typically have to be created manually, which can be costly and inefficient for complex systems.</p><p>2.2.2 Data-driven Method. Early research in data-driven method learns the rules embedded in data and combines them with predefined animation units to generate new gestures. For example, <ref type="bibr" target="#b36">Kopp et al. [2006]</ref>; <ref type="bibr" target="#b42">Levine et al. [2010]</ref> use probabilistic models to build correspondence between speech and gestures. <ref type="bibr" target="#b52">Neff et al. [2008]</ref> build a statistical model to learn the personal style of each speaker. The model is combined with the input text tagged with the theme, utterance focus, and rheme to generate gesture scripts, which are then mapped to a sequence of gestures selected from an animation lexicon. <ref type="bibr" target="#b12">Chiu et al. [2015]</ref> train a neural classification model to select a proper gesture unit based on the speech input. More recent research has started to take advantage of deep learning and trains end-to-end models using raw gesture data directly, which frees the manual efforts of designing the gesture lexicon and mapping rules. Gestures can be synthesized using deterministic models such as multilayer perceptron (MLP) , recurrent neural networks <ref type="bibr" target="#b6">[Bhattacharya et al. 2021a;</ref><ref type="bibr" target="#b24">Hasegawa et al. 2018;</ref><ref type="bibr" target="#b71">Yoon et al. 2020</ref><ref type="bibr" target="#b72">Yoon et al. , 2019</ref>, convolutional networks <ref type="bibr">[Habibie et al. 2021]</ref>, and transformers <ref type="bibr">[Bhattacharya et al. 2021b</ref>], or by learning generative models such as normalizing flow , VAEs <ref type="bibr" target="#b43">[Li et al. 2021a;</ref>, and learnable noise codes <ref type="bibr" target="#b54">[Qian et al. 2021</ref>]. Our method is also a data-driven framework. We learn the motion generator and the mapping between the speech and gestures from data using a combined network structure of the vector quantized variational autoencoder (VQ-VAE) <ref type="bibr">[Oord et al. 2017]</ref> and LSTM. To capture the rhythmic and semantic correspondences between the speech and gestures, we propose a multi-stage architecture that explicitly models the rhythm and semantics in different stages. An earlier system proposed by <ref type="bibr" target="#b41">Kucherenko et al. [2021b]</ref> shares a similar high-level architectural design to our framework. However, there are two key differences: (a) our method is essentially an unsupervised learning approach, which learns the gesture lexeme, style code, and the generator directly from the data without detailed annotations; and (b) our system employs an explicit beat-based segmentation scheme which is shown to be effective in ensuring temporal coherence between the speech and the gesture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Modal Data Processing</head><p>Co-speech gesture generation is a cross-modal process involving audio, text, motion, and other information related to the speaker and the content of the speech. The representation and alignment of each modality are essential for high-quality results <ref type="bibr" target="#b4">[Baltru?aitis et al. 2019</ref>]. Mel-spectrogram and MFCC acoustic features are commonly used as audio features <ref type="bibr" target="#b54">Qian et al. 2021]</ref>, typically resampled into the same framerate of the motion. For the text features, pre-trained language models like BERT <ref type="bibr" target="#b14">[Devlin et al. 2019;</ref> and FastText <ref type="bibr" target="#b8">[Bojanowski et al. 2017;</ref><ref type="bibr" target="#b71">Yoon et al. 2020]</ref> have been used to encode text transcripts into frame-wise latent codes, where paddings, fillers, or empty words are inserted into a sentence to make the world sequence the same length as the motion <ref type="bibr" target="#b71">Yoon et al. 2020</ref>]. Speaker's style and emotions can also be encoded by learnable latent codes <ref type="bibr" target="#b6">[Bhattacharya et al. 2021a;</ref><ref type="bibr" target="#b71">Yoon et al. 2020]</ref> and are resampled or padded to match the length of the speech. In this work, we employ a pre-trained speech model to extract audio features and fine-tune it using a contrastive learning strategy. We also utilize a BERT-based model to vectorize the text. These multimodal data are then aligned explicitly using the standard approaches discussed above. Notably, a concurrent study ] also extracts audio features using contrastive learning. Their framework considers the learning of the audio features as a part of the training of the gesture generator. Instead, our framework trains the audio encoder in a separate pre-training stage using only the audio data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluation of Motion Synthesis Models</head><p>Evaluating the generated co-speech gestures is often difficult because the motion quality is a very subjective concept. Previous works have proposed several evaluation criteria. <ref type="bibr" target="#b68">Wolfert et al. [2022]</ref> have made a comprehensive review of them. User studies are widely adopted to evaluate different aspects of motion quality, such as human-likeliness and speech-gesture matching <ref type="bibr" target="#b71">Yoon et al. 2020</ref>], but can be expensive and hard to exclude uncontrolled factors. The absolute difference of joint positions or other motion features, such as velocity and acceleration between a reconstructed motion and the ground truth, is used by several works as an objective metric <ref type="bibr" target="#b19">[Ginosar et al. 2019;</ref><ref type="bibr" target="#b32">Joo et al. 2019;</ref><ref type="bibr" target="#b38">Kucherenko et al. 2019</ref>]. However, this metric is not suitable for evaluating motions that are natural but not the same as the reference. Fr?chet Inception Distance (FID) <ref type="bibr" target="#b26">[Heusel et al. 2017</ref>] is a widely used criterion in image generation tasks that measures the difference between the distributions of the dataset and generated samples in the latent space. It successfully reflects the perceptual quality of generated samples. Similarly, <ref type="bibr" target="#b71">Yoon et al. [2020]</ref> and <ref type="bibr" target="#b54">Qian et al. [2021]</ref> propose Fr?chet Gesture Distance (FGD) and Fr?chet Template Distance (FTD) metrics, respectively. These metrics measure the perceptual quality of generated gestures. In this paper, we compare our framework with several baseline methods </p><formula xml:id="formula_0">! ! ! " ? ? Encoder Decoder ! ! ! " * ? ? $%" $%" &amp;'( motion blocks training set Motion Encoder Audio Encoder $ &amp;'( $)" &amp;'( $ $%" $ $ motion block ? ? !?# ! low-level audio blocks $%&amp; ? ? !!?#! $ * gesture style code ? ? # " ? * ? +,- ? &amp;,.,/, gesture lexeme ? ? #"</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generator</head><formula xml:id="formula_1">$ * predicted gesture lexeme block * ? ? ,?." $ ' &amp; ( )" ? high-level audio blocks $ text block ? ? !?# 5 $%&amp;$ ? ? 6)?8) gesture lexeme ? ? # " Fig. 2.</formula><p>Our system is composed of three core components: (a) the data module preprocesses a speech, segments it into normalized blocks based on the beats, and extracts speech features from these blocks; (b) the training module learns a gesture lexicon from the normalized motion blocks and trains the generator to synthesize gesture sequences, conditioned on the gesture lexemes, the style codes, as well as the features of previous motion blocks and adjacent speech blocks; and (c) the inference module employs interpreters to transfer the speech features to gesture lexemes and style codes, which are then used by the learned generator to predict future gestures.</p><p>using both user studies and objective metrics like FGD. We further propose a simple but effective rhythmic metric to measure the percentage of matched beats by dynamically adjusting the matching threshold, which provides a more informative picture of the rhythm performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYSTEM OVERVIEW</head><p>Our goal is to synthesize realistic co-speech upper-body gestures that match a given speech context both temporally and semantically. To achieve this goal, we build a system using neural networks that takes speech audio as input and generates gesture sequences accordingly. Additional speech modalities, such as text and speaker identity, will also be considered by the system when available to enhance semantic coherence and generate stylized gestures. A gesture motion consists of a sequence of gesture units, which can be further broken down into a number of gesture phases that align with intonational units, such as pitch accents or stressed syllables <ref type="bibr" target="#b33">[Kendon 2004;</ref><ref type="bibr" target="#b47">Loehr 2012]</ref>. The action in each of these gesture phases is typically a specific movement such as lifting a hand, holding an arm at a position, or moving both arms down together, which is often referred to as a gesture lexeme by linguists <ref type="bibr" target="#b35">[Kipp 2004;</ref><ref type="bibr" target="#b52">Neff et al. 2008;</ref><ref type="bibr" target="#b65">Webb 1996]</ref>. It is also revealed in the literature that there are only a limited number of lexemes used in everyday conversation. These lexemes form a gesture lexicon. A typical speaker may only use a subset of this lexicon and apply slight variations to the motion. We assume such variations cannot be inferred directly from the speech but can be characterized by some latent variables, which we refer to as the gesture style codes. Our system then generates gestures in a hierarchical order. It first determines the sequence of gesture lexemes and style codes and then generates gestural moves based on these motion-related features and other speech modalities.</p><p>Our system processes the input speech in a block-wise manner. Considering the temporal and structural synchrony between the gesture and the speech, we leverage a segmentation that aligns with the rhythm of the speech to ensure temporal coherence between the two modalities. Specifically, our system extracts beats from the input speech based on audio onsets and segments the speech into short clips at every beat. These clips are then time-scaled and converted into normalized blocks with the same length. We extract features at multiple levels for each block, where the high-level features are translated into a gesture lexeme, and the low-level features determine the style code. The generated gesture motions are then denormalized to match the length of the input speech.</p><p>As illustrated in <ref type="figure">Figure 2</ref>, our system consists of three core components: (a) the data module preprocesses a speech, segments it into normalized blocks based on the beats, and extracts speech features from these blocks; (b) the training module learns a gesture lexicon from the normalized motion blocks and trains the generator to synthesize gesture sequences, conditioned on the gesture lexemes, the style codes, as well as the features of previous motion blocks and adjacent speech blocks; and (c) the inference module employs interpreters to transfer the speech features to gesture lexemes and style codes, which are then used by the learned generator to predict future gestures. We train our system on a speech-gesture dataset with accompanying text and speaker identity (ID). The gesture lexicon is constructed using unsupervised learning based on a vector quantized variational autoencoder (VQ-VAE) <ref type="bibr">[Oord et al. 2017</ref>]. The generator is trained as an autoregressive encoder-decoder network, where we use an LSTM-based decoder combined with a vector quantized encoder to generate gesture motions. We train two separate interpreters to translate speech features into gesture lexemes and style codes, respectively. These interpreters can work with only the audio features and can be retrained to accept other speech modalities. In the following sections, we will provide details about these components and how they are trained in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DATA PREPARATION</head><p>The data module of our system preprocesses an input speech, segments it into uniform blocks based on speech rhythms, and extracts features that will be used to generate co-speech gestures. In this section, we first introduce the representations of different speech modalities and gesture motion and then describe details of the data preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representation of Speech Modalities</head><p>4.1.1 Motion Representation. We focus on upper-body gestures in this work. Our system employs a character model consisting of 16 upper-body joints, including a rotational root, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. A gesture pose is then represented as a list of joint rotations, parameterized using the exponential map, in the hierarchical order. We use ? R to represent the gesture pose at frame , and a clip of gestures is represented collectively as = { 1 , . . . , }, where is the number of frames. We retarget training motions onto this model by copying the rotations of corresponding joints. The translation and the rotation around the vertical axis are excluded from the root joint, ensuring a normalized body orientation.</p><formula xml:id="formula_2">! ! ! " ? ? ! ! ! ! ? ? text</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Text Representation.</head><p>Text transcription is an important speech modality that provides high-level linguistic information in a compact format. It is typically given as a word sequence, where the number of words per unit time can vary depending on the speed of the speech. Following , we align the words to the speech and convert the text into frame-level features to overcome this issue, which is done using an off-the-shelf text-speech alignment tool combined with a pre-trained language model.</p><p>Text-speech alignment is a standard technique in the field of speech synthesis. In our system, we employ Montreal Forced Aligner (MFA) <ref type="bibr" target="#b48">[McAuliffe et al. 2017]</ref> for this task, which pinpoints the beginning and end frames of every word in the speech. MFA also identifies silences and represents them as empty words. Since a speaker typically stops gesticulating in a long silence <ref type="bibr">[Graziano and</ref> Gullberg 2018], our system records those silences and uses them during training to reproduce such behaviors, as will be detailed later.</p><p>We then pass the text and the empty words into BERT <ref type="bibr" target="#b14">[Devlin et al. 2019</ref>], a popular and powerful pre-trained language model, to extract a high-level representation of the text. BERT computes an encoding vector for each word in an input sentence, which is then repeated and used for all the frames that the word occupies. We represent these word vectors collectively as = { 1 , . . . , } for a speech clip of frames, where each ? R .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Audio Representation.</head><p>Many recent studies use deep encoders to extract audio features from raw audio signals or audio spectrograms, where only the features extracted at the last layer of the encoder are used to generate gestures <ref type="bibr" target="#b43">Li et al. 2021a;</ref><ref type="bibr" target="#b54">Qian et al. 2021;</ref><ref type="bibr" target="#b71">Yoon et al. 2020]</ref>. Such a configuration potentially encourages the encoder to mix information from multiple levels into the same feature, which can be difficult to disentangle in the downstream generation tasks.</p><p>In our system, we propose to decouple the multi-level audio features in the encoder and use them in different scenarios. We assume the high-level features correspond to the speech semantics that determines the gesture lexemes, while the low-level features relate to the other audio information and can be used to control the gesture styles. As shown in <ref type="figure">Figure 4</ref>, we employ a pre-trained speech model, vq-wav2vec <ref type="bibr">[Baevski et al. 2020]</ref>, to extract audio features from raw audio signals and fine-tune it using a contrastive learning strategy.</p><p>The encoder of vq-wav2vec has = 8 convolutional layers. When taking a block of audio signals of frames, , as input, this encoder produces a representation for each frame of the audio. In this computation, the outputs of every layer can be considered as a set of multi-level features { }, = 1, . . . , , and notably, = . We then encourage the highest-level feature to match the speech content and push apart the features of the lower levels { }, &lt; to capture crucial content-irrelevant information. Specifically, we utilize the contrastive loss</p><formula xml:id="formula_3">L cont = ? log exp (sim(?,?)/ ) =1 =1 exp (sim(?,?)/ ) ,<label>(1)</label></formula><p>where the text feature is extracted from the speech transcription, the sim(?, ?) function computes the cosine similarity between two vectors as</p><formula xml:id="formula_4">sim(?,?) =????? ??? ,<label>(2)</label></formula><p>and is the temperature hyperparameter. All the feature vectors are projected into the same vector space using learnable linear projections?= ( ) and?= ( ), = 1, . . . , , respectively. Notably, we consider the highest-level audio feature of the current frame as the positive example and audio features of the other levels and the other frames as the negative examples in this contrastive learning process. This contrastive learning strategy is partially inspired by the HA2G model proposed by . However, unlike their approach, which considers contrastive learning as a part of the training of the gesture generator, we train the audio encoder in a separate pre-training stage using only the speech data. After the training, the features extracted at the second and the last layers of the encoder, represented by low ? R and high ? R , respectively, are then used in different training and inference stages in the downstream generation task. They can be represented collectively as low and high for a speech clip. Although the gesture motions are not considered here, we find that the results of this encoder still demonstrate correlations between the high-level audio features and the gestures. We will discuss these results later in Section 7.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Identity Representation.</head><p>Similar to previous studies <ref type="bibr" target="#b6">[Bhattacharya et al. 2021a;</ref><ref type="bibr" target="#b71">Yoon et al. 2020]</ref>, our system can leverage the speaker identity (ID) to help distinguish different gesture styles and achieve stylized gesture generation. We represent each speaker as a one-hot vector ? {0, 1} , where is the number of speakers in a dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Rhythm-Based Speech Segmentation</head><p>In this section, we describe how our system segments and normalizes an input speech into uniform blocks. This procedure is crucial for generating a gesture motion that is temporally synchronized with the rhythm of the speech. To that end, our system first identifies beats in the input audio, which generally corresponds to phonetic properties such as stress or accent, then segments the speech at every beat and time-scales the audio to the same length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Beat</head><p>Identification. Rhythm can be characterized by a pattern of beats. In music-related tasks, such as dance generation <ref type="bibr" target="#b2">[Aristidou et al. 2022;</ref>, identifying beats using the onsets of audio signals is a standard technique <ref type="bibr" target="#b5">[Bello et al. 2005;</ref><ref type="bibr" target="#b17">Ellis 2007]</ref>, where off-the-shelf tools such as librosa library <ref type="bibr" target="#b50">[McFee et al. 2015]</ref> can be employed to extract those audio features.</p><p>However, unlike the rhythm in music that is typically consistent over time, the pattern of beats in a speech can vary significantly according to the context and pace of the speech. Taking a close look at the time intervals between consecutive audio onsets in our training dataset, we notice that the majority of those intervals fall within a range roughly between = 0.2 ? 0.3 seconds and = 0.5 ? 0.6 seconds, as illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>, though the actual values of and may vary among datasets depending on the personality of the speakers and the language they speak. We also observe that the time intervals shorter than are often caused by noise, filler words, or stuttering. On the other hand, the intervals that are excessively long often correspond to pauses or silent periods.</p><p>Based on these observations, our system employs a simple heuristic strategy to identify beats based on the audio onsets. An onset will be recognized as a beat unless the time interval between it and the previous beat is shorter than , in which case the onset will be ignored. If an interval is longer than , we will insert a necessary number of pseudo-beats to make the duration of every new interval within the range <ref type="bibr">[ , ]</ref>. More specifically, we insert a pseudo-beat at the first frame which is seconds away from any preceding beat and where the volume of the audio is greater than a threshold?. Other pseudo-beats are then added recursively in the same way. We set the threshold?as the average volume of the environmental noise. If the entire interval is quieter than?, a minimal number of pseudo-beats will be placed evenly in it so that each new interval is shorter than .</p><p>4.2.2 Normalization. Our system then segments the speech into short clips at every beat. These clips are then time-scaled into uniform blocks of length . The speech modalities are segmented and time-scaled as well in this process. For the motion, , and text representation, , of a clip, we resample the corresponding features to match the new length. The audio is processed with additional care, where we use the TSM (Time-Scale Modification) algorithm to change the duration of the audio while preserving the pitch. The audio features low and high are then recomputed for the timescaled audio blocks. The speaker ID is a constant of the whole speech, which will not be changed during the normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GESTURE GENERATION</head><p>The generator module is the core component of our system. It synthesizes realistic gesture motions according to a sequence of gesture lexemes, the corresponding style codes, and the low-level features of the audio. In this section, we first introduce how we construct the gesture lexicon and then describe the design and training of the gesture generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Construction of Gesture Lexicon</head><p>As revealed in several pieces of literature in linguistics <ref type="bibr" target="#b35">[Kipp 2004;</ref><ref type="bibr" target="#b52">Neff et al. 2008;</ref><ref type="bibr" target="#b65">Webb 1996]</ref>, only a limited number of lexemes are used in everyday conversation. We assume that each lexeme corresponds to a specific motion category. Our goal is then to extract those motion categories from a large gesture dataset. To achieve this goal, we employ the vector quantized variational autoencoder (VQ-VAE) model <ref type="bibr">[Oord et al. 2017</ref>] to learn a categorical representation of the motion and construct the gesture lexicon.</p><p>VQ-VAE has been widely used to learn categorical spaces in many successful temporal models <ref type="bibr">[Baevski et al. 2020;</ref><ref type="bibr" target="#b15">Dhariwal et al. 2020;</ref><ref type="bibr" target="#b55">Ramesh et al. 2021;</ref><ref type="bibr" target="#b70">Yan et al. 2021</ref>]. Similar to a regular autoencoder, a VQ-VAE also has an encoder-decoder structure but quantizes the latent space using a discrete codebook. The codebook consists of a list of vectors and their associated indices. The output of the encoder network is compared to every vector in the codebook, where the vector that is the closest in Euclidean distance is considered to be the latent representation of the input and will be fed to the decoder. The training of a VQ-VAE is achieved by pulling together the latent code of input and its corresponding codebook vector.</p><p>As illustrated in <ref type="figure">Figure 2</ref>, we construct the gesture lexicon by learning the categorical representations of the normalized motion blocks using VQ-VAE. Following <ref type="bibr">[Oord et al. 2017]</ref>, the loss function is defined as</p><formula xml:id="formula_5">L lexicon = ? ? D ( )? 2 2 + ?E ( ) ? sg( )? 2 2 + ?sg(E ( )) ? ? 2 2 , (3) where = arg min ? ?S ? ? ? E ( )? 2 ,<label>(4)</label></formula><p>is a normalized motion block, E and D represent the encoder and decoder, respectively, sg stands for the stop gradient operator that prevents the gradient from backpropagating through it, S represents the codebook, or the lexicon, and is a codebook vector, or a lexeme. The first term of Equation (3) penalizes the reconstruction error, while the other two terms pull together the latent representation of motion and its corresponding lexeme. Notably, since S is discrete, the arg min operator in Equation <ref type="formula" target="#formula_5">(4)</ref> does not generate a gradient. The gradient of the reconstruction error with respect to the latent code is passed unaltered to the encoder during the backward pass as suggested in <ref type="bibr">[Oord et al. 2017</ref>].</p><p>We train the VQ-VAE on each dataset in a separate pre-training stage. The encoder is a multi-layer network consisting of four 1-D convolutional layers followed by a fully connected layer, which encodes a motion block into a vector ? R . The decoder is a mirror of the encoder structurally. The size of the lexicon is a hyperparameter, which is chosen empirically based on the size and complexity of the dataset. <ref type="figure">Figure 6</ref> shows the t-SNE visualization of the training results on two speech-gesture datasets, along with sample gestures of several lexemes. Once learned, the gesture lexicon and the gesture lexeme of every motion block are fixed and used by the generator and interpreters in both the training and inference stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Architecture of Generator</head><p>As illustrated in <ref type="figure">Figure 2</ref>, the generator module of our system is an autoregressive encoder-decoder network, where a new motion block is conditioned on not only the input speech but also the preceding block, the gesture lexeme, and the style code. More specifically, the generation of a motion block can be formulated as * = G(?? 1 ,?, , , ),</p><p>where?? 1 represents the features extracted from the preceding motion block,?stands for the representation of the input audio, and are the gesture lexeme and style code of the new block, respectively. Note that we use an asterisk ( * ) to indicate a generated quantity. All these motion and feature blocks have the same length of frames, where and are repeated and stacked into the corresponding blocks, represented as and , respectively.</p><p>We extract the motion feature?? 1 ? R ??a s</p><formula xml:id="formula_7">?1 = E ( ?1 ),<label>(6)</label></formula><p>where the encoder E is a 1-D convolutional network with three layers. The audio feature?? R ??i s computed using three consecutive audio blocks</p><formula xml:id="formula_8">= E ( low ?1 , low , low +1 )<label>(7)</label></formula><p>to allow the generator to prepare for the future gestures. Notably, the original duration of an audio block is characterized by the onset interval <ref type="bibr">[ , ]</ref>, which is typically [0.2 , 0.5 ] in our experiments. Thus the temporal window of this encoder is roughly [0.6 , 1.5 ]. Each low is the low-level feature pre-computed from the raw speech audio. We assume that low already captures necessary information and use a simple network consisting of one fully connected layer as the encoder E .</p><p>In the training stage, the gesture lexeme of each motion block is determined during the construction of the gesture lexicon, while the style code is a learnable variable that will be trained along with the generator. In the inference stage, both and are provided by the interpreters, as will be discussed below.</p><p>In addition to these features, we include a positional encoding block, ? R ? , to let the generator know the progress of the generation in a motion block, which is a standard component of transformers <ref type="bibr" target="#b63">[Vaswani et al. 2017</ref>] and many sequence generation tasks <ref type="bibr" target="#b23">[Harvey et al. 2020]</ref>. For our normalized blocks with frames, we compute = { 1 , . . . , } as</p><formula xml:id="formula_9">2 = sin 2 / 2 +1 = cos 2 / ,<label>(8)</label></formula><p>where is the dimension of the encoding and = 10, 000 is a constant controlling the rate of change in frequencies along the embedding dimensions.</p><p>The generator G consists of an MLP-based encoder followed by an LSTM-based decoding network. Inspired by the successful systems in generating sequential output <ref type="bibr">[Oord et al. 2017;</ref><ref type="bibr" target="#b56">Richard et al. 2021]</ref>, we quantized the latent space of the encoder into groups of -way categories, which provides different configurations. We use = 64 and = 128 to ensure a large enough categorical space. In addition, we leverage Gumbel-softmax <ref type="bibr" target="#b31">[Jang et al. 2017</ref>] to convert a latent code into a codebook vector, which can be viewed as a differentiable sampling operator for the discrete codebook search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training</head><p>We train the generator G, the encoders E and E , and the learnable style codes { } by minimizing a combination of loss terms:</p><formula xml:id="formula_10">L gen = rec L rec + perc L perc + lexeme L lexeme + L . (9)</formula><p>The reconstruction loss</p><formula xml:id="formula_11">L rec = ? ? * ? 2 2<label>(10)</label></formula><p>is simply the MSE loss between the generated motion block and the ground truth. We additionally include a perceptual loss to ensure the similarity between the generated motion and the ground truth in the feature level as well, which is defined as</p><formula xml:id="formula_12">L perc = ?E ( ) ? E ( * )? 2 2 ,<label>(11)</label></formula><p>where E is the motion encoder pre-trained in Section 5.1. We assume that the gesture lexeme determines the type of gesture motion, and the other speech modalities only affect the motion variations. To enforce this assumption, we develop a new perceptual loss, namely the lexeme loss. We first generate a number of new motion blocks using the current gesture lexeme but random sets of </p><p>where?? 1 ,?, and correspond to a random speech block . Then the lexeme loss is defined as</p><formula xml:id="formula_14">L lexeme = 1 ?? ? ? ? E ( * )? 2 2 ,<label>(13)</label></formula><p>where is a random subset of all the motion blocks in the training dataset. The size of , , is chosen based on the size of the dataset. Lastly, we regularize the learning of the style code by applying a KL-divergence loss</p><formula xml:id="formula_15">L = (N ( , 2 )?N (0, 1)),<label>(14)</label></formula><p>where and 2 are the mean and variance vectors of the style codes in a mini-batch, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CO-SPEECH GESTURE INFERENCE</head><p>When given a speech as input, our system segments it into normalized feature blocks { low , high , , } and then generates motion blocks</p><formula xml:id="formula_16">{ * } recursively, where * = G E * ?1 , E low ?1 , low , low +1 , * , * , ,<label>(15)</label></formula><p>and G, E , E are the components of the learned gesture generator. The generated motion blocks are then denormalized to their original length in the input speech, producing a realistic co-speech gesture animation. Note that we again use the asterisk ( * ) to indicate a computed quantity that is not provided directly in the speech. All the variables in Equation <ref type="formula" target="#formula_3">(15)</ref> are known except the gesture lexeme * and style code * . As shown in <ref type="figure">Figure 2</ref>, our system learns two interpreters to compute them: the lexeme interpreter P translates high-level speech features into the gesture lexemes * , and the style interpreter P predicts the style code * according to the low-level speech features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Lexeme Interpreter</head><p>As illustrated in <ref type="figure" target="#fig_5">Figure 7</ref>, the lexeme interpreter is formulated as * = P ( * ?1 ,?H,?, ),</p><p>which is conditioned on the gesture lexeme of the last motion block * ?1 , and the high-level features of the current speech block. Like the generator, the high-level audio features?H ? R ??a re computed using three consecutive audio blocks</p><formula xml:id="formula_18">H = E lex ( high ?1 , high , high +1 ),<label>(17)</label></formula><p>where each high contains the high-level representation of the input speech audio, and the encoder E H is a single-layer fully connected network. The text feature?? R ??i s also extracted from the text representation of the speech block as</p><formula xml:id="formula_19">= E lex ( ),<label>(18)</label></formula><p>where E lex is again a single-layer network. Lastly, the one-hot representation of the speaker ID, , is repeated times and converted into a feature block. Those feature blocks can then be concatenated together and fed to an LSTM-based decoder to predict the next gesture lexeme * . However, considering that the lexemes are selected from the discrete gesture lexicon, we can convert this regression problem into a classification problem. Specifically, instead of directly evaluating Equation <ref type="formula" target="#formula_3">(16)</ref>, we can let P predict the probability that * is a specific lexeme in the gesture lexicon, then the lexeme with the maximum likelihood will be considered as the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Style Interpreter</head><p>The style interpreter shares a similar structure with the lexeme interpreter. It computes * as * = P *</p><formula xml:id="formula_20">?1 , * , E style ( ) , E style low ?1 , low , low +1 ,<label>(19)</label></formula><p>which is conditioned on the last style code and the new gesture lexeme computed by the lexeme interpreter. The low-level audio representation low is used in the style interpreter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Audio-Only Inference</head><p>Both the two interpreters can be reformulated to take only the speech audio as input, where the features related to the text representation , and optionally the speaker ID , will be removed from Equation <ref type="formula" target="#formula_3">(16)</ref> and <ref type="formula" target="#formula_3">(19)</ref>.</p><p>In practice, these audio-only interpreters allow cross-language gesture generation, where the speech audio in another language can be taken as input to synthesize realistic gestures without further training. For example, we can utilize a pre-trained model on an English dataset to generate gestures that accompany a Chinese speech. We will show related experiments in Section 7.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Training</head><p>During the training of the generator, we have computed the gesture lexeme and the style code of every motion block in the training dataset. We then train the two interpreters using these results as the ground truth. We minimize the standard categorical cross-entropy loss to train the lexeme interpreter, while the MSE loss is used for the style interpreter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Silent Period Hint.</head><p>A speaker typically stops gesticulating during a silent pause <ref type="bibr" target="#b20">[Graziano and Gullberg 2018]</ref>. Such behaviors are often crucial to the naturalness of a co-speech gesture animation. However, we find that it is often difficult for a gesture generator to The speechgesture datasets may lack necessary motion, and some specific generator models, such as LSTM, may exhibit generative inertia that makes it difficult to become stationary in time.</p><p>To solve this problem, we develop a new approach, which we refer to as the silent period hint, to encourage the lexeme interpreter to compute a specific silent lexeme that corresponds to a silent gesture when encountering a silent period. We check all the lexemes in the lexicon and label a number of stationary ones as the silent lexemes. Notably, the silent lexemes can be automatically labeled by finding such a lexeme corresponding to an empty text word. Then, when a training audio block is in a silent period, which can be detected by the data module of our system, we will force the lexeme interpreter to output the silent lexeme that is the nearest to the current lexeme in the latent space. Moreover, a silent data augmentation is applied when training the generator. We find data blocks that contain empty words and randomly insert 0 ? 10 consecutive silent blocks after them. The silent block above includes four different features: (a) the audio feature is the environmental noise; (b) the style code is set to zero; (c) the gesture lexeme is the silent lexeme that is the nearest to the previous lexeme in the latent space; and (d) the motion is a stationary pose that is the same as the last frame of the previous motion block. In total, the amount of the inserted silent blocks accounts for 5% of the whole training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head><p>In this section, we first introduce the setup of our system and then evaluate its performance, followed by quantitative and qualitative comparisons with other recent systems. Lastly, we do the ablation study to make an overall analysis of our system.  <ref type="bibr" target="#b18">[Ferstl and McDonnell 2018]</ref>, the TED dataset <ref type="bibr" target="#b72">[Yoon et al. 2019]</ref>, and a Chinese dataset collected for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">System Setup</head><p>Trinity Gesture dataset is a large database of speech and gestures jointly collected by <ref type="bibr" target="#b18">Ferstl and McDonnell [2018]</ref>. This dataset consists of 242 minutes of motion capture and audio of one male actor talking on different topics. The actor's motion was captured with a 20-camera Vicon system and solved onto a skeleton with 69 joints.</p><p>In this paper, we use the official release version of The GENEA Challenge 2020 Dataset <ref type="bibr" target="#b40">[Kucherenko et al. 2021a]</ref>, where 221 minutes are used as training data, and the remaining 21 minutes are kept for testing.</p><p>TED Gesture dataset <ref type="bibr" target="#b72">[Yoon et al. 2019</ref>] is a 3D upper-body gesture dataset collected using 3D pose estimation from English TED videos. This dataset includes 3D upper-body gestures of the speakers, the aligned speech audios, and text transcripts. In total, there are 253, 186 data samples, 80% of which are training samples, 10% belong to the validation set, and the remaining 10% are test samples. The duration of each data sample is 34 frames at a rate of 15 fps, so the total length of this dataset is about 97 hours. Notably, we adapt our model to take 3D joint positions instead of rotations for the TED dataset. The generated gestures are also represented in 3D joint positions, which are then converted into joint rotations for visualization.</p><p>Additionally, we collected a 4-hours (80% are the training data and 20% are used for testing) Chinese Gesture dataset using the Noitom Perception Neuron Pro system. This dataset contains 3D fullbody gestures of five speakers, aligned speech audios, and Chinese text transcripts. The text transcripts were recognized by Alibaba Cloud Automatic Speech Recognition (ASR) service. The skeleton of this dataset is retargeted to be consistent with the Trinity Gesture dataset. To ensure semantic richness, speakers are instructed to cover a diverse set of topics, such as cooking, fiction, philosophy of life, and academic reporting. <ref type="figure" target="#fig_6">Figure 8a</ref> illustrates the distribution of onset intervals in our dataset, and <ref type="figure" target="#fig_6">Figure 8b</ref> shows the visualization of the learned gesture lexicon on our dataset.  <ref type="bibr">128,</ref><ref type="bibr">128,</ref><ref type="bibr">128,</ref><ref type="bibr">192,</ref><ref type="bibr">32</ref>, and 32 respectively. The size of the gesture lexicon is 50 for both the Trinity and Chinese datasets but 100 for the TED dataset. We train our framework using the Adam optimizer with 1 = 0.9, 2 = 0.999 and a learning rate of 0.0003. The loss weights , , rec , perc , lexeme , and are set as 1.0, 1.0, 1.0, 0.5, 0.2, and 1.0, respectively. At runtime, we use a Gaussian filter with a kernel size of to smooth the denormalized gesture sequence, where = 5 is chosen to generate the results presented in this paper.</p><p>We train separate gesture generators on the Trinity, TED, and Chinese datasets. The cross-language capability of a generator can be further enhanced by pre-training the audio encoder (Section 4.1.3) using datasets in different languages. We have tried pre-training the audio encoder using both an English dataset (such as the Trinity or TED datasets) and our Chinese dataset and using it to train the generator on the Chinese dataset only. The gesture results can be found in the supplementary video. <ref type="figure">Figure 9</ref> shows the gesture synthesis results for the speech excerpts from the test set of the Trinity dataset. Our system generates different types of realistic gestures. The character makes a metaphoric gesture when saying fine and an iconic gesture for defend. There are beat gestures for words like thing and selling, and a deictic gesture appears when the character says me.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Evaluation</head><p>We also did a cross-language synthesis experiment to test the robustness of our system. We use the pre-trained model trained on the Trinity dataset (an English dataset) to generate gestures for a Chinese speech clip. Since different languages do not share the same word embedding, we generate gestures by taking only the speech audio as input (Section 6.3). As illustrated in <ref type="figure" target="#fig_1">Figure 13b</ref>, when encountering a different language, our system still generates beat-matched and stylized gestures, reflecting the robustness of our system. We also trained our system on the Chinese dataset and then did another cross-language experiment by generating gestures for an English speech excerpt. Please refer to the supplementary video for the visualization results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Style Editing. Inspired by the idea of Alexanderson et al.</head><p>[2020], we augment our system to achieve motion editing by adding a feature block ? R ? of the gesture motion as an extra input of the generator and the lexeme interpreter <ref type="figure" target="#fig_5">(Figure 7)</ref>. The computations in Equation <ref type="formula" target="#formula_6">(5)</ref> and <ref type="formula" target="#formula_3">(16)</ref> are then reformulated as * = G <ref type="figure">(?? 1 ,?, , , , )</ref>,</p><formula xml:id="formula_21">(20) * = P ( * ?1 ,?H,?, , ),<label>(21)</label></formula><p>which allows the network to learn the relationship between a desired motion feature and a gesture motion. During inference phase, we can easily edit the motion style feature of the generated gestures by modifying the feature block as needed. Similar to , we have experimented with three different style features using our system: the height of the right-hand (hand height), the average speed of both hands (hand speed), and the average distance from the hand positions to the up-axis through the root joint of the character (hand radius). We train a separate generator for each style feature. The training data is computed using the reference motions and averaged within a four-second sliding window, forming -frame feature blocks.</p><p>We have synthesized three animations for each of the motion styles. Each animation has a constant desired low, mid, or high feature value, as shown in the first three columns of <ref type="figure" target="#fig_7">Figure 10</ref>. The last column of <ref type="figure" target="#fig_7">Figure 10</ref> shows the accuracy of the generated motion features. These results indicate that all the editing signals could efficiently affect the generated gestures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Comparisons</head><p>In this section, we compare our system with several state-of-theart systems to demonstrate the advances made by our system. We first briefly introduce these systems and then our quantitative and qualitative comparisons. We also propose a simple but effective objective metric (PMB) to evaluate the rhythm performance for co-speech gesture synthesis. ? that was writing for me ? <ref type="figure">Fig. 9</ref>. Qualitative results on the gestures synthesized by our method for four sample speech excerpts from the Trinity Gesture dataset <ref type="bibr" target="#b18">[Ferstl and McDonnell 2018]</ref>. The character makes a metaphoric gesture when saying fine and an iconic gesture for defend. There are beat gestures for the words like thing and selling, and a deictic gesture appears when the character says me. The drawing of this figure is inspired by <ref type="bibr" target="#b71">[Yoon et al. 2020</ref>].    , GTC <ref type="bibr" target="#b71">[Yoon et al. 2020]</ref>, and S2AG <ref type="bibr" target="#b6">[Bhattacharya et al. 2021a</ref>] on the TED and Trinity datasets. The system without beat segmentation (w/o BC) uses a fixed interval of for segmentation, which is 0.5 ? 0.6 depending on which dataset is used. The system without gesture lexeme (w/o SC) excludes the gesture lexicon and lexeme interpreter modules. The generator is retrained to predict future gestures based on only the previous motion, the audio, and the style code. Similarly, the system without gesture style code (w/o ZC) excludes the style code and the style interpreter modules. Only the motion, the audio, and the lexeme are used by the generator. Ours (audio only) denotes the audio-only inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>System    <ref type="bibr" target="#b71">[Yoon et al. 2020</ref>] and Speech to Affective Gestures (S2AG) <ref type="bibr" target="#b6">[Bhattacharya et al. 2021a</ref>]. GTC uses speech audio, text transcript, and speaker identity to generate gestures. Based on the three modalities used in GTC, S2AG adds another new modality of affective expressions from the seed poses into their model.</p><formula xml:id="formula_22">MAJE ( ) ? MAD ( / 2 ) ? FGD ? PMB (%) ?<label>Trinity</label></formula><p>Because we have no access to the official pretrained model of SG, we strictly follow the official configuration and run the codes offered by authors to train a model. For other systems, we use the pretrained models provided by the authors. For a fair comparison, we use the same skeleton and motion frame rate as the baselines. 7.3.2 Quantitative Evaluation. We first adopt three commonly used evaluation metrics (MAJE, MAD, and FGD) to compare these systems quantitatively. MAJE measures the mean of the absolute errors between the generated joint positions and the ground truth over all the time steps and joints, which indicates how closely the generated joint positions follow the ground truth. MAD measures the mean of the ? 2 norm differences between the generated joint accelerations and the ground truth over all the time steps and joints, which indicates how closely the ground truth and the generated joint movements match. Fr?chet Gesture Distance (FGD) was proposed by <ref type="bibr" target="#b71">Yoon et al. [2020]</ref>, which measures the difference between the distributions of the latent features of the generated gestures and ground truth, where the latent features are extracted using an auto-encoder trained on the Human 3.6M dataset <ref type="bibr" target="#b30">[Ionescu et al. 2013]</ref>. FGD could assess the perceived plausibility of the synthesized gestures.</p><p>Calculating the matching rate between audio and motion beats is a standard method of evaluating rhythm performance, which has been widely used in music-driven dance synthesis <ref type="bibr" target="#b44">Li et al. 2021b</ref>]. However, we cannot simply apply this method to the gesture generation task because the onset of beat gesture usually precedes the corresponding speech beat by a small amount of time <ref type="bibr" target="#b53">[Pouw and Dixon 2019]</ref>. Thus, we need a distance threshold to determine robustly whether the audio and motion beats match each other.</p><p>Percentage of correctly predicted keypoints (PCK) is a widely used metric in human pose estimation <ref type="bibr">[Mehta et al. 2017;</ref><ref type="bibr" target="#b67">Wei et al. 2016]</ref>, where a predicted key point is considered correct if its distance to the ground truth is smaller than an adjustable threshold. Inspired by this metric, we propose a new metric, PMB, as the percentage of matched beats, where a motion beat is considered to be matched if its temporal distance to a nearby audio beat is smaller than a threshold . Specifically,</p><formula xml:id="formula_23">PMB( , ) = 1 ?? =1 ?? = * [ ?1] +1 1 ? ? ? 1 &lt; . (22)</formula><p>The sequence of motion beats = { 1 , . . . , }, where is the number of motion beats, is identified using the algorithm proposed by <ref type="bibr" target="#b27">Ho et al. [2013]</ref> based on the local minima of joint deceleration. The audio beats = { 1 , . . . , } are the onset sequence, where is the number of onsets. * [ ?1] represents the index of the audio beats that the last motion beat matches. 1 denotes the indicator function. The threshold is set to 0.2s by default, but we can adjust continuously and observe the changes of the PMB values, which provides a more comprehensive picture of the rhythmic performance. <ref type="table" target="#tab_4">Table 1</ref> summarizes the performance of all the systems on the two English datasets, Trinity and TED. For the MAJE, MAD, and FGD metrics, our system achieves the lowest values on both datasets. Note that the FGD values of our system are significantly lower than other systems, which indicates the better perceptual quality of gestures synthesized by our system. It is interesting that the FGD values rise rapidly without the gesture lexeme component (w/o SC), which means the gesture lexeme is crucial to gesture quality. There is a decline in the generation performance in the audio-only inference because of the lack of sufficient input information. However, the generated gestures are still acceptable.</p><p>As for the rhythm performance, our system achieves the highest PMB values on both datasets. The PMB values drop rapidly without the beat segmentation component (w/o BC), indicating that beat segmentation is vital to rhythm awareness. <ref type="figure">Figure 11</ref> shows the PMB results with different thresholds. It can be seen that even the ground-truth motion does not match the speech beats precisely, while the behavior of our method is closer to the ground truth compared with the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.3">User Study.</head><p>We further conduct user studies to assess the performance of our system qualitatively, where SG  and Ges ] are used as the baseline methods. We generate 14 test video clips, each consisting of the results synthesized by our methods, the baselines, and the ground truth in random order. Among the 14 clips, nine are English clips generated from the test set of the Trinity dataset, and five are Chinese clips generated from the test set of our Chinese dataset. Notably, all the clips are synthesized using the models trained on the English Trinity dataset. The duration of each clip is around 30s.</p><p>We have recruited 30 volunteer participants to participate in our study, of which 19 are male and 11 are female. 19 participants are 18 ? 22 years of age, 10 are between 23 ? 30, and one is above 30. When participants watch each video clip, they will be asked to answer three questions and rate the video from 1 to 5, with 1 being the worst, 3 being average, and 5 being the best. The three questions are: (1) human likeness (neglecting the speech audio), (2) speech-to-gesture content matching, and (3) speech-to-gesture beat matching.</p><p>The results of these user studies are shown in <ref type="figure">Figure 12</ref>, our system receives higher scores than the other systems and is closer to the real gestures (GT). A one-way ANOVA reveals main effects of human likeness, content matching, and beat matching, and a post-hoc Tukey multiple comparison test identifies a significant difference ( &lt; 0.005) between our system and all the other methods. As also illustrated in <ref type="figure" target="#fig_1">Figure 13a</ref>, the end-to-end systems SG and Ges are less sensitive to rhythm than ours, and the resulting motions lack diversity, which affects their performance in the user studies.</p><p>The statistical results of the cross-language test <ref type="figure">(Figure 12b</ref>) demonstrate the better robustness of our system. When dealing with a completely different language <ref type="figure" target="#fig_1">(Figure 13b)</ref>, the gestures generated by SG are more rigid and do not match the beats correctly. In contrast, our model (audio-only) is still able to perceive beats ?but then there's stuff, when you start selling it as a (like) self-defense thing which a lot of martial arts like, ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ges</head><p>Ours Audio:</p><p>Text:</p><p>(a) Gesture results for an English speech clip. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Ablation Study</head><p>We conduct a variety of ablation studies to analyze the performance of our system. Notably, only the ablation of the lexicon size <ref type="figure" target="#fig_6">(Figure 18)</ref> uses the validation set of the dataset to determine the hyperparameter of the model. All other experiments are based on the test set.</p><p>7.4.1 Hierarchical Audio Feature Disentanglement. We presume that the high-level audio feature high contains semantics-relevant information that determines the gesture lexeme . To justify this assumption, we apply the K-means algorithm to all the high-level audio blocks of the TED Gesture dataset and get 50 clusters, where each cluster essentially indexes the audio clips with similar semantics. We can find several representative clusters high 0 whose corresponding text transcripts contain words with a similar meaning, such as many, quite a few, lots of, much, and more, etc. Meanwhile, these audio clips also correspond to a set of generated motion blocks { * 0 , * 1 , . . . }. By encoding these motion blocks using the pre-trained encoder E, we can obtain their corresponding motion latent codes. As illustrated in <ref type="figure">Figure 14a</ref>, these latent codes (gray dots) only appear in a few gesture lexemes (orange, purple, and red), and it can be seen that the sample gestures of these latent codes indeed convey the semantics of the cluster high 0 . The same observation is not true for the low-level audio features. If we also cluster all low-level audio features and pick a representative cluster low 0 , <ref type="figure">Figure 14b</ref> shows that the corresponding motion latent codes (various color dots) appear nearly uniformly in most of the gesture lexemes. The experiments above confirm the correlation between the high-level audio features and the gesture lexemes, as well as the semantic disentanglement between high-level and low-level audio features. system, we introduce the learnable gesture style code combined with the low-level audio feature to jointly determine the motion variations, which can be considered a fine-grained style. As illustrated in <ref type="figure" target="#fig_3">Figure 15b</ref>, the low-level audio feature increases the variety of the generated gestures but cannot fully decide the variations. With the introduction of the gesture style code <ref type="figure" target="#fig_3">(Figure 15c</ref>), the distribution of generated gestures becomes closer to the ground-truth distribution. <ref type="table" target="#tab_8">Table 2</ref> compares several settings in terms of motion variance, FGD, and PMB. We measure the Euclidean distance from the motion latent code of a motion to the corresponding gesture lexeme and compute the variance of these distances corresponding to the same lexeme. The motion variance is then defined as the average of such variances of every lexeme. denotes the output of the style interpreter, while random is a random style code sampled from the normal distribution in the latent space. Consistent with <ref type="figure" target="#fig_3">Figure 15</ref>, <ref type="table" target="#tab_8">Table 2</ref> also shows that combining the low-level audio feature and styles achieves more significant motion variance and lower FGD values. Besides, the FGD values in <ref type="table" target="#tab_8">Table 2</ref> also indicate that the style codes computed by the interpreter generate gestures that are more perceptually acceptable than that created using random style codes.</p><p>We have also evaluated the importance of the text features in interpreting the gesture style codes (see <ref type="table" target="#tab_4">Equation 19</ref>). The result of <ref type="table">Table 3</ref> shows that interpreting the style codes with text features does improve the FGD value. However, the improvement is marginal. Considering the inference efficiency, we can interpret the style codes conditional on only the low-level audio features. 7.4.3 Range of Onset Intervals. Choosing a proper range of onset intervals is crucial to achieving quality gestures. Intuitively, the lower bound of this range affects the model's sensitivity to beat. If the lower bound is too high (such as 0.5s), the interval between the identified beats becomes large, causing the model to respond sluggishly to beats. The upper bound of the onset intervals regularizes the variance of the duration of the speech blocks, which should  not be too large either. If the length of the speech blocks differs too much, the side effect of the normalization becomes visible, causing unnatural transitions between the generated gesture blocks. To verify the above hypothesis, we compare the performance of three different interval ranges: 0.2-0.5s, 0.2-1.0s, and 0.5-1.0s, where 0.2-0.5s is our default setting (Section 4.2.1). As shown in <ref type="table" target="#tab_9">Table  4</ref>, our default range of onset intervals achieves the best FGD and PMB values on both the Trinity and TED Gesture datasets. PMB drops significantly when the minimum distance between onsets is high (0.5-1.0s), which indicates that the model is insensitive to the rhythm. When the variance is too large (0.2-1.0s), PMB is barely affected, but FGD drops a lot, showing that the generated motions exhibit a lower quality. This result is consistent with our hypothesis.</p><p>Moreover, to explicitly show the effect of the minimum onset interval, we visualize the synchronization between the motion and audio beats under different interval ranges in <ref type="figure" target="#fig_11">Figure 16</ref>. Similar to <ref type="bibr" target="#b2">Aristidou et al. [2022]</ref>, we calculate the motion beats based on the motion deceleration <ref type="bibr" target="#b13">[Davis and Agrawala 2018]</ref>. As shown in <ref type="figure" target="#fig_11">Figure 16</ref>, the motion beats extracted under the interval range of 0.2-0.5s (denoted by the green stars) synchronize with the audio beats (dashed red lines). In contrast, the motion beats extracted under the interval range of 0.5-1.0s (denoted by the orange stars) do not match well with the audio beats, which proves that a high minimum interval (0.5s) will cause a low beat sensitivity of the model. 7.4.4 Gesture Lexeme. <ref type="table" target="#tab_4">Table 1</ref> has shown that FGD increases significantly without the gesture lexeme, indicating the importance of the gesture lexeme in achieving quality motions. Besides, as demonstrated in <ref type="figure" target="#fig_5">Figure 17</ref>, the variety of the generated gestures is  also significantly reduced without the gesture lexeme. To show this conclusion quantitatively, we calculate the entropy of the gesture lexemes to measure the motion diversity as</p><formula xml:id="formula_24">Diversity = ? ?? =1 log ,<label>(23)</label></formula><p>where is the size of gesture lexicon, indicates the occurrence frequency of the -th lexeme in the generated gestures. As shown in <ref type="table" target="#tab_10">Table 5</ref>, our system with the gesture lexeme creates much higher diversity than the system without it, which further testifies the conclusion of <ref type="figure" target="#fig_5">Figure 17</ref> and again emphasizes the importance of the gesture lexeme. 7.4.5 Size of Gesture Lexicon. <ref type="figure" target="#fig_6">Figure 18</ref> shows the performance of our system under different gesture lexicon sizes. It can be seen that neither too small nor too large lexicons achieve good results, measured as FGD values. On the one hand, a small gesture lexicon forces a diverse range of gesture motions to be merged into the same gesture lexeme, which aggravates the one-to-many mapping issue and causes the generator hard to learn all the motions. On the other hand, an excessively large lexicon forcibly splits many lexemes into sub-lexemes. These sub-lexemes are typically close together, making the gesture lexeme interpretation more challenging and thus negatively affecting the gesture quality. We note that the PMB metric is less affected by the gesture lexicon size, possibly because our beat-based segmentation and normalization mechanism explicitly enforces the gesture rhythm. Based on these experiments, we set  <ref type="figure" target="#fig_6">Fig. 18</ref>. Effects of the size of the gesture lexicon. The gray dashed lines mark the optimal lexicon size. <ref type="table">Table 6</ref>. Comparison of interpreters. The statistical interpreter selects gesture lexemes based on the frequency distribution of lexemes but neglects the input speech. The learning-based interpreter is our default interpreter that translates the input speech into gesture lexemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Interpreter the sizes of the gesture lexicon for the Trinity and TED dataset to 50 and 100, respectively, as shown in <ref type="figure">Figure 6</ref>.</p><p>7.4.6 Different Interpreters. In our system, the lexeme interpreter determines the sequence of gesture lexemes. Our learning-based interpreter described in Section 6.1 accomplishes this task according to the speech features and the previous lexemes. Besides, inspired by <ref type="bibr" target="#b2">[Aristidou et al. 2022]</ref>, we further experiment with a statistical interpreter that matches the frequency of each lexeme in the generated gestures with the reference. Specifically, for a speaker , we calculate the frequency distribution of gesture lexemes, ? R , and a transition matrix, ? R ? , describing the frequency of transitions between lexemes using the training data. These quantities can be considered as a global representation of speaker's gesture style. During inference, we configure the statistical interpreter to ensure that the lexeme distribution of the generated gesture sequence matches the corresponding global distribution . To achieve this goal, at each generation step, we compute a multinomial distribution characterized by * +1 using the transition matrix and the difference between the current and target lexeme frequencies and , where * +1 = softmax( ? ) ? .</p><p>Then the next lexeme is sampled from the multinomial distribution. This statistical interpreter does not consider the input speech but only the statistics of the generated gestures when selecting the lexemes. In practice, the result motions still look acceptable but more random. This can be confirmed by <ref type="table">Table 6</ref>, where the statistical interpreter exhibits a lower prediction accuracy and higher FGD values than our learning-based method because of the lack of semantic information brought by the input speech.</p><p>7.4.7 Positional Encoding. The positional encoding block (Equation 8) informs the generator about the frame-level progress of the synthesis in a motion block, which helps the generator model the temporal structure, especially the rhythm, of the sequence. As shown in <ref type="table" target="#tab_12">Table 7</ref>, the positional encoding block can improve the beat-matched rate (higher PMB) while enhancing the perceptual quality of generated movements (lower FGD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we present a rhythm-and semantics-aware co-speech gesture synthesis system that can generate realistic gestures to accompany a speech. For the rhythm, we utilize a segmentation pipeline that explicitly enforces beat alignment to ensure the temporal coherence between the speech and gestures. For the semantics, we successfully disentangle both low-and high-level neural embeddings of speech and motion based on linguistic theory. Then, we devise two neural interpreters to build correspondence between the hierarchical embeddings of the speech and the motion. To evaluate the rhythmic performance, we propose a new objective metric, PMB, to measure the percentage of matched beats. Our method outperforms state-of-the-art systems both objectively and subjectively, as indicated by the MAJE, MAD, FGD, PMB metrics, and human feedback. The cross-language synthesis experiment demonstrates the robustness of our system for rhythmic perception. In terms of application, We show our system's flexible and effective style editing ability that allows editing of several directorial styles of the generated gestures without manual annotation of the data. Lastly, we have systematically conducted detailed ablation studies that justify the design choices of our system. There is still room for improvement in our current research. First, our beat detection algorithm is not perfect. We have assumed that the gesture beats coincide with the verbal stresses, but in practice, it has been observed that gesture beats may not always correspond to stressed syllables <ref type="bibr" target="#b49">[McClave 1994]</ref>. How to accurately model the complex gestural rhythm is an exciting topic for further exploration. Second, our system can only capture semantics-related gestures repeatedly appearing in the dataset. Learning semantically meaningful gestures that are sparsely distributed in a dataset and allowing a user to control the gesture corresponding to specific semantics is still challenging. Third, our system hypothesizes that each audio onset should correspond to a beat gesture. However, in reality, humans do not make a beat gesture at every point of verbal emphasis. We believe our framework can be easily augmented by employing another model to predict whether the character should gesture at a specific moment, as suggested by <ref type="bibr">[Speech2Properties2Gestures]</ref>, and replacing the corresponding lexeme with the silent lexeme. Finally, we only consider the upper body gestures in this work. Generating full-body gestures that include locomotion, facial expressions, finger motions, and the temporal and semantic correspondence among them is a valuable topic for future exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The character model used in our system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>A contrastive learning task is performed to disentangle multi-level audio features. We use the text feature as the anchor of this learning. The highest-level audio feature high is considered as the positive sample, while the features of the lower levels are all treated as negative samples. After training, both high and the feature extracted at the second level, low , are used for gesture generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FrequencyFig. 5 .</head><label>5</label><figDesc>Distribution of time intervals between consecutive audio onsets in two open-source speech-gesture datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>t-SNE visualization of gesture lexicons. Each color stands for a gesture lexeme. (a) lexicon learned on the Trinity Gesture dataset with 50 lexemes. (b) lexicon learned on the TED Gesture dataset with 100 lexemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Architecture of the lexeme interpreter.other features, * = G(?? 1 ,?, , , ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Properties of our Chinese Gesture dataset. (a) Distribution of onset intervals; (b) t-SNE visualization of gesture lexicon. deal with silent periods well, even in recent successful systems such as [Alexanderson et al. 2020; Kucherenko et al. 2020].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Results of style editing for the right-hand height (the first row), the hand speed (the second row), and the hand radius (the third row). The graphs on the right show the editing input (flat line) and the corresponding values of the output motions. The box plots show the statistics of the output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>. PMB results of continuously adjusting the threshold on the Trinity and TED datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Mean ratings for the Chinese speech clips.Fig. 12. User study results with 95% confidence intervals. Asterisks indicate the significant effects ( * : &lt; 0.05, * * : &lt; 0.01, * * * : &lt; 0.001). All the models are trained on the Trinity dataset (an English dataset). See Section 7.3.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>7.4.2 Gesture Style Code. The low-level audio feature low contains semantic-irrelevant information, e.g., pitch and volume. Presumably, it should affect the motion variations within a gesture lexeme. In our(a) High-level (b) Low-level Fig. 14. t-SNE visualization of motion latent codes. Each color (except gray) stands for a gesture lexeme. (a) latent codes (gray dots) corresponding to cluster high 0 only appear in specific gesture lexemes (orange, purple, and red). (b) latent codes (various color dots) corresponding to cluster low 0 are distributed in most of the gesture lexemes. See Section 7.4.1 for details. (a) w/o low , (b) w/ low + w/o (c) w/ low , Fig. 15. t-SNE visualization of real gestures (GT) versus generated gestures in the motion latent space. We randomly choose two gesture lemexes for visualization. (a) results of our system without low-level audio feature and gesture style code. (b) with low-level audio feature but without gesture style code. (c) with low-level audio feature and gesture style code. See Section 7.4.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>FramesFig. 16 .</head><label>16</label><figDesc>Audio-to-gesture synchronization under different ranges of onset interval. Motion beats are computed based on the local minima of joint deceleration. Audio beats are identified using the audio onsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>. t-SNE visualization of motion latent codes on the TED dataset, computed using the pre-trained encoder E. Colors represent gesture lexemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>7.1.1 Datasets. Three speech-gesture datasets are used in this paper: the Trinity dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>7.1.2 System Settings. All the motion data are downsampled to 20, 20, and 15 frames per second on the Trinity, Chinese, and TED datasets, respectively. The range of onset intervals [ , ] is [0.2 , 0.5 ] for both the Trinity and TED datasets, but [0.3 , 0.6 ] for the Chinese dataset. The length of each normalized block =</figDesc><table><row><cell>?</cell><cell>?</cell><cell cols="2">?. The generator synthesizes 4 seconds of gestures at</cell></row><row><cell cols="3">a time. The dimensions of ,?,?,?, , , and</cell><cell>are 768,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Low right hand, middle right hand, high right hand and time series statistics of right-hand height editing. Low average speed, middle average speed, high average speed and time series statistics of speed editing. Low average radius, middle average radius, high average radius and time series statistics of radius editing.</figDesc><table><row><cell>low right hand</cell><cell>middle right hand</cell><cell>high right hand</cell></row><row><cell>height</cell><cell>height</cell><cell>height</cell></row><row><cell>(a) low hand speed</cell><cell>middle hand speed</cell><cell>high hand speed</cell></row><row><cell>(b) low hand radius</cell><cell>middle hand radius</cell><cell>high hand radius</cell></row><row><cell>(c)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Comparison of our system to SG [Alexanderson et al. 2020], Ges</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Generated motions of SG, Ges, and our system for the same input speech used in the user study. All the models are trained on the Trinity dataset (an English dataset). The red words indicate beats. The red arrows show the movement of corresponding beat gestures. A green check indicates a correct beat match, while a red cross indicates a wrong beat match.</figDesc><table><row><cell>SG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(audio only)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Audio:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Text:</cell><cell>?</cell><cell>?? ?</cell><cell>? ? ??</cell><cell>? ? ? ? ? ???</cell><cell>????? ?? ? ?? ???</cell></row><row><cell></cell><cell></cell><cell cols="2">(It doesn't matter.)</cell><cell>(Please crouch down like me.)</cell><cell>(And then, open your arms.)</cell></row><row><cell></cell><cell></cell><cell cols="4">(b) Gesture results for a Chinese speech clip.</cell></row><row><cell>Fig. 13. accurately and generate dynamic gestures. Notably, we do not com-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pare with Ges in this cross-language test because this model only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>supports English text transcripts.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 .</head><label>2</label><figDesc>Comparison of style interpreters w/ and w/o low-level audio features and gesture style code. See Section 7.4.1 for details.</figDesc><table><row><cell cols="2">Dataset System</cell><cell cols="3">Variance ? FGD ? PMB (%) ?</cell></row><row><cell></cell><cell>Real Gesture</cell><cell>0.41</cell><cell>-</cell><cell>95.74</cell></row><row><cell></cell><cell>w/o low ,</cell><cell>0.09</cell><cell>17.99</cell><cell>89.35</cell></row><row><cell>Trinity</cell><cell>w/ low + w/o</cell><cell>0.21</cell><cell>12.53</cell><cell>91.36</cell></row><row><cell></cell><cell>w/ low , random</cell><cell>0.30</cell><cell>11.96</cell><cell>91.28</cell></row><row><cell></cell><cell>w/ low ,</cell><cell cols="2">0.37 10.78</cell><cell>91.36</cell></row><row><cell></cell><cell>Real Gesture</cell><cell>2.72</cell><cell>-</cell><cell>93.10</cell></row><row><cell></cell><cell>w/o low ,</cell><cell>0.89</cell><cell>3.71</cell><cell>84.57</cell></row><row><cell>TED</cell><cell>w/ low + w/o</cell><cell>1.87</cell><cell>2.47</cell><cell>88.67</cell></row><row><cell></cell><cell>w/ low , random</cell><cell>2.49</cell><cell>2.13</cell><cell>88.89</cell></row><row><cell></cell><cell>w/ low ,</cell><cell>2.45</cell><cell>2.04</cell><cell>89.52</cell></row><row><cell cols="5">Table 3. Comparison of style interpreters w/ and w/o text features.</cell></row><row><cell>Dataset</cell><cell>System</cell><cell></cell><cell>FGD ?</cell><cell>PMB (%) ?</cell></row><row><cell>Trinity</cell><cell cols="2">w/o text feature w/ text feature</cell><cell>10.91 10.78</cell><cell>91.36 91.36</cell></row><row><cell>TED</cell><cell cols="2">w/o text feature w/ text feature</cell><cell>2.09 2.04</cell><cell>89.22 89.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>Effects of the range of onset intervals.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Range of Onset Intervals</cell><cell>FGD ?</cell><cell></cell><cell>PMB (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell></cell><cell>0.5-1.0s</cell><cell></cell><cell></cell><cell>25.45</cell><cell></cell><cell>73.87</cell></row><row><cell>Trinity</cell><cell>0.2-1.0s</cell><cell></cell><cell></cell><cell>19.16</cell><cell></cell><cell>90.75</cell></row><row><cell></cell><cell>0.2-0.5s</cell><cell></cell><cell></cell><cell>10.78</cell><cell></cell><cell>91.36</cell></row><row><cell></cell><cell>0.5-1.0s</cell><cell></cell><cell></cell><cell>3.61</cell><cell></cell><cell>65.34</cell></row><row><cell>TED</cell><cell>0.2-1.0s</cell><cell></cell><cell></cell><cell>2.55</cell><cell></cell><cell>89.10</cell></row><row><cell></cell><cell>0.2-0.5s</cell><cell></cell><cell></cell><cell>2.04</cell><cell></cell><cell>89.52</cell></row><row><cell>200</cell><cell>250</cell><cell>300</cell><cell>350</cell><cell>400</cell><cell>450</cell><cell>500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 .</head><label>5</label><figDesc>Diversity of motions generated w/ and w/o gesture lexemes.</figDesc><table><row><cell cols="2">Dataset System</cell><cell>Diversity ?</cell></row><row><cell></cell><cell>Real Gesture</cell><cell>3.79</cell></row><row><cell>Trinity</cell><cell>w/o gesture lexeme</cell><cell>1.91</cell></row><row><cell></cell><cell>w/ gesture lexeme</cell><cell>3.40</cell></row><row><cell></cell><cell>Real Gesture</cell><cell>4.32</cell></row><row><cell>TED</cell><cell>w/o gesture lexeme</cell><cell>2.99</cell></row><row><cell></cell><cell>w/ gesture lexeme</cell><cell>4.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 .</head><label>7</label><figDesc>Effects of the positional encoding block.</figDesc><table><row><cell>Dataset</cell><cell>System</cell><cell>FGD ?</cell><cell>PMB (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell>Trinity</cell><cell>w/o positional encoding w/ positional encoding</cell><cell>11.15 10.78</cell><cell>89.98 91.36</cell></row><row><cell>TED</cell><cell>w/o positional encoding w/ positional encoding</cell><cell>2.19 2.04</cell><cell>88.13 89.52</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Graph., Vol. 41, No. 6, Article 209. Publication date: December 2022.Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings ? 209:5</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Graph., Vol. 41, No. 6, Article 209. Publication date: December 2022.Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings ? 209:9</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans.Graph., Vol. 41, No. 6, Article 209. Publication date: December 2022.Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings ? 209:11</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans. Graph., Vol. 41, No. 6, Article 209. Publication date: December 2022.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Trans.Graph., Vol. 41, No. 6, Article 209. Publication date: December 2022.Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings ? 209:15</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their constructive comments. This work was supported in part by NSFC Projects of International Cooperation and Exchanges (62161146002).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Style-Controllable Speech-Driven Gesture Synthesis Using Normalising Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Simon Alexanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beskow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="487" to="496" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep motifs and motion signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Aristidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiorgos</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rhythm is a Dancer: Music-Driven Motion Synthesis with Global Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Aristidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Yiannakidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiorgos</forename><surname>Chrysanthou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2020. vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal Machine Learning: A Survey and Taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Tutorial on Onset Detection in Music Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daudet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duxbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1035" to="1047" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Childs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rewkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia (Virtual Event, China) (MM &apos;21)</title>
		<meeting>the 29th ACM International Conference on Multimedia (Virtual Event, China) (MM &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2027" to="2036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Aniket Bera, and Dinesh Manocha. 2021b. Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rewkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooja</forename><surname>Guhan</surname></persName>
		</author>
		<idno>abs/2101.11101</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the association for computational linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonverbal behaviors, persuasion, and credibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Judee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Birk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human communication research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="140" to="169" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beat: the behavior expression animation toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justine</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>H?gni Vilhj?lmsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bickmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Life-Like Characters</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="163" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Choreomaster: choreography-oriented music-driven dance synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Hai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Chen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting coverbal gestures: A deep and temporal modeling approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacy</forename><surname>Marsella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Virtual Agents</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="152" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual rhythm and beat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2532" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>north american chapter of the association for computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Jukebox: A Generative Model for Music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/2005.00341</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The repertoire of nonverbal behavior: Categories, origins, usage, and coding. semiotica</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace V Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beat tracking by dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of New Music Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Investigating the use of recurrent motion modelling for speech gesture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ylva</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Mcdonnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Intelligent Virtual Agents</title>
		<meeting>the 18th International Conference on Intelligent Virtual Agents</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning individual styles of conversational gesture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiry</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gefen</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3497" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">When Speech Stops, Gesture Stops: Evidence From Developmental and Crosslinguistic Comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Graziano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianne</forename><surname>Gullberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="page">0</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Predicting head pose from speech with a conditional variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Laycock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning speechdriven 3d conversational gestures from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents</title>
		<editor>Hans-Peter Seidel, Gerard Pons-Moll, Mohamed Elgharib, and Christian Theobalt. 2021</editor>
		<meeting>the 21st ACM International Conference on Intelligent Virtual Agents</meeting>
		<imprint>
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust Motion In-Betweening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>F?lix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Yurick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. 39, 4, Article</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluation of speech-to-gesture generation using bi-directional LSTM network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoshi</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinichi</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Sakuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhiko</forename><surname>Sumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Intelligent Virtual Agents</title>
		<meeting>the 18th International Conference on Intelligent Virtual Agents</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Moglow: Probabilistic and controllable motion synthesis using normalising flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Alexanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Beskow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extraction and alignment evaluation of motion beats for street dance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Tze</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keng-Sheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Homer H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2429" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Phase-functioned neural networks for character control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robot behavior toolkit: generating effective social behaviors for robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Categorical Reparameterization with Gumbel-Softmax. ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards social artificial intelligence: Nonverbal social signal prediction in a triadic interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mina</forename><surname>Cikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10873" to="10883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gesture: Visible Action as Utterance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kendon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Making Them Dance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesham</forename><surname>Fouad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James K</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Fall Symposium: Aurally Informed Performance</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gesture Generation by Imitation: From Human Behavior to Computer Character Animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kipp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dissertation.com</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards a common framework for multimodal generation: The behavior markup language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brigitte</forename><surname>Krenn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacy</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pelachaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on intelligent virtual agents</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="205" to="217" />
		</imprint>
	</monogr>
	<note>Hannes Pirker, Kristinn R Th?risson, and Hannes Vilhj?lmsson</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Motion Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="482" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Analyzing input and output representations for speech-driven gesture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoshi</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kjellstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents</title>
		<meeting>the 19th ACM International Conference on Intelligent Virtual Agents</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gesticulator: A framework for semantically-aware speech-driven gesture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Jonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Sanne Van Waveren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iolanda</forename><surname>Alexandersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kjellstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimodal Interaction</title>
		<meeting>the 2020 International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="242" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Large, Crowdsourced Evaluation of Gesture Generation Systems on Common Data: The GENEA Challenge 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Jonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwoo</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Wolfert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Henter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Intelligent User Interfaces</title>
		<meeting><address><addrLine>College Station, TX, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
	<note>IUI &apos;21)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Speech2Properties2Gestures: Gesture-Property Prediction as a Tool for Generating Representational Gestures from Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajmund</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Jonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kjellstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Henter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents (Virtual Event, Japan) (IVA &apos;21)</title>
		<meeting>the 21st ACM International Conference on Intelligent Virtual Agents (Virtual Event, Japan) (IVA &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="145" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gesture Controllers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2010-07" />
		</imprint>
	</monogr>
	<note>Article</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Audio2Gestures: Generating Diverse Gestures from Speech Audio with Conditional Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefei</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11293" to="11302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">AI Choreographer: Music Conditioned 3D Dance Generation With AIST++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13401" to="13412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Character Controllers Using Motion VAEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><forename type="middle">Yu</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Zinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Van De Panne</surname></persName>
		</author>
		<idno>40:40:1-40:40:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal, Structural, and Pragmatic Synchrony between Intonation and Gesture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Laboratory Phonology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="89" />
			<date type="published" when="2012-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Socolof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Mihuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Sonderegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="498" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gestural beats: The rhythm hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evelyn</forename><surname>Mcclave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of psycholinguistic research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Librosa: Audio and Music Signal Analysis in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Python in Science Conference</title>
		<meeting>the 14th Python in Science Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcneill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Semiotics</title>
		<editor>351. Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko, Helge Rhodin, Mohammad Shafiei, Hans-Peter Seidel, Weipeng Xu, Dan Casas, and Christian Theobalt</editor>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>Hand and Mind</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gesture Modeling and Animation Based on a Probabilistic Re-Creation of Speaker Style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">:24. van den Aaron Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural Discrete Representation Learning</title>
		<imprint>
			<date type="published" when="2008-03" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>ACM Transactions on Graphics</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Quantifying gesture-speech synchrony. In the 6th gesture and speech in interaction conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><surname>Pouw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James A Dixon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="75" to="80" />
		</imprint>
		<respStmt>
			<orgName>Universitaetsbibliothek Paderborn</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Shenhan Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11077" to="11086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Zero-Shot Text-to-Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning. PMLR</title>
		<meeting>the 38th International Conference on Machine Learning. PMLR</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Novel realizations of speech-driven head movements with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najmeh</forename><surname>Sadoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<biblScope unit="page" from="6169" to="6173" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Bailando: 3D Dance Generation by Actor-Critic GPT With Choreographic Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianpei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunze</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="11050" to="11059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Naoqi api documentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robotics</forename><surname>Softbank</surname></persName>
		</author>
		<ptr target="http://doc.aldebaran.com/2-5/homepepper.html" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">DeepPhase: Periodic Autoencoders for Learning Motion Phase Manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="136" />
			<date type="published" when="2022-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Local motion phases for learning multi-contact character movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><surname>Zaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="54" to="55" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Transflower: probabilistic autoregressive dance generation with multimodal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Valle-P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Beskow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Holzapfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Yves</forename><surname>Oudeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Alexanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Gesture and speech in interaction: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zofia</forename><surname>Malisz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kopp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="209" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Linguistic Features of Metaphoric Gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">A</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. Dissertation. University of Rochester</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with</title>
	</analytic>
	<monogr>
		<title level="j">Hierarchical Neural Embeddings ?</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2022-12" />
		</imprint>
	</monogr>
	<note>ACM Trans. Graph.</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Convolutional Pose Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A Review of Evaluation Practices of Gesture Generation in Embodied Conversational Agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Wolfert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Belpaeme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="379" to="389" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02291</idno>
		<title level="m">Freeform Body Motion Generation from Speech</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10157</idno>
		<title level="m">Pieter Abbeel, and Aravind Srinivas. 2021. Videogpt: Video generation using vq-vae and transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Speech gesture generation from the trimodal context of text, audio, and speaker identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwoo</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bok</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Haeng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geehyuk</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwoo</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo-Ri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geehyuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4303" to="4309" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
