<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Refinement Network for Oriented and Densely Packed Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjia</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<address>
									<country>CAS</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">UCAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Ren</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Youtu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kekai</forename><surname>Sheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Youtu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<address>
									<country>CAS</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">UCAS</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">CASIA-LLVision Joint Lab 5 Y-Tech</orgName>
								<address>
									<settlement>Kuaishou Technology</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haolei</forename><surname>Yuan</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Youtu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Youtu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
							<email>chongyangma@kuaishou.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
							<email>changsheng.xu@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<address>
									<country>CAS</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">UCAS</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">CASIA-LLVision Joint Lab 5 Y-Tech</orgName>
								<address>
									<settlement>Kuaishou Technology</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Refinement Network for Oriented and Densely Packed Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection has achieved remarkable progress in the past decade. However, the detection of oriented and densely packed objects remains challenging because of following inherent reasons: (1) receptive fields of neurons are all axis-aligned and of the same shape, whereas objects are usually of diverse shapes and align along various directions;</p><p>(2) detection models are typically trained with generic knowledge and may not generalize well to handle specific objects at test time; (3) the limited dataset hinders the development on this task. To resolve the first two issues, we present a dynamic refinement network which consists of two novel components, i.e., a feature selection module (FSM) and a dynamic refinement head (DRH). Our FSM enables neurons to adjust receptive fields in accordance with the shapes and orientations of target objects, whereas the DRH empowers our model to refine the prediction dynamically in an object-aware manner. To address the limited availability of related benchmarks, we collect an extensive and fully annotated dataset, namely, SKU110K-R, which is relabeled with oriented bounding boxes based on SKU110K. We perform quantitative evaluations on several publicly available benchmarks including DOTA, HRSC2016, SKU110K, and our own SKU110K-R dataset. Experimental results show that our method achieves consistent and substantial gains compared with baseline approaches. The code and dataset are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection has achieved remarkable progress on a few benchmarks (e.g., VOC <ref type="bibr" target="#b5">[6]</ref> and COCO <ref type="bibr" target="#b23">[24]</ref>) with * Corresponding author Dynamic Refinement Module the help of deep learning. Numerous well-designed methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b2">3]</ref> have demonstrated promising results. However, majority of these detectors encounter problems when objects, such as those in aerial images, are in arbitrary orientations and present dense distribution. Moreover, almost all detectors optimize model parameters on the training set and keep them fixed afterward. This static paradigm, which uses general knowledge, may not be flexible enough to detect specific samples during test time.</p><p>Most of the recent progress on oriented object detection is based on R-CNN series frameworks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35]</ref>. These methods first generate numerous horizontal bounding boxes as region of interests (RoIs) and then predict classification and location on the basis of regional features. Unfortunately, horizontal RoIs typically suffer from severe misalignment between the bounding boxes and oriented objects <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b28">29]</ref>. For example, objects in aerial images are usually with arbitrary orientations and densely packed, leading to artifacts wherein several instances are often crowded and contained by a single horizontal RoI <ref type="bibr" target="#b4">[5]</ref>. Consequently, extracting accurate visual features becomes difficult. Other methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28]</ref> leverage oriented bounding boxes as anchors to handle rotated objects. However, these methods suffer from high computational complexity because they acquire numerous well-designed anchors with different angles, scales, and aspect ratios. Recently, RoI Trans <ref type="bibr" target="#b4">[5]</ref> has transformed horizontal RoIs into oriented ones by rotating RoI learners and extracting rotation-invariant region features using a rotated position-sensitive RoI alignment module. However, such approach still needs well-designed anchors and is not flexible enough.</p><p>Model training is a procedure from special to general, whereas inference is from general to special. However, almost all methods follow the stationary paradigm and cannot make flexible inference based on samples. Dynamic filters are a simple yet effective approach to enable the model to change over different samples. Existing methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">38]</ref> resort to feature reassembly via dynamic filters and achieve promising results. However, detectors have two different tasks, namely, classification and regression. <ref type="figure" target="#fig_0">Fig. 1</ref> shows some illustrative examples. For a classification task, the key is to refine the feature embedding for improved discriminability. However, for a regression problem, refining the predicted value directly is desirable. We propose two versions of dynamic refinement heads (DRHs) tailored for the above two aspects.</p><p>In this work, we adopt CenterNet <ref type="bibr" target="#b43">[44]</ref>, with an additional angle prediction head as our baseline and present dynamic refinement network (DRN). Our DRN consists of two novel parts: feature selection module (FSM) and dynamic refinement head (DRH). FSM empowers neurons with the ability to adjust receptive fields in accordance with the object shapes and orientations, thus passing accurate and denoised features to detectors. DRH enables our model to make flexible inferences in an object-aware manner. Specifically, we propose two DRHs for classification (DRH-C) and regression (DRH-R) tasks. In addition, we carefully relabel oriented bounding boxes for SKU110K <ref type="bibr" target="#b8">[9]</ref> and called them SKU110K-R; in this manner, oriented object detection is facilitated. To evaluate the proposed method, we conduct extensive experiments on the DOTA, HRSC2016, and SKU110K datasets.</p><p>In summary, our contributions include:</p><p>? We propose a novel FSM to adaptively adjust the receptive fields of neurons based on object shapes and orientations. The proposed FSM effectively alleviates the misalignment between receptive fields and objects. ? We present two DRHs, namely, DRH-C and DRH-R, for classification and regression tasks, respectively. These DRHs can model the uniqueness and particularity of each sample and refine the prediction in an objectwise manner.</p><p>? We collect a carefully relabeled dataset, namely, SKU110K-R, which contains accurate annotations of oriented bounding boxes, to facilitate the research on oriented and densely packed object detection. ? Our method shows consistent and substantial gains across DOTA, HRSC2016, SKU110K, and SKU110K-R on oriented and densely packed object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most object detection methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b36">37]</ref> focus on axis-aligned or upright objects and may encounter problems when the targets are of arbitrary orientations or present dense distribution <ref type="bibr" target="#b8">[9]</ref>. For oriented object detection, some methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28]</ref> adopt the R-CNN <ref type="bibr" target="#b34">[35]</ref> framework and use numerous anchors with different angles, scales, and aspect ratios, at the expense of considerably increasing computation complexity. The SRBBS <ref type="bibr" target="#b28">[29]</ref> uses rotated region of interest (RoI) warping to extract features of rotated RoIs; however, it is difficult to embed in a neural network because rotated proposal generation consumes additional time. Ding et al. <ref type="bibr" target="#b4">[5]</ref> proposed an RoI transformer to transform axis-aligned RoIs into rotated ones to address the misalignment between RoIs and oriented objects. SCRDet <ref type="bibr" target="#b41">[42]</ref> added an IOU constant factor to the L1 loss term to address the boundary issue for oriented bounding boxes. In contrast to the aforementioned methods, we propose FSM to adjust receptive fields of neurons adaptively and reassemble appropriate features for various objects with different angles, shapes, and scales.</p><p>FPN <ref type="bibr" target="#b21">[22]</ref> proposes a feature pyramid network to perform object detection at multiple scales. They select features of the proposals in accordance with area sizes. FSAF <ref type="bibr" target="#b45">[46]</ref> learns an anchor-free module to select the most suitable feature level dynamically. Li et al. <ref type="bibr" target="#b18">[19]</ref> presented a dynamic feature selection module to select pixels on basis of the position and size of new anchors. These methods aim to select additional suitable features at the object level. To become more fine-grained, SKN <ref type="bibr" target="#b19">[20]</ref> learned to select features with different receptive fields at each position using different kernels. SENet <ref type="bibr" target="#b10">[11]</ref> explicitly recalibrates channel-wise feature responses adaptively, whereas CBAM <ref type="bibr" target="#b38">[39]</ref> adopts one more spatial attention module to model inter spatial relationships. Our FSM learns to extract shape-and rotationinvariant features in a pixel-wise manner.</p><p>Spatial transformer network <ref type="bibr" target="#b12">[13]</ref> are the first to learn spatial transformation and affine transformation in deep learning frameworks to warp feature maps. Active convolution <ref type="bibr" target="#b13">[14]</ref> augments the sampling locations in the convolutional layers with offsets. It shares the offsets all over the different spatial locations and the model parameters are static after training. Deformable convolutional network (DCN) <ref type="bibr" target="#b3">[4]</ref> models the dense spatial transformation in the images and the offsets are dynamic model outputs. Our rotated convolution layer </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hourglass Network</head><p>Dynamic refinement for regression <ref type="figure">Figure 2</ref>. Overall framework of our Dynamic Refinement Network. The backbone network is followed by two modules, i.e., feature selection module (FSM) and dynamic refinement heads (DRHs). FSM selects the most suitable features by adaptively adjusting receptive fields. The DRHs dynamically refine the predictions in an object-aware manner.</p><p>in FSM learns the rotation transformation in a dense fashion. RoI Trans <ref type="bibr" target="#b4">[5]</ref> learns five offsets to transform the axis-aligned ROIs into rotated ones in a manner similar to that of positionsensitive ROI Align <ref type="bibr" target="#b34">[35]</ref>. ORN <ref type="bibr" target="#b44">[45]</ref> proposes active rotating filters which actively rotate during convolution. The rotation angle is a hyper-parameter which is rigid and all the locations share the same rotation angle. On the contrary, our rotation transformation is learnable and can predict angles at each position.</p><p>Neural networks are conditioned on the input features and change over samples by introducing dynamic filters. Dynamic filters <ref type="bibr" target="#b14">[15]</ref> learns filter weights in the training phase and thus can extract example-wise features at the inference time. Similarly, CARAFE <ref type="bibr" target="#b37">[38]</ref> proposes a kernel prediction module which is responsible for generating the reassembly kernels in a content-aware manner. Although DCN <ref type="bibr" target="#b3">[4]</ref> and RoI Trans <ref type="bibr" target="#b4">[5]</ref> model the offset prediction in a dynamic manner, they do not change the kernel weight. In contrast to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">38]</ref>, our DRHs aim to refine the detection results in a content-aware manner by introducing dynamic filters instead of feature reassembly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method and Dataset</head><p>The overall framework of our approach is shown in <ref type="figure">Fig. 2</ref>. We first introduce our network architecture in Sec. 3.1. The misalignment between various objects and simplex receptive fields in each network layer is ubiquitous; hence, we propose an FSM to reassemble the most suitable feature automatically, as described in Sec. 3.2. To empower a model with the ability to refine predictions dynamically in accordance with different examples, we propose the use of DRHs to achieve object-aware predictions in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>We use CenterNet <ref type="bibr" target="#b43">[44]</ref> as our baseline, which models an object as a single point (i.e., the center point of the bounding box) and regresses the object size and offset. To predict oriented bounding boxes, we add a branch to regress the orientations of the bounding boxes, as illustrated in <ref type="figure">Fig. 2</ref>. Let (c x , c y , h, w, ?, ? x , ? y ) be one output septet from the model. Then, we construct the oriented bounding box by:</p><formula xml:id="formula_0">P lt = M r [?w/2, ?h/2] T + [c x + ? x , c y + ? y ] T , P rt = M r [+w/2, ?h/2] T + [c x + ? x , c y + ? y ] T , P lb = M r [?w/2, +h/2] T + [c x + ? x , c y + ? y ] T , P rb = M r [+w/2, +h/2] T + [c x + ? x , c y + ? y ] T ,<label>(1)</label></formula><p>where (c x , c y ) and (? x , ? y ) are the center point and the offset prediction; (w, h) is the size prediction; M r is the rotation matrix; and P lt , P rt , P lb and P rb are the four corner points of the oriented bounding box. Following CenterNet for regression tasks, we use L1 loss for the regression of rotation angles:</p><formula xml:id="formula_1">L ang = 1 N N k=1 |? ??|,<label>(2)</label></formula><p>where ? and? are the target and predicted rotation angles, respectively; and N is the number of positive samples. Thus, the overall training objective of our model is</p><formula xml:id="formula_2">L det = L k + ? size L size + ? of f L of f + ? ang L ang ,<label>(3)</label></formula><p>where L k , L size and L of f are the losses of center point recognition, scale regression, and offset regression, which are the same as CenterNet; and ? size , ? of f and ? ang are constant factors, which are all set to 0.1 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Selection Module</head><p>To alleviate the mismatches between various objects and axis-aligned receptive fields of neurons, we propose an Feature Selection Module (FSM) to aggregate the information extracted using different kernel sizes, shapes (aspect ratios), and orientations adaptively (see <ref type="figure" target="#fig_2">Fig. 3</ref>). Each split extracts different information by using Rotation Convolution Layer with 3 ? 3, 1 ? 3, and 3 ? 1 kernels. We adopt the attention mechanism to aggregate the information.</p><p>Multiple features. Given a feature map X ? R H?W ?C , we first compress the feature with a 1 ? 1 convolution layer, followed by Batch Normalization <ref type="bibr" target="#b11">[12]</ref> and ReLU <ref type="bibr" target="#b30">[31]</ref> function in sequence for improved information aggregation. Next, we extract multiple features by using Rotation Convolution Layers (RCLs) with different kernels from X c ? R H?W ?C . <ref type="figure" target="#fig_2">Fig. 3</ref> shows a three-split example with 3 ? 3, 1?3, and 3?1 kernels. Each split is responsible for different receptive fields, and we call it X i ? R H?W ?C , where i ? {1, 2, 3}. The RCL draws inspiration from DCN <ref type="bibr" target="#b3">[4]</ref>, and the implementation details are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Akin to DCN, we use R to represent the regular grid receptive field and dilation. For a kernel of size 3 ? 3, we have</p><formula xml:id="formula_3">R = {(?1, ?1), (?1, 0), ..., (0, 1), (1, 1)}.<label>(4)</label></formula><p>Given the pre-defined offset p i ? R for the i-th location and learned angle ?, the learned offset is</p><formula xml:id="formula_4">?p i = M r (?) ? p i ? p i ,<label>(5)</label></formula><p>where M r (?) is the rotation matrix defined in Eqn. <ref type="bibr" target="#b0">(1)</ref>. For each location p 0 in the output feature map X i , we have</p><formula xml:id="formula_5">X i (p 0 ) = pn?R w(p n ) ? X c (p 0 + p n + ?p n ),<label>(6)</label></formula><p>where p n denotes the locations in R, and w is the kernel weight.  Feature selection. To enforce neurons with adaptive receptive fields, we adopt an attention mechanism to fuse the feature in a position-wise manner. X i is first to feed into an attention block (composed of a convolution with kernel 1 ? 1, Batch Normalization and ReLU in sequence) to obtain the attention map A i ? R H?W ?1 (i ? 1, 2, 3). Then, we concatenate A i in the channel direction, followed with a SoftMax operation to obtain the normalized selection weight A i as:</p><formula xml:id="formula_6">A i = Sof tM ax([A 1 , A 2 , A 3 ]).<label>(7)</label></formula><p>A soft attention fuses features from multiple branches:</p><formula xml:id="formula_7">Y = i A i ? X i ,<label>(8)</label></formula><p>where Y ? R H?W ?C is the output feature. We omit the channel expansion layer before Y for similarity. Here, we show a three-branch case, and one can easily extend to more branches with different kernel sizes and shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic Refinement Head</head><p>In standard machine learning frameworks, people usually learn a model through a large annotated training set. At the inference time, the test example is fed to the model with parameters fixed to obtain the prediction results. A problem occurs when the well-trained model can only respond on the basis of the general knowledge learned from the training set while ignoring the uniqueness of each example.</p><p>To enable the model to respond on the basis of each sample, we propose the use of DRHs to model the particularity of each input object. Specifically, two different modules, i.e., DRH-C and DRH-R, can be used for classification and regression, respectively.</p><p>We illustrate our motivation with an example for a threeclass classification problem, as shown by the left image in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. The gray circular area represents the feature space and solid dots are examples that belong to three classes. Some samples are located far from the discrimination boundary, indicating that these samples possess good semantic discriminability. By contrast, the samples with a small margin to the boundary are unfortunately not much compatible with the model. To enhance the flexibility of the model, we resort to an object-aware classification/regression module.</p><p>Dynamic refinement for classification. The architecture of DRH-C is shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. Given an input feature map F in ? R H?W ?C , we first obtain an object-aware filter:</p><formula xml:id="formula_8">K c = G c (F in ; ?),<label>(9)</label></formula><p>where G c represents the dynamic filter generator, and ? is the parameter set of G c . K c are the learned example-wise kernel weights. Then, we obtain the feature refinement F via a convolution operation:</p><formula xml:id="formula_9">F = F mid * K c ,<label>(10)</label></formula><p>where F mid is the base feature by processing F in through a Conv-BN-ReLU block with kernel 3 ? 3, and * represents the convolution operator. Finally, we obtain the classification prediction H c :</p><formula xml:id="formula_10">H c = C (1 + ? ? F / F ) ? F mid ; ? ,<label>(11)</label></formula><p>where C(?, ?) represents the classifier with parameter ?, and ? is a modulus operation. We normalize F in the channel direction for each location. The normalized F indicates the modification direction for base feature F mid . We adaptively refine the basic feature according to its length. ? is a constant factor to control the scope of refinement.</p><p>Dynamic refinement for regression. We also show a simple example for regression tasks in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. The orange solid dots represent the target values of examples, and the orange curve represents the learned regression model. For regression tasks, researchers usually minimize the average L1 or L2 distances; thus, the learned model cannot fit the target value accurately. To predict exact values without increasing the risk of overfitting, we design an object-aware regression head similar to the classifier shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. Given the feature map F in ? R H?W ?C , we first calculate the dynamic filter weight K r via G r (?; ?) and then predict the refinement factor H similar to Eqn. (10) to obtain the final object-aware regression result H r :</p><formula xml:id="formula_11">H b = R(F mid ; ?), H r = 1 + ? tanh(H ) ? H b ,<label>(12)</label></formula><p>where R(?; ?) is the regressor with parameters ?. H b is the base prediction value according to the general knowledge. The refinement factor ranges in [?1, 1] via a tanh activation function. is the control factor which prevents the model from being confused by big refinement. This factor is set to 0.1 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">SKU110K-R Dataset</head><p>Our SKU110K-R dataset is an extended version of SKU110K <ref type="bibr" target="#b8">[9]</ref>. The original SKU110K dataset contains 11, 762 images in total (8, 233 for training, 588 for validation, and 2, 941 for testing) and 1, 733, 678 instances. The images are collected from thousands of supermarket stores and are of various scales, viewing angles, lighting conditions, and noise levels. All the images are resized into a resolution of one megapixel. Most of the instances in the dataset are tightly packed and typically of a certain orientation in the rage of [?15 ? , 15 ? ]. To enrich the dataset, we perform data augmentation by rotating the images by six different angles, i.e., -45 ? , -30 ? , -15 ? , 15 ? , 30 ? , and 45 ? . Then, we annotate the oriented bounding box for each instance via crowdsourcing to obtain our SKU110K-R dataset. Please refer to our supplementary materials for more details about SKU110-R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Dataset. We conduct experiments on three datasets, i.e., DOTA <ref type="bibr" target="#b39">[40]</ref>, HRSC2016 <ref type="bibr" target="#b28">[29]</ref>, and our own SKU110K-R (Sec. <ref type="bibr">3.4</ref> boxes. The objects are of various scales, orientations, and shapes. Before training, we crop a series of patches of the same resolution 1024 ? 1024 from the original images with a stride of 924 and get about 25000 patches. To alleviate the class imbalance, we perform data augmentation by random rotation for those categories with very few samples, and finally obtain approximately 40000 patches in total. The HRSC2016 dataset contains 1061 aerial images and more than 20 categories of ships in various appearance, scales, and orientations. The training, validation, and test sets include 436, 181, and 444 images, respectively. We did not conduct any data augmentation on this dataset. For the DOTA and HRSC2016 datasets, we use the same mAP calculation as PASCAL VOC <ref type="bibr" target="#b5">[6]</ref>. For SKU110K and SKU110K-R, we use the same evaluation method as COCO <ref type="bibr" target="#b24">[25]</ref>, which reported an mean average precision (mAP) at IoU = 0.5 : 0.05 : 0.95. Moreover, we report AP at IoU = 0.75 (AP 75 ) and average recall 300 (AR 300 ) at IoU = 0.5 : 0.05 : 0.95 (300 is the maximal number of objects) following Goldman et al. <ref type="bibr" target="#b8">[9]</ref>. Implementation details. We use an hourglass-104 network as the backbone. To implement RCL, we use the released code of DCNV2 <ref type="bibr" target="#b46">[47]</ref> and replace the original predicted offset with the offset deduced from the predicted angle in Eqn. 5.</p><p>The input resolutions of DOTA, HRSC2016, and SKU110K-R are 1024 ? 1024, 768 ? 768, and 768 ? 768, respectively. We used random scaling (in the range of [0.7, 1.3]), random flipping, and color jittering for data augmentation. For DOTA and HRSC, the models are trained with 140 epochs in total. The learning rate is reduced by a factor of 10 after the 90th and the 120th epochs from an initial value of 4e ? 4 to 4e ? 6 finally. For SKU110K-R, we train for 25 epochs, with a learning rate of 2e ? 4 which is decreased by 10 at the 20 epoch. We use Adam <ref type="bibr" target="#b16">[17]</ref> as the optimizer and set the batch size to 8. For improved convergence, we calculate the offsets from target angles instead of predicted ones during the training phase. We deduce the offset in RCL using predicted angles at the test time. As set in CenterNet, we adopt three levels of test augmentation. First, we evaluate our method without any augmentation. Then, we add multi-scale testing with (0.5, 1.0, 1.5). To merge the detection, we adopt a variant of Soft-NMS [2] that faces oriented bounding boxes (anglesoftnms). Specifically, we use the linear method to adjust the score value, set the IoU threshold, and suppress the threshold to 0.5 and 0.03, respectively. Lastly, we add horizontal flipping and average the network predictions before decoding oriented bounding boxes. <ref type="table" target="#tab_0">Table 1</ref> shows quantitative results comparing our approach with state-of-the-art methods on the DOTA test set for the oriented bounding box (OBB) task. Other methods are all anchor-based and most of them are based on the framework of Faster R-CNN <ref type="bibr" target="#b34">[35]</ref>. By contrast, we follow an anchor-free paradigm and demonstrate comparable results with SCRDet <ref type="bibr" target="#b41">[42]</ref>. Compared to the baseline, our method ahchieves a remarkable gain of 3.3% in terms of mAP. <ref type="table" target="#tab_1">Table 2</ref> shows the results on HRSC2016 in Pascal VOC fashion. Our method achieves a significant gain of 6.4% in terms of mAP. Such improvement indicates that the proposed FSM effectively addresses the misalignment issue Method CP <ref type="bibr" target="#b27">[28]</ref>   by adjusting the receptive fields adaptively. We further show evaluation results on COCO fashion in <ref type="table">Table 3</ref>. Our method provides 1.9% mAP gain. Moreover, as the IoU increases, our method improves. <ref type="figure">Fig. 7</ref> shows some qualitative results on DOTA and HRSC2016 datasets using our method. <ref type="table" target="#tab_2">Table 4</ref> shows the results on SKU110K-R and SKU110K. For oriented object detection, we reimplement YoloV3 <ref type="bibr" target="#b33">[34]</ref> by introducing angle prediction. CenterNet-4point ? represents regressing the four corners of each bounding box, and CenterNet ? indicates that we add center pooling and DCN <ref type="bibr" target="#b3">[4]</ref> to our baseline. We improve the mAP by 1.5% and also report superior results on the original SKU110K dataset. These numbers further demonstrate the effectiveness of our proposed DRN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We conduct a series of ablation studies on the DOTA validation set and report quantitative results in COCO fashion to verify the effectiveness of our method. We use the hourglass-52 as our backbone in this section.</p><p>Our FSM aims to select compact receptive fields for each object adaptively. To match the objects as much as possible, we set up three shapes of kernels, i.e., square, flat, and slender rectangles. <ref type="table">Table 5</ref> shows the results when we use different settings. The first row is the baseline. We first construct the FSM with only one branch by using a 3 ? 3 kernel and shield the RCL. This setting achieves almost the same results as the baseline since our network is the same as the baseline, except for the addition of one convolution layer To model the uniqueness and particularity of each object and empower the network to handle flexible samples, we design two DRHs for classification and regression tasks. For the classifier, we report the accuracy (Acc), recall (Rec), and AP to reveal the quality of center point prediction. Specifically, we select the top 300 points as the predicted object centers for each image in our experiments. <ref type="table">Table 6</ref> shows the results of the ablation study on DRH-C. The performance of the classifier is considerably improved when DRH-C is introduced. Specifically, the Acc and Rec are increased from 0.21 to 0.32 and from 0.81 to 0.89, respectively. For the detection, the DHR-C provides 0.7% AP 50 and 0.6% AP 75 gains. In <ref type="table">Table 7</ref>, to evaluate the impact of DRH-R, we report the prediction errors, AP 50 , and AP 75 when we replace the original heads with our DRH-R for scale, angle, and offset regression. We use the standard L 1 distance between the predicted and ground-truth values to measure the errors. The first three rows in DRH-R show results when we replace the corresponding single head with DRH-R. Our DHR-R provides consistent improvement albeit slight on angle and offset regression tasks. The reason is that these two tasks are relatively easy and have almost achieved the optimal point. On scale regression tasks, DRH-R reduces L 1 error by 1.24 and improves AP 50 and AP 75 by 0.7% and 0.6%, respectively. <ref type="table">Table 8</ref> compares our method with the baseline in terms of the average time to process an image, numbers of model parameters, as well as model performance (AP 50 and AP 75 ). Our method has achieved remarkable improvement over the baseline with very limited increased number of parameters. Here, we only apply DRH-R on the scale head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present a unified framework for oriented and densely packed object detection. To adjust receptive fields of neurons in accordance with object shapes and orientations, we propose an FSM to aggregate information and thus address the misalignment issue between receptive fields and various objects. We further present DRH-C and DRH-R to refine the prediction dynamically, thereby alleviating the contradiction between the model equipped by generic knowledge and specific objects. In addition, we relabel SKU110K with oriented bounding boxes and obtain a new dataset, called SKU110K-R to facilitate the development of detection models on oriented and densely packed objects. We conduct extensive experiments to show that our method achieves consistent gains across multiple datasets in comparison with baseline approaches. In the future, we plan to explore a more effective mechanism of dynamic models and investigate oriented object detection in few-shot settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustrations of dynamic refinement on classification (a) and regression (b). Each solid dot represents a sample. With the general knowledge learned in training procedure, classifiers and regressors make predictions while suffering from lack of flexibility. Model should changes over samples. The arrows show the promising refinements for improved performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Top: Feature Selection Module. Bottom: Rotation Convolution Layer. The illustration shows a three-split example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Dynamic Refinement Head for classification (DRH-C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Dynamic Refinement Head for regression (DRH-R).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Example images with annotated oriented bounding boxes in our SKU110K-R dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>). The DOTA dataset contains 2, 806 images and covers 15 object categories. It is mainly used for object detection in aerial images with annotations of oriented bounding 20.<ref type="bibr" target="#b28">29</ref> 36.58 23.42 8.85 2.09 4.82 44.34 38.35 34.65 16.02 37.62 47.23 25.5 7.45 21.39 FR-O [40] 79.42 44.13 17.7 64.05 35.3 38.02 37.16 89.41 69.64 59.28 50.3 52.91 47.89 47.4 46.3 54.13 two-stage method ICN [1] 81.40 74.30 47.70 70.30 64.90 67.80 70.00 90.80 79.10 78.20 53.60 62.90 67.00 64.20 50.20 68.20 R-DFPN [41] 80.92 65.82 33.77 58.94 55.77 50.94 54.78 90.33 66.34 68.66 48.73 51.76 55.10 51.32 35.88 57.94 R 2 CNN [16] 80.94 65.67 35.34 67.44 59.92 50.91 55.81 90.67 66.92 72.39 55.06 52.23 55.14 53.35 48.22 60.67 RRPN [30] 88.52 71.20 31.66 59.30 51.85 56.19 57.25 90.81 72.84 67.38 56.69 52.84 53.08 51.94 53.58 61.01 RoI-Transformer * [5] 88.64 78.52 43.44 75.92 68.81 73.6 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 69.56 SCRDet [42] 89.41 78.83 50.02 65.59 69.96 57.63 72.26 90.73 81.41 84.39 52.76 63.62 62.01 67.62 61.16 69.83 SCRDet * [42] 89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61 anchor-free method baseline [44] 89.02 69.71 37.62 63.42 65.23 63.74 77.28 90.51 79.24 77.93 44.83 54.64 55.93 61.11 45.71 65.04 baseline .222 43.52 63.35 73.48 70.69 84.94 90.14 83.85 84.11 50.12 58.41 67.62 68.60 52.50 70.70 DRN * (Ours) 89.45 83.16 48.98 62.24 70.63 74.25 83.99 90.73 84.60 85.35 55.76 60.79 71.56 68.82 63.92 72.95 DRN * * (Ours) 89.71 82.34 47.22 64.10 76.22 74.43 85.84 90.57 86.18 84.89 57.65 61.93 69.30 69.63 58.48 73.23 Evaluation results of the OBB task on the DOTA dataset. The category names are abbreviated as follows: PL-PLane, BD-Baseball Diamond, BR-BRidge, GTF-Ground Field Track, SV-Small Vehicle, LV-Large Vehicle, SH-SHip, TC-Tennis Court, BC-Basketball Court,</figDesc><table><row><cell>Method</cell><cell>PL</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP</cell></row><row><cell>one-stage method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSD [27]</cell><cell>39.83</cell><cell>9.09</cell><cell cols="3">0.64 13.18 0.26</cell><cell>0.39</cell><cell cols="6">1.11 16.24 27.57 9.23 27.16 9.09</cell><cell>3.03</cell><cell>1.05</cell><cell cols="2">1.01 10.59</cell></row><row><cell cols="3">YOLOv2 [33] 39.57 *  *  [44] 89.56 79.83</cell><cell cols="14">43.8 66.54 65.58 66.09 83.11 90.72 83.72 84.3 55.62 58.71 62.48 68.33 50.77 69.95</cell></row><row><cell>DRN (Ours)</cell><cell cols="2">88.91 80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>ST-Storage Tank, SBF-Soccer-Ball Field, RA-RoundAbout, HA-Harbor, SF-Swimming Pool, and HC-HeliCopter. (?)* represents testing in multi-scale, and (?) * * represents testing with both flip and multi-scale. The other results of our approach are all without any test augmentation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>BL 2 [28] RC 1 [28] RC 2 [28] R 2 PN [43] RRD [21] RoI Trans [5] Ours Evaluation results on the HRSC2016 dataset.</figDesc><table><row><cell cols="2">mAP</cell><cell>55.7</cell><cell>69.6</cell><cell cols="2">75.7</cell><cell>75.7</cell><cell>79.6</cell><cell>84.3</cell><cell>86.2</cell><cell>92.7</cell></row><row><cell></cell><cell cols="4">Method mAP AP 50 AP 75</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Baseline 63.5</cell><cell>92.3</cell><cell>75.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Ours</cell><cell>65.6</cell><cell>92.0</cell><cell>77.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Table 3. Comparison of our method with the baseline on the</cell><cell></cell><cell></cell></row><row><cell cols="4">HRSC2016 dataset in COCO fashion.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Method</cell><cell></cell><cell></cell><cell cols="3">mAP AP 75 AR 300</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Faster-RCNN [35]</cell><cell></cell><cell>4.5</cell><cell>1.0</cell><cell>6.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">YOLO9000 [33]</cell><cell></cell><cell>9.4</cell><cell>7.3</cell><cell>11.1</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">RetinaNet [23]</cell><cell></cell><cell>45.5</cell><cell>38.9</cell><cell>53.0</cell><cell></cell><cell></cell></row><row><cell>SKU110K</cell><cell cols="4">RetinaNet with EM-Merger [9] 49.2</cell><cell>55.6</cell><cell>55.4</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">YoloV3 [34]</cell><cell></cell><cell>55.4</cell><cell>76.8</cell><cell>56.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell></cell><cell>55.8</cell><cell>62.8</cell><cell>62.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell>56.9</cell><cell>64.0</cell><cell>63.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">YoloV3-Rotate</cell><cell></cell><cell>49.1</cell><cell>51.1</cell><cell>58.2</cell><cell></cell><cell></cell></row><row><cell>SKU110K-R</cell><cell cols="2">CenterNet-4point  ? [44] CenterNet  ? [44]</cell><cell></cell><cell>34.3 54.7</cell><cell>19.6 61.1</cell><cell>42.2 62.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell></cell><cell>54.4</cell><cell>60.6</cell><cell>61.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell>55.9</cell><cell>63.1</cell><cell>63.3</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Evaluation results on SKU110K and SKU110K-R.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6 .Table 7 .Table 8 .</head><label>5678</label><figDesc>Ablation studies about FSM on the DOTA validation set. MK denotes the multiple kernels used in FSM.<ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b12">13</ref>, and 31 represent kernel sizes of (3, 3), (1, 3) and (3, 1), respectively. DCN and ROT are the deformable and rotation convolution layers. Evaluation results on the validation partition of the DOTA dataset using DRH-C. Evaluation results on the DOTA validation set using DRH-R. before the head branches. When we add the RCL, some improvement (0.5%) is observed because the RCL enables the neurons to adjust receptive fields by rotation. Next, we add a flat kernel (1?3) and the model demonstrates improved performance. Lastly, we add a slender kernel (3 ? 1) and the model shows consistent gains. The FSM with three splits enables the neurons to adjust receptive fields in two degrees of freedom, namely, shape and rotation. A slight improvement of a few more flat-shaped objects is observed when 1?3 kernel is added. To further reveal the effectiveness of FSM, we visualize the attention map in FSM. Details are available in our supplementary materials. In our experiments, we set up simple kernels to demonstrate the effectiveness of FSM and leave the design of more complex kernel shapes as future work.Figure 7. Example detection results of our method. The top row is from DOTA while and the bottom row is from HRSC2016. Comparison of our method with the baseline in terms of speed, complexity, and accuracy. The timing information is measured using images resolution 1024 ? 1024 on a single NVIDIA Tesla V100. The time of post-processing (i.e., NMS) is not included.</figDesc><table><row><cell>Method</cell><cell cols="2">MK</cell><cell cols="4">DCN ROT AP 50 AP 75</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>63.4</cell><cell>34.6</cell></row><row><cell></cell><cell>33</cell><cell></cell><cell></cell><cell></cell><cell>63.3</cell><cell>34.5</cell></row><row><cell></cell><cell>33</cell><cell></cell><cell></cell><cell></cell><cell>63.5</cell><cell>34.8</cell></row><row><cell></cell><cell>33</cell><cell></cell><cell></cell><cell></cell><cell>63.9</cell><cell>35.1</cell></row><row><cell></cell><cell cols="2">33, 13</cell><cell></cell><cell></cell><cell>63.5</cell><cell>34.7</cell></row><row><cell>FSM</cell><cell cols="2">33, 13</cell><cell></cell><cell></cell><cell>63.6</cell><cell>34.9</cell></row><row><cell></cell><cell cols="2">33, 13</cell><cell></cell><cell></cell><cell>64.2</cell><cell>35.4</cell></row><row><cell></cell><cell cols="2">33, 13, 31</cell><cell></cell><cell></cell><cell>63.7</cell><cell>34.8</cell></row><row><cell></cell><cell cols="2">33, 13, 31</cell><cell></cell><cell></cell><cell>63.9</cell><cell>35.2</cell></row><row><cell></cell><cell cols="2">33, 13, 31</cell><cell></cell><cell></cell><cell>64.4</cell><cell>35.7</cell></row><row><cell cols="2">Method</cell><cell cols="4">Acc Rec AP 50 AP 75</cell></row><row><cell cols="5">Baseline 0.21 0.89 63.4</cell><cell>34.6</cell></row><row><cell cols="2">DRH-C</cell><cell cols="3">0.27 0.95 64.1</cell><cell>35.2</cell></row><row><cell>Method</cell><cell cols="4">L 1 scale angle offset</cell><cell cols="2">AP 50 AP 75</cell></row><row><cell cols="2">Baseline 5.34</cell><cell></cell><cell>0.21</cell><cell>0.39</cell><cell>63.4</cell><cell>34.6</cell></row><row><cell></cell><cell>4.12</cell><cell></cell><cell>-</cell><cell>-</cell><cell>64.1</cell><cell>35.2</cell></row><row><cell>DRH-R</cell><cell>--</cell><cell></cell><cell>0.19 -</cell><cell>-0.36</cell><cell>63.5 63.4</cell><cell>34.8 34.5</cell></row><row><cell></cell><cell>4.10</cell><cell></cell><cell>0.18</cell><cell>0.35</cell><cell>64.1</cell><cell>35.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Seyed Majid Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Soft-NMS -Improving Object Detection With One Line of Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="150" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qikai</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Precise detection in densely packed scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dronebased object counting by spatially regularized regional proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Ru</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4145" to="4153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Active convolution: Learning the shape of convolution for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunho</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4201" to="4209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic anchor feature selection for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6609" to="6618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast multiclass vehicle detection on aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gellert</forename><surname>Mattyus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1938" to="1942" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rotated region based cnn for ship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ship rotated bounding box space for ship extraction from highresolution optical satellite images with complex backgrounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1074" to="1078" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dsod: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1919" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A three-stage real-time detector for traffic signs in large panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofeng</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="403" to="416" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Carafe: Content-aware reassembly of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SCRDet: Towards More Robust Detection for Small, Cluttered and Rotated Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxian</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1745" to="1749" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Oriented response networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
