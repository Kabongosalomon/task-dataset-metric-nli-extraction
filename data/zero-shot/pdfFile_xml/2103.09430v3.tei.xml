<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">TU Dortmund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maho</forename><surname>Nakata</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Enabling effective and efficient machine learning (ML) over large-scale graph data (e.g., graphs with billions of edges) can have a great impact on both industrial and scientific applications. However, existing efforts to advance large-scale graph ML have been largely limited by the lack of a suitable public benchmark.</p><p>Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of three real-world datasets for facilitating the advancements in large-scale graph ML. The OGB-LSC datasets are orders of magnitude larger than existing ones, covering three core graph learning tasks-link prediction, graph regression, and node classification. Furthermore, we provide dedicated baseline experiments, scaling up expressive graph ML models to the massive datasets. We show that expressive models significantly outperform simple scalable baselines, indicating an opportunity for dedicated efforts to further improve graph ML at scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and attracted more than 500 team registrations globally, during which significant performance improvements were made by a variety of innovative techniques. We summarize the common techniques used by the winning solutions and highlight the current best practices in large-scale graph ML. Finally, we describe how we have updated the datasets after the KDD Cup to further facilitate research advances. The OGB-LSC datasets, baseline code, and all the information about the KDD Cup are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine Learning (ML) on graphs has attracted immense attention in recent years because of the prevalence of graphstructured data in real-world applications. Modern application domains include Webscale social networks <ref type="bibr" target="#b56">(Ugander et al., 2011)</ref>, recommender systems <ref type="bibr" target="#b71">(Ying et al., 2018)</ref>, hyperlinked Web documents <ref type="bibr" target="#b26">(Kleinberg, 1999)</ref>, knowledge graphs (KGs) <ref type="bibr" target="#b0">(Bollacker et al., 2008;</ref><ref type="bibr" target="#b59">Vrande?i? and Kr?tzsch, 2014)</ref>, as well as the molecule simulation data generated by the ever-increasing scientific computation <ref type="bibr" target="#b37">(Nakata and Shimazaki, 2017;</ref><ref type="bibr" target="#b6">Chanussot et al., 2021)</ref>. All these domains involve large-scale graphs with billions of edges or a dataset with millions of graphs. Deploying accurate graph ML at scale will have a huge practical im-However, in deep learning, it has been demonstrated over and over again that one needs big expressive models and train them on big data to achieve the best performance <ref type="bibr" target="#b19">(He et al., 2016;</ref><ref type="bibr" target="#b42">Russakovsky et al., 2015;</ref><ref type="bibr" target="#b57">Vaswani et al., 2017;</ref><ref type="bibr" target="#b10">Devlin et al., 2018;</ref><ref type="bibr" target="#b2">Brown et al., 2020)</ref>. In graph ML, the trend has been the opposite-models get simplified and less expressive to be able to scale to large graphs <ref type="bibr" target="#b64">(Wu et al., 2019)</ref>. Thus, there is a massive opportunity to enable graph ML techniques to work with realistic and large-scale graph datasets, exploring the potential of expressive models for big graphs.</p><p>Here we present a large-scale graph ML challenge, OGB Large-Scale Challenge (OGB-LSC), to facilitate the development of state-of-the-art graph ML models for massive modern datasets. Specifically, we introduce three large-scale, realistic, and challenging datasets-MAG240M, WikiKG90M, and PCQM4M-that are unprecedentedly large in scale (see <ref type="table" target="#tab_0">Table 1</ref>; the sizes are 10 to 100 times larger than the corresponding original OGB datasets 1 ) and cover predictions at the level of nodes, links, and graphs, respectively. An overview of the datasets is provided in <ref type="figure">Figure 1</ref>.</p><p>Beyond providing the datasets, we perform an extensive baseline analysis on each dataset and implement both simple baseline models and advanced expressive models at scale. We find that advanced expressive models-despite requiring more efforts to scale up-do benefit from the large data and significantly outperform simple baseline models that are easy to scale.</p><p>To facilitate the community engagement, we recently organized the ACM KDD Cup 2021 around the OGB-LSC datasets. The competition attracted more than 500 team registrations and 150 leaderboard submissions. Within the three-month duration of the competition (March 15 to June 15, 2021), we have already witnessed innovative methods being developed to provide impressive performance gains 2 , further solidifying the value of the OGB-LSC datasets to advance state-of-the-art. We summarize the common techniques shared by the winning solutions, highlighting the current best practices of large-scale graph ML. Moreover, based on the lessons learned from the KDD Cup, we describe the future plan to update the datasets so that they can be further used to advance large-scale graph ML.</p><p>In addition, we highlight the top 3 winning results from our KDD Cup 2021 that significantly advance state-of-the-art and summarize common techniques used by the winning solutions. Note that while our baselines only used a single model for simplicity, all the winners used extensive model ensembling for their test submissions in order to maximize the performance. For a more direct comparison, we also report the winners' self-reported validation accuracy in the main text, which still exhibits significant accuracy improvement over our strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MAG240M: Node-Level Prediction</head><p>Practical relevance and dataset overview. The volume of scientific publication has been increasing exponentially, doubling every 12 years <ref type="bibr" target="#b11">(Dong et al., 2017)</ref>. Currently, subject areas of ARXIV papers are manually determined by the paper's authors and ARXIV moderators. An accurate automatic predictor of papers' subject categories not only reduces the significant burden of manual labeling, but can also be used to classify the vast number of non-ARXIV papers, thereby allowing better search and organization of academic papers.</p><p>MAG240M is a heterogeneous academic graph extracted from the Microsoft Academic Graph (MAG) . Given arXiv papers situated in the heterogeneous graph, whose schema diagram is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, we aim to automatically annotate their topics, i.e., predicting the primary subject area of each ARXIV paper.</p><p>Graph. We extract 121M academic papers in English from MAG (version: 2020-11-23) to construct a heterogeneous academic graph. The resultant paper set is written by 122M author entities, who are affiliated with 26K institutes. Among these papers, there are 1.3 billion citation links captured by MAG. Each paper is associated with its natural language title and most papers' abstracts are also available. We concatenate the title and abstract by period and pass it to a ROBERTA sentence encoder <ref type="bibr" target="#b40">Reimers and Gurevych, 2019)</ref>, generating a 768-dimensional vector for each paper node. Among the 121M paper nodes, approximately 1.4M nodes are ARXIV papers annotated with 153 ARXIV subject areas, e.g., cs.</p><p>LG (Machine Learning). On the paper nodes, we attach the publication years as meta information.</p><p>Prediction task and evaluation metric. The task is to predict the primary subject areas of the given ARXIV papers, which is cast as an ordinary multi-class classification problem. The metric is the classification accuracy.</p><p>To understand the relation between the prediction task and the heterogeneous graph structure, we analyze the graph homophily <ref type="bibr" target="#b32">(McPherson et al., 2001</ref>)-tendency of two adjacent nodes to share the same labels-to better understand the interplay between heterogeneous graph connectivity and the prediction task. Homophily is normally analyzed over a homogeneous graph, but we extend the analysis to the heterogenous graph by considering meta-paths <ref type="bibr" target="#b52">(Sun et al., 2011</ref>)-a path consisting of a sequence of relations defined between different node types. Given a meta-path, we can say two nodes are adjacent if they are connected by the meta-path. <ref type="table">Table 3</ref> shows the homophily for different kinds of meta-paths with different levels of connection strength. Compared to the direct citation connection (i.e., P-P), certain meta-paths (i.e., P-A-P) give rise to much higher degrees of homophiliness, while other meta-paths (i.e., P-A-I-A-P) provide much less homophily. As homophily is the central graph property exploited by many graph ML models, we believe that discovering essential heterogeneous connectivity is important to achieve good performance on this dataset.</p><p>Dataset split. We split the data according to time. Specifically, we train models on ARXIV papers published until 2018, validate the performance on the 2019 papers, and finally test the performance on the 2020 papers. The split reflects the practical scenario of helping the authors and moderators annotate the subject areas of the newly-published ARXIV papers.</p><p>Baseline. We benchmark a broad range of graph ML models in both homogeneous (where only paper to paper relations are considered) and full heterogeneous settings. For both settings, we convert the directed graph into an undirected graph for simplicity. First, for the homogeneous setting, we benchmark the simple baseline models: graph-agnostic MLP, Label Propagation, and the recently-proposed simplified graph methods: SGC <ref type="bibr" target="#b64">(Wu et al., 2019)</ref>, SIGN <ref type="bibr" target="#b41">(Rossi et al., 2020)</ref> and MLP+C&amp;S , which are inherently scalable by decoupling predictions from propagation. Furthermore, we benchmark state-of-the-art expressive GNNs trained with neighborhood sampling (NS) <ref type="bibr" target="#b17">(Hamilton et al., 2017)</ref>, where we recursively sample 25 neighbors in the first layer and 15 neighbors in the second layer during training time. At inference time, we sample at most 160 neighbors for each layer. Here, we benchmark two types of strong models: the GRAPHSAGE (Hamilton et al., 2017) model (performing mean aggregation and utilizing skip-connections), and the more advanced GRAPH ATTENTION NETWORK (GAT) model <ref type="bibr" target="#b58">(Velickovic et al., 2018)</ref>. For the full heterogeneous setting, we follow <ref type="bibr" target="#b44">Schlichtkrull et al. (2018)</ref> and learn distinct weights for each individual relation type (denoted by R-GRAPHSAGE and R-GAT, where "R" stands for "Relational"). We obtain the input features of authors and institutions by averaging the features of papers belonging to the same author and institution, respectively. The models are trained with NS. We note that the expressive GNNs trained with NS require more efforts to scale up, but are more expressive than the simple baselines.</p><p>Hyper-parameters. Hyper-parameters are selected based on their best validation performance. For all the models without NS, we tuned the hidden dimensionality ? {128, 256, 512, 1024}, MLP depth ? {1, 2, 3, 4}, dropout ratio ? {0, 0.25, 0.5}, propagation layers (for SGC, SIGN, and C&amp;S) ? {2, 3}. For all the GNN models with NS, we use a hidden dimensionality of 1024. We make use of batch normalization <ref type="bibr" target="#b24">(Ioffe and Szegedy, 2015)</ref> and ReLU activation in all models.</p><p>Discussion. Validation and test performances of all models considered are shown in <ref type="table" target="#tab_2">Table 2</ref>. First, the graph-agnostic MLP and Label Propagation algorithm perform poorly, indicating that both graph structure and feature information are indeed important for the given task. Across the graph ML models operating on the homogeneous paper graph, GNNs with NS perform the best, with slight gains compared to their simplified versions. In particular, the advanced expressive graph attention aggregation is favourable compared to the uniform mean aggregation in GRAPHSAGE. Furthermore, considering all available heterogeneous relational structure in the heterogeneous graph setting yields significant improvements, with performance gains up to 3 percentage points. Again, the advanced attention aggregation provides favorable performance. Overall, our experiments highlight the benefits of developing and evaluating advanced expressive models on the larger scale.</p><p>KDD Cup 2021 summary. In <ref type="table" target="#tab_2">Table 2</ref>, we show the results of the top 3 winners of the KDD Cup: BD-PGL Team , Academic Team (Addanki et al., 2021), and Synerise AI Team <ref type="bibr" target="#b9">(Daniluk et al., 2021)</ref>. All the solutions outperform our baselines significantly, yielding 5-6% gain in test accuracy. For a more direct comparison, with a single model (no model ensembling), the BD-PGL Team reports a validation accuracy of 73.71% , improving our best R-GAT baseline by 3.7%.</p><p>Notably, all the winning solutions used the target labels as input to their models, which allows the models to propagate labels together with the features. Regarding the GNN architectures, the BD-PGL adopted the expressive Transformer-based UniMP architecture <ref type="bibr" target="#b46">(Shi et al., 2020)</ref>, while the Academic adopted the standard MPNN <ref type="bibr" target="#b14">(Gilmer et al., 2017)</ref> but trained it with self-supervised contrastive learning on unlabeled paper nodes <ref type="bibr" target="#b54">(Thakoor et al., 2021)</ref>. These results suggest that expressive GNNs are indeed promising for this dataset. Finally, both the BD-PGL and Academic teams exploited the temporal aspect of the academic graph by using the publication years either as input positional encoding  or as a way to sample mini-batch subgraphs for    <ref type="table">Table 3</ref>: Analysis of graph homophily for different meta-paths connecting 1,251,341 arXiv papers (only train+validation). Connection strength indicates the number of different possible paths along the template meta-path, e.g., meta-path "Paper-Author-Paper (P-A-P)" with connection strength 3 means that at least 3 authors are shared for the two papers of interest. Homophily ratio is the ratio of two nodes having the same target labels. GNNs (Addanki et al., 2021). As real-world large-scale graphs are almost always dynamic, exploiting the temporal information is a promising direction of future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">WikiKG90M: Link-Level Prediction</head><p>Practical relevance and dataset overview. Large encyclopedic Knowledge Graphs (KGs), such as Wikidata <ref type="bibr" target="#b59">(Vrande?i? and Kr?tzsch, 2014)</ref> and Freebase <ref type="bibr" target="#b0">(Bollacker et al., 2008)</ref>, represent factual knowledge about the world through triplets connecting different entities, e.g., Hinton citizen of ??????? Canada. They provide rich structured information about many entities, aiding a variety of knowledgeintensive downstream applications such as information retrieval, question answering <ref type="bibr" target="#b48">(Singhal, 2012)</ref>, and recommender systems <ref type="bibr" target="#b16">(Guo et al., 2020)</ref>. However, these large KGs are known to be far from complete <ref type="bibr" target="#b33">(Min et al., 2013)</ref>, missing many relational information between entities.</p><p>WikiKG90M is a Knowledge Graph (KG) extracted from the entire Wikidata knowledge base. The task is to automatically impute missing triplets that are not yet present in the current KG. Accurate imputation models can be readily deployed on the Wikidata to improve its coverage.</p><p>Graph. Each triplet (head, relation, tail) in WikiKG90M represents an Wikidata claim, where head and tail are the Wikidata items, and relation is the Wikidata predicate. We extracted triplets from the public Wikidata dump downloaded at three time-stamps: September 28, October 26, and November 23 of 2020, for training, validation, and testing, respectively. We retain all the entities and relations in the September dump, resulting in 87,143,637 entities, 1,315 relations, and 504,220,369 triplets in total.</p><p>In addition to extracting triplets, we provide text features for entities and relations. Specifically, each entity/relation in Wikidata is associated with a title and a short description, e.g., one entity is associated with the title 'Geoffrey Hinton' and the description 'computer scientist and psychologist'. Similar to MAG240M, we provide ROBERTA embeddings <ref type="bibr" target="#b40">(Reimers and Gurevych, 2019;</ref> as node and edge features. 3</p><p>Prediction task and evaluation metric. The task is the KG completion, i.e., given a set of training triplets, predict a set of test triplets. For evaluation, we follow the protocol similar to how KG completion is evaluated <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref>. Specifically, for each validation/test triplet, (head, relation, tail), we corrupt tail with randomly-sampled 1000 negative entities, e.g., tail neg, such that (head, relation, tail neg) does not appear in the train/validation/test KG. The model is asked to rank the 1001 candidates (consisting of 1 positive and 1000 negatives) for each triplet and predict the top 10 entities that are most likely to be positive. The goal is to rank the ground-truth positive entity as high in the rank as possible, which is measured by Mean Reciprocal Rank (MRR). 4</p><p>Dataset split. We split the triplets according to time, simulating a realistic KG completion scenario of imputing missing triplets not present at a certain timestamp. Specifically, we construct three KGs using the aforementioned September, October, and November KGs, where we only retain entities and relation types that appear in the earliest September KG. We use the triplets in the September KG for training, and use the additional triplets in the October and November KGs for validation and test, respectively.</p><p>We analyze the effect of the time split. We find that head entities of validation triplets tend to be less popular entities; on average, they only have 6.5 out-degrees in the training KG, which is less than a quarter of the out-degree averaged over training triplets (i.e., 28.0). This suggests that learning signals for predicting validation (and test) triplets are sparse. Nonetheless, even for the sparsely-connected triplets, we find the textual information provides important clues, as illustrated in <ref type="table" target="#tab_4">Table 4</ref>. Hence, we expect that advanced graph models that effectively incorporate textual information will be key to achieve good performance on the challenging time split.</p><p>Baseline. We consider two representative KG embedding models: TRANSE <ref type="bibr" target="#b1">(Bordes et al., 2013)</ref> and COMPLEX <ref type="bibr" target="#b55">(Trouillon et al., 2016)</ref>. These models define their own decoders to score knowledge triplets using the corresponding entity and relation embeddings. For instance, TRANSE uses ? h + r ? t 2 as the decoder, where h, r, and t are embeddings of head, relation, and tail, respectively. For the encoder function (mapping each entity and relation to its embedding), we consider the following three options. Shallow: We use the distinct embedding for each entity and relation, as normally done in KG embedding models. RoBERTa: We use two MLP encoders (one for entity and another for relation) that transform the ROBERTA features into entity and relation embeddings. Concat: To enhance the expressive power of the previous encoder, we concatenate the shallow learnable embeddings into the ROBERTA features, and use the MLPs to transform the concatenated vectors to get the final embeddings. This way, the MLP encoders can adaptively utilize the ROBERTA features and the shallow embeddings to fit the large amount of triplet data. To implement our baselines, we utilize DGL-KE <ref type="bibr" target="#b74">(Zheng et al., 2020)</ref>.</p><p>Hyper-parameters. For the loss function, we use the negative sampling loss from <ref type="bibr" target="#b53">Sun et al. (2019)</ref>, where we pick margin ? from {1,4,8,10,100}. In order to balance the performance and the memory cost, we use the embedding dimensionality of 200 for all the models.</p><p>Discussion. <ref type="table" target="#tab_5">Table 5</ref> shows the validation and test performance of the six different models, i.e., combination of two decoders (TRANSE and COMPLEX) and three encoders (SHALLOW, ROBERTA, and CONCAT). Notably, in terms of the encoders, we see that the most expressive CONCAT outperforms both SHALLOW and ROBERTA, indicating that both the textual information (captured by the ROBERTA embeddings) and structural information (captured by node-wise learnable embeddings) are useful in predicting validation and test triplets. In terms of the decoders, TRANSE and COMPLEX show similar performance with the CONCAT encoder, while they show somewhat mixed results with the SHALLOW and ROBERTA encoders.</p><p>Overall, our experiments suggest that the expressive encoder that combines both textual information and structural information gives the most promising performance. In the KG completion literature, the design of the encoder has been much less studied compared to the decoder designs. Therefore, we believe there is a huge opportunity in scaling up more advanced encoders, especially GNNs <ref type="bibr" target="#b44">(Schlichtkrull et al., 2018)</ref>, to further improve the performance on this dataset.</p><p>KDD Cup 2021 summary. <ref type="table" target="#tab_5">Table 5</ref> shows the results of the top 3 winners of the KDD Cup: BD-PGL Team , OhMyGod Team <ref type="bibr">(Peng et al., 2021)</ref>, and the GraphMIRAcles Team <ref type="bibr" target="#b4">(Cai et al., 2021)</ref>. All the winning solutions outperform our strong baselines significantly,   , improving our best COMPLEX-CONCAT baseline by 0.07 points in validation MRR. Similar to our baselines, all the winners utilize the KG embedding approach as the backbone, and adopt the encoder that takes both shallow embedding and textual embeddings into account. Specifically, BD-PGL proposed the NOTE model  which makes the ROTATE model more expressive, while OhMyGod adopted the ensemble of several existing KG embedding models. On the other hand, GraphMIRAcles explored different design choices for the encoder and found that adding residual connection for shallow embeddings significantly improved the model performance.</p><p>In addition to the model advances, all the winners exploited some statistical property of candidate tail entities. Most notably, <ref type="bibr" target="#b67">Yang et al. (2021)</ref> found that simply by sorting the candidate tails by the frequency they appear in the training KG, it was possible to achieve validation MRR of 0.75, rivaling our TRANSE-SHALLOW baseline. This highlights that our negative tail candidates are mostly rare entities that can be easily distinguished from the true tail entity. On the other hand, the practical KG completion presents a much harder challenge: the candidate tails are not provided, and a model needs to predict the true tail entity out of all the possible 87M entities. As the performance on WikiKG90M has already saturated, we have updated WikiKG90M to WikiKG90Mv2 to reflect the realistic setting in large-scale KG completion. See Section 3 for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">PCQM4M: Graph-Level Prediction</head><p>Practical relevance and dataset overview. Density Functional Theory (DFT) is a powerful and widely-used quantum physics calculation that can accurately predict various molecular properties such as the shape of molecules, reactivity, responses by electromagnetic fields <ref type="bibr" target="#b3">(Burke, 2012)</ref>. However, DFT is time-consuming and takes up to several hours per small molecule. Using fast and accurate ML models to approximate DFT enables diverse downstream applications, such as property prediction for organic photovaltaic devices <ref type="bibr" target="#b5">(Cao and Xue, 2014)</ref> and structure-based virtual screening for drug discovery <ref type="bibr" target="#b12">(Ferreira et al., 2015)</ref>.</p><p>PCQM4M is a quantum chemistry dataset originally curated under the PubChemQC project <ref type="bibr">(Nakata, 2015;</ref><ref type="bibr" target="#b37">Nakata and Shimazaki, 2017)</ref>. Based on the PubChemQC, we define a meaningful ML task of predicting DFT-calculated HOMO-LUMO energy gap of molecules given their 2D molecular graphs. The HOMO-LUMO gap is one of the most practically-relevant quantum chemical properties of molecules since it is related to reactivity, photoexcitation, and charge transport <ref type="bibr" target="#b15">(Griffith and Orgel, 1957)</ref>. Moreover, predicting the quantum chemical property only from 2D molecular graphs without their 3D equilibrium structures is also practically favorable. This is because obtaining 3D equilibrium structures requires DFT-based geometry optimization, which is expensive on its own.</p><p>To ensure the resulting models are practically useful, we limit the average inference budget per molecule (including both pre-processing and model inference) to be less than 0.1 second using a single GPU and CPU (multi-threading on a multi-core CPU is allowed). This means that expensive (quantum) calculations cannot be used to perform inference. As our test set contains 377,423 molecules, we require the all the prediction to be made within 12 hours. Note that this time constraint is quite generous for ordinary GNNs-each of our baseline GNN only took about 3 minutes to perform inference over the entire test data.</p><p>Graph. We provide molecules as the SMILES strings <ref type="bibr" target="#b63">(Weininger, 1988)</ref>, from which 2D molecule graphs (nodes are atoms and edges are chemical bonds) as well as molecular fingerprints (handengineered molecular feature developed by the chemistry community) can be obtained. By default, we follow OGB <ref type="bibr" target="#b20">(Hu et al., 2020a)</ref> to convert the SMILES string into a molecular graph representation, where each node is associated with a 9-dimensional feature (e.g., atomic number, chirality) and each edge comes with a 3-dimensional feature (e.g., bond type, bond stereochemistry), although the optimal set of input graph features remains to be explored.</p><p>Prediction task and evaluation metric. The task is graph regression: predicting the HOMO-LUMO energy gap in electronvolt (eV) given 2D molecular graphs. Mean Absolute Error (MAE) is used as evaluation metric.</p><p>Dataset split. We split molecules by their PubChem ID (CID) with ratio 80/10/10. Our original intention was to provide the scaffold split <ref type="bibr" target="#b20">(Hu et al., 2020a;</ref><ref type="bibr" target="#b65">Wu et al., 2018)</ref>, but the provided data turns out to be split by the CID due to some pre-processing bug. The CID number itself does not indicate particular meaning about the molecule, but splitting by CID may provide a moderate distribution shift (most likely not as severe as the scaffold split). We empirically compared the CID and scaffold splits and found the model performances were consistent between the two splits. 5</p><p>Baseline. We benchmark two types of models: a simple MLP over the Morgan fingerprint (Morgan, 1965) and more advanced GNN models. For GNNs, we use the four strong models developed for graph-level prediction: Graph Convolutional Network (GCN) (Kipf and Welling, 2017) and Graph Isomorphism Network (GIN) <ref type="bibr" target="#b66">(Xu et al., 2019)</ref>, as well as their variants, GCN-VIRTUAL and GIN-VIRTUAL, which augment graphs with a virtual node that is bidirectionally connected to all nodes in the original graph <ref type="bibr" target="#b14">(Gilmer et al., 2017)</ref>. Adding the virtual node is shown to be effective across a wide range of graph-level prediction datasets in OGB <ref type="bibr" target="#b20">(Hu et al., 2020a)</ref>. Edge features are incorporated following <ref type="bibr" target="#b21">Hu et al. (2020b)</ref>. At inference time, the model output is clamped between 0 and 50 to avoid model's anomalously large/small prediction.</p><p>Hyper-parameters. For the MLP over Morgan fingerprint, we set the fingerprint dimensionality to be 2048, and tune the fingerprint radius ? {2, 3}, as well as MLP's hyper-parameters: hidden dimensionality ? {1200, 1600}, number of hidden layers ? {2, 4, 6}, and dropout ratio ? {0, 0.2}. For GNNs, we tune hidden dimensionality, i.e., width ? {300, 600}, number of GNN layers, i.e., depth ? {3, 5}. Simple summation is used for graph-level pooling. For all MLPs (including GIN's), we use batch normalization <ref type="bibr" target="#b24">(Ioffe and Szegedy, 2015)</ref> and ReLU activation.</p><p>Discussion. The validation and test results are shown in <ref type="table" target="#tab_6">Table 6</ref>. We see both the GNN models significantly outperform the simple fingerprint baseline. Expressive GNNs (GIN and GIN-VIRTUAL) outperform less expressive ones (GCN and GCN-VIRTUAL); especially, the most advanced and expressive GIN-VIRTUAL model significantly outperforms the other GNNs. Nonetheless, the current performance is still much worse than the chemical accuracy of 0.043eV-an indicator of practical usefulness established by the chemistry community. In the same <ref type="table" target="#tab_6">Table 6</ref>, we show our ablation, where we use only 10% of data to train the GIN-VIRTUAL model. We see the performance significantly deteriorate, indicating the importance of training the model on large data. Finally, in <ref type="table" target="#tab_8">Table 7</ref>, we show the relation between model sizes and validation performance. We see that the largest models always achieve the best performance.</p><p>Overall, we find that advanced, expressive, and large GNN model gives the most promising performance on the PCQM4M dataset, although the performance still needs to be improved for practical use. We believe further advances in advanced modeling, expressive architectures, and larger model sizes could yield breakthrough in the large-scale molecular property prediction task.</p><p>KDD Cup 2021 summary. In  <ref type="bibr" target="#b69">(Ying et al., 2021a)</ref>, which is 0.04 points lower than our best GIN-VIRTUAL baseline.  In terms of methodology, we find that the winning solutions share three important components in common.</p><p>(1) Their winning GNN models are indeed large and deep; the number of learnable parameters (single model) ranges from 50M up to 450M, while the number of GNN layers ranges from 11 up to 50, being significantly larger than our baseline models.</p><p>(2) All the GNNs perform global message passing at each layer, either through the virtual nodes <ref type="bibr" target="#b14">(Gilmer et al., 2017)</ref> or fully-connected Transformer-style self-attention <ref type="bibr" target="#b69">(Ying et al., 2021a)</ref>.</p><p>(3) All the winners utilize 3D structure of molecules to supervise their GNNs. As 3D structure was not provided at our KDD Cup, the winners generate the 3D structure themselves using RDkit <ref type="bibr" target="#b28">(Landrum et al., 2006)</ref> or PySCF , both of which provide cheap but less accurate 3D structure of molecules.</p><p>As modeling 3D molecular graphs is a promising direction in graph ML <ref type="bibr" target="#b45">(Sch?tt et al., 2017;</ref><ref type="bibr" target="#b27">Klicpera et al., 2020;</ref><ref type="bibr">Sanchez-Gonzalez et al., 2020;</ref>, we have updated PCQM4M to PCQM4Mv2 to include DFT-calculated 3D structures for training molecules. Details are provided in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Updates after the KDD Cup</head><p>To facilitate further research advances, we have updated the datasets and leaderboards based on the lessons learned from our KDD Cup 2021. Here we briefly describe our updates. More details are provided in Appendix C.</p><p>Updates on WikiKG90M. From the KDD Cup results, we learned that most of our provided negative entities in the large-scale WikiKG90M are "easy negatives", and our current task gives overly-optimistic performance scores. In a realistic large-scale KG completion setting, ML models are required to predict the true tail entity from nearly 90M entities, which is much more challenging.</p><p>To reflect this challenge, we have updated WikiKG90M to WikiKG90Mv2, where we do not provide any candidate entities for validation/test triples. Our initial experiments using the same set of baseline models, shows that WikiKG90Mv2 indeed provides a much harder challenge; our best model COMPLEX-CONCAT only achieves 0.1833 MRR on WikiKG90Mv2 as opposed to achieving 0.8637 MRR on WikiKG90M, leaving significant room for further improvement.</p><p>Updates on PCQM4M. From the KDD Cup results, we saw that the winners effectively utilized (self-calculated) 3D structure of molecules. Modeling molecular graphs in 3D space is of great interest to the graph ML community; We therefore have updated PCQM4M to PCQM4Mv2, where we provide DFT-calculated 3D structure for training molecules. For validation and test molecules, 3D structures is not be provided, and ML models still need to make prediction based on the 2D molecular graphs. In updating to PCQM4Mv2, we are also fixing subtle but important mismatch between some of the 2D molecular graphs and the corresponding 3D molecular graphs. Our preliminary experiments on PCQM4Mv2 suggest that the all the baseline models' MAE is improved by ? 0.04 [eV] compared to PCQM4M, although the trends in model performance stay almost the same as PCQM4M.</p><p>Updates on leaderboards. We are introducing public leaderboards to facilitate further research advances after our KDD Cup. The test submissions of the KDD Cup 2021 were evaluated on the entire hidden test set. After the KDD Cup, we are randomly splitting the test set into two: "test-dev" and "test-challenge". The test-dev set is be used for public leaderboards that evaluate test submissions any time during a year. The test-challenge set is be left for future competitions, which we plan to hold annually to facilitate community engagement. The leaderboards have been released together with the updated datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Modern applications of graph ML involve large-scale graph data with billions of edges or millions of graphs. ML advances on large graph data have been limited due to the lack of a suitable benchmark.</p><p>Here we present OGB-LSC, with the goal of advancing state-of-the-art in large-scale graph ML. OGB-LSC provides the three large-scale realistic benchmark datasets, covering the core graph ML tasks of node classification, link prediction, and graph regression. We perform dedicated baseline analysis, scaling up advanced graph models to large graphs. We show that advanced and expressive models can significantly outperform simpler baseline models, suggesting opportunities for further dedicated effort to yield even better performance.</p><p>We used our datasets for the recent ACM KDD Cup 2021, where we have attracted huge engagement from the community and have already witnessed significant performance improvement. We summarize the winning solutions for each dataset, highliting the current best practices in large-scale graph ML. Finally, we describe how we have updated our datasets after the KDD Cup to further facilitate research advances. Overall, we hope OGB-LSC encourages dedicated community efforts to tackle the important but challenging problem of large-scale graph ML.</p><p>(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] All of our relevant URLs are described in Appendix A. (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? <ref type="bibr">[Yes]</ref> We are using public datasets and closely follow the license rules. (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] Our datasets do not contain any private nor offensive information. 5. If you used crowdsourcing or conducted research with human subjects... A Key Information about OGB-LSC Dataset documentation. All of our datasets as well as how to use them through our Python package are documented at https://ogb.stanford.edu/kddcup2021/. Our baseline code to reproduce all the results for each dataset is available at https://github.com/snap-stanford/ ogb/tree/master/examples/lsc.</p><p>Intended use. OGB-LSC is intended for machine learning and data scientists to develop ML models to tackle the challenge of large-scale graph ML.</p><p>Relevant URLs. OGB-LSC maintains the following:</p><p>? Official website (https://ogb.stanford.edu/kddcup2021/) is the main reference of OGB-LSC. It provides an overview of the OGB-LSC, descriptions of the datasets as well as detailed documentations of how to use the datasets through the OGB Python package. The subpage (https://ogb.stanford.edu/kddcup2021/results/) also contains the leaderboards during the KDD Cup 2021 as well as the technical reports and code provided by the winners.</p><p>? Github repository (https://github.com/snap-stanford/ogb) hosts the source code for the OGB Python package. OGB-LSC datasets and evaluation are all managed by the Python package. We also release all the baseline code that we used in our experiments.</p><p>? Datasets are extremely large (around 300GB in total) and are hosted under AWS with the help of the DGL Team. Our users do not need to directly interact with the URL, as the dataset download and processing are all managed by our Python package.</p><p>? Mailing list (https://groups.google.com/g/open-graph-benchmark) is used for making any announcements about OGB/OGB-LSC.</p><p>Hosting and maintenance plan. OGB-LSC's Python package is hosted and version-tracked via Github. All the datasets are hosted under the AWS with the help of the DGL Team. We design the Python package to handle downloading and processing of the datasets. OGB is a community-driven initiative that has been actively maintained by our team members.</p><p>Licensing. The OGB Python package uses the MIT license. Each dataset has its own license. Specifically, MAG240M uses ODC-BY, WikiKG90M uses CC-0, and PCQM4M uses CC BY 4.0.</p><p>Author statement. We bear all responsibility in case of violation of rights, etc., and confirmation of the data license.</p><p>Computing resources. We ran all the experiments on a server with 10 GeForce RTX 2080 GPUs and an Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz.</p><p>Limitations. Large-scale graph ML has a wide variety of application domains and there are representative graphs that we cannot cover in the current OGB-LSC datasets. Examples include large-scale recommender systems, social networks, and financial networks. These graphs are hard to obtain due to privacy and cooperative concerns, but we hope to include these realistic large graphs in the future if we have a chance. That being said, it is our hope that many methodological insights on our large graphs (training strategy, GNN architecture, regularization, etc) still transfer well to a variety of large-scale graphs. We leave the thorough investigation to future work.</p><p>Potential negative social impacts. All of our datasets are derived from practically-relevant tasks in the real world; hence, developing models and deploying them to the real-world could potentially produce predictions that are influenced by the bias in the datasets. For example, regarding the MAG240M dataset, we may use the resulting paper and author embeddings to perform a variety of downstream ML tasks such as searching for similar papers or recommending author collaboration and paper citations. Thus, it is critical to ensure there is no undesirable bias in the embeddings. There could be also misuse of highly accurate ML models. For instance, regarding the PCQM4M dataset, we  <ref type="bibr" target="#b29">(Leskovec and Sosi?, 2016)</ref> is then used to compute the graph statistics. MAG240M (homo) represents the homogenized MAG240M graph with only paper nodes and citation links. Some graph statistics were omitted due to their high computational cost (the calculation did not complete in two weeks). need to make sure that the trained molecular property predictor is used in the right way to develop useful drugs/materials rather than harmful ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Basic Graph Statistics of the Datasets</head><p>The basic graph statistics of the OGB-LSC datasets are provided in <ref type="table" target="#tab_9">Table 8</ref>. MPNet <ref type="bibr" target="#b40">(Reimers and Gurevych, 2019;</ref>, which is shown to be significantly better sentence encoder <ref type="bibr" target="#b40">(Reimers and Gurevych, 2019</ref> In <ref type="table" target="#tab_0">Table 10</ref>, we show head entities that have very sparse connection in the training KG. We see that textual features could provide important signals for predicting these triples.</p><p>We perform an extensive baseline analysis on WikiKG90Mv2. We used the same set of hyper-parameters and baseline models as our original WikiKG90M. Different from WikiKG90M, WikiKG90Mv2 does not provide any candidate tail entities. A na?vely approach is to use the entire entities as the tail candidates. However, this approach does not scale well to a KG with nearly 90M entities because we need to predict scores for all the 90M entities for every triple. Nonetheless, in practice, most of the entities are obvious negatives: e.g., for the relation type "is located in", any entities that are not locations can be easily filtered out as negatives. Based on the the above intuition, we consider the relation-specific tail candidate sets. Specifically, on training triples, we pre-compute 20K most frequent tail entities for each relation and treat them as candidate tail entities for that relation. At inference time, we use our KG model to score among those relation-specific candidates.</p><p>The results are provided in <ref type="table" target="#tab_0">Table 10</ref>. Overall, we observe that the relative trends are similar to the original WikiKG90Mv2. Especially the CONCAT encoder provides the best MRR performance. Different from WikiKG90M, the MRR score on the new WikiKG90Mv2 is far perfect score of 1 and leaves a lot of room for improvement. Overall, we believe it is promising to explore how to quickly generate a small number of high-quality candidate tail entities out of all the entities so that KG models only need to score a much fewer number of candidate entities.</p><p>PCQM4Mv2 updates. The PCQM4M dataset has been updated to PCQM4Mv2. Below we summarize the updates we have applied to the dataset.</p><p>? 3D molecular structures provided. We additionally provide 3D structures for training molecules. These structures are calculated by DFT and are obtained together with the HOMO-LUMO gap. ? SMILES strings are partly updated. In the process of preparing the 3D structures, we found a subtle mismatch between SMILES strings (i.e., 2D molecular graphs) and the HOMO-LUMO gap for about 10% of the entire molecules. Specifically, the SMILES strings can be changed in the course of DFT's geometry optimization, but in PCQM4M, we provided the initial SMILES strings. In the updated PCQM4Mv2, we provide SMILES strings corresponding to the final optimized 3D structures. Note that the HOMO-LUMO gap was calculated by DFT based on the final 3D structures <ref type="bibr" target="#b37">(Nakata and Shimazaki, 2017)</ref>; hence, it makes more sense to correspond the HOMO-LUMO gap with the SMILES string associated with the final 3D structures. ? Number of molecules decreased slightly. As a result of the SMILES update, some molecules can no longer be parsed by the commonly-used chemistry toolkit, i.e., rdkit <ref type="bibr" target="#b28">(Landrum et al., 2006)</ref>. As a result, the total number of molecules has been slightly reduced to 3,746,619. ? Split ratio changed. For PCQM4Mv2, we set the split ratio for train/validation/test-dev/testchallenge to 90/2/4/4. The split is still done by PubChem compound ID so that there is no test label leakage, i.e., all the test molecules in PCQM4Mv2 is in the test split of PCQM4M.</p><p>Similar to PCQM4M, we also provide our baseline analysis on the updated PCQM4Mv2 dataset. At inference time, we clamped the output values to be between 0 and 20, which prevents our models from predicting erroneous values for some test molecules. We show the results in <ref type="table" target="#tab_0">Tables 12 and  13</ref>. We found that all the models were able to achieve lower MAE compared to PCQM4M, probably because we have fixed the mismatch bug described above. Beyond the overall better MAE, we see that the trend in model performance is mostly preserved; larger and more expressive GNN models achieve better results. For the GNNs, we observe that the depth helps more than width. Interestingly, too-wide models often make unstable prediction on validation molecules.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>A schema diagram of MAG240M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Basic statistics of the OGB-LSC datasets used in KDD Cup 2021. Datasets marked by ? has been updated to v2 after the KDD Cup (cf. Section 3).</figDesc><table><row><cell>Task type</cell><cell cols="2">Dataset</cell><cell cols="2">Statistics</cell></row><row><cell>Node-level</cell><cell cols="2">MAG240M</cell><cell>#nodes: #edges:</cell><cell>244,160,499 1,728,364,232</cell></row><row><cell>Link-level</cell><cell cols="3">? #nodes: WikiKG90M #edges:</cell><cell>87,143,637 504,220,369</cell></row><row><cell>Graph-level</cell><cell>PCQM4M</cell><cell>?</cell><cell>#graphs: #edges (total):</cell><cell>3,803,453 55,399,880</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of MAG240M measured by the accuracy (%).</figDesc><table><row><cell>Model</cell><cell cols="2">#Params Validation Test</cell></row><row><cell>MLP</cell><cell></cell><cell>0.5M 52.67 52.73</cell></row><row><cell>LABELPROP</cell><cell></cell><cell>0 58.44 56.29</cell></row><row><cell>SGC</cell><cell></cell><cell>0.7M 65.82 65.29</cell></row><row><cell>SIGN</cell><cell></cell><cell>3.8M 66.64 66.09</cell></row><row><cell>MLP+C&amp;S</cell><cell></cell><cell>0.5M 66.98 66.18</cell></row><row><cell>GRAPHSAGE (NS)</cell><cell></cell><cell>4.9M 66.79 66.28</cell></row><row><cell>GAT (NS)</cell><cell></cell><cell>4.9M 67.15 66.80</cell></row><row><cell>R-GRAPHSAGE (NS)</cell><cell></cell><cell>12.2M 69.86 68.94</cell></row><row><cell>R-GAT (NS)</cell><cell></cell><cell>12.3M 70.02 69.42</cell></row><row><cell>KDD 1ST: BD-PGL</cell><cell></cell><cell>75.49</cell></row><row><cell>KDD 2ND: ACADEMIC</cell><cell></cell><cell>75.19</cell></row><row><cell cols="2">KDD 3RD: SYNERISE AI</cell><cell>74.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Textual representation of validation triplets whose head entities only appear once as head in the training WikiKG90M.</figDesc><table><row><cell>Head</cell><cell>Relation</cell><cell>Tail</cell></row><row><cell cols="3">Food and drink companies of Bulgaria combines topics Bulgaria</cell></row><row><cell>Performing arts in Denmark</cell><cell cols="2">combines topics performing arts</cell></row><row><cell>Anglicanism in Grenada</cell><cell cols="2">combines topics Anglicanism</cell></row><row><cell>Chuan Li</cell><cell>occupation</cell><cell>researcher</cell></row><row><cell>Petra Junkova</cell><cell>given name</cell><cell>Petra</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results of WikiKG90M measured by Mean Reciprocal Rank (MRR).</figDesc><table><row><cell>Model</cell><cell cols="2">#Params Validation Test</cell></row><row><cell>TRANSE-SHALLOW</cell><cell cols="2">17.4B 0.7559 0.7412</cell></row><row><cell>COMPLEX-SHALLOW</cell><cell cols="2">17.4B 0.6142 0.5883</cell></row><row><cell>TRANSE-ROBERTA</cell><cell cols="2">0.3M 0.6039 0.6288</cell></row><row><cell>COMPLEX-ROBERTA</cell><cell cols="2">0.3M 0.7052 0.7186</cell></row><row><cell>TRANSE-CONCAT</cell><cell cols="2">17.4B 0.8494 0.8548</cell></row><row><cell>COMPLEX-CONCAT</cell><cell cols="2">17.4B 0.8425 0.8637</cell></row><row><cell>KDD 1ST: BD-PGL</cell><cell></cell><cell>0.9727</cell></row><row><cell cols="2">KDD 2ND: OHMYGOD</cell><cell>0.9712</cell></row><row><cell cols="2">KDD 3RD: GRAPHMIRACLES</cell><cell>0.9707</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>, we show the results of the top 3 winners of the KDD Cup: Machine Learning Team (Ying et al., 2021b), SuperHelix Team (Zhang et al., 2021), and Quantum Team (Addanki et al., 2021). The winners have significantly reduced the MAE compared our baselines, yielding around 0.03 points improvement in test MAE. For a more direct comparison, with a single model, the Machine Learning reports the validation MAE of 0.097 for their Graphormer model</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results of PCQM4M measured by MAE[eV]. The lower, the better. Ablation study of using only 10% of training data is also shown. Chemical accuracy indicates the final goal for practical usefulness.</figDesc><table><row><cell>Model</cell><cell cols="2">#Params Validation Test</cell></row><row><cell>MLP-FINGERPRINT</cell><cell cols="2">16.1M 0.2044 0.2070</cell></row><row><cell>GCN</cell><cell cols="2">2.0M 0.1684 0.1842</cell></row><row><cell>GCN-VIRTUAL</cell><cell cols="2">4.9M 0.1510 0.1580</cell></row><row><cell>GIN</cell><cell cols="2">3.8M 0.1536 0.1685</cell></row><row><cell>GIN-VIRTUAL</cell><cell cols="2">6.7M 0.1396 0.1494</cell></row><row><cell>MLP-FINGERPRINT (10% train)</cell><cell cols="2">6.8M 0.2708 0.2659</cell></row><row><cell>GIN-VIRTUAL (10% train)</cell><cell cols="2">6.7M 0.1790 0.1892</cell></row><row><cell cols="2">KDD 1ST: MACHINELEARNING</cell><cell>0.1208</cell></row><row><cell>KDD 2ND: SUPERHELIX</cell><cell></cell><cell>0.1210</cell></row><row><cell>KDD 3RD: QUANTUM</cell><cell></cell><cell>0.1211</cell></row><row><cell>Chemical accuracy (goal)</cell><cell>-</cell><cell>0.0430</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Model size and the MAE performance[eV]. For both models, the width indicates the hidden dimensionality. For GIN-VIRTUAL, the depth represents the number of GNN layers, while for the MLP-FINGERPRINT, the depth represents the the number of hidden layers in MLP.</figDesc><table><row><cell>Model</cell><cell cols="3">Width Depth #Params Validation</cell></row><row><cell></cell><cell>1600</cell><cell>6</cell><cell>16.1M 0.2044</cell></row><row><cell>MLP-FINGERPRINT</cell><cell>1600 1600</cell><cell>4 2</cell><cell>11.0M 0.2044 5.8M 0.2220</cell></row><row><cell></cell><cell>1200</cell><cell>6</cell><cell>9.7M 0.2083</cell></row><row><cell></cell><cell>600</cell><cell>5</cell><cell>6.7M 0.1410</cell></row><row><cell></cell><cell>600</cell><cell>3</cell><cell>3.7M 0.1462</cell></row><row><cell>GIN-VIRTUAL</cell><cell>300</cell><cell>5</cell><cell>1.7M 0.1442</cell></row><row><cell></cell><cell>300</cell><cell>3</cell><cell>1.0M 0.1512</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Basic graph statistics of the OGB-LSC datasets. The last three graph statistics are calculated over the 'standardized' graphs, where the graphs are first converted into undirected and unlabeled homogeneous graphs with duplicated edges removed. The SNAP library</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>C Details about Dataset Updates after the KDD Cup 2021 MAG240M updates. The MAG240M dataset itself has not been changed. The only update is on the test set. InTable 8, we report the test-dev accuracy of all the models.WikiKG90Mv2 updates. The WikiKG90M dataset has been updated to WikiKG90Mv2. Below we summarize the updates we have applied to the dataset.? No candidate tails provided. The most important update is that we do not provide any candidate tail entities for validation/test triples. Hence, a model needs to predict the target tail entity out of all the entities in Wikidata. ? Created from more recent Wikidata. The WikiKG90Mv2 is based on the public Wikidata dump downloaded at three time-stamps: May 17th, June 7th, and June 28th of 2021, for training, validation, and testing, respectively. We retain all the entities and relations in the September dump, resulting in 91,230,610 entities, 1,387 relations, and 601,062,811 triplets in total. ? A better text encoder used. The text features of WikiKG90Mv2 are obtained by using</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>). ? Balancing relation types in validation/test triples. On the new Wikidata dumps, we found the relation types of the raw validation/test triples are highly-skewed; the most frequent relation, "cites work (P2860)", occupies 60% and 85% of the entire validation and test triples, respectively. To test a model's capability to perform well across all types of relations, we subsample 15,000 triples from the entire validation/test triples such that the resulting relation counts are proportional to the cubic-root of the original relation counts.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Results of MAG240M measured by the accuracy (%). R-GRAPHSAGE/-GAT utilize the full heterogeneous graph information, while the other models operate on the homogeneous paper citation graph. Test accuracy is evaluated on the test-dev set.</figDesc><table><row><cell>Model</cell><cell cols="3">#Params Validation Test-dev</cell></row><row><cell>MLP</cell><cell>0.5M</cell><cell>52.67</cell><cell>52.76</cell></row><row><cell>LABELPROP</cell><cell>0</cell><cell>58.44</cell><cell>56.38</cell></row><row><cell>SGC</cell><cell>0.7M</cell><cell>65.82</cell><cell>65.30</cell></row><row><cell>SIGN</cell><cell>3.8M</cell><cell>66.64</cell><cell>66.03</cell></row><row><cell>MLP+C&amp;S</cell><cell>0.5M</cell><cell>66.98</cell><cell>66.05</cell></row><row><cell>GRAPHSAGE (NS)</cell><cell>4.9M</cell><cell>66.79</cell><cell>66.21</cell></row><row><cell>GAT (NS)</cell><cell>4.9M</cell><cell>67.15</cell><cell>66.71</cell></row><row><cell>R-GRAPHSAGE (NS)</cell><cell>12.2M</cell><cell>69.86</cell><cell>68.78</cell></row><row><cell>R-GAT (NS)</cell><cell>12.3M</cell><cell>70.02</cell><cell>69.31</cell></row><row><cell>KDD 1ST: BD-PGL</cell><cell cols="2">Ensemble</cell><cell>75.39</cell></row><row><cell>KDD 2ND: ACADEMIC</cell><cell cols="2">Ensemble</cell><cell>75.07</cell></row><row><cell>KDD 3RD: SYNERISE AI</cell><cell cols="2">Ensemble</cell><cell>74.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Textual representation of validation triplets whose head entities only appear once as head in the training WikiKG90Mv2.</figDesc><table><row><cell>Head</cell><cell>Relation</cell><cell>Tail</cell></row><row><cell>Herbert Hoover's Inaugural Address</cell><cell>country</cell><cell>United States of America</cell></row><row><cell cols="2">Jussi Award for Best Sound Recording instance of</cell><cell>class of award</cell></row><row><cell>organ dose</cell><cell>calculated from</cell><cell>absorbed dose</cell></row><row><cell>British Endurance Racing Team</cell><cell>country</cell><cell>United Kingdom</cell></row><row><cell>Knee bursae</cell><cell>anatomical location</cell><cell>knee</cell></row><row><cell>Churches in Dekanat Leuchtenberg</cell><cell>is a list of</cell><cell>church building</cell></row><row><cell>web content management system</cell><cell>model item</cell><cell>workflow management system</cell></row><row><cell>Stephan von Divonne</cell><cell>given name</cell><cell>Stephan</cell></row><row><cell>Minecraft mod</cell><cell cols="2">depends on software Minecraft</cell></row><row><cell>beer pouring</cell><cell>uses</cell><cell>beer engine</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Results of WikiKG90Mv2 measured by the Mean Reciprocal Rank (MRR).</figDesc><table><row><cell>Model</cell><cell cols="3">#Params Validation Test-dev</cell></row><row><cell>TRANSE-SHALLOW</cell><cell>18.2B</cell><cell>0.1103</cell><cell>0.0824</cell></row><row><cell>COMPLEX-SHALLOW</cell><cell>18.2B</cell><cell>0.1150</cell><cell>0.0985</cell></row><row><cell>TRANSE-MPNET</cell><cell>0.3M</cell><cell>0.1128</cell><cell>0.0860</cell></row><row><cell>COMPLEX-MPNET</cell><cell>0.3M</cell><cell>0.1258</cell><cell>0.0988</cell></row><row><cell>TRANSE-CONCAT</cell><cell>18.2B</cell><cell>0.2060</cell><cell>0.1761</cell></row><row><cell>COMPLEX-CONCAT</cell><cell>18.2B</cell><cell>0.2048</cell><cell>0.1761</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Results of PCQM4Mv2 measured by MAE [eV]. The lower, the better. Ablation study of using only 10% of training data is also shown. Chemical accuracy indicates the final goal for practical usefulness.</figDesc><table><row><cell>Model</cell><cell cols="3">#Params Validation Test-dev</cell></row><row><cell>MLP-FINGERPRINT</cell><cell>16.1M</cell><cell>0.1753</cell><cell>0.1760</cell></row><row><cell>GCN</cell><cell>2.0M</cell><cell>0.1379</cell><cell>0.1398</cell></row><row><cell>GCN-VIRTUAL</cell><cell>4.9M</cell><cell>0.1153</cell><cell>0.1152</cell></row><row><cell>GIN</cell><cell>3.8M</cell><cell>0.1195</cell><cell>0.1218</cell></row><row><cell>GIN-VIRTUAL</cell><cell>6.7M</cell><cell>0.1083</cell><cell>0.1084</cell></row><row><cell>MLP-FINGERPRINT (10% train)</cell><cell>16.1M</cell><cell>0.2429</cell><cell>0.2445</cell></row><row><cell>GIN-VIRTUAL (10% train)</cell><cell>6.7M</cell><cell>0.1442</cell><cell>0.1446</cell></row><row><cell>Chemical accuracy (goal)</cell><cell>-</cell><cell cols="2">0.0430</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13 :</head><label>13</label><figDesc>Model size and the MAE performance[eV]. For both models, the width indicates the hidden dimensionality. For GIN-VIRTUAL, the depth represents the number of GNN layers, while for the MLP-FINGERPRINT, the depth represents the the number of hidden layers in MLP.</figDesc><table><row><cell>Model</cell><cell cols="4">Width Depth #Params Validation</cell></row><row><cell></cell><cell>1600</cell><cell>6</cell><cell>16.1M</cell><cell>0.1753</cell></row><row><cell>MLP-FINGERPRINT</cell><cell>1600 1600</cell><cell>4 2</cell><cell>11.0M 5.8M</cell><cell>0.1752 0.1954</cell></row><row><cell></cell><cell>1200</cell><cell>6</cell><cell>9.7M</cell><cell>0.1804</cell></row><row><cell></cell><cell>600</cell><cell>5</cell><cell>6.7M</cell><cell>0.1083</cell></row><row><cell></cell><cell>600</cell><cell>3</cell><cell>3.7M</cell><cell>0.1239</cell></row><row><cell>GIN-VIRTUAL</cell><cell>300</cell><cell>5</cell><cell>1.7M</cell><cell>0.1100</cell></row><row><cell></cell><cell>300</cell><cell>3</cell><cell>1.0M</cell><cell>0.1181</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Specifically, MAG240M is 126 times larger than ogbn-mag in terms of the number of nodes, WikiKG90M is 35 times larger than ogbl-wikikg2 in terms of the number of nodes, and PCQM4M is 9 times larger than ogbg-molpcba in terms of the number of graphs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">OGB-LSC Datasets, Baselines, and KDD Cup SummaryWe describe the OGB-LSC datasets, covering three key task categories (node-, link-, and graph-level prediction tasks) of ML on graphs. We emphasize the practical relevance and data split for each dataset, making our task closely aligned to realistic applications. Through our extensive baseline experiments, we show that advanced expressive models tend to give much better performance than simple graph ML models, leaving room for further improvement. All the OGB-LSC datasets are available through the OGB Python package<ref type="bibr" target="#b20">(Hu et al., 2020a)</ref>. All the baseline and package code is available at https://github.com/snap-stanford/ogb.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See the results at https://ogb.stanford.edu/kddcup2021/results/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We concatenate the title and description with comma, e.g., 'Geoffrey Hinton, computer scientist and psychologist', and pass the sentence to a ROBERTA sentence encoder (Note that the ROBERTA model was trained before September 2020, so there is no obvious information leak). The title or/and description are sometimes missing, in which case we simply use the blank sentence to replace it.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that this is more strict than the standard MRR since there is no partial score for positive entities being ranked outside of top 10.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Detailed discussion can be found at https://github.com/snap-stanford/ogb/ discussions/162</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Michele Catasta and Larry Zitnick for helpful discussion, Shigeru Maya for motivating the project, Adrijan Bradaschia for setting up the server for the project, and Amit Bleiweiss, Benjamin Braun and Hanjun Dai for providing helpful feedback on our baseline code, and the DGL Team for hosting our large datasets. The HOKUSAI facility was used to perform some of the quantum calculations. This work was supported by the Japan Society for the Promotion of Science (JSPS KAKENHI Grant no. 18H03206). We are also grateful to Maeda Toshiyuki for helpful discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Ravichandra Addanki, Peter W Battaglia, David Budden, Andreea Deac, Jonathan Godwin, Thomas   Keck, Wai Lok Sibon Li, Alvaro Sanchez-Gonzalez, Jacklynn Stott, Shantanu Thakoor, et al.</p><p>Large-scale graph representation learning with very deep gnns and self-supervision. arXiv preprint arXiv:2107.09422, 2021.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Interest Group on Management of Data (SIGMOD)</title>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Perspective on density functional theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Burke</surname></persName>
		</author>
		<idno type="DOI">http:/link.aip.org/link/doi/10.1063/1.4704546</idno>
		<ptr target="http://link.aip.org/link/doi/10.1063/1.4704546" />
	</analytic>
	<monogr>
		<title level="j">J. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page">150901</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taoxing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Technical report of team graphmiracles in the wikikg90m-lsc track of ogb-lsc@ kdd cup 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recent progress in organic photovoltaics: Device architecture and optical design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangeng</forename><surname>Xue</surname></persName>
		</author>
		<idno type="DOI">10.1039/C4EE00260A</idno>
		<ptr target="http://dx.doi.org/10.1039/C4EE00260A" />
	</analytic>
	<monogr>
		<title level="j">Energy Environ. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2123" to="2144" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Open catalyst 2020 (oc20) dataset and community challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lowik</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgane</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Heras-Domingo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caleb</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aini</forename><surname>Palizhati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwoong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ulissi</surname></persName>
		</author>
		<idno type="DOI">10.1021/acscatal.0c04525</idno>
		<ptr target="https://doi.org/10.1021/acscatal.0c04525" />
	</analytic>
	<monogr>
		<title level="j">ACS Catal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="6059" to="6072" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Synerise at kdd cup 2021: Node classification in massive heterogeneous graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Daniluk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Dabrowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Rychalska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Go?uchowski</surname></persName>
		</author>
		<ptr target="https://ogb.stanford.edu/paper/kddcup2021/mag240m_SyneriseAI.pdf" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A century of science: Globalization of scientific collaborations, citations, and innovations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1437" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Molecular docking and structure-based drug design strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><forename type="middle">G</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><forename type="middle">N Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glaucius</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><forename type="middle">D</forename><surname>Andricopulo</surname></persName>
		</author>
		<idno type="DOI">10.3390/molecules200713384</idno>
		<ptr target="https://www.mdpi.com/1420-3049/20/7/13384" />
	</analytic>
	<monogr>
		<title level="j">Molecules</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="13384" to="13421" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<title level="m">Fast graph representation learning with pytorch geometric</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1273" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ligand-field theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Griffith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Orgel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Reviews, Chemical Society</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="381" to="393" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey on knowledge graph-based recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Array programming with numpy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarrod</forename><surname>Charles R Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauli</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><forename type="middle">J</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="issue">7825</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Forcenet: A graph neural network for large-scale quantum calculations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Shuaibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01436</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin R</forename><surname>Benson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13993</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast and uncertainty-aware directional message passing for non-equilibrium molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankari</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">T</forename><surname>Margraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS-W</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Landrum</surname></persName>
		</author>
		<title level="m">Open-source cheminformatics</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Snap: A general-purpose network analysis and graph-mining library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rok</forename><surname>Sosi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">eccv</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miller</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harry L Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Documentation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franka</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Maho Nakata. the PubChemQC Project: A Large Chemical Database from the First Principle Calculations</title>
		<idno type="DOI">10.1063/1.4938866</idno>
		<ptr target="http://dx.doi.org/10.1063/1.4938866" />
	</analytic>
	<monogr>
		<title level="j">AIP Conf. Proc</title>
		<imprint>
			<biblScope unit="volume">1702</biblScope>
			<biblScope unit="page">90058</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pubchemqc project: A large-scale first-principles electronic structure database for data-driven chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomomi</forename><surname>Shimazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1300" to="1308" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghai</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhi</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>of wikikg90m-lsc. 2021</idno>
		<ptr target="https://ogb.stanford.edu/paper/kddcup2021/wikikg90m_OhMyGod.pdf" />
		<imprint/>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1908.10084" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huziel Enoc Sauceda</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Masked label prediction: Unified message passing model for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03509</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Runimp: Solution for kddcup 2021 mag240m-lsc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjie</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
		<ptr target="https://ogb.stanford.edu/paper/kddcup2021/mag240m_BD-PGL.pdf" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Introducing the knowledge graph: things, not strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Official google blog</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09297</idno>
		<title level="m">Mpnet: Masked and permuted pre-training for language understanding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01892</idno>
		<title level="m">Solution for kdd-cup 2021 wikikg90m-lsc</title>
		<meeting><address><addrLine>Note</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Recent developments in the pyscf program package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samragni</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Barbry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nikolay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bogdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24109</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shantanu</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06514</idno>
		<title level="m">Bootstrapped representation learning on graphs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">The anatomy of the facebook social graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Ugander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Marlow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1111.4503</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.01315" />
	</analytic>
	<monogr>
		<title level="m">Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weininger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="36" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Team littleant&apos;s solution of task2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Ye</surname></persName>
		</author>
		<ptr target="https://ogb.stanford.edu/paper/kddcup2021/wikikg90m_littleant.pdf" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05234</idno>
		<title level="m">Do transformers really perform bad for graph representation? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08279</idno>
		<title level="m">Awardee solution of kdd cup 2021 ogb large-scale challenge graph-level track</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Graph-Saint: Graph sampling based inductive learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Litegem: Lite geometry enhanced molecular representation learning for quantum property prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanzhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weibin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14494</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08532</idno>
		<title level="m">Dgl-ke: Training knowledge graph embeddings at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
