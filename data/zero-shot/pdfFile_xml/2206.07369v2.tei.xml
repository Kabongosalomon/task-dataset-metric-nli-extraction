<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DiffWire: Inductive Graph Rewiring via the Lov?sz Bound</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Arnaiz-Rodriguez</surname></persName>
							<email>adrian@ellisalicante.org</email>
							<affiliation key="aff0">
								<orgName type="institution">ELLIS Alicante</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Begga</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Alicante</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Escolano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ELLIS Alicante</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Alicante</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuria</forename><surname>Oliver</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ELLIS Alicante</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DiffWire: Inductive Graph Rewiring via the Lov?sz Bound</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have been shown to achieve competitive results to tackle graph-related tasks, such as node and graph classification, link prediction and node and graph clustering in a variety of domains. Most GNNs use a message passing framework and hence are called MPNNs. Despite their promising results, MPNNs have been reported to suffer from over-smoothing, over-squashing and under-reaching. Graph rewiring and graph pooling have been proposed in the literature as solutions to address these limitations. However, most state-of-the-art graph rewiring methods fail to preserve the global topology of the graph, are neither differentiable nor inductive, and require the tuning of hyper-parameters. In this paper, we propose DIFFWIRE, a novel framework for graph rewiring in MPNNs that is principled, fully differentiable and parameter-free by leveraging the Lov?sz bound. Our approach provides a unified theory for graph rewiring by proposing two new, complementary layers in MPNNs: CT-LAYER, a layer that learns the commute times and uses them as a relevance function for edge re-weighting; and GAP-LAYER, a layer to optimize the spectral gap, depending on the nature of the network and the task at hand. We empirically validate the value of each of these layers separately with benchmark datasets for graph classification. DIFFWIRE brings together the learnability of commute times to related definitions of curvature, opening the door to creating more expressive MPNNs.</p><p>Arnaiz-Rodriguez A., Begga A., Escolano F., Oliver N. DiffWire: Inductive Graph Rewiring via the Lov?sz Bound. 2022. Preprint.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> are a class of deep learning models applied to graph structured data. They have been shown to achieve state-of-the-art results in many graph-related tasks, such as node and graph classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, link prediction <ref type="bibr" target="#b4">[5]</ref> and node and graph clustering <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, and in a variety of domains, including image or molecular structure classification, recommender systems and social influence prediction <ref type="bibr" target="#b7">[8]</ref>.</p><p>Most GNNs use a message passing framework and thus are referred to as Message Passing Neural Networks (MPNNs) <ref type="bibr" target="#b3">[4]</ref> . In these networks, every node in each layer receives a message from its adjacent neighbors. All the incoming messages at each node are then aggregated and used to update the node's representation via a learnable non-linear function -which is typically implemented by means of a neural network. The final node representations (called node embeddings) are used to perform the graph-related task at hand (e.g. graph classification). MPNNs are extensible, simple and have proven to yield competitive empirical results. Examples of MPNNs include GCN <ref type="bibr" target="#b2">[3]</ref>, GAT <ref type="bibr" target="#b8">[9]</ref>, GATv2 <ref type="bibr" target="#b9">[10]</ref>, GIN <ref type="bibr" target="#b10">[11]</ref> and GraphSAGE <ref type="bibr" target="#b11">[12]</ref>. However, they typically use transductive learning, i.e. the model observes both the training and testing data during the training phase, which might limit their applicability to graph classification tasks.</p><p>MPNNs have important limitations due to the inherent complexity of graphs, the limited depth of most state-of-the-art MPNNs and the inability of current methods to the capture global structural information of the graph. The literature has reported best results when MPNNs have a small number of layers, because networks with many layers tend to suffer from over-smoothing <ref type="bibr" target="#b12">[13]</ref> and oversquashing <ref type="bibr" target="#b13">[14]</ref>. Over-smoothing takes place when the embeddings of nodes that belong to different classes become indistinguishable. Over-squashing refers to the distortion of information flowing from distant nodes due to graph bottlenecks 1 that emerge when the number of k-hop neighbors grows exponentially with k. They both tend to occur in networks with a large number of layers <ref type="bibr" target="#b14">[15]</ref>. Moreover, simple MPNNs with a small number of layers fail to capture information that depends on the entire structure of the graph (e.g., random walk probabilities <ref type="bibr" target="#b15">[16]</ref>) and prevent the information flow to reach distant nodes. This phenomenon is called under-reaching <ref type="bibr" target="#b16">[17]</ref> and occurs when the MPNNs depth is smaller than the graph's diameter.</p><p>Graph pooling and graph rewiring have been proposed in the literature as solutions to address these limitations <ref type="bibr" target="#b13">[14]</ref>. Given that the main infrastructure for message passing in MPNNs are the edges in the graph, and given that many of these edges might be noisy or inadequate for the downstream task <ref type="bibr" target="#b17">[18]</ref>, graph rewiring aims to identify such edges and edit them.</p><p>Many graph rewiring methods rely on edge sampling strategies: first, the edges are assigned new weights according to a relevance function and then they are re-sampled according to the new weights to retain the most relevant edges (i.e. those with larger weights). Edge relevance might be computed in different ways, including randomly <ref type="bibr" target="#b18">[19]</ref>, based on similarity <ref type="bibr" target="#b19">[20]</ref> or on the edge's curvature <ref type="bibr" target="#b20">[21]</ref>.</p><p>Due to the diversity of possible graphs and tasks to be performed with those graphs, optimal graph rewiring should include a variety of strategies that are suited not only to the task at hand but also to the nature and structure of the graph.</p><p>Motivation. State-of-the-art edge sampling strategies have three significant limitations. First, most of the proposed methods fail to preserve the global topology of the graph. Second, most graph rewiring methods are neither differentiable nor inductive <ref type="bibr" target="#b20">[21]</ref>. Third, relevance functions that depend on a diffusion measure (typically in the spectral domain) are not parameter-free, which adds a layer of complexity in the models. In this paper, we address these three limitations.</p><p>Contributions and outline. The main contribution of our work is to propose a theoretical framework called DIFFWIRE for graph rewiring in MPNNs that is principled, fully differentiable, inductive, and parameter-free by leveraging the Lov?sz bound <ref type="bibr" target="#b15">[16]</ref> given by Eq. 1. This bound is a mathematical expression of the relationship between the commute times (effective resistance distance) and the network's spectral gap. Inductive means that given an unseen test graph, DIFFWIRE predicts the optimal graph structure for the task at hand without any parameter tuning. Given the recently reported connection between commute times and curvature <ref type="bibr" target="#b21">[22]</ref>, and between curvature and the spectral gap <ref type="bibr" target="#b20">[21]</ref>, our framework provides a unified theory linking these concepts. Our aim is to leverage diffusion and curvature theories to propose a new approach for graph rewiring that preserves the graph's structure.</p><p>We first propose using the commute times as a relevance function for edge re-weighting. Moreover, we develop a differentiable, parameter-free layer in the GNN (CT-LAYER) to learn the commute times. Second, we propose an alternative graph rewiring approach by adding a layer in the network (GAP-LAYER) that optimizes the spectral gap according to the nature of the network and the task at hand. Finally, we empirically validate the proposed layers with state-of-the-art benchmark datasets in a graph classification task. We select a graph classification task to emphasize the inductive nature of DIFFWIRE: the layers in the GNN (CT-LAYER and GAP-LAYER) are trained to predict the CTs embedding and minimize the spectral gap for unseen graphs, respectively. This approach gives a great advantage when compared to SoTA methods that require optimizing the parameters of the models for each graph. CT-LAYER and GAP-LAYER learn the weights during training to predict the optimal changes in the topology of any unseen graph in test time.</p><p>The paper is organized as follows: Section 2 provides a summary of the most relevant related literature. Our core technical contribution is described in Section 3, followed by our experimental evaluation and discussion in Section 4. Finally, Section 5 is devoted to conclusions and an outline of our future lines of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section we provide an overview of the most relevant works that have been proposed in the literature to tackle the challenges of over-smoothing, over-squashing and under-reaching in MPNNs by means of graph rewiring and pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations of MPNNs.</head><p>MPNNs are widely used to tackle many real-world tasks -from social network analysis to protein modeling-with competitive results. However, MPNNs also have important limitations due to the inherent complexity of graphs. Despite such complexity, the literature has reported best results when MPNNs have a small number of layers, because networks with many layers tend to suffer from over-smoothing <ref type="bibr" target="#b12">[13]</ref> and over-squashing <ref type="bibr" target="#b13">[14]</ref>.</p><p>Over-smoothing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> takes place when the embeddings of nodes that belong to different classes become indistinguishable. It tends to occur in MPNNs with many layers that are used to tackle short-range tasks, i.e. tasks where a node's correct prediction mostly depends on its local neighborhood. Given this local dependency, it makes intuitive sense that adding layers to the network would not help the network's performance.</p><p>Conversely, long-range tasks require as many layers in the network as the range of the interaction between the nodes. However, as the number of layers in the network increases, the number of nodes feeding into each of the node's receptive field also increases exponentially, leading to oversquashing <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>: the information flowing from the receptive field composed of many nodes is compressed in fixed-length node vectors, and hence the graph fails to correctly propagate the messages coming from distant nodes. Thus, over-squashing emerges when there is a bottleneck in the graph and a long-range task.</p><p>To prevent over-smoothing and over-squashing, the number of layers in MPNNs is typically kept small. However, simple models with a small number of layers fail to capture information that depends on the entire structure of the graph (e.g., random walk probabilities <ref type="bibr" target="#b15">[16]</ref>) and prevent the information flow to reach distant nodes. This phenomenon is called under-reaching <ref type="bibr" target="#b16">[17]</ref> and occurs when the MPNN's depth is smaller than the graph's diameter.</p><p>Graph rewiring in MPNNs. Rewiring is a process of changing the graph's structure to control the information flow and hence improve the ability of the network to perform the task at hand (e.g. node or graph classification, link prediction...). Several approaches have been proposed in the literature for graph rewiring, such as connectivity diffusion <ref type="bibr" target="#b24">[25]</ref> or evolution <ref type="bibr" target="#b20">[21]</ref>, adding new bridge-nodes <ref type="bibr" target="#b25">[26]</ref> and multi-hop filters <ref type="bibr" target="#b26">[27]</ref>, and neighborhood <ref type="bibr" target="#b11">[12]</ref>, node <ref type="bibr" target="#b27">[28]</ref> and edge <ref type="bibr" target="#b18">[19]</ref> sampling.</p><p>Edge sampling methods sample the graph's edges based on their weights or relevance, which might be computed in different ways. Rong et al. <ref type="bibr" target="#b18">[19]</ref> prove that randomly dropping edges during training improves the performance of GNNs. Klicpera et al. <ref type="bibr" target="#b24">[25]</ref>, define edge relevance according to the coefficients of a parameterized diffusion process over the graph. Then, the k-hop diffusion matrix is truncated to discard long-range interactions. For Kazi et al. <ref type="bibr" target="#b19">[20]</ref>, edge relevance is given by the similarity between the nodes' attributes In addition, a reinforcement learning process rewards edges leading to a correct classification and penalizes the rest.</p><p>Edge sampling-based rewiring has been proposed to tackle over-smoothing and over-squashing in MPNNs. Over-smoothing may be relieved by removing inter-class edges <ref type="bibr" target="#b28">[29]</ref>. However, this strategy is only valid when the graph is homophilic, i.e. connected nodes tend to share similar attributes. Otherwise, removing these edges could lead to over-squashing <ref type="bibr" target="#b20">[21]</ref> if their removal obstructs the message passing between distant nodes belonging to the same class (heterophily). Increasing the size of the bottlenecks of the graph via rewiring has been shown to improve node classification performance in heterophilic graphs, but not in homophilic graphs <ref type="bibr" target="#b20">[21]</ref>. Recently, Topping et al. <ref type="bibr" target="#b20">[21]</ref> propose an edge relevance function given by the edge curvature to mitigate over-squashing. They identify the bottleneck of the graph by computing the Ricci curvature of the edges. Next, they remove edges with high curvature and add edges around minimal curvature edges.</p><p>Graph Structure Learning (GSL). GSL methods <ref type="bibr" target="#b29">[30]</ref> aim to learn an optimized graph structure and its corresponding representations at the same time. DIFFWIRE could be seen from the perspective of GSL: CT-LAYER, as a metric-based, neural approach, and GAP-LAYER, as a direct-neural approach to optimize the structure of the graph to the task at hand.</p><p>Pooling in MPNNs. In addition to graph rewiring, pooling layers simplify the original graph by compressing it into a smaller graph or a vector via pooling operators, which range from simple <ref type="bibr" target="#b30">[31]</ref> to more sophisticated approaches, such as DiffPool <ref type="bibr" target="#b31">[32]</ref> and MinCut pool <ref type="bibr" target="#b32">[33]</ref>. Although graph pooling methods do not consider the edge representations, there is a clear relationship between pooling methods and rewiring since both of them try to reduce the flow of information through the graph's bottleneck. 3 Proposed Approach: DIFFWIRE for Inductive Graph Rewiring DIFFWIRE provides a unified theory for graph rewiring by proposing two new, complementary layers in MPNNs: first, CT-LAYER, a layer that learns the commute times and uses them as a relevance function for edge re-weighting; and second, GAP-LAYER, a layer to optimize the spectral gap, depending on the nature of the network and the task at hand.</p><p>In this section, we present the theoretical foundations for the definitions of CT-LAYER and GAP-LAYER. First, we introduce the bound that our approach is based on: The Lov?sz bound. <ref type="table" target="#tab_2">Table 2</ref> in A.1 summarizes the notation used in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Lov?sz Bound</head><p>The Lov?sz bound, given by Eq. 1, was derived by Lov?sz in <ref type="bibr" target="#b15">[16]</ref> as a means of linking the spectrum governing a random walk in an undirected graph G = (V, E) with the hitting time H uv between any two nodes u and v of the graph. H uv is the expected number of steps needed to reach (or hit) v from u; H vu is defined similarly. The sum of both hitting times between the two nodes, v and u, is the commute time CT uv = H uv + H vu . Thus, CT uv is the expected number of steps needed to hit v from u and go back to u. According to the Lov?sz bound:</p><formula xml:id="formula_0">1 vol(G) CT uv ? 1 d u + 1 d v ? 1 ? 2 2 d min<label>(1)</label></formula><p>where ? 2 ? 0 is the spectral gap, i.e. the first non-zero eigenvalue of L = I ? D ?1/2 AD ?1/2 (normalized Laplacian <ref type="bibr" target="#b33">[34]</ref>, where D is the degree matrix and A, the adjacency matrix); vol(G) is the volume of the graph (sum of degrees); d u and d v are the degrees of nodes u and v, respectively; and d min is the minimum degree of the graph.</p><p>The term CT uv /vol(G) in Eq. 1 is referred to as the effective resistance, R uv , between nodes u and v. The bound states that the effective resistance between two nodes in the graph converges to or diverges from (1/d u + 1/d v ), depending on whether the graph's spectral gap diverges from or tends to zero. The larger the spectral gap, the closer CT uv /vol(G) will be to 1 du + 1 dv and hence the less informative the commute times will be.</p><p>We propose two novel MPNNs layers based on each side of the inequality in Eq. 1: CT-LAYER, focuses on the left-hand side, and GAP-LAYER, on the right-hand side. The use of each layer depends on the nature of the network and the task at hand. In a graph classification task (our focus), CT-LAYER is expected to yield good results when the graph's spectral gap is small; conversely, GAP-LAYER would be the layer of choice in graphs with large spectral gap.</p><p>The Lov?sz bound was later refined by von Luxburg et al. <ref type="bibr" target="#b34">[35]</ref>. App. A.2.2 presents this bound along with its relationship with R uv as a global measure of node similarity. Once we have defined both sides of the Lov?sz bound, we proceed to describe their implications for graph rewiring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CT-LAYER: Commute Times for Graph Rewiring</head><p>We focus first on the left-hand side of the Lov?sz bound which concerns the effective resistances CT uv /vol(G) = R uv (or commute times) 2 between any two nodes in the graph.</p><p>Spectral Sparsification leads to Commute Times. Graph sparsification in undirected graphs may be formulated as finding a graph H = (V, E ) that is spectrally similar to the original graph G = (V, E) with E ? E. Thus, the spectra of their Laplacians, L G and L H should be similar. Theorem 1 (Spielman and Srivastava <ref type="bibr" target="#b35">[36]</ref>). Let Sparsify(G, q) -&gt; G' be a sampling algorithm of graph G = (V, E), where edges e ? E are sampled with probability q ? R e (proportional to the effective resistance). For n = |V | sufficiently large and 1/ ? n &lt; ? 1, O(n log n/ 2 ) samples are needed to satisfy ?x ? R n :</p><formula xml:id="formula_1">(1 ? )x T L G x ? x T L G x ? (1 + )x T L G x , with probability ? 1/2.</formula><p>The above theorem has a simple explanation in terms of Dirichlet energies. The Laplacian L = D ? A 0, i.e. it is semi-definite positive (all its eigenvalues are non-negative). Then, if we consider</p><formula xml:id="formula_2">x : V ? R as a real-valued function of the n nodes of G = (V, E), we have that E(x) := x T L G x = e=(u,v)?E (x u ? x v ) 2 ? 0 for any x.</formula><p>In particular, the eigenvectors f := {x i : Lf i = ? i x i } are the set of special functions (mutually orthogonal and normalized) that minimize the energies E(f i ), i.e. they are the orthogonal functions with the minimal variabilities achievable by the topology of G. Therefore, Theorem 1 states that any minimal variability of G is bounded by (1 ? ) times that of G if we sample enough edges with probability q ? R e . Therefore, the effective resistance is a principled relevance function, since the resulting graph G retains the main properties of G. In particular, we have that the spectra of L G and L G are related by</p><formula xml:id="formula_3">(1 ? )? G i ? ? G i ? (1 + )? G i : in short (1 ? )L G L G (1 + )L G . This is a direct result of the theorem since ? i = E(fi) f T i fi are the normalized minimal variabilities.</formula><p>This first result implies that edge sampling based on effective resistances (or commute times) is a principled way to rewire a graph while preserving its original structure. Next, we present what is a commute times embedding and how it can be spectrally computed.</p><p>Commute Times Embedding. The choice of effective resistances in Theorem 1 is explained by the fact that R uv can be computed from R uv = (e u ? e v ) T L + (e u ? e v ), where e u is the unit vector with a unit value at u and zero elsewhere.</p><formula xml:id="formula_4">L + = i?2 ? ?1 i f i f T i , where f i , ? i</formula><p>are the eigenvectors and eigenvalues of L, is the pseudo-inverse or Green's function of G = (V, E) if it is connected, and from the theorem we also have</p><formula xml:id="formula_5">(1 + ) ?1 L + G L + G (1 ? ) ?1 L + G .</formula><p>The Green's function leads to envision R uv (and therefore CT uv ) as metrics relating pairs of nodes of G. For instance R uv = L + uu + L + vv ? 2L + uv , is the resistance distance <ref type="bibr" target="#b36">[37]</ref> i.e., as noted by Qiu and Hancock <ref type="bibr" target="#b37">[38]</ref> the elements L + uv encode dot products between the embeddings z u and z v of u and v. As a result, the latent space can not only be described spectrally but also in a parameter free-manner, which is not the case for other spectral embeddings, such as heat kernel or diffusion maps as they rely on a time parameter t. More precisely, the embedding matrix Z whose columns contain the nodes' embeddings is given by:</p><formula xml:id="formula_6">Z := vol(G)? ?1/2 F T = vol(G)? ?1/2 G T D ?1/2</formula><p>(2) where ? is the diagonal matrix of the unnormalized Laplacian L eigenvalues and F is the matrix of their associated eigenvectors. Similarly, ? contains the eigenvalues of the normalized Laplacian L and G the eigenvectors. We have</p><formula xml:id="formula_7">F = GD ?1/2 or f i = g i D ?1/2 , where D is the degree matrix.</formula><p>Finally, the commute times are given by the Euclidean distances between the embeddings CT uv = z u ? z v 2 . Their spectral form is</p><formula xml:id="formula_8">R uv = CT uv vol(G) = n i=2 1 ? i (f i (u) ? f i (v)) 2 = n i=2 1 ? i g i (u) ? d u ? g i (v) ? d v 2<label>(3)</label></formula><p>Note how in Eq. 3 the commute times rely on the Fiedler vector f 2 (or g 2 ) downscaled by the spectral gap ? 2 (or more formally ? 2 ). The downscaled Fiedler vector dominates the expansion because the Fiedler vector is the solution to the relaxed ratio-cut problem. This is consistent with the fact that p?resistances become the inverse of mincut when p ? ?.</p><p>Commute Times as an Optimization Problem. In this section, we demonstrate how the CTs may be computed as an optimization problem by means of a differentiable layer in a GNN. Constraining neighboring nodes to have a similar embedding leads to</p><formula xml:id="formula_9">Z = arg min Z T Z=I u,v z u ? z v 2 A uv u,v Z 2 uv d u = (u,v)?E z u ? z v 2 u,v Z 2 uv d u = T r[Z T LZ] T r[Z T DZ] ,<label>(4)</label></formula><p>which reveals that CTs embeddings result from a Laplacian regularization down-weighted by the degree. As a result, frontier nodes or hubs -i.e. nodes with inter-community edges-which tend to have larger degrees than those lying inside their respective communities will be embedded far away from their neighbors, increasing the distance between communities. Note that the above quotient of traces formulation is easily differentiable and different from T r[ Z T LZ Z T DZ ] proposed in <ref type="bibr" target="#b37">[38]</ref>. With the above elements we define CT-LAYER, the first rewiring layer proposed in this paper. See <ref type="figure" target="#fig_2">Figure 2</ref> for a graphical representation of the layer.</p><p>Definition 1 (CT-Layer). Given the matrix X n?F encoding the features of the nodes after any message passing (MP) layer, Z n?O(n) = tanh(MLP(X)) learns the association X ? Z while Z is</p><formula xml:id="formula_10">optimized according to the loss L CT = T r[Z T LZ] T r[Z T DZ] + Z T Z Z T Z F ? I n F</formula><p>. This results in the following</p><formula xml:id="formula_11">resistance diffusion T CT = R(Z) A, i.e.</formula><p>the Hadamard product between the resistance distance and the adjacency matrix, providing as input to the subsequent MP layer a learnt convolution matrix.</p><p>Thus, CT-LAYER learns the CTs and rewires an input graph according to them: the edges with maximal resistance will tend to be the most important edges so as to preserve the topology of the graph. <ref type="bibr">CT</ref>  T CT and Graph Bottlenecks. Beyond the principled sparsification of T CT (enabled by Theorem 1), this layer rewires the graph G = (E, V ) in such a way that edges with maximal resistance will tend to be the most critical to preserve the topology of the graph. More precisely, although e?E R e = n ? 1, the bulk of the resistance distribution will be located at graph bottlenecks, if they exist. Otherwise, their magnitude is upper-bounded and the distribution becomes more uniform.</p><formula xml:id="formula_12">= [ ] [ ] + ? Pool -tanh A ? ? ? ( ) ? ? ? = cdist( ) ( ) ?A</formula><p>Graph bottlenecks are controlled by the graph's conductance or Cheeger constant,</p><formula xml:id="formula_13">h G = min S?V h S , where: h S = |?S| min(vol(S),vol(S)) , ?S = {e = (u, v) : u ? S, v ?S} and vol(S) = u?S d u .</formula><p>The interplay between the graph's conductance and effective resistances is given by:</p><formula xml:id="formula_14">Theorem 2 (Alev et al. [39]). Given a graph G = (V, E), a subset S ? V with vol(S) ? vol(G)/2, h S ? c vol(S) 1/2? ?? |?S| ? c ? vol(S) 1/2? ,<label>(5)</label></formula><p>for some constant c and ? [0, 1/2]. Then, R uv ? 1</p><formula xml:id="formula_15">d 2 u + 1 d 2 v ? 1 ?c 2 for any pair u, v.</formula><p>According to this theorem, the larger the graph's bottleneck, the tighter the bound on R uv are. Moreover, max(R uv ) ? 1/h 2 S , i.e., the resistance is bounded by the square of the bottleneck. This bound partially explains the rewiring of the graph in <ref type="figure" target="#fig_0">Figure 1</ref>-center. As seen in the Figure, rewiring using CT-LAYER sparsifies the graph and assigns larger weights to the edges located in the graph's bottleneck. The interplay between the above theorem and Theorem 1 is described in App. A.1.</p><p>Recent work has proposed using curvature for graph rewiring. We outline below the relationship between CTs and curvature.</p><p>Effective Resistances and Curvature. Topping et al. <ref type="bibr" target="#b20">[21]</ref> propose an approach for graph rewiring, where the relevance function is given by the Ricci curvature. However, this measure is nondifferentiable. More recent definitions of curvature <ref type="bibr" target="#b21">[22]</ref> have been formulated based on resistance distances that would be differentiable using our approach. The resistance curvature of an edge</p><formula xml:id="formula_16">e = (u, v) is ? uv := 2(p u + p v )/R uv where p u := 1 ? 1 2 u?w R uv is the node's curvature.</formula><p>Relevant properties of the edge resistance curvature are discussed in App. A.1.3, along with a related Theorem proposed in Devriendt and Lambiotte <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GAP-LAYER: Spectral Gap Optimization for Graph Rewiring</head><p>The right-hand side of the Lov?sz bound in Eq. 1 relies on the graph's spectral gap ? 2 , such that the larger the spectral gap, the closer the commute times would be to their non-informative regime. Note that the spectral graph is typically large in commonly observed graphs -such as communities in social networks which may be bridged by many edges <ref type="bibr" target="#b39">[40]</ref>-and, hence, in these cases it would be desirable to rewire the adjacency matrix A so that ? 2 is minimized.</p><p>In this section, we explain how to rewire the graph's adjacency matrix A to minimize the spectral gap. We propose using the gradient of ? 2 wrt each component of?. Then, we can compute these gradient either using Laplacians (L, with Fiedler ? 2 ) or normalized Laplacians (L, with Fiedler ? 2 ). We also present an approximation of the Fiedler vectors needed to compute those gradients, and propose computing them as a GNN Layer called the GAP-LAYER. A detailed schematic of GAP-LAYER is shown in <ref type="figure">Figure 3</ref>.</p><p>Ratio-cut (Rcut) Approximation. We propose to rewire the adjacency matrix, A, so that ? 2 is minimized. We consider a matrix? close to A that satisfiesLf 2 = ? 2 f 2 , where f 2 is the solution to the ratio-cut relaxation <ref type="bibr" target="#b40">[41]</ref>. Following <ref type="bibr" target="#b41">[42]</ref>, the gradient of ? 2 wrt each component of? is given by</p><formula xml:id="formula_17">??? 2 := T r (?L? 2 ) T ? ??L = diag(f 2 f T 2 )11 T ? f 2 f T 2<label>(6)</label></formula><p>where 1 is the vector of n ones; and [??? 2 ] ij is the gradient of ? 2 wrt? uv . The driving force of this gradient relies on the correlation f 2 f T 2 . Using this gradient to minimize ? 2 results in breaking the graph's bottleneck while preserving simultaneously the inter-cluster structure. We delve into this matter in App. A.2.</p><p>Normalized-cut (Ncut) Approximation. Similarly, considering now ? 2 for rewiring leads to</p><formula xml:id="formula_18">??? 2 := T r (?L? 2 ) T ? ??L = d g T 2? TD?1/2 g 2 1 T + d g T 2?D ?1/2 g 2 1 T +D ?1/2 g 2 g T 2D ?1/2<label>(7)</label></formula><p>where d is a n ? 1 vector including derivatives of degree wrt adjacency and related terms. This gradient relies on the Fiedler vector g 2 (the solution to the normalized-cut relaxation), and on the incoming and outgoing one-hop random walks. This approximation breaks the bottleneck while preserving the global topology of the graph <ref type="figure" target="#fig_0">(Figure 1-left)</ref>. More details and proof are included in App. A.2.</p><p>We present next an approximation of the Fiedler vector, followed by a proposed new layer in the GNN called the GAP-LAYER to learn how to minimize the spectral gap of the graph.</p><p>Approximating the Fiedler vector. Given that g 2 =D 1/2 f 2 , we can obtain the normalized-cut gradient in terms of f 2 . From <ref type="bibr" target="#b22">[23]</ref> we have that . Then the Fiedler vector f 2 is approximated by appyling a softmaxed version of Eq. 8 and considering the loss</p><formula xml:id="formula_19">f 2 (u) = +1/ ? n if u belongs to the first cluster ?1/ ? n if u belongs to the second cluster + O log n n (8) MLP -? A ? ? ?2 ? A?A = [ ] [ ] + ? 2 2 ( ) ? = ? 2 ? ? ? = ? ? ??? 2 = ? ? A + ?(? 2 ) 2 ??? 2 = 2 ? ? + (diag 2 2 ? 2 2 ) ? 2</formula><formula xml:id="formula_20">L F iedler = ? ? A F + ?(? * 2 ) 2 , where ? * 2 = ? 2 if</formula><p>we use the ratio-cut approximation (and gradient) and ? * 2 = ? 2 if we use the normalized-cut approximation and gradient. This returns? and the GAP diffusion T GAP =?(S) A results from minimizing L GAP := L Cut + L F iedler .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Discussion</head><p>In this section, we study the properties and performance of CT-LAYER and GAP-LAYER in a graph classification task with several benchmark datasets. To illustrate the merits of our approach, we compare CT-LAYER and GAP-LAYER with 3 state-of-the-art diffusion and curvature-based graph rewiring methods. Note that the aim of the evaluation is to shed light on the properties of both layers and illustrate their inductive performance, not to perform a benchmark comparison with all previously proposed graph rewiring methods. Baselines:. The first baseline architecture is based on MINCUT Pool <ref type="bibr" target="#b32">[33]</ref> and it is shown in <ref type="figure" target="#fig_4">Figure 4a</ref>. It is the base GNN that we use for graph classification without rewiring. MINCUT Pool layer learns (A n?n , X n?F ) ? (A k?k , X k?F ), being k &lt; n the new number of node clusters. The next two baselines are graph rewiring methods that belong to the same family of methods as DIFFWIRE, i.e. methods based on diffusion and curvature, namely DIGL (PPR) <ref type="bibr" target="#b24">[25]</ref> and SDRF <ref type="bibr" target="#b20">[21]</ref>. DIGL is a diffusion-based preprocessing method within the family of metric-based GSL approaches. We set the teleporting probability ? = 0.001 and is set to keep the same average degree for each graph. Once preprocessed with DIGL, the graphs are provided as input to the MinCut Pool (Baseline1) arquitecture. The third baseline model is SDRF, which performs curvature-based rewiring. SDRF is also a preprocessing method which has 3 parameters that are highly graph-dependent. We set these parameters to ? = 20 and C + = 0 for all experiments as per <ref type="bibr" target="#b20">[21]</ref>. The number of iterations is estimated dynamically according to 0.7 * |V | for each graph.</p><formula xml:id="formula_21">LINEAR CONV MINCUT CONV READOUT MLP X X X A X A X X Y<label>(</label></formula><p>Both DIGL and SDRF aim to preserve the global topology of the graph but require optimizing their parameters for each input graph via hyper-parameter search. In a graph classification task, this search is O(n 3 ) per graph. Details about the parameter tuning in these methods can be found in App. A.3.3.</p><p>To shed light on the performance and properties of CT-LAYER and GAP-LAYER, we add the corresponding layer in between Linear(X) * ? ? Conv1(A, X). We build 3 different models: CT-LAYER, GAP-LAYER (Rcut), GAP-LAYER (Ncut), depending on the layer used. For CT-LAYER, we learn T CT which is used as a convolution matrix afterwards. For GAP-LAYER, we learn T GAP either using the Rcut or the Ncut approximations. A schematic of the architectures is shown in <ref type="figure" target="#fig_4">Figure 4b</ref> and in App. A.3.2.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, we use in our experiments common benchmark datasets for graph classification. We select datasets both with features and featureless, in which case we use the degree as the node features. These datasets are diverse regarding the topology of their networks: The experiments support our hypothesis that rewiring based on CT-LAYER and GAP-LAYER improves the performance of the baselines on graph classification. Since both layers are differentiable, they learn how to rewire unseen graphs. The improvements are significant in graphs where social components arise (REDDITB, IMDBB, COLLAB), i.e. graphs with small world properties and power-law degree distributions with a topology based on hubs and authorities. These are graphs where bottlenecks arise easily and our approach is able to properly rewire the graphs. However, the improvements observed in planar or grid networks (MUTAG and PROTEINS) are more limited: the bottleneck does not seem to be critical for the graph classification task.</p><p>Moreover, our method performs better in graphs with featureless nodes than graphs with node features because it is able to leverage the information encoded in the topology of the graphs. Note that in attribute-based graphs, the weights of the attributes typically overwrite the graph's structure in the classification task, whereas in graphs without node features, the information is encoded in the graph's structure. App. A.3.4 contains an in-depth analysis of the latent space produced by each method.</p><p>CT-Layer vs GAP-Layer. The real-world datasets explored in this paper are characterized by mild bottlenecks from the perspective of the Lov?sz bound. For completion, we have included two synthetic datasets (SBM and Erd?s-R?nyi) where the Lov?sz bound is very restrictive. As a result, CT-LAYER is outperformed by GAP-LAYER in SBM. Note that the results on the synthetic datasets suffer from large variability. The smaller the graph's bottleneck, the more useful the CT-Layer is. Conversely, the larger the bottleneck, the more useful GAP-Layer is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we have proposed DIFFWIRE, a unified framework for graph rewiring that links the two components of the Lov?sz bound: CTs and the spectral gap. We have presented two novel, fully differentiable and inductive rewiring layers: CT-LAYER and GAP-LAYER. We have empirically evaluated these layers on benchmark datasets for graph classification with competitive results when compared to SoTA baselines, specially in graphs where the the nodes have no attributes and have small-world properties.</p><p>In future work, we plan to test our approach in other graph-related tasks and intend to apply DIFFWIRE to real-world applications, particularly in social networks, which have unique topology, statistics and direct implications in society.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>A. Arnaiz-Rodriguez and N. Oliver are supported by a nominal grant received at the ELLIS Unit Alicante Foundation from the Regional Government of Valencia in Spain (Convenio Singular signed with Generalitat Valenciana, Conselleria d'Innovaci?, Universitats, Ci?ncia i Societat Digital, Direcci?n General para el Avance de la Sociedad Digital). A. Arnaiz-Rodriguez is also funded by a grant by the Banc Sabadell Foundation. F. Escolano is funded by the project RTI2018-096223-B-I00 of the Spanish Government.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>In Appendix A we include a <ref type="table">Table with</ref> the notation used in the paper and we provide an analysis of the diffusion and its relationship with curvature. In Appendix B, we study in detail GAP-LAYER and the implications of the proposed spectral gradients. Appendix C reports statistics and characteristics of the datasets used in the experimental section, provides more information about the experiments results, describes additional experimental results, and includes a summary of the computing infrastructure used in our experiments.  The <ref type="table" target="#tab_2">Table 2</ref> summarizes the notation used in the paper.</p><formula xml:id="formula_22">Symbol Description G = (V, E) Graph = (Nodes, Edges) A Adjacency matrix: A ? R n?n X Feature matrix: X ? R n?F v Node v ? V or u ? V e Edge e ? E x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Analysis of Commute Times rewiring</head><p>First, we provide an answer to the following question:</p><p>Is resistance diffusion via T CT a principled way of preserving the Cheeger constant?</p><p>We answer the question above by linking Theorems 1 and 2 in the paper with the Lov?sz bound. The outline of our explanation follows three steps.</p><p>? Proposition 1: Theorem 1 (Sparsification) provides a principled way to bias the adjacency matrix so that the edges with the largest weights in the rewired graph correspond to the edges in graph's bottleneck. ? Proposition 2: Theorem 2 (Cheeger vs Resistance) can be used to demonstrate that increasing the effective resistance leads to a mild reduction of the Cheeger constant. ? Proposition 3: (Conclusion) The effectiveness of the above theorems to contain the Cheeger constant is constrained by the Lov?sz bound.</p><p>Next, we provide a thorough explanation of each of the propositions above. Proposition 1 (Biasing). Let G' = Sparsify(G, q) be a sampling algorithm of graph G = (V, E), where edges e ? E are sampled with probability q ? R e (proportional to the effective resistance). This choice is necessary to retain the global structure of G, i.e., to satisfy</p><formula xml:id="formula_23">?x ? R n : (1 ? )x T L G x ? x T L G x ? (1 + )x T L G x ,<label>(9)</label></formula><p>with probability at least 1/2 by sampling O(n log n/ 2 ) edges , with 1/ ? n &lt; ? 1, instead of O(m), where m = |E|. In addition, this choice biases the uniform distribution in favor of critical edges in the graph. Proof. We start by expressing the Laplacian L in terms of the edge-vertex incidence matrix B m?e :</p><formula xml:id="formula_24">B eu = 1 if u is the head of e ?1 if u is the tail of e 0 otherwise .<label>(10)</label></formula><p>where edges in undirected graphs are counted once, i.e. e = (u, v) = (v, u). Then, we have L = B T B = e b e b T e , where b e is a row vector (incidence vector) of B, with b e=(u,v) = (e u ?e v ). In addition, the Dirichlet energies can be expressed as norms:</p><formula xml:id="formula_25">E(x) = x T Lx = x T B T Bx = Bx 2 2 = e=(u,v)?E (x u ? x v ) 2 .<label>(11)</label></formula><p>As a result, the effective resistance R e between the two nodes of an edge e = (u, v) can be defined as</p><formula xml:id="formula_26">R e = (e u ? e v ) T L + (e u ? e v ) = b T e L + b e<label>(12)</label></formula><p>Next, we reformulate the spectral constraints in Eq. 9, i.e.</p><formula xml:id="formula_27">(1 ? )L G L G (1 + )L G as L G L G ?L G , ? = 1 + 1 ? .<label>(13)</label></formula><p>This simplifies the analysis, since the above expression can be interpreted as follows: </p><formula xml:id="formula_28">G L G L +/2 G L +/2 G L G L +/2 G L +/2 G ?L +/2 G ,<label>(14)</label></formula><p>which leads to I n L +/2</p><formula xml:id="formula_29">G L G L +/2 G ?I n .<label>(15)</label></formula><p>We seek a Laplacian L G satisfying the similarity constraints in Eq. 13. Since E ? E, i.e. we want to remove structurally irrelevant edges, we can design L G in terms of considering all the edges E:</p><formula xml:id="formula_30">L G := B T G B G = e s e b e b T e<label>(16)</label></formula><p>and let the similarity constraint define the sampling weights and the choice of e (setting s e ? 0 propertly). More precisely:</p><formula xml:id="formula_31">I n L +/2 G e b e b T e L +/2 G ?I n .<label>(17)</label></formula><p>Then if we define v e := L +/2 G b e as the projected incidence vector, we have</p><formula xml:id="formula_32">I n e s e v e v T e ?I n .<label>(18)</label></formula><p>Consequently, a spectral sparsifier must find s e ? 0 so that the above similarity constraint is satisfied.</p><p>Since there are m edges in E, s e must be zero for most of the edges. But, what are the best candidates to retain? Interestingly, the similarity constraint provides the answer. From Eq. 12 we have v T e v e = v e 2 = L</p><formula xml:id="formula_33">+/2 G b e 2 2 = b T e L + G b e = R e .<label>(19)</label></formula><p>This result explains why sampling the edges with probability q ? R e leads to a ranking of m edges of G = (V, E) such that edges with large R e = v e 2 are preferred 4 .</p><p>Algorithm 1 implements a deterministic greedy version of Sparsify(G, q), where we build incrementally E ? E by creating a budget of decreasing resistances R e1 ? R e2 ? . . . ? R e O(n log n/ 2 ) .</p><p>Note that this rewiring strategy preserves the spectral similarities of the graphs, i.e. the global structure of G = (V, E) is captured by G = (V, E ).</p><p>Moreover, the maximum R e in each graph determines an upper bound on the Cheeger constant and hence an upper bound on the size of the graph's bottleneck, as per the following proposition. </p><formula xml:id="formula_34">h G ? ? ? R diam ? vol(S) ?1/2 ,<label>(20)</label></formula><p>with 0 &lt; &lt; 1/2 and d u ? 1/? for all u ? V .</p><p>Proof. The fact that the maximum resistance R diam is located in an edge is derived from two observations: a) Resistance is upper bounded by the shortest-path distance; and b) edges with maximal resistance are prioritized in (Proposition 1).</p><p>Theorem 2 states that any attempt to increase the graph's bottleneck in a multiplicative way (i.e. multiplying it by a constant c ? 0) results in decreasing the effective resistances as follows:</p><formula xml:id="formula_35">R uv ? 1 d 2 u + 1 d 2 v ? 1 ? c 2<label>(21)</label></formula><p>with ? [0, 1/2]. This equation is called the resistance bound. Therefore, a multiplicative increase of the bottleneck leads to a quadratic decrease of the resistances.</p><p>Following Corollary 2 of <ref type="bibr" target="#b38">[39]</ref>, we obtain an upper bound of any h S , i.e. the Cheeger constant for S ? V with vol(S) ? vol(G)/2 -by defining c properly. In particular we are seeking a value of c that would lead to a contradiction, which is obtained by setting</p><formula xml:id="formula_36">c = 1 d 2 u * + 1 d 2 v * R diam ? ,<label>(22)</label></formula><p>where (u * , v * ) is a pair of nodes with maximal resistance, i.e. R u * v * = R diam .</p><p>Consider now any other pair of nodes (s, t) with R st &lt; R diam . Following Theorem 2, if the bottleneck of h S is multiplied by c, we should have</p><formula xml:id="formula_37">R st ? 1 d 2 s + 1 d 2 s ? 1 ? c 2 = 1 d 2 s + 1 d 2 s ? R diam 1 d 2 u * + 1 d 2 v * .<label>(23)</label></formula><p>However, since R diam ? 1</p><formula xml:id="formula_38">d 2 u * + 1 d 2 v *</formula><p>we have that R st can satisfy</p><formula xml:id="formula_39">R st &gt; 1 d 2 s + 1 d 2 s ? 1 ? c 2<label>(24)</label></formula><p>which is a contradiction and enables</p><formula xml:id="formula_40">h S ? c vol(S) 1/2? ?? |?S| ? c ? vol(S) 1/2? .<label>(25)</label></formula><p>Using c as defined in Eq. 22 and d u ? 1/? we obtain</p><formula xml:id="formula_41">c = 1 d 2 u * + 1 d 2 v * R diam ? ? ? R diam ? ? ? ? R diam ? .<label>(26)</label></formula><p>Therefore,</p><formula xml:id="formula_42">h S ? c vol(S) 1/2? ? ? ? R diam ? vol(S) 1/2? = ? ? R diam ? ? vol(S) ?1/2 .<label>(27)</label></formula><p>As a result, the Cheeger constant of G = (V, E) is mildly reduced (by the square root of the maximal resistance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 3 (Conclusion).</head><p>Let (u * , v * ) be a pair of nodes (may be not unique) in G = (V, E) with maximal resistance, i.e. R u * v * = R diam . Then, the Cheeger constant h G relies on the ratio between the maximal resistance R diam and its uninformative approximation 1</p><formula xml:id="formula_43">d * u + 1 d * v .</formula><p>The closer this ratio is to the unit, the easier it is to contain the Cheeger constant. Proof. The referred ratio above is the ratio leading to a proper c in Proposition 2. This is consistent with a Lov?sz regime where the spectral gap ? 2 has a moderate value. However, for regimes with very small spectral graphs, i.e. ? 2 ? 0, according to the Lov?sz bound, R diam</p><formula xml:id="formula_44">1 d * u + 1 d * v</formula><p>and hence the Cheeger constant provided by Proposition 2 will tend to zero.</p><p>We conclude that we can always find an moderate upper bound for the Cheeger constant of G = (V, E), provided that the regime of the Lov?sz bound is also moderate. Therefore, as the global properties of G = (V, E) are captured by G = (V, E ), a moderate Cheeger constant, when achievable, also controls the bottlenecks in G = (V, E ).</p><p>Our methodology has focused on first exploring the properties of the commute times / effective resistances in G = (V, E). Next, we have leveraged the spectral similarity to reason about the properties -particularly the Cheeger constant-of G = (V, E ). In sum, we conclude that resistance diffusion via T CT is a principled way of preserving the Cheeger constant of G = (V, E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 Resistance-based Curvatures</head><p>We refer to recent work by Devriendt and Lambiotte <ref type="bibr" target="#b21">[22]</ref> to complement the contributions of Topping et al. <ref type="bibr" target="#b20">[21]</ref> regarding the use of curvature to rewire the edges in a graph. Theorem 3 (Devriendt and Lambiotte <ref type="bibr" target="#b21">[22]</ref>). The edge resistance curvature has the following properties: (1) It is bounded by The new definition of curvature given in <ref type="bibr" target="#b20">[21]</ref> is related to the resistance distance and thus it is learnable with the proposed framework (CT-LAYER). Actually, the Balanced-Forman curvature (Definition 1 in <ref type="bibr" target="#b20">[21]</ref>) relies on the uniformative approximation of the resistance distance. <ref type="figure" target="#fig_8">Figure 5</ref> illustrates the relationship between effective resistances / commute times and curvature on an exemplary graph from the COLLAB dataset.</p><formula xml:id="formula_45">(4 ? d u ? d v ) ? ? uv ? 2/R uv ,</formula><p>As seen in the Figure, effective resistances prioritize the edges connecting outer nodes with hubs or central nodes, while the intra-community connections are de-prioritized. This observation is consistent with the aforementioned theoretical explanations about preserving the bottleneck while breaking the intra-cluster structure. In addition, we also observe that the original edges between hubs have been deleted o have been extremely down-weighted.</p><p>Regarding curvature, hubs or central nodes have the lowest node curvature (this curvature increases with the number of nodes in a cluster/community). Edge curvatures, which rely on node curvatures, depend on the long-term neighborhoods of the connecting nodes. In general, edge curvatures can be seen as a smoothed version -since they integrate node curvatures-of the inverse of the resistance distances.</p><p>We observe that edges linking nodes of a given community with hubs tend to have similar edgecurvature values. However, edges linking nodes of different communities with hubs have different edge curvatures ( <ref type="figure" target="#fig_8">Figure 5-right)</ref>. This is due to the different number of nodes belonging to each community, and to their different average degree inside their respective communities (property 1 of Theorem 3).</p><p>Finally, note that the range of edge curvatures is larger than that of resistance distances. The sparsifier transforms a uniform distribution of the edge weights into a less entropic one: in the example of <ref type="figure" target="#fig_8">Figure 5</ref> we observe a power-law distribution of edge resistances. As a result, ? uv := 2(p u +p v )/T CT uv becomes very large on average (edges with infinite curvature are not shown in the plot) and a log scale is needed to appreciate the differences between edge resistances and edge curvatures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Appendix B: GAP-LAYER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Spectral Gradients</head><p>The proposed GAP-LAYER relies on gradients wrt the Laplacian eigenvalues, and particularly the spectral gap (? 2 for L and ? 2 wrt L). Although the GAP-LAYER inductively rewires the adjacency matrix A so that ? 2 is minimized, the gradients derived in this section may also be applied for gap maximization.</p><p>Note that while our cost function L F iedler = ? ? A F + ?(? * 2 ) 2 , with ? * 2 ? {? 2 , ? 2 }, relies on an eigenvalue, we do not compute it explicitly, as its computation has a complexity of O(n 3 ) and would need to be computed in every learning iteration. Instead, we learn an approximation of ? 2 's eigenvector f 2 and use its Dirchlet energy E(f 2 ) to approximate the eigenvalue. In addition, since g 2 = D 1/2 f 2 , we first approximate g 2 and then approximate ? 2 from E(g 2 ).</p><p>Gradients of the Ratio-cut Approximation. Let A be the adjacency matrix of G = (V, E); and A, a matrix similar to the original adjacency but with minimal ? 2 . Then, the gradient of ? 2 wrt each component of? is given by</p><formula xml:id="formula_46">??? 2 := T r (?L? 2 ) T ? ??L = diag(f 2 f T 2 )11 T ? f 2 f T 2 ,<label>(28)</label></formula><p>where 1 is the vector of n ones; and [??? 2 ] ij is the gradient of ? 2 wrt? uv . The above formula is an instance of the network derivative mining mining approach <ref type="bibr" target="#b41">[42]</ref>. In this framework, ? 2 is seen as a function of? and ??? 2 , the gradient of ? 2 wrt?, comes from the chain rule of the matrix derivative T r (?L? 2 ) T ? ??L . More precisely,</p><formula xml:id="formula_47">?L? 2 := ?? 2 ?L = f 2 f T 2 ,<label>(29)</label></formula><p>is a matrix relying on an outer product (correlation). In the proposed GAP-LAYER, since f 2 is approximated by:</p><formula xml:id="formula_48">f 2 (u) = +1/ ? n if u belongs to the first cluster ?1/ ? n if u belongs to the second cluster ,<label>(30)</label></formula><p>i.e. we discard the O log n n from Eq. 30 (the non-liniarities conjectured in <ref type="bibr" target="#b22">[23]</ref>) in order to simplify the analysis. After reordering the entries of f 2 for the sake of clarity, f 2 f T 2 is the following block matrix:</p><formula xml:id="formula_49">f 2 f T 2 = 1/n ?1/n ?1/n 1/n whose diagonal matrix is diag(f 2 f T 2 ) = 1/n 0 0 1/n<label>(31)</label></formula><p>Then, we have ??? 2 = 1/n 1/n 1/n 1/n ? 1/n ?1/n ?1/n 1/n = 0 2/n 2/n 0</p><p>which explains the results in <ref type="figure" target="#fig_0">Figure 1</ref>-left: edges linking nodes belonging to the same cluster remain unchanged whereas inter-cluster edges have a gradient of 2/n. This provides a simple explanation for T GAP =?(S) A. The additional masking added by the adjacency matrix ensures that we do not create new links.</p><p>Gradients Normalized-cut Approximation. Similarly, using ? 2 for graph rewiring leads to the following complex expression:</p><formula xml:id="formula_51">??? 2 := T r (?L? 2 ) T ? ??L = d g T 2? TD?1/2 g 2 1 T + d g T 2?D ?1/2 g 2 1 T +D ?1/2 g 2 g T 2D ?1/2 .<label>(33)</label></formula><p>However, since g 2 = D 1/2 f 2 and f 2 = D ?1/2 g 2 , the gradient may be simplified as follows:</p><formula xml:id="formula_52">??? 2 := T r (?L? 2 ) T ? ??L = d f T 2D 1/2?T f 2 1 T + d f T 2D 1/2? f 2 1 T +D ?1/2 f 2 f T 2D ?1/2 .<label>(34)</label></formula><p>In addition, considering symmetry for the undirected graph case, we obtain:</p><formula xml:id="formula_53">??? 2 := T r (?L? 2 ) T ? ??L = 2d f T 2D 1/2? f 2 1 T +D ?1/2 f 2 f T 2D ?1/2 .<label>(35)</label></formula><p>where d is a n ? 1 negative vector including derivatives of degree wrt adjacency and related terms. The obtained gradient is composed of two terms.</p><p>The first term contains the matrixD 1/2? which is the adjacency matrix weighted by the square root of the degree; f T 2D 1/2? f 2 is a quadratic form (similar to a Dirichlet energy for the Laplacian) which approximates an eigenvalue ofD 1/2? . We plan to further analyze the properties of this term in future work.</p><p>The second term,D ?1/2 f 2 f T The Lov?sz bound was later refined by von Luxburg et al. <ref type="bibr" target="#b34">[35]</ref> via a new, tighter bound which replaces d min by d 2 min in Eq. 1. Given that ? 2 ? (0, 2], as the number of nodes in the graph (n = |V |) and the average degree increase, then R uv ? 1/d u + 1/d v . This is likely to happen in certain types of graphs, such as Gaussian similarity-graphs -graphs where two nodes are linked if the neg-exponential of the distances between the respective features of the nodes is large enough; -graphs -graphs where the Euclidean distances between the features in the nodes are ? ; and k?NN graphs with large k wrt n. The authors report a linear collapse of R uv with the density of the graph in scale-free networks, such as social network graphs, whereas a faster collapse of R uv has been reported in community graphs -congruent graphs with Stochastic Block Models (SBMs) <ref type="bibr" target="#b39">[40]</ref>.</p><p>Given the importance of the effective resistance, R uv , as a global measure of node similarity, the von Luxburg et al.'s refinement motivated the development of robust effective resistances, mostly in the form of p?resistances given by R p uv = arg min f { e?E r e |f e | p }, where f is a unit-flow injected in u and recovered in v; and r e = 1/w e with w e being the edge's weight <ref type="bibr" target="#b43">[44]</ref>. For p = 1, R p uv corresponds to the shortest path; p = 2 results in the effective resistance; and p ? ? leads to the inverse of the unweighted u-v-mincut 5 . Note that the optimal p value depends on the type of graph <ref type="bibr" target="#b43">[44]</ref> and p?resistances may be studied from the perspective of p?Laplacians <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>While R uv could be unbounded by minimizing the spectral gap ? 2 , this approach has received little attention in the literature of mathematical characterization of graphs with small spectral gaps <ref type="bibr" target="#b45">[46]</ref>[47], i.e., instead of tackling the daunting problem of explicitly minimizing the gap, researchers in this field have preferred to find graphs with small spectral gaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Appendix C: Experiments</head><p>In this section, we provide details about the graphs contained in each of the datasets used in our experiments, a detailed clarification about architectures and experiments, and, finally, report additional experimental results. <ref type="table" target="#tab_3">Table 3</ref> depicts the number of nodes, edges, average degree, assortativity, number of triangles, transitivity and clustering coefficients (mean and standard deviation) of all the graphs contained in each of the benchmark datasets used in our experiments. As seen in the <ref type="table">Table, the</ref>   In addition, <ref type="figure">Figure 6</ref> depicts the histograms of the average node degrees for all the graphs in each of the eight datasets used in our experiments. The datasets are also very diverse in terms of topology, corresponding to social networks, biochemical networks and meshes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Datasets statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 Training parameters</head><p>The value of the hyperparameters used in the experiments are the ones by default in the anonymous repository <ref type="bibr" target="#b5">6</ref> . We report average accuracies and standard deviation on 10 random iterations, using different 85/15 train-test stratified split (we do not perform hyperparameter search), training during 60 epochs and reporting the results of the last epoch for each random run. We have used an Adam optimizer, with a learning rate of 5e ? 4 and weight decay of 1e ? 4. In addition, the batch size used for the experiments are shown in   <ref type="bibr" target="#b3">(4)</ref> and it is nor computationally feasible optimize parameters for each specific graph. For DIGL, we use a fixed ? = 0.001 and based on keeping the same average degree for each graph, i.e., we use a different dynamically chosen for each graph in each dataset which maintain the same number of edges as the original graph. In the case of SDRF, the parameters define how stochastic the edge addition is (? ), the graph edit distance upper bound (number of iterations) and optional Ricci upper-bound above which an edge will be removed each iteration (C + ). We set the parameters ? = 20 (the edge added is always near the edge of lower curvature), C + = 0 (to force one edge is removed every iteration), and number of iterations dynamic according to 0.7 * |V |. Thus, we maintain the same number of edges in the new graph (? = 20 C + = 0), i.e., same average degree, and we keep the graph distance to the original bounded by 0.7 * |V |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.4 Latent Space Analysis</head><p>In this section, we analyze the latent space produced by the models that use MINCUTPOOL <ref type="figure">(Figure 7a</ref>), GAP-LAYER ( <ref type="figure">Figure 7b</ref>) and CT-LAYER <ref type="figure">(Figure 7c</ref>). We plot the output of the readout layer for each model, and then perform dimensionality reduction with TSNE.</p><p>Observing the latent space of the REDDIT-BINARY dataset <ref type="figure" target="#fig_14">(Figure 8</ref>), CT-LAYER creates a disperse yet structured latent space for the embeddings of the graphs. This topology in latent spaces show that this method is able to capture different topological details. The main reason is the expressiveness of the commute times as a distance metric when performing rewiring, which has been shown to be a optimal metric to measure node structural similarity. In addition, GAP-LAYER creates a latent space where, although the 2 classes are also separable, the embeddings are more compressed, due to a more aggressive -yet still informative-change in topology. This change in topology is due to the change in bottleneck size that GAP-LAYER applies to the graph. Finally, MINCUT creates a more squeezed and compressed embedding, where both classes lie in the same spaces and most of the graphs have collapsed representations, due to the limited expressiveness of this architecture. A.3.5 Computing infrastructure <ref type="table" target="#tab_6">Table 5</ref> summarizes the computing infrastructure used in our experiments. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>DiffWire. Left: Original graph. Center: Rewired graph after CT-LAYER. Right: Rewired graph after GAP-LAYER. Colors indicate the strength of the edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>-Layer ? ? ? n : 1 ? T ? T ? ? (1 + ) T Structure preservation: Dirichlet energies in new graph G? are bounded in (1??) of the Dirichlet energies of the original graph G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Detailed depiction of CT-LAYERBelow, we present the relationship between the CTs and the graph's bottleneck and curvature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 : 7 Definition 2 (</head><label>372</label><figDesc>GAP-LAYER (Rcut). For GAP-LAYER (Ncut), substitute ??? 2 by Eq. GAP-Layer). Given the matrix X n?F encoding the features of the nodes after any message passing (MP) layer, S n?2 = Softmax(MLP(X)) learns the association X ? S while S is optimized according to the lossL Cut = ? T r[S T AS] T r[S T DS] + S T S S T S F ? In ? 2 F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>CT-LAYER or GAP-LAYER GNN models used in the experiments. Left: MinCut Baseline model. Right: CT-LAYER or GAP-LAYER models, depending on what method is used for rewiring.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>E 2 2 u?w</head><label>22</label><figDesc>Features of node v: x ? X n Number of nodes: n = |V | F Number of features D Degree diagonal matrix where dv in Dvv dv Degree of node v vol(G) Sum of the degrees of the graph vol(G) = T r[D]L Laplacian: L = D ? A BSigned edge-vertex incidence matrix be Incidence vector:Row vector of B, with b e=(u,v) = (eu ? ev) ve Projected incidence vector: ve = L +/2 be ? Ratio ? = 1+ 1? Dirichlet Energy wrt L: E(x) := x T Lx L Normalized Laplacian: L = I ? D ?1/2 AD ?1/2 ? Eigenvalue matrix of L ? Eigenvalue matrix of L ?i i-th eigenvalue of L ?2Second eigenvalue of L:Spectral gap ? i i-th eigenvalue of L ? Second eigenvalue of L: Spectral gap F Matrix of eigenvectors of L G Matrix of eigenvectors of L fi i eigenvector of L f2 Second eigenvector of L: Fiedler vector gi i eigenvector of L g2 Second eigenvector of L: Fiedler vector A New Adjacency matrix E New edges Huv Hitting time between u and v CTuv Commute time: CTuv = Huv + Hvu Ruv Effective resistance: Ruv = CTuv/vol(G) Z Matrix of commute times embeddings for all nodes in G zu Commute time embedding of node u T CT Resistance diffusion S Cluster assignment matrix: S ? R n?2 T GAP GAP diffusion eu Unit vector with unit value at u and 0 elsewhere ???2 Gradient of ?2 wrt? [???2]ij Gradient of ?2 wrt?uv pu Node curvature: pu := 1 ? 1 Ruv ?uv Edge curvature: ?uv := 2(pu + pv)/Ruv A.1 Appendix A: CT-LAYER A.1.1 Notation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>the Dirichlet energies of L G are lower-bounded by those of L G and upper-bounded by ? times the energies of L G . Considering that the energies define hyper-ellipsoids, the hyper-ellipsoid associated with L G is between the hyper-ellipsoids of L G and ? times the L G . The hyper-ellipsoid analogy provides a framework to proof that the inclusion relationships are preserved under scaling: M L G M M L G M M ?L G M where M can be a matrix. In this case, if we set M := (L + G ) 1/2 = L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1 :</head><label>1</label><figDesc>GREEDYSparsify Input :G = (V, E), ? (1/ ? n, 1], n = |V | . Output :G = (V, E ) with E ? E such that |E | = O(n log n/ 2 ). L ? List({v e : e ? E}) Q ? Sort(L, descending, criterion= v e 2 ) Sort candidate edges by descending Resistance E ? ? I ? 0 n?n repeat v e ? pop(Q) Remove the head of the queue I ? I + v e v T e if I ?I n then E ? E ? {e} Update the current budget of edges else return G = (V, E ) until Q = ? Proposition 2 (Resistance Diameter). Let G' = Sparsify(G, q) be a sampling algorithm of graph G = (V, E), where edges e ? E are sampled with probability q ? R e (proportional to the effective resistance). Consider the resistance diameter R diam := max u,v R uv . Then, for the pair of (u, v) does exist an edge e = (u, v) ? E in G = (V, E ) such that R e = R diam . A a result the Cheeger constant of G h G is upper-bounded as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Left: Original graph with nodes colored as Louvain communities. Middle: T CT learnt by CT-LAYER with edges colors as node importance [0,1]. Right: Node and edge curvature: T CT using p u := 1 ? 1 2 u?w T CT uv and ? uv := 2(p u + p v )/T CT uv with edge an node curvatures as color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>with equality in the lower bound iff all incident edges to u and v are cut links; (2) It is upper-bounded by the Ollivier-Ricci curvature ? OR uv ? ? uv , with equality if (u, v) is a cut link; and (3) Forman-Ricci curvature is bounded as follows: ? F R uv /R uv ? ? uv with equality in the bound if the edge is a cut link.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2D? 1 / 2 ,</head><label>12</label><figDesc>downweights the correlation term for the Ratio-cut case f 2 f T 2 by the degrees as in the normalized Laplacian. This results in a normalization of the Fiedler vector: ?1/n becomes ? ? d u d v /n at the uv entry and similarly for 1/n, i.e. each entry contains the average degree assortativity. A.2.2 Beyond the Lov?sz Bound: the von Luxburg et al. bound</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>datasets are very diverse in their characteristics. In addition, we use two synthetic datasets with 2 classes: Erd?s-R?nyi with p 1 ? [0.3, 0.5] and p 2 ? [0.4, 0.8] and Stochastic block model (SBM) with parameters p 1 = 0.8, p 2 = 0.5, q 1 ? [0.1, 0.15] and q 2 ? [0.01, 0.1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Degree histogram of the average degree of all the graphs in each of the datasets.A.3.2 GNN architecturesFigure 7 shows the specific GNN architectures used in the experiments explained in section 4 in the manuscript. Although the specific calculation of T GAP and T CT are given in Theorems 2 and 1, we also provide a couple of pictures for a better intuition. Diagrams of the GNNs used in the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>REDDIT embeddings produced by GAP-LAYER (Ncut) CT-LAYER and MINCUT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on common graph classification benchmarks. Red denotes the best model row-wise and blue marks the runner-up. '*' means degree as node feature. 66.53 ? 4.47 76.02 ? 4.31 65.3 ? 7.7 78.45 ? 4.59 77.63 ? 4.96 76.00 ? 5.30 IMDB-B* 60.75 ? 7.03 59.35 ? 7.76 59.2 ? 6.9 69.84 ? 4.60 69.93 ? 3.32 68.80 ? 3.10</figDesc><table><row><cell></cell><cell>MinCutPool</cell><cell>DIGL</cell><cell>SDRF</cell><cell>CT-LAYER</cell><cell>GAP-LAYER (Rcut)</cell><cell>GAP-LAYER (Ncut)</cell></row><row><cell>REDDIT-B*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>COLLAB*</cell><cell>58.00 ? 6.22</cell><cell>57.51 ? 5.95</cell><cell>56.6 ? 10</cell><cell>69.87 ? 2.40</cell><cell>64.47 ? 4.07</cell><cell>65.89 ? 4.90</cell></row><row><cell>MUTAG</cell><cell>84.21 ? 6.34</cell><cell>85.00 ? 5.65</cell><cell>82.4 ? 6.8</cell><cell>86.05 ? 4.99</cell><cell>86.90 ? 4.00</cell><cell>86.90 ? 4.00</cell></row><row><cell>PROTEINS</cell><cell>74.84 ? 2.39</cell><cell>74.49 ? 2.88</cell><cell>74.4 ? 2.7</cell><cell>75.38 ? 2.97</cell><cell>75.03 ? 3.09</cell><cell>75.34 ? 2.10</cell></row><row><cell>SBM*</cell><cell>53.00 ? 9.90</cell><cell>56.93 ? 12.8</cell><cell>54.1 ? 7.1</cell><cell>81.40 ? 11.7</cell><cell>90.80 ? 7.00</cell><cell>92.26 ? 2.92</cell></row><row><cell>Erd?s-R?nyi*</cell><cell>81.86 ? 6.26</cell><cell>81.93 ? 6.32</cell><cell>73.6 ? 9.1</cell><cell>79.06 ? 9.89</cell><cell>79.26 ? 10.46</cell><cell>82.26 ? 3.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Notation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Dataset statistics. ? 62 2457 ? 6438 37.36 ? 44 12?10 4 ? 48?10 4 0.76 ? 0.21 0.89 ? 0.? 30 0.48 ? 0.20 0.51 ? 0.23</figDesc><table><row><cell></cell><cell>Nodes</cell><cell cols="2">Egdes AVG Degree</cell><cell>Triangles</cell><cell>Transitivity</cell><cell>Clustering</cell></row><row><cell cols="2">REDDIT-BINARY 429.6 ? 554</cell><cell>497.7 ? 622</cell><cell>2.33 ? 0.3</cell><cell cols="2">24 ? 41 0.01 ? 0.02 0.04 ? 0.06</cell></row><row><cell>IMDB-BINARY</cell><cell>19.7 ? 10</cell><cell>96.5 ? 105</cell><cell>8.88 ? 5.0</cell><cell cols="2">391 ? 868 0.77 ? 0.15 0.94 ? 0.03</cell></row><row><cell>COLLAB</cell><cell cols="5">74.5 08</cell></row><row><cell>MUTAG</cell><cell>2.2 ? 0.1</cell><cell>19.8 ? 5.6</cell><cell>2.18 ? 0.1</cell><cell cols="2">0.00 ? 0.0 0.00 ? 0.00 0.00 ? 0.00</cell></row><row><cell>PROTEINS</cell><cell>39.1 ? 45.8</cell><cell>72.8 ? 84.6</cell><cell>3.73 ? 0.4</cell><cell>27.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Regarding the synthetic datasets, the parameters are: Erd?s-R?nyi with p 1 ? [0.3, 0.5] and p 2 ? [0.4, 0.8] and Stochastic block model (SBM) p 1 = 0.8, p 2 = 0.5, q 1 ? [0.1, 0.15] and q 2 ? [0.01, 0.1].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Dataset Batch size number of graphs are huge</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Computing infrastructure.</figDesc><table><row><cell>Component</cell><cell>Details</cell></row><row><cell>GPU</cell><cell>2x A100-SXM4-40GB</cell></row><row><cell>RAM</cell><cell>1 TiB</cell></row><row><cell>CPU</cell><cell>255x AMD 7742 64-Core @ 2.25 GHz</cell></row><row><cell>OS</cell><cell>Ubuntu 20.04.4 LTS</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A graph bottleneck is defined a a topological property of the graph that leads to over-squashing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use commute times and effective resistances interchangeably as per their use in the literature</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://anonymous.4open.science/r/DiffWireLoG22/readme.md</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Although some of the elements of this section are derived from<ref type="bibr" target="#b42">[43]</ref>, we note that the Nikhil Srivastava's lectures at The Simons Institute (2014) are by far more clarifying.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The link between CTs and mincuts is leveraged in the paper as an essential element of our approach.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://anonymous.4open.science/r/DiffWireLoG22/readme.md</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our experiments also use 2 preprocessing methods DIGL and SDRF. Unlike our proposed methods, both SDRF <ref type="bibr" target="#b20">[21]</ref> and DIGL <ref type="bibr" target="#b24">[25]</ref> use a set of hyperparamerters to optimize for each specific graph, because both are also not inductive. This approach could be manageable for the task of node classification, where you only have one graph. However, when it comes to graph classification, the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/1555942.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE international joint conference on neural networks</title>
		<meeting>2005 IEEE international joint conference on neural networks</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/4700287.1" />
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJU4ayYgl.1" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML</title>
		<meeting>the 34th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="http://bayesiandeeplearning.org/2016/papers/BDL_16.pdf.1" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/10179.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/8916.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/9046288.1" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ.1" />
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How attentive are graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaked</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=F72ximsx7C1.1" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km.1" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf.1" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/11604.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=i80OPhOCVH2.1" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1812.08434</idno>
		<ptr target="http://arxiv.org/abs/1812.08434" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random walks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?szl?</forename><surname>Lov?sz</surname></persName>
		</author>
		<ptr target="https://web.cs.elte.hu/~lovasz/erdos.pdf.2" />
	</analytic>
	<monogr>
		<title level="j">Combinatorics, Paul erdos is eighty</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The logical expressiveness of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Barcel?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Egor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Kostylev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Monet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Reutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silva</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1lZ7AEKvB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Message passing all the way up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bc8GiEZkTe5.2" />
	</analytic>
	<monogr>
		<title level="m">ICLR 2022 Workshop on Geometrical and Topological Representation Learning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hkx1qkrKPr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Differentiable graph module (dgm) for graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anees</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cosmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/9763421" />
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding over-squashing and bottlenecks on graphs via curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Paul</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note>id=7UmjRGzp-A. 2, 3, 7, 8</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Discrete curvature on graphs from the effective resistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Devriendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Lambiotte</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2201.06385</idno>
		<idno type="arXiv">arXiv:2201.06385</idno>
		<ptr target="https://arxiv.org/abs/2201.06385.2" />
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Revisiting graph neural networks: Graph filtering perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Nt Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsuyoshi</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murata</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/9412278.3" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1ldO2EFPr.3" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/23c894276a2c5a16470e6a31f4618d73-Paper.pdf.3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<ptr target="https://arxiv.org/abs/1806.01261.3" />
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sign: Scalable inception graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<ptr target="https://grlplus.github.io/papers/77.pdf.3" />
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DropGNN: Random dropouts increase the expressiveness of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolis</forename><surname>P?l Andr?s Papp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Martinkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wattenhofer</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=fpQojkIV5q8.3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Measuring and relieving the oversmoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i04.5747</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/5747.3" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2103.03036.3" />
		<title level="m">A survey on graph structure learning: Progress and opportunities</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv PrePrint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking pooling in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kaski</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1764183ef03fc7324eb58c3842bd9a57-Paper.pdf.3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/file/e77dbaf6759253c7c6d0efc5690369c7-Paper.pdf.3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alippi</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v119/bianchi20a.html.3,8" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Spectral Graph Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<ptr target="https://www.bibsonomy.org/bibtex/295ef10b5a69a03d8507240b6cf410f8a/folke.4" />
		<imprint>
			<date type="published" when="1997" />
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hitting and commute times in large random neighborhood graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnes</forename><surname>Ulrike Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Radl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hein</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/vonluxburg14a.html.4" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">52</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph sparsification by effective resistances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
		<idno type="DOI">10.1137/080734029</idno>
		<ptr target="https://doi.org/10.1137/080734029.5" />
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1913" to="1926" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Resistance distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Randi?</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF01164627</idno>
		<ptr target="https://doi.org/10.1007/BF01164627.5" />
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Chemistry</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="95" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Clustering and embedding using commute times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaijun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2007.1103</idno>
		<ptr target="https://ieeexplore.ieee.org/document/4302755.5" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph Clustering using Effective Resistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Vedat Levi Alev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Lap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oveis Gharan</surname></persName>
		</author>
		<idno type="DOI">10.4230/LIPIcs.ITCS.2018.41</idno>
		<ptr target="http://drops.dagstuhl.de/opus/volltexte/2018/8369.6" />
	</analytic>
	<monogr>
		<title level="m">9th Innovations in Theoretical Computer Science Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>ITCS 2018</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Community detection and stochastic block models: Recent developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v18/16-480.html.7" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">177</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spectral clustering based on the graph p-laplacian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>B?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553385</idno>
		<ptr target="https://doi.org/10.1145/1553374.1553385" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">N2n: Network derivative mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357384.3357910</idno>
		<idno>9781450369763. doi: 10.1145/ 3357384.3357910</idno>
		<ptr target="https://doi.org/10.1145/3357384.3357910" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spectral sparsification of graphs: Theory and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Hua</forename><surname>Teng</surname></persName>
		</author>
		<idno type="DOI">10.1145/2492007.2492029</idno>
		<ptr target="https://doi.org/10.1145/2492007.2492029.15" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="87" to="94" />
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Phase transition in the family of p-resistances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morteza</forename><surname>Alamgir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Luxburg</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2011/file/07cdfd23373b17c6b337251c22b7ea57-Paper.pdf.19" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Phase transition in the family of p-resistances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morteza</forename><surname>Alamgir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Luxburg</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2011/file/07cdfd23373b17c6b337251c22b7ea57-Paper.pdf.19" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Edge connectivity and the spectral gap of combinatorial and quantum graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Berkolaiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delio</forename><surname>Kurasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mugnolo</surname></persName>
		</author>
		<idno type="DOI">10.1088/1751-8121/aa8125</idno>
		<ptr target="https://doi.org/10.1088/1751-8121/aa8125.19" />
	</analytic>
	<monogr>
		<title level="j">Journal of Physics A: Mathematical and Theoretical</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">36</biblScope>
			<biblScope unit="page">365201</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graphs with small spectral gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Stani?</surname></persName>
		</author>
		<ptr target="https://journals.uwyo.edu/index.php/ela/article/view/1259.19" />
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Linear Algebra</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
