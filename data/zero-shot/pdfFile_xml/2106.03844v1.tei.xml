<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mean-Shifted Contrastive Loss for Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Reiss</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mean-Shifted Contrastive Loss for Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep anomaly detection methods learn representations that separate between normal and anomalous samples. Very effective representations are obtained when powerful externally trained feature extractors (e.g. ResNets pre-trained on Ima-geNet) are fine-tuned on the training data which consists of normal samples and no anomalies. However, this is a difficult task that can suffer from catastrophic collapse, i.e. it is prone to learning trivial and non-specific features. In this paper, we propose a new loss function which can overcome failure modes of both center-loss and contrastive-loss methods. Furthermore, we combine it with a confidence-invariant angular center loss, which replaces the Euclidean distance used in previous work, that was sensitive to prediction confidence. Our improvements yield a new anomaly detection approach, based on Mean-Shifted Contrastive Loss, which is both more accurate and less sensitive to catastrophic collapse than previous methods. Our method achieves state-of-the-art anomaly detection performance on multiple benchmarks including 97.5% ROC-AUC on the CIFAR-10 dataset 1 . <ref type="bibr" target="#b0">1</ref> The code is available at github.com/talreiss/Mean-Shifted-Anomaly-Detection arXiv:2106.03844v1 [cs.CV] 7 Jun 2021 functions of the data. Although past methods <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> attempted to mitigate this collapse by various techniques such as: architectural modifications, auxiliary tasks with extra supervision and continual learning, they do not solve the problem of catastrophic collapse.</p><p>Our contributions: we propose several advances that significantly improve the accuracy of anomaly detection and reduce catastrophic collapse. i) We introduce a new loss function that relies on ideas from contrastive learning for visual recognition tasks. Unlike the standard contrastive learning, where the angular distances are measured in relation to the origin, we measure the angular distances in relation to the normalized center of the extracted features. We show that this modification is crucial for achieving strong performance for adapting features in the OCC setting. ii) We find that learning a deep representation and rescaling it to the unit sphere provides a substantial boost in accuracy. This constraint dramatically improves performance across fine-tuning settings. We show that this is related to disambiguating semantics from confidence. iii) we demonstrate a limitation of our proposed mean-shifted contrastive loss, and show that the combination of our new loss with an angular center loss solves this limitation. In extensive experiments we demonstrate that our method is able to both achieve the top anomaly detection performance (e.g. 97.5% ROC-AUC on CIFAR-10) and nearly entirely eliminates catastrophic collapse.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Anomaly detection is a fundamental task for intelligent agents that aims to detect if an observed pattern is normal or anomalous (unusual or unlikely). Anomaly detection has broad applications in scientific and industrial tasks such as detecting new physical phenomena (black holes, supernovae) or genetic mutations, as well as production line inspection and video surveillance. Due to the significance of the task, many efforts have been focused on automatic anomaly detection, particularly on statistical and machine learning methods. A common paradigm used by many anomaly detection methods is measuring the probability of samples and assigning high-probability samples as normal and low-probability samples as anomalous. The quality of the density estimators is closely related to the quality of features used to represent the data. Classical methods used statistical estimators such as K nearest-neighbors (kNN) or Gaussian mixture models (GMMs) on raw features, however this often results in sub-optimal results on high-dimensional data such as images. Many recent methods, learn features in a self-supervised way and use them in order to detect anomalies. Their main weakness is that anomaly detection datasets are typically small and do not include anomalous samples resulting in weak features. An alternative direction, which achieved better results, is to transfer features learned from auxiliary tasks on large-scale external datasets such as ImageNet classification. It was found that fine-tuning the pre-trained features on the normal training data can result in significant performance improvements, however it is quite challenging. The main issue with fine-tuning on one-class classification (OCC) tasks such as anomaly detection is catastrophic collapse i.e. after an initial improvement in efficacy, the features degrade and become uninformative. This phenomenon is caused by trivial solutions allowed by OCC tasks such as the center-loss used by PANDA <ref type="bibr" target="#b0">[1]</ref> and Deep-SVDD <ref type="bibr" target="#b1">[2]</ref>, which can achieve perfect score by either ignoring the data or by learning simple 2 Related Work Classical anomaly detection methods: Detecting anomalies in images has been researched for several decades. The methods follow three main paradigms: i) Reconstruction -this paradigm first attempts to characterize the normal data by a set of basis functions and then attempts to reconstruct a new example using these basis functions, typically under some constraint such as sparsity or weights with small norm. Samples with high reconstruction errors are atypical of the normal data distribution and are denoted anomalous. Some notable methods include: principal component analysis <ref type="bibr" target="#b3">[4]</ref> and K nearest neighbors (kNN) <ref type="bibr" target="#b4">[5]</ref>. ii) Density estimation -another paradigm is to first estimate the density of normal data. A new test sample is denoted as anomalous if its estimated density is low. Parametric density estimation methods include Ensembles of Gaussian Mixture Models (EGMM) <ref type="bibr" target="#b5">[6]</ref>, and non-parametric methods include kNN (which is also a reconstruction-based method) as well as kernel density estimation <ref type="bibr" target="#b6">[7]</ref>. Both types of methods have weaknesses: parametric methods are sensitive to the parametric assumptions about the nature of the data whereas non-parametric methods suffer from the difficulty to accurately estimate density in high-dimensions. iii) One-class classification (OCC) -this paradigm attempt to fit a parametric classifier to distinguish between normal training data and all other data. The classifier is then used to classify new samples as normal or anomalous. Such methods include one-class support vector machine (OCSVM) <ref type="bibr" target="#b7">[8]</ref> and support vector data description (SVDD) <ref type="bibr" target="#b8">[9]</ref>.</p><p>Self-supervised deep learning methods: Instead of using supervision for learning deep representations, self-supervised methods train neural networks to solve an auxiliary task for which obtaining data is free or at least very inexpensive. Auxiliary tasks for learning high-quality image features include: video frame prediction <ref type="bibr" target="#b9">[10]</ref>, image colorization <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> and puzzle solving <ref type="bibr" target="#b12">[13]</ref>. RotNet <ref type="bibr" target="#b13">[14]</ref> used a set of image processing rotations around the image axis, and predicted the true image orientation to learn high-quality image features. GEOM <ref type="bibr" target="#b14">[15]</ref>, have used similar image-processing task prediction for detecting anomalies in images. This method was improved by <ref type="bibr" target="#b15">[16]</ref>, and extended to tabular data by <ref type="bibr" target="#b16">[17]</ref>. Another commonly used self-supervised paradigm is contrastive learning <ref type="bibr" target="#b17">[18]</ref>, which learns representations by distinguishing similar views (augmentations) of the same samples from other data samples. Recently, variants of contrastive learning were also introduced to OCC. CSI <ref type="bibr" target="#b18">[19]</ref> treats augmented input as positive samples and the distributionally-shifted input as negative samples. DROC <ref type="bibr" target="#b19">[20]</ref> shares a similar technical formulation as CSI without any test-time augmentation nor ensemble of models.</p><p>Feature adaptation for one-class classification: This line of work is based on the idea of initializing a neural network with pre-trained weights and then obtaining stronger performance by further adaptation of the training data. Although feature adaptation has been extensively studied in the multiclass classification setting, limited work was done in the OCC setting. DeepSVDD <ref type="bibr" target="#b1">[2]</ref> suggested to first train an auto-encoder on the normal training data, and then using the encoder as the initial feature extractor. Moreover, since the features of the encoder are not specifically fitted to anomaly detection, DeepSVDD adapts on the encoder training data. However, this naive training procedure leads to catastrophic collapse. An alternative direction, is to use features learned from auxiliary tasks on large-scale external datasets such as ImageNet classification. Deep features representations trained on the ImageNet dataset have been shown by <ref type="bibr" target="#b20">[21]</ref> to significantly boost performance on other datasets that are only vaguely related to some of the ImageNet classes. Transferring ImageNet pre-trained features for out-of-distribution detection has been proposed by <ref type="bibr" target="#b15">[16]</ref>. Analogous pre-training for OCC has been proposed by <ref type="bibr" target="#b2">[3]</ref>, where they jointly train anomaly detection with the original task, which achieves only limited adaptation success. PANDA <ref type="bibr" target="#b0">[1]</ref> proposed techniques based on early stopping and EWC <ref type="bibr" target="#b21">[22]</ref>, a continual learning method, to mitigate catastrophic collapse. Although PANDA achieved state-of-the-art performance on most datasets, it has yet to solve the problem of catastrophic collapse.</p><p>3 Background: Learning Representations for One-Class Classification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>In the one-class classification task, we are given a set of training samples that are all normal (and contain no anomalies) x 1 , x 2 ..x N ? X train . The objective is to classify a new sample x as being normal or anomalous. The methods considered here learn a deep representation of a sample parametrized by the neural network function ? :</p><formula xml:id="formula_0">X ? R d , where d ? N is the feature dimension.</formula><p>In several methods, ? is initialized by pre-trained weights ? 0 , which can be learned either using external datasets (e.g. ImageNet classification) or using self-supervised tasks on the training set (e.g. RotNet <ref type="bibr" target="#b13">[14]</ref>). The representation is further tuned on the training data to form the final representation ?. Finally, an anomaly scoring function s(x) which utilizes the representation of the sample ?(x) and predicts how anomalous the sample x is. The binary anomaly classification can be predicted by applying a threshold on s(x). In Sec. 3.2 and Sec. 3.3, we review the most relevant methods for learning the representation ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Center Loss</head><p>One simple method for adapting an initial feature extractor ? 0 for OCC is using a center loss. The idea simply states that the features are adapted such that the training data, which consist only of normal samples, lie as near as possible to the center. Specifically, the center loss for an input sample x ? X train can be written as follows:</p><formula xml:id="formula_1">L center (x) = ?(x) ? c 2<label>(1)</label></formula><p>The feature extractor ? is initialized with some pre-trained feature extractor ? 0 (an ImageNet pretrained ResNet was shown by <ref type="bibr" target="#b0">[1]</ref> to be very effective). The center can be set to be the mean of the training set pre-trained feature representation:</p><formula xml:id="formula_2">c = E x?Xtrain [? 0 (x)]<label>(2)</label></formula><p>This loss suffers from catastrophic collapse, i.e., the discriminative properties of the pre-trained extractor ? 0 may be lost due to training on the simple center loss task. After an initial improvement in efficacy, the features degrade and become uninformative due to a trivial solution where ?(x) = c independently of the sample x. Such a representation cannot, of course, discriminate between normal and anomalous samples.</p><p>DeepSVDD <ref type="bibr" target="#b1">[2]</ref> suggested various architectural modifications to improve the utilization of the center loss. However, the proposed architectures were too constrained. PANDA <ref type="bibr" target="#b0">[1]</ref> showed that the DeepSVDD feature adaptation does not perform better than linear post-processing of the initial feature extractor ? 0 . Instead, they use the center loss without any architectural modifications, and propose techniques based on early stopping and continual learning to mitigate catastrophic collapse. While achieving state-of-the-art performance on most datasets, this does not completely resolve the issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-supervised Auxiliary Tasks</head><p>An alternative line of work proposed to first learn the representations ? by using state-of-the-art self-supervised learning techniques. Although earlier works were based on RotNet ideas <ref type="bibr" target="#b13">[14]</ref>, top performing methods are now based on contrastive learning. In the contrastive training procedure a mini-batch of size N is randomly sampled and the contrastive prediction task is defined on pairs of augmented examples derived from the mini-batch, resulting in 2N data points. The typical contrastive loss for a positive pair (i , i ), where x i and x i are augmentations of x ? X train , is written below:</p><formula xml:id="formula_3">L con (x i , x i ) = ? log exp((?(x i ) ? ?(x i ))/? ) 2N m=1 1[i = m] ? exp((?(x i ) ? ?(x m ))/? )<label>(3)</label></formula><p>where ?m ? [2N ] : x m is an augmented view of some x ? X train , ? denotes a temperature hyper-parameter and it holds that ?x ? X train : ||?(x)|| = 1. Augmentations include crops, flips, color jitter, grayscale and Gaussian blurs. The contrastive loss was shown by <ref type="bibr" target="#b22">[23]</ref> to stimulate two properties i) uniform distribution of {?(x)} x?Xtrain across the unit sphere ii) different augmentations of the same sample are mapped to the same representation.</p><p>Constrative methods currently achieve the top performance for anomaly detection without utilization of externally trained network weights. We remind the reader that PANDA uses ImageNet pre-trained weights while contrastive loss methods, such as CSI <ref type="bibr" target="#b18">[19]</ref>, only use the normal training data. The fact that contrastive losses achieve better self-supervised anomaly detection performance than the center loss might suggest that combining pre-trained features with the contrastive methods can result in further improvement over PANDA. However, we empirically find that this is not the case, rather it results in very fast catastrophic collapse and no improvement over the pre-trained feature extractor ? 0 . We will present a new loss function: Mean-Shifted Contrastive Loss to overcome this limitation in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Mean-Shifted Contrastive Loss</head><p>In this section, we introduce our new approach for OCC feature adaptation. In Sec 4.1 we present our new loss function, the mean-shifted contrastive loss, where we operate in the angular space with respect to the extracted features center. In Sec 4.2 we explain the advantages of using the angular distance as a metric in place of the Euclidean distance and leverage it to propose the angular center loss. In Sec 4.3 we combine the above to construct our final approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mean-Shifted Loss: Modifying the Contrastive Loss for OCC Transfer Learning</head><p>Previous contrastive learning methods were mostly evaluated by their ability to perform interclass separation. While contrastive methods have achieved state-of-the-art performance on visual recognition tasks, they are not apriori designed for OCC feature adaptation. In order to minimize the contrastive loss, the angles between representations of negative pairs x, y ? X train need to be maximized, even though x, y are both normal samples, i.e., from the same class. By maximizing these angles, the distance to the normalized center increases as well, as illustrated in <ref type="figure" target="#fig_0">Fig. 1 (top)</ref>. This behaviour is in contrast to the optimization of the center loss (Eq. 1), which learns representations by minimizing the Euclidean distance between normal representations and the center. PANDA <ref type="bibr" target="#b0">[1]</ref> has shown that the optimized center loss results in high anomaly detection performance. Furthermore, forcing the representations of the normal training data to be uniformly distributed on the unit sphere is not well suited for OCC, since every anomalous samplex / ? X train will have a nearby normal sample x ? X train .</p><p>We propose a modification to the contrastive loss that alleviates these issues. Instead of measuring the angular distance between samples in relation to the origin, we measure the angular distance in relation to the normalized center of the normal features. As can be seen in <ref type="figure" target="#fig_0">Fig. 1 (bottom)</ref>, in our proposed mean-shifted representation the contrastive loss maximizes the angles between the negative pairs while maintaining their distance to the normalized center. Our new loss function is named -Mean-Shifted Contrastive Loss.</p><p>By a slight abuse of notation, let us denote the normalized center of the feature representation of the training set by c (that is, the 2 normalized form of Eq. 2). For each image x, we create two different augmentations of the image, denoted x , x . All images are passed through a feature extractor, then, as in most contrastive learning methods, the extracted features are scaled to the unit sphere (by 2 normalization) resulting in their respective ? feature representations. For each representation, we define its mean-shifted counterpart, by subtracting the center and normalizing to the unit sphere. For a sample x, its mean-shifted representation is defined:</p><formula xml:id="formula_4">?(x) = ?(x) ? c ?(x) ? c<label>(4)</label></formula><p>The mean-shifted loss for two augmentations x , x of image x from an augmented mini-batch of size 2N is defined as follows:</p><formula xml:id="formula_5">L msc (x , x ) = L con (?(x ), ?(x )) = ? log exp((?(x ) ? ?(x ))/? ) 2N i=1 1[x i = x ] ? exp((?(x ) ? ?(x i ))/? )<label>(5)</label></formula><p>where ? denotes a temperature hyper-parameter. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the training process. Since the mean-shifted representation operates in the space of angles around c, it does not directly encourage increasing the distance between the center and the features of the training images. Furthermore, the loss forms a uniform distribution around the center of normal data, rather than forming a uniform distribution of the normal data on the unit sphere, thus allowing discrimination of anomalies (which will often be farther from the center). This behaviour mostly eliminates the collapse encountered when adapting ? using the standard contrastive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Angular Center Loss for Anomaly Detection</head><p>One limitation of the mean-shifted loss is that although it does not directly encourage increasing the distance of features of training images from the center, it might still do so indirectly. Specifically, if the Euclidean distance between ?(x ) and c is small, the angle ?(x ) is very susceptible to changes in ?(x ). Indeed, a small change in ?(x ) would induce a large change in the angle, since it induces sensitive behaviour. This may eventually increase the distance to c. In accordance to the intuition of the center loss used by DeepSVDD <ref type="bibr" target="#b1">[2]</ref> and PANDA <ref type="bibr" target="#b0">[1]</ref>, we expect that having the normal data lie in a small region around the center will be more discriminative than when it is allowed to occupy larger distances. We propose to support the shrinkage of the distance of normal samples from the center by adding a center loss.</p><p>Breaking away from previous deep learning approaches that used the center loss, we propose to use the angular center loss. The angular center loss encourages the angular distance between each sample and the center to be minimal. This contrasts with the standard center loss, which opts towards minimal Euclidean distance to the center. Although a simple change, the angular center loss achieves much better results than the regular center loss (see Sec. 5.3). We remind the reader that our feature extractor ? yields unit vectors, and that c is the normalized center. Therefore, the angular center loss is defined as: In what follows we analyse the reasons for the strong performance of the angular center loss. Our initial feature extractor ? 0 is pre-trained on a classification task (specifically ImageNet classification). To obtain class probabilities from the features ? 0 (x), which are subsequently multiplied by classifier matrix C and passed through a softmax layer. The logits are therefore given by C ? ? 0 (x). As softmax is a monotonic function, scaling of the logits does not change the order of probabilities. However, scaling does determine the degree of confidence in the decision. We propose to disambiguate the representation ? 0 (x) into two components: i) the semantic class ?0(x) ?0(x) , and the confidence ? 0 (x) . The confidence acts as a per-sample temperature that determines how confident the discrimination between the classes is. In <ref type="figure" target="#fig_2">Fig. 3</ref>, we compare the histogram of confidence values between the normal and anomalous values on a particular class of the CIFAR-10 dataset ("Bird"). We observe that confidence does not discriminate between normal and anomalous images in this dataset. A thorough investigation that we conducted, showed that the confidence of an ImageNet pre-trained feature representation did not help the anomaly detection performance. It is possible that this is partially caused by the protocol used to convert multi-class datasets into anomaly detection datasets. This motivates the angular center loss, which is only sensitive to the semantic similarity with the center of the normal images, and does not use classifier confidence, which acts as a nuisance factor.</p><formula xml:id="formula_6">L angular = ??(x) ? c<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The best of both worlds: A new feature adaptation approach for OCC</head><p>Our entire approach is formed by combining the two losses: i) the mean-shifted contrastive loss (Sec. 4.1) ii) the angular center loss (Sec. <ref type="bibr">4.2)</ref>.</p><formula xml:id="formula_7">min ? L f inal = L msc + L angular<label>(7)</label></formula><p>This combination allows us to enjoy the best of both worlds and alleviates some of the limitations of each loss. The ability of the mean-shifted loss to arbitrarily increase the feature angular distance of the normal samples from the center is kept in check by the angular center loss. On the other hand, the trivial solution of the angular center loss, where ?(x) = c independently of the sample x is mitigated by the combination with the mean-shifted contrastive loss. Therefore, the combination of the two This should be compared to the collapse encountered after less than ten epochs for the standard contrastive loss, and after tens of epochs for the center loss in PANDA <ref type="bibr" target="#b0">[1]</ref>.</p><p>Anomaly criterion: In order to classify a sample as normal or anomalous, we use a simple criterion based on kNN using the cosine distance. We first compute the cosine distance between the features of the target image x and those of all training images. The anomaly score is given by:</p><formula xml:id="formula_8">s(x) = ?(y)?N k (x) 1 ? ?(x) ? ?(y)<label>(8)</label></formula><p>where N k (x) denotes the k nearest features to ?(x) in the training feature set {?(z)} z?Xtrain . By checking if the anomaly score s(x) is larger than a threshold, we determine if the image x is normal or anomalous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we extensively evaluate our method and demonstrate that it outperforms the state-ofthe-art. In Sec 5.2, we report our OCC results with a comparison to previous works on the standard benchmark datasets. In Sec 5.3 we analyse the ability of our method to mitigate catastrophic collapse and present an ablation study.</p><p>Building up on the framework suggested in <ref type="bibr" target="#b0">[1]</ref>, we use ResNet152 pre-trained on ImageNet classification task as ? 0 , and adding an additional final 2 normalization layer -this is our initialized feature extractor ?. We adopt the data augmentation module proposed by <ref type="bibr" target="#b23">[24]</ref>; we sequentially apply a 224 ? 224-pixel crop from a randomly resized image, random color jittering, random grayscale conversion, random Gaussian blur and random horizontal flip. By default, we fine-tune our model with the loss function in Eq. 7. For inference we use the criterion described in Sec. 4.3. We adopt the ROC-AUC metric as detection performance score. Full training and implementation details are provided in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmarks</head><p>We evaluated our approach on a wide range of anomaly detection benchmarks. Following <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> we run our experiments on commonly used datasets: CIFAR-10 <ref type="bibr" target="#b24">[25]</ref>, CIFAR-100 coarse-grained version that consists of 20 classes <ref type="bibr" target="#b24">[25]</ref>, and CatsVsDogs <ref type="bibr" target="#b25">[26]</ref>. In order to demonstrate different challenges in image anomaly detection, we further extend our results on small datasets from different domains. Following the setting presented in <ref type="bibr" target="#b0">[1]</ref> we tested our method on: 102 Category Flowers <ref type="bibr" target="#b26">[27]</ref>, Caltech-UCSD Birds 200 <ref type="bibr" target="#b27">[28]</ref>, MVTec <ref type="bibr" target="#b28">[29]</ref>, WBC <ref type="bibr" target="#b29">[30]</ref>, DIOR <ref type="bibr" target="#b30">[31]</ref>. Following standard protocol, multi-class dataset are converted to anomaly detection by setting a class as normal and all other classes as anomalies. This is performed for all classes, in practice turning a single dataset with C classes into C datasets. For the full dataset descriptions and details see Appendix A.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison on Standard Datasets</head><p>We compare our approach with the top current self-supervised and pre-trained feature adaptation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b0">1]</ref>. Results that were reported in the original papers were copied. When the results were not reported in the original papers, we ran the experiments (where possible).</p><p>Tab. 1 shows that our proposed approach surpasses the previous state-of-the-art on the common OCC benchmarks. This establishes the superiority of our approach over previous self-supervised and pre-trained methods. This is due to our new loss function and the removal of confidence information (see Sec. 5.3). The full additional class-wise results for CIFAR-10, CIFAR-100 and CatsVsDogs see Appendix A.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis &amp; Ablation Study</head><p>Small datasets: In Tab. 2 we present a comparison between (i) top self-supervised contrastivelearning based method -CSI <ref type="bibr" target="#b18">[19]</ref> (ii) PANDA based on the Euclidean metric <ref type="bibr" target="#b0">[1]</ref> (iii) our methodwhich uses 2 normalized representations. We see that the self-supervised method does not perform well on such small datasets, whereas our method achieves very strong performance. The reason for the poor performance of self-supervised methods on small dataset, is due to the fact that the only training data they see is the small dataset, and they cannot learn strong features using such a small amount of data. This is particularly severe for contrastive methods (but is also the case for all other self-supervised methods). As pre-trained methods transfer features from external datasets, they do not have this failure mode. Note that similarly to PANDA <ref type="bibr" target="#b0">[1]</ref>, our method does not benefit from adaptation on small datasets -and relies on the initial feature extractor ? 0 , after normalizing away the confidence ? 0 (x) . Moreover, unlike PANDA which uses kNN with Euclidean distance (DN2) we use the angular distance. We find that our confidence-invariant approach outperforms PANDA in all benchmarks but one, in which we also achieve comparable performance. Training objective: The individual effects of each of the components that comprise our final train loss is appraised in Tab. 3 as well as the unadapted ImageNet pre-trained ResNet features combined with kNN as anomaly scoring (DN2). We note that both the confidence-invariant form of the pretrained model and the angular center loss achieve better performance than the raw features and the vanilla center loss respectively. We further notice that the mean-shifted loss outperforms the rest, and combining it with the angular center loss results in further improvements.</p><p>Catastrophic collapse: In <ref type="figure" target="#fig_3">Fig. 4</ref>.a, we evaluated the collapse of different training objectives on the CIFAR-10 "Bird" class. We notice that the contrastive loss is unsuitable for OCC feature adaptation as it results in very fast catastrophic collapse. PANDA-ES (early-stopping) results in initial improvement in accuracy, but after few epochs the features degrade and become uninformative. PANDA-EWC has the same itinerary; it postpones the collapse, but does not prevent it. Finally, we see that the mean-shifted loss nearly entirely eliminates catastrophic collapse. In <ref type="figure" target="#fig_3">Fig. 4</ref>.b-c we zoom in on the collapse slopes of the different components that comprise our final loss. While our final loss results in higher accuracy than its components, its feature degradation is similar to that of the mean-shifted  loss. Moreover, the feature degradation of the angular center loss is faster than the above but is still much better than that of the standard center loss.</p><p>Negative samples: In additional to contrastive self-supervised learning methods such as SimCLR <ref type="bibr" target="#b17">[18]</ref> and MoCo <ref type="bibr" target="#b31">[32]</ref>, other non-contrastive methods have been proposed (e.g. BYOL <ref type="bibr" target="#b32">[33]</ref> and SimSiam <ref type="bibr" target="#b33">[34]</ref>) which only use positive pairs but no negative pairs. We evaluated our method with SimSiam (using the mean-shifted representations), which is the same as using our loss without negative examples. We found that the method experiences an immediate catastrophic collapse. This indicates that negative examples are necessary for good performance when using mean-shifted representations. To give some intuition, note that the SimSiam objective (with or without mean-shifted representations), can in fact be optimized by having all representations mapped to a constant value. Although it does not happen when SimSiam is initialized from scratch, it appears that in the OCC case, it does degrade to the trivial solution. This establishes the need for a contrastive approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a novel feature adaptation approach for deep anomaly detection. We improved over PANDA <ref type="bibr" target="#b0">[1]</ref> and surpassed it by introducing two new components; First, we presented a new loss function for feature adaptation for one-class classification that measures the angular distance in relation to the center of the extracted features rather than the origin. Second, we used the angular distance as a metric in place of the Euclidean distance, as the latter was shown to be sensitive to prediction confidence, which was found to be uncorrelated with anomalies. Our method achieves the top anomaly detection performance and appears to eliminate catastrophic collapse nearly entirely. We pre-processed the DIOR aerial image dataset by taking the segmented object in classes that have more than 50 images with size larger than 120 ? 120 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean-Shifted Contrastive Loss for Anomaly Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Baselines</head><p>DROC <ref type="bibr" target="#b19">[20]</ref>: We used the numbers reported in the paper.</p><p>For the evaluation of the other competing method, we trained using the official repositories of their authors and make an effort to select the best configurations available.</p><p>DeepSVDD <ref type="bibr" target="#b1">[2]</ref>: We resize all the images to 32 ? 32 pixels and use the official pyTorch implementation with the CIFAR-10 configuration.</p><p>MHRot <ref type="bibr" target="#b15">[16]</ref>: An improved version of the original RotNet approach. For high-resolution images we used the current GitHub implementation. For low resolution images, we modified the code to the architecture described in the paper, replicating the numbers in the paper on CIFAR-10.</p><p>CSI <ref type="bibr" target="#b18">[19]</ref>, PANDA [1]: We run the code and used the exact protocol as described in the official repositories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Implementation details</head><p>We fine-tune the two last blocks of an ImageNet pre-trained ResNet152 with an additional 2 normalization layer for 100 epochs by minimizing L f inal where the temperature ? is set as 0.25. We use SGD optimizer with weight decay of w = 5 ? 10 ?5 , and no momentum. The size of the mini-batches is set to be 64. Finally, for anomaly scoring we use kNN with k = 2 nearest neighbours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Training Resources</head><p>Training each dataset class presented in this paper takes approximately 3 hours on a single NVIDIA RTX-2080 TI.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Per-class results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Top: The angular representation in relation to the origin. L con enlarging the angles between postive and negative samples, thus increasing their Euclidean distance to c. Bottom: The meanshifted representation. L msc does not affect the Euclidean distance between c and the mean-shifted representations while maximizes the angles between the negative pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) The initialized feature space derived by ? 0 . (b) L con forces {?(x)} x?Xtrain to be equally distributed across the unit sphere, resulting that every anomalous samplex / ? X train will have a nearby normal sample (c) L msc operates in the space of angles around the center forming a uniform distribution of {?(x)} x?Xtrain across the unit sphere that surrounds the center (d) Projecting the mean-shifted features to the unit sphere after optimizing L msc yields an informative compact representation of normal samples features around the center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Confidence histogram of CIFAR-10 "Bird" class. The 2 norm confidence of the extracted features derived by ? does not differentiate between normal and anomalous samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>CIFAR-10 "Bird" class: (a) While the contrastive loss and PANDA suffers from catastrophic collapse the mean-shifted loss eliminates it nearly entirely. (b) We zoom in to the ROC-AUC according to epoch with different training objectives. (c) We zoom in further to the maximum of the curves and observe a faster decline for the angular center loss accuracy than for the others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Anomaly detection performance (mean ROC-AUC %) on datasets containing more than 500 train samples. For our method we report the means and standard deviations averaged over five runs. Bold denotes the best results. losses eliminates these faults, achieving high accuracy and training stability. In fact, we show in Sec. 5 that our combined losses encounter virtually no collapse after hundreds of training epochs.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">Self-supervised</cell><cell></cell><cell cols="2">Pre-trained</cell></row><row><cell></cell><cell cols="5">D-SVDD [2] MHRot [16] DROC [20] CSI [19] PANDA [1]</cell><cell>Ours</cell></row><row><cell>CIFAR-10</cell><cell>64.8</cell><cell>90.1</cell><cell>92.5</cell><cell>94.3</cell><cell>96.2</cell><cell>97.5?0.1</cell></row><row><cell>CIFAR-100</cell><cell>67.0</cell><cell>80.1</cell><cell>86.5</cell><cell>89.6</cell><cell>94.1</cell><cell>96.5?0.1</cell></row><row><cell>CatsVsDogs</cell><cell>50.5</cell><cell>86.0</cell><cell>89.6</cell><cell>86.3</cell><cell>97.3</cell><cell>99.4?0.0</cell></row><row><cell>DIOR</cell><cell>70.0</cell><cell>73.3</cell><cell>-</cell><cell>78.5</cell><cell>94.3</cell><cell>97.2?0.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Anomaly detection accuracy (mean ROC-AUC %) on various small dataset (&lt; 250 images). Self-supervised methods fail while pre-trained features achieves strong results.</figDesc><table><row><cell></cell><cell cols="4">Birds Flowers MvTec WBC</cell></row><row><cell>CSI [19]</cell><cell>52.4</cell><cell>60.8</cell><cell>63.6</cell><cell>50.4</cell></row><row><cell>PANDA [1]</cell><cell>95.3</cell><cell>94.1</cell><cell>86.5</cell><cell>87.4</cell></row><row><cell cols="2">PANDA + 2 norm (ours) 96.7</cell><cell>96.5</cell><cell>87.2</cell><cell>87.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Training objective ablation study (CIFAR-10, mean ROC-AUC %).</figDesc><table><row><cell>Dataset</cell><cell>DN2</cell><cell></cell><cell>Center</cell><cell cols="2">L msc L msc + L angular</cell></row><row><cell cols="4">Raw Angular L center L angular</cell><cell></cell><cell></cell></row><row><cell>CIFAR-10 92.5</cell><cell>95.8</cell><cell>96.2</cell><cell>96.8</cell><cell>97.3</cell><cell>97.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>We evaluate our method on a set of commonly used datasets: CIFAR-10<ref type="bibr" target="#b24">[25]</ref>: Consists of RGB images of 10 object classes. CIFAR-100<ref type="bibr" target="#b24">[25]</ref>: We use the coarse-grained version that consists of 20 classes. DogsVsCats: High resolution color images of two classes: cats and dogs. The data were extracted from the ASIRRA dataset<ref type="bibr" target="#b25">[26]</ref>, we split each class to the first 10,000 images as train and the last 2,500 as test. To further extend our results, we compared the methods on a number of small datasets from different domains: 102 Category Flowers &amp; Caltech-UCSD Birds 200<ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>: For each of those datasets we evaluated the methods using only each of the first 20 classes as normal, and using the entire test set for evaluation. MvTec<ref type="bibr" target="#b28">[29]</ref>: This dataset contains 15 different industrial products, with normal images of proper products for train and 1 ? 9 types of manufacturing errors as anomalies. The anomalies in MvTec are in-class i.e. the anomalous images come from the same class of normal images with subtle variations. We evaluated our method on datasets that contain symmetries, such as images that have no preferred angle (microscopy, aerial images.): WBC<ref type="bibr" target="#b29">[30]</ref>: We used the 4 big classes in "Dataset 1" of microscopy images of white blood cells, and a 80%/20% train-test split. DIOR<ref type="bibr" target="#b30">[31]</ref>:</figDesc><table><row><cell>A Experimental details</cell></row><row><cell>A.1 Dataset Descriptions</cell></row><row><cell>Standard datasets: Small datasets: Symmetric datasets:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>CIFAR-10 anomaly detection performance (mean ROC-AUC %). Bold denotes the best results.</figDesc><table><row><cell></cell><cell cols="6">DeepSVDD [2] MHRot [16] DROC [20] CSI [19] PANDA [1] Ours</cell></row><row><cell>0</cell><cell>61.7</cell><cell>77.5</cell><cell>90.9</cell><cell>89.9</cell><cell>97.4</cell><cell>97.7</cell></row><row><cell>1</cell><cell>65.9</cell><cell>96.9</cell><cell>98.9</cell><cell>99.1</cell><cell>98.4</cell><cell>98.9</cell></row><row><cell>2</cell><cell>50.8</cell><cell>87.3</cell><cell>88.1</cell><cell>93.1</cell><cell>93.9</cell><cell>95.8</cell></row><row><cell>3</cell><cell>59.1</cell><cell>80.9</cell><cell>83.1</cell><cell>86.4</cell><cell>90.6</cell><cell>94.5</cell></row><row><cell>4</cell><cell>60.9</cell><cell>92.7</cell><cell>89.9</cell><cell>93.9</cell><cell>97.5</cell><cell>97.3</cell></row><row><cell>5</cell><cell>65.7</cell><cell>90.2</cell><cell>90.3</cell><cell>93.2</cell><cell>94.4</cell><cell>97.1</cell></row><row><cell>6</cell><cell>67.7</cell><cell>90.9</cell><cell>93.5</cell><cell>95.1</cell><cell>97.5</cell><cell>98.4</cell></row><row><cell>7</cell><cell>67.3</cell><cell>96.5</cell><cell>98.2</cell><cell>98.7</cell><cell>97.5</cell><cell>98.3</cell></row><row><cell>8</cell><cell>75.9</cell><cell>95.2</cell><cell>96.5</cell><cell>97.9</cell><cell>97.6</cell><cell>98.7</cell></row><row><cell>9</cell><cell>73.1</cell><cell>93.3</cell><cell>95.2</cell><cell>95.5</cell><cell>97.4</cell><cell>98.4</cell></row><row><cell cols="2">Mean 64.8</cell><cell>90.1</cell><cell>92.5</cell><cell>94.3</cell><cell>96.2</cell><cell>97.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>CIFAR-100 coarse-grained version anomaly detection performance (mean ROC-AUC %). Bold denotes the best results.</figDesc><table><row><cell></cell><cell cols="6">DeepSVDD [2] MHRot [16] DROC [20] CSI [19] PANDA [1] Ours</cell></row><row><cell>0</cell><cell>66.0</cell><cell>77.6</cell><cell>82.9</cell><cell>86.3</cell><cell>91.5</cell><cell>96.2</cell></row><row><cell>1</cell><cell>60.1</cell><cell>72.8</cell><cell>84.3</cell><cell>84.8</cell><cell>92.6</cell><cell>95.9</cell></row><row><cell>2</cell><cell>59.2</cell><cell>71.9</cell><cell>88.6</cell><cell>88.9</cell><cell>98.3</cell><cell>98.4</cell></row><row><cell>3</cell><cell>58.7</cell><cell>81.0</cell><cell>86.4</cell><cell>85.7</cell><cell>96.6</cell><cell>97.7</cell></row><row><cell>4</cell><cell>60.9</cell><cell>81.1</cell><cell>92.6</cell><cell>93.7</cell><cell>96.3</cell><cell>97.6</cell></row><row><cell>5</cell><cell>54.2</cell><cell>66.7</cell><cell>84.5</cell><cell>81.9</cell><cell>94.1</cell><cell>96.5</cell></row><row><cell>6</cell><cell>63.7</cell><cell>87.9</cell><cell>73.4</cell><cell>91.8</cell><cell>96.4</cell><cell>98.6</cell></row><row><cell>7</cell><cell>66.1</cell><cell>69.4</cell><cell>84.2</cell><cell>83.9</cell><cell>91.2</cell><cell>94.1</cell></row><row><cell>8</cell><cell>74.8</cell><cell>86.8</cell><cell>87.7</cell><cell>91.6</cell><cell>94.7</cell><cell>97.1</cell></row><row><cell>9</cell><cell>78.3</cell><cell>91.7</cell><cell>94.1</cell><cell>95.0</cell><cell>94.0</cell><cell>96.6</cell></row><row><cell>10</cell><cell>80.4</cell><cell>87.3</cell><cell>85.2</cell><cell>94.0</cell><cell>96.4</cell><cell>97.4</cell></row><row><cell>11</cell><cell>68.3</cell><cell>85.4</cell><cell>87.8</cell><cell>90.1</cell><cell>92.6</cell><cell>96.3</cell></row><row><cell>12</cell><cell>75.6</cell><cell>85.1</cell><cell>82.0</cell><cell>90.3</cell><cell>93.1</cell><cell>95.6</cell></row><row><cell>13</cell><cell>61.0</cell><cell>60.3</cell><cell>82.7</cell><cell>81.5</cell><cell>89.4</cell><cell>93.0</cell></row><row><cell>14</cell><cell>64.3</cell><cell>92.7</cell><cell>93.4</cell><cell>94.4</cell><cell>98.0</cell><cell>98.9</cell></row><row><cell>15</cell><cell>66.3</cell><cell>70.4</cell><cell>75.8</cell><cell>85.6</cell><cell>89.7</cell><cell>92.6</cell></row><row><cell>16</cell><cell>72.0</cell><cell>78.3</cell><cell>80.3</cell><cell>83.0</cell><cell>92.1</cell><cell>95.4</cell></row><row><cell>17</cell><cell>75.9</cell><cell>93.5</cell><cell>97.5</cell><cell>97.5</cell><cell>97.7</cell><cell>98.5</cell></row><row><cell>18</cell><cell>67.4</cell><cell>89.6</cell><cell>94.4</cell><cell>95.9</cell><cell>94.7</cell><cell>97.4</cell></row><row><cell>19</cell><cell>65.8</cell><cell>88.1</cell><cell>92.4</cell><cell>95.2</cell><cell>92.7</cell><cell>97.0</cell></row><row><cell cols="2">Mean 67.0</cell><cell>80.1</cell><cell>86.5</cell><cell>89.6</cell><cell>94.1</cell><cell>96.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>CatsVsDogs anomaly detection performance (mean ROC-AUC %). Bold denotes the best results.</figDesc><table><row><cell></cell><cell cols="6">DeepSVDD [2] MHRot [16] DROC [20] CSI [19] PANDA [1] Ours</cell></row><row><cell>Cat</cell><cell>49.2</cell><cell>87.7</cell><cell>91.7</cell><cell>85.7</cell><cell>99.2</cell><cell>99.7</cell></row><row><cell>Dog</cell><cell>51.8</cell><cell>84.2</cell><cell>87.5</cell><cell>86.9</cell><cell>95.4</cell><cell>99.1</cell></row><row><cell cols="2">Mean 50.5</cell><cell>86.0</cell><cell>89.6</cell><cell>86.3</cell><cell>97.3</cell><cell>99.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Panda-adapting pretrained features for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05903</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Gornitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning deep features for one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="5450" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A geometric framework for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleazar</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Prerau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Portnoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sal</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of data mining in computer security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="77" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ensemble gaussian mixture models for probability density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glodek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedhelm</forename><surname>Schwenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Outlier detection with kernel density functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Longin Jan Latecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragoljub</forename><surname>Lazarevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pokrajac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Machine Learning and Data Mining in Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="61" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Support vector data description. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Csi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08176</idno>
		<title level="m">Novelty detection via contrastive learning on distributionally shifted instances</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning and evaluating representations for deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minho</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02578</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<title level="m">What makes imagenet good for transfer learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Asirra: a captcha that exploits interest-aligned manual image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>John R Douceur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Computer and Communications Security</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="366" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive realworld dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast and robust segmentation of white blood cell images by self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Micron</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="55" to="71" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images: A survey and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="296" to="307" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
