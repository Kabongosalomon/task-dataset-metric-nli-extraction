<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Top-down Discourse Parsing via Sequence Labelling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koto</forename><surname>Fajri</surname></persName>
							<email>ffajri@student.unimelb.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Jey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
							<email>tbaldwin@unimelb.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Top-down Discourse Parsing via Sequence Labelling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a top-down approach to discourse parsing that is conceptually simpler than its predecessors <ref type="bibr" target="#b19">(Kobayashi et al., 2020;</ref><ref type="bibr" target="#b42">Zhang et al., 2020)</ref>. By framing the task as a sequence labelling problem where the goal is to iteratively segment a document into individual discourse units, we are able to eliminate the decoder and reduce the search space for splitting points. We explore both traditional recurrent models and modern pre-trained transformer models for the task, and additionally introduce a novel dynamic oracle for top-down parsing. Based on the Full metric, our proposed LSTM model sets a new state-of-the-art for RST parsing. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discourse analysis involves the modelling of the structure of text in a document. It provides a systematic way to understand how texts are segmented hierarchically into discourse units, and the relationships between them. Unlike syntax parsing which models the relationship of words in a sentence, discourse parsing operates at the document-level, and aims to explain the flow of writing. Studies have found that discourse parsing is beneficial for downstream NLP tasks including document-level sentiment analysis <ref type="bibr" target="#b1">(Bhatia et al., 2015)</ref> and abstractive summarization <ref type="bibr" target="#b20">(Koto et al., 2019)</ref>.</p><p>Rhetorical Structure Theory (RST; <ref type="bibr" target="#b26">Mann and Thompson (1988)</ref>) is one of the most widely used discourse theories in NLP <ref type="bibr" target="#b12">(Hernault et al., 2010;</ref><ref type="bibr" target="#b8">Feng and Hirst, 2014;</ref><ref type="bibr" target="#b15">Ji and Eisenstein, 2014;</ref><ref type="bibr" target="#b22">Li et al., 2016;</ref>. RST organizes text spans into a tree, where the leaves represent the basic unit of discourse, known as elementary discourse units <ref type="bibr">(EDUs)</ref>. EDUs are typically clauses 1 Code and trained models: https://github.com/ fajri91/NeuralRST-TopDown  EDU-3 elab EDU-1: Roy E. Parrott, the company's president and chief operating officer since Sept. 1, was named to its board. EDU-2: The appointment increased the number of directors to 10, EDU-3: three of whom are company employees. EDU-4: Simpson is an auto parts maker. of a sentence. Non-terminal nodes in the tree represent discourse unit relations.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, we present an example RST tree with four EDUs spanning two sentences. In this discourse tree, EDUs are hierarchically connected with arrows and the discourse label elab. The direction of arrows indicates the nuclearity of relations, wherein a "satellite" points to its "nucleus". The satellite unit is a supporting sentence for the nucleus unit and contains less prominent information. It is standard practice that the RST tree is trained and evaluated in a right-heavy binarized manner, resulting in three forms of binary nuclearity relationships between EDUs: Nucleus-Satellite, Satellite-Nucleus, and Nucleus-Nucleus. In this work, eighteen coarse-grained relations are considered as discourse labels, consistent with earlier work . <ref type="bibr">2</ref> Work on RST parsing has been dominated by the bottom-up paradigm <ref type="bibr" target="#b12">(Hernault et al., 2010;</ref><ref type="bibr" target="#b8">Feng and Hirst, 2014;</ref><ref type="bibr" target="#b15">Ji and Eisenstein, 2014;</ref><ref type="bibr" target="#b2">Braud et al., 2017;</ref><ref type="bibr" target="#b30">Morey et al., 2017;</ref>. These methods produce very competitive benchmarks, but in practice it is not a straightforward  <ref type="figure">Figure 2</ref>: Comparison of our top-down models with <ref type="bibr" target="#b42">Zhang et al. (2020)</ref> and <ref type="bibr" target="#b19">Kobayashi et al. (2020).</ref> approach (e.g. transition-based parser with actions prediction steps). Furthermore, bottom-up parsing limits the tree construction to local information, and macro context such as global structure/topic is prone to be under-utilized. As a result, there has recently been a move towards top-down approaches <ref type="bibr" target="#b19">(Kobayashi et al., 2020;</ref><ref type="bibr" target="#b42">Zhang et al., 2020)</ref>.</p><p>The general idea behind top-down parsing is to find splitting points in each iteration of tree construction. In <ref type="figure">Figure 2</ref>, we illustrate how our architecture differs from <ref type="bibr" target="#b42">Zhang et al. (2020)</ref> and <ref type="bibr" target="#b19">Kobayashi et al. (2020)</ref>. First, <ref type="bibr" target="#b42">Zhang et al. (2020)</ref> utilize four levels of encoder that comprise 3 Bi-GRUs and 1 CNN layer. The splitting mechanism is applied through a decoder, a stack, and bi-affine attention mechanisms. <ref type="bibr" target="#b19">Kobayashi et al. (2020)</ref> use the gold paragraph and sentence boundaries to aggregate a representation for each unit, and generate the tree based on these granularities. Two Bi-LSTMs are used, with splitting points determined by exhaustively calculating the bi-affine score of each possible split. The use of paragraph boundaries can explicitly lower the difficulty of the task, as 77% of paragraphs in the English RST Discourse Treebank ("RST-DT") are actually text spans <ref type="bibr" target="#b5">(Carlson et al., 2001)</ref>. These boundaries are closely related to gold span boundaries in evaluation.</p><p>In this paper, we propose a conceptually simpler top-down approach for RST parsing. The core idea is to frame the problem as a sequence labelling task, where the goal is to iteratively find a segmentation boundary to split a sequence of discourse units into two sub-sequences of discourse units. This way, we are able to simplify the architecture, in eliminating the decoder as well as reducing the search space for splitting points. Specifically, we use an LSTM <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997)</ref> or pre-trained BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> as the segmenter, enhanced in a number of key ways.</p><p>Our primary contributions are as follows: (1) we propose a novel top-down approach to RST parsing based on sequence labelling; (2) we explore both traditional sequence models such as LSTMs and also modern pre-trained encoders such as BERT;</p><p>(3) we demonstrate that adding a weighting mechanism during the splitting of EDU sequences improves performance; and (4) we propose a novel dynamic oracle for training top-down discourse parsers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous work on RST parsing has been dominated by bottom-up approaches <ref type="bibr" target="#b12">(Hernault et al., 2010;</ref><ref type="bibr" target="#b17">Joty et al., 2013;</ref><ref type="bibr" target="#b22">Li et al., 2016;</ref><ref type="bibr" target="#b2">Braud et al., 2017;</ref><ref type="bibr" target="#b39">Wang et al., 2017)</ref>. For example, <ref type="bibr" target="#b15">Ji and Eisenstein (2014)</ref> introduce DPLP, a transition-based parser based on an SVM with representation learning, combined with some heuristic features. <ref type="bibr" target="#b4">Braud et al. (2016)</ref> propose joint text segment representation learning for predicting RST discourse trees using a hierarchical Bi-LSTM. Elsewhere,  showed that implicit syntax features extracted from a dependency parser <ref type="bibr" target="#b7">(Dozat and Manning, 2017)</ref> are highly effective for discourse parsing.</p><p>Top-down parsing is well established for constituency parsing and language modelling <ref type="bibr" target="#b16">(Johnson, 1995;</ref><ref type="bibr" target="#b35">Roark and Johnson, 1999;</ref><ref type="bibr" target="#b34">Roark, 2001;</ref><ref type="bibr" target="#b9">Frost et al., 2007)</ref>, but relatively new to discourse parsing. <ref type="bibr" target="#b24">Lin et al. (2019)</ref> propose a unified framework based on pointer networks for sentence-level discourse parsing, while  employ hierarchical pointer network parsers. <ref type="bibr" target="#b30">Morey et al. (2017)</ref> found that most previous studies on parsing RST discourse tree were incorrectly benchmarked, e.g. one study uses macroaveraging while another use micro-averaging. 3 They also advocate for evaluation based on microaveraged F-1 scores over labelled attachment decisions (a la the original Parseval).</p><p>Pre-trained language models <ref type="bibr" target="#b33">(Radford et al., 2018;</ref><ref type="bibr" target="#b6">Devlin et al., 2019)</ref> have been shown to benefit a multitude of NLP tasks, including discourse analysis. For example, BERT models have been used for classifying discourse markers <ref type="bibr">(Sileo et al.,</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2019</head><p>) and discourse relations <ref type="bibr" target="#b31">(Nie et al., 2019;</ref><ref type="bibr" target="#b36">Shi and Demberg, 2019)</ref>. To the best of our knowledge, however, pre-trained models have not been applied in the generation of full discourse trees, which we address here by experimenting with BERT for topdown RST parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Top-down RST Parsing</head><p>We frame RST parsing as a sequence labelling task, where given a sequence of input EDUs, the goal is to find a segmentation boundary to split the sequence into two sub-sequences. This is realized by training a sequence labelling model to predict a binary label for each EDU, and select the EDU with the highest probability to be the segmentation point. After the sequence is segmented, we repeat the same process for the two sub-sequences in a divide-and-conquer fashion, until all sequences are segmented into individual units, producing the binary RST tree (e.g. <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LSTM Model</head><p>As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, our LSTM parser consists of two main blocks: an encoder and a segmenter. For the encoder, we follow  in using two LSTMs (Bi-LSTM 1 and Bi-LSTM 2 ) to produce EDU encodings by processing: (1) x i , the concatenation of word embedding w i and POS tag embedding p i ; and (2) syntax embedding s i , the output of the MLP layer of the bi-affine dependency parser <ref type="bibr" target="#b7">(Dozat and Manning, 2017)</ref>. Similar to , we then take the average of the output states for both LSTMs over the EDU, and concatenate it with an EDU type embedding t E j (which distinguishes the last EDU in a paragraph from other EDUs) to produce the final encoding:</p><formula xml:id="formula_0">x i = w i ? p i {a w 1 , .., a w p } = Bi-LSTM 1 ({x 1 , .., x p }) {a s 1 , ..., a s p } = Bi-LSTM 2 ({s 1 , .., s p }) g E j = Avg-Pool({a w 1 , .., a w p })? Avg-Pool({a s 1 , .., a s p }) ? t E j (1)</formula><p>where E j is an EDU, p is the number of words in E j , and ? denotes the concatenate operation. t E j is generally an implicit paragraph boundary feature, and provides a fair benchmark with previous models. In Section 4.3, we also show results without paragraph boundary features. As each EDU is processed independently, we use another LSTM (Bi-LSTM 3 ) to capture the inter-EDU relationship to obtain a contextualized representation h E j :</p><formula xml:id="formula_1">{h E 1 , ..., h Eq } = Bi-LSTM 3 ({g E 1 , ..., g Eq })</formula><p>where q is the number of EDUs in the document. Note that h E j is the final encoder output (see <ref type="figure">Figure</ref> 3) and is only computed once for each document.</p><p>The second part is the segmenter. We frame segmentation as a sequence labelling problem with y E j ? {0, 1}, where 1 denotes the splitting point, and 0 a non-splitting point. For each EDU sequence there is exactly one EDU that is labeled 1, and we start from the full EDU sequence (whole document) and iteratively perform segmentation until we are left with individual EDUs. We use a queue to store the two EDU sub-sequences as the result of the segmentation process. In total, there are q ? 1 iterations of segmentation (recall that q is the total number of EDUs in the document).</p><p>As segmentation is done iteratively in a divideand-conquer fashion, h E j serves as the input to the segmenter, which takes a (sub)sequence of EDUs to predict the segmentation position:</p><formula xml:id="formula_2">{h Em , .., h En } = Bi-LSTM 4 ({h Em , .., h En }) y E j = ?(MLP(h E j ))</formula><p>where m/n are the starting/ending index of the EDU sequence, 4 and? E j gives the probability of a segmentation. From preliminary experiments we found that it's important to have this additional Bi-LSTM 4 to perform the EDU sub-sequence segmentation point prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformer Model</head><p>Adapting BERT to discourse parsing is not trivial due to the limited number of input tokens it takes (typically 512 tokens), which is often too short for documents. Moreover, BERT is designed to encode sentences (and only two at maximum), where in our case we want to encode sequences of EDUs that span multiple sentences.</p><p>In our case, EDU truncation is not an option (since that would produce an incomplete RST tree), and the average number of words per document in our data is 521 (741 word pieces after BERT tokenization), which is much larger than the 512 limit. We therefore break the document into a number of partial documents, each consisting of multiple sentences that fit into the 512 token limit. This way, we allow the model to capture the fine-grained wordto-word relationships across (most) EDUs. Each partial document is then processed based on Liu and Lapata (2019) trick where we use an alternating even/odd segmentation embedding to encode all the EDUs in a document.</p><p>We illustrate this approach in <ref type="figure">Figure 4</ref>. First, all EDUs are formatted to start with [CLS] and end with [SEP], and words are tokenized using WordPiece. If the document has more than 512 tokens, we break it into multiple partial documents based on EDU boundaries, and pad accordingly (e.g. in <ref type="figure">Figure 4</ref> we break the example document of 3 EDUs into 2 partial documents), and process each partial document independently with BERT.</p><p>We also experimented with the second alternative by encoding each EDU independently first with BERT, and use a second inter-EDU transformer to capture the relationships between EDUs. Preliminary experiments, however, suggest that this approach produces sub-optimal performance.</p><p>In <ref type="figure">Figure 4</ref> each token is assigned three kinds of embeddings: (1) word, (2) segment, and (3) position. The input vector is computed by summing these three embeddings, and fed into BERT (initialized with bert-base). The output of BERT 4 In the first iteration, m = 1 and n = q (number of EDUs in the document).</p><formula xml:id="formula_3">BERT [CLS] EDU One [SEP] [CLS] EDU Two [SEP] [PAD] [CLS] EDU Three [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] T A T A T A T A T B T B T B T B T P T A T A T A T A T P T P T P T P T P P 2 P 1 g E1 g E2 g E3 t E2 t E3 t E1 Word Segment Position</formula><p>Bert Out type P 4 P 3 P 6 P 5 P 8 P 7 P 9 P 2 P 1 P 4 P 3 P 6 P 5 P 8 P 7 P 9 g E1 g EDU g One g SEP g E2 g EDU g Two g SEP g PAD g E3 g EDU g Three g SEP g PAD g PAD g PAD g PAD  <ref type="figure">Figure 4</ref>: Architecture of the transformer model. In practice, 1 row of input can have more than two EDUs.</p><p>gives us a contextualized embedding for each token, and we use the [CLS] embedding as the encoding for each EDU (g E j ).</p><p>Unlike the LSTM model, we do not incorporate syntax embeddings into the transformer model as we found no empirical benefit (see Section 4.3). This observation is in line with other studies (e.g. <ref type="bibr" target="#b14">Jawahar et al. (2019)</ref>) that have found BERT to implicit encode syntactic knowledge.</p><p>For the segmenter we use a second transformer (initialized with random weights) to capture the inter-EDU relationships for sub-sequences of EDUs during iterative segmentation:</p><formula xml:id="formula_4">{h Em , .., h En } = transformer({h Em , .., h En }) y E j = ?(MLP(h E j ))</formula><p>where? E j gives the probability of a segmentation, and h E j is the concatenation of the output of BERT (g E j ) and the EDU type embedding (t E j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Nuclearity and Discourse Relation Prediction</head><p>In <ref type="figure" target="#fig_2">Figure 5</ref>, we give an example of the iterative segmentation process to construct the RST tree. In each iteration, we pop a sequence from the queue (initialized with the original sequence of EDUs in  the document) and compute the segmentation label for each EDU using an LSTM (Section 3.1) or transformer (Section 3.2). After the sequence is segmented (using the ground truth label during training, or the highest-probability label at test time), we push to the queue the two sub-sequences (if they contain at least two EDUs) and repeat this process until the queue is empty.</p><p>In addition to segmentation, we also need to predict the nuclearity/satellite relationship (3 classes) and the discourse label (18 classes) for the segmented pairs. To that end, we average the EDU encodings for the segments, and feed them to a MLP layer to predict the nuclearity and discourse labels:</p><formula xml:id="formula_5">u l = Avg-Pool(h Em , ..., h E m+ind ) u r = Avg-Pool(h E m+ind+1 , ..., h En ) z nuc+dis = softmax(MLP(u l , u r ))</formula><p>where ind is the index of the segmentation point (given by the ground truth during training, or argmax of the segmentation probabilities? E j at test time), and z nuc+dis gives the joint probability distribution over the nuclearity and discourse classes. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Segmentation Loss with Penalty</head><p>One drawback of the top-down approach is that segmentation errors incurred closer to the root can be detrimental, as the error will propagate to the rest of the sub-trees. To address this, we explore scaling the segmentation loss based on the current tree depth and the number of EDUs in the input sequence. Preliminary experiments found that both approaches work, but that the latter is marginally better, and so we present results using the latter.</p><p>Formally, the modified segmentation loss of an example (document) is given as follows:</p><formula xml:id="formula_6">L(E m:n ) = ? n i=m y E i log(? E i )+ (1 ? y E i ) log(1 ?? E i ) L seg = 1 |S| (m,n)?S (1 + (n ? m) ? )L(E m:n )</formula><p>where y E i ? {0, 1} is the ground truth segmentation label, L(E m:n ) is the cross-entropy loss for an EDU sequence, S is the set of all EDU sequences (based on ground truth segmentation), and ? is a scaling hyper-parameter.</p><p>To summarize, the total training loss of our model is a (weighted) combination of segmentation loss (L seg ) and nuclearity-discourse prediction loss (L nuc+dis ):</p><formula xml:id="formula_7">L = ? 1 L seg + ? 2 L nuc+dis<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Dynamic Oracle</head><p>The training regimen for discourse parsing creates an exposure bias, where the parser may struggle to recover when it makes a mistake at test time. Goldberg and Nivre (2012) propose a dynamic oracle for transition-based dependency parsing to tackle this. The idea is to allow the model during training to use its predictions (instead of ground truth actions), and introduce a dynamic oracle to find the next best/optimal action sequences. It does so by comparing the current state of the constructed tree and the gold-standard tree, and aims to minimize the deviation. As the model is exposed to prediction errors during training time, it has a better chance of recovering from them at test time.</p><p>We explore a similar idea, and propose a dynamic oracle for our top-down discourse parser. A crucial question to ask when designing a dynamic oracle is: how can we compare the current state to the gold tree to obtain the next best series of actions when an error occurs during training? In transition-based parsing, <ref type="bibr" target="#b10">Goldberg and Nivre (2012)</ref>  while queue is not empty do 8:</p><p>Em:n = queue.pop() 9:</p><p>id gold , r gold = match(Em:n, O, R) 10:</p><p>id pred = predictSplit(Em:n) 11:</p><p>r pred1 = predictLabel(Em:n, id gold ) # for loss 12:</p><p>r pred2 = predictLabel(Em:n, id pred ) # ignored 13:</p><p>if random() &gt; ? then 14:</p><p>L, R = separate(Em:n, id gold ) 15: else 16:</p><p>L, R = separate(Em:n, id pred ) 17:</p><p>end if 18:</p><p>queue.push(L) if len(L) &gt; 1 19:</p><p>queue.push(R) if len(R) &gt; 1 20: end while 21: end function counting the gold arcs that are no longer reachable based on the action taken (e.g. SHIFT, REDUCE). We apply similar reasoning when finding the next best segmentation sequence in our dynamic oracle, which we illustrate below with an example.</p><p>Say we have a document with 4 EDUs (E 1:4 ), and the gold tree given in <ref type="figure" target="#fig_3">Figure 6</ref> (left). The correct sequence of segmentation is given by O 1:4 = [2, 1, 3, ?], which means we should first split at E 2 (creating E 1:2 and E 3:4 ), and then at E 1 (creating E 1 , E 2 , E 3:4 ), and lastly at E 3 , producing E 1 , E 2 , E 3 , E 4 as the leaves with the gold tree structure. We give the last EDU E 4 a "?" label (i.e. O 4 ='?') because no segmentation is needed for the last EDU.</p><p>Suppose the model predicts to do the first segmentation at E 3 . This produces E 1:3 and E 4 . What is the best way to segment E 1:3 to produce a tree that is as close as possible to the gold tree? The canonical segmentation order O 1:3 is [2, 1, ?] (the label of the last EDU is replaced by '?'), from which we can see the next best segmentation is to segment at E 2 to create E 1:2 and E 3 . Creating the canonical segmentation order O, and following it as much as possible, ensures the sub-tree that we're creating for E 1:3 mimics the structure of the gold tree.</p><p>The dynamic oracle labels nuclearity-discourse relations following the same idea. We introduce R, a list of gold nuclearity-discourse relations. For our example R 1:4 = [r 2 , r 1 , r 3 , ?] (based on the gold tree; see <ref type="figure" target="#fig_3">Figure 6 (left)</ref>). If the model decides to first segment at E 3 and creates E 1:3 and E 4 , when  we segment at E 2 (next best choice of segmentation), we will follow R and label the nuclearitydiscourse relation with r 1 . As before, following the original label list R ensures we keep the nuclearitydiscourse relation as faithful as possible <ref type="figure" target="#fig_3">(Figure 6  (right bottom)</ref>).</p><p>The dynamic oracle of our top-down parser is arguably quicker than that of a transition-based parser, as we do not need to accumulate cost for every transition taken. Instead, the dynamic oracle simply follows the gold segmentation order O to preserve as many subtrees as possible when an error occurs. We present pseudocode for the proposed dynamic oracle in Algorithm 1.</p><p>The probability of using the ground truth segmentation or predicted segmentation during training is controlled by the hyper-parameter ? ? [0, 1] (see Algorithm 1). Intuitively, this hyper-parameter allows the model to alternate between exploring its (possibly erroneous) segmentation or learning from the ground truth segmentation. The oracle reverts to its static variant when ? = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We use the English RST Discourse Treebank <ref type="bibr" target="#b5">(Carlson et al., 2001)</ref> for our experiments, consistent with recent studies <ref type="bibr" target="#b15">(Ji and Eisenstein, 2014;</ref><ref type="bibr" target="#b21">Li et al., 2014;</ref><ref type="bibr" target="#b8">Feng and Hirst, 2014;</ref>. The dataset is based on the Wall Street Journal portion of the Penn Treebank <ref type="bibr" target="#b29">(Marcus et al., 1993)</ref>, with 347 documents for training, and the remaining 38 documents for testing. We use the same development set as , which consists of 35 documents selected from the training set. We also use the same 18 discourse labels. Stanford CoreNLP  is used for POS tagging. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Configurations</head><p>We experiment with two segmentation models -LSTM (Section 3.1) and transformer (Section 3.2) -both implemented in PyTorch framework. 7 As EDUs are provided in the dataset, no automatic segmentation of EDU is required in our experiments. For the LSTM model, the dimensionality of the Bi-LSTMs in the encoder is 256, while the segmenter (Bi-LSTM 4 ) is 128 <ref type="figure" target="#fig_1">(Figure 3)</ref>. The embedding dimensions of words, POS tags, EDU type, and syntax features are 200, 200, 100, and 1,200, respectively, and we initialize words in EDU with GloVe embedding <ref type="bibr" target="#b32">(Pennington et al., 2014)</ref>. 8 For hyper-parameters, we use the following: batch size = 4, gradient accumulation = 2, learning rate = 0.001, dropout probability = 0.5, and optimizer = Adam (with epsilon of 1e-6). The loss scaling hyper-parameters (Equation <ref type="formula" target="#formula_7">(2)</ref>), are tuned based on the development set, and set to ? 1 = 1.0, and ? 2 = 1.0.</p><p>For the transformer model, the document length limit is set to 512 tokens, and longer documents are broken into smaller partial documents. As before, we truncate each EDU to the first 50 words. We initialize the transformer in the encoder with bert-base, and the transformer in the segmenter with random weights <ref type="figure">(Figure 4)</ref>. The transformer segmenter has 2 layers with 8 heads and 2048 feedforward hidden size. The training hyper-parameters are: initial learning rate = 5e-5, maximum epochs = 250, warm up = 2000 steps, and drop out = 0.2. For the ? hyper-parameters, we use the same 6 https://stanfordnlp.github.io/CoreNLP 7 We use the Huggingface framework for the transformer models. 8 https://nlp.stanford.edu/projects/ glove configuration as for the LSTM model.</p><p>We tuned the segmentation loss penalty hyperparameter ? (Section 3.4) and the dynamic oracle hyper-parameter ? (Section 3.5) based on the development set. Both the LSTM and transformer models use the same ? = 0.35 and ? = 0.65. We activate the dynamic oracle after training for 50 epochs for both models.</p><p>In terms of evaluation, we use the standard metrics introduced by Marcu <ref type="formula" target="#formula_7">(2000)</ref>: Span, Nuclearity, Relation, and Full. We report micro-averaged F-1 scores on labelled attachment decisions (original Parseval), following the recommendation of <ref type="bibr" target="#b30">Morey et al. (2017)</ref>. Additionally, we also present the evaluation with RST-Parseval procedure in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We first perform a feature addition study over our models to find the best model configuration; results are presented in <ref type="table">Table 1</ref>. Note that these results are computed over the development set, based on a static oracle.</p><p>For the vanilla models, the transformer model performs much better than the LSTM model. Adding syntax features (+Syntax) improves both models, although it's more beneficial for the LSTM. A similar trend is observed when we modify the segmentation loss to penalize the model if a segmentation error is made with more EDUs in the input sequence (+Penalty; Section 3.4): the transformer model sees an improvement of +0.8 while the LSTM model improves by +1.2. Lastly, when we combine both syntax features and the segmentation penalty, the LSTM model again shows an appreciable improvement, while the transformer model drops in performance marginally. 9 Given these results, we use both syntax features and the segmentation penalty for the LSTM model, but only the segmentation penalty for the transformer model in the remainder of our experiments.</p><p>We next benchmark our models against state-ofthe-art RST parsers over the test set, as presented in <ref type="table">Table 2 (original Parseval)</ref> and <ref type="table">Table 5</ref> (RST-Parseval as additional result). Except , all bottom-up results are from <ref type="bibr" target="#b30">Morey et al. (2017)</ref>. We present the labelled attachment decision performance for  by running the code of the authors for three runs and taking <ref type="bibr">9</ref> The result is consistent with the test set (see Appendix B) Method S N R F <ref type="table">Table 2</ref>: Results over the test set calculated using micro-averaged F-1 on labelled attachment decisions (original Parseval). All metrics (S: Span, N: Nuclearity, R: Relation, F: Full) are averaged over three runs. "*" denotes reported performance. " ?" and " ?" denote that the model uses sentence and paragraph boundary features, respectively. In this evaluation, <ref type="bibr" target="#b19">Kobayashi et al. (2020)</ref> does not report the original Parseval result.</p><p>the average. <ref type="bibr">10</ref> We also present the reported scores for the other top-down RST parsers <ref type="bibr" target="#b42">(Zhang et al., 2020;</ref><ref type="bibr" target="#b19">Kobayashi et al., 2020)</ref>. 11 Human performance in <ref type="table">Table 2</ref> and <ref type="table">Table 5</ref> is the score of human agreement reported by <ref type="bibr" target="#b18">Joty et al. (2015)</ref> ad <ref type="bibr" target="#b30">Morey et al. (2017)</ref>. Overall, in <ref type="table">Table 2</ref> our top-down models (LSTM and transformer) outperform all bottom-up and topdown baselines across all metrics. As we saw in the feature addition study, the LSTM model outperforms the transformer model, even though the transformer uses pre-trained BERT. We hypothesize that this may be because BERT is trained over shorter texts (paragraphs or sentence pairs), while our documents are considerably longer. Also, due to memory constraints, we break long documents into partial documents (Section 3.2), limiting 10 https://github.com/yunan4nlp/ NNDisParser. 11 Neither <ref type="bibr" target="#b42">Zhang et al. (2020)</ref> nor <ref type="bibr" target="#b19">Kobayashi et al. (2020)</ref> released their code, so we were unable to rerun their models.  fine-grained word-to-word attention to only nearby EDUs.</p><p>In <ref type="table">Table 2</ref>, we also present results for our model without paragraph features, and compare against other models which don't use paragraph features (each marked with " ?"). 12 First, we observe that our best model substantially outperforms all models with paragraph boundary features in terms of the Full metric. Compared to <ref type="bibr" target="#b42">Zhang et al. (2020)</ref>, our models (without this feature) achieve an improvement of +0.1, +1.9, +3.2, and +3.1 for Span, Nuclearity, Relation, and Full respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In <ref type="table" target="#tab_8">Table 3</ref> we present the impact of the dynamic oracle over documents of differing length for the LSTM model. Generally, we found that the static model performs better for shorter documents, and the dynamic oracle is more effective for longer documents. For instance, for documents with 50-100 EDUs, the dynamic oracle improves the Span, Nuclearity, and Relation metrics substantially. We also observe that the longer the document, the more difficult the tree prediction is. It is confirmed by the decreasing trends of all metrics for longer documents in <ref type="table" target="#tab_8">Table 3</ref>.</p><p>In total, our best model obtains 1,698 out of 2,308 spans of original Parseval trees, and correctly predict 1,517 segmentation points (pairs). We further analyze these pairs by presenting the confusion matrices of nuclearity and relation prediction in <ref type="figure">Figure 7</ref> and <ref type="figure" target="#fig_4">Figure 8,</ref>   In <ref type="figure" target="#fig_4">Figure 8</ref> we present analysis over top-7 relations and a relation other that represents the rest of 11 classes. Similar to the nuclearity prediction, the relation class distribution is also imbalance where elab accounts for 37% of the examples. Some relations are related to elab (see <ref type="table" target="#tab_10">Table 4</ref> for examples), such as back, cause, and list which we see some false positives. This produces the low precision of elab (74%). Unlike elab, relation attr is also a major class (represents 14% of the training data) but its precision and recall is substantially higher, at 94% and 96% respectively, suggesting it is less ambiguous. For other, its recall is 45%, and most of the errors are classified as elab (31%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce a top-down approach for RST parsing via sequence labelling. Our model is conceptually simpler than previous top-down discourse parsers  and can leverage pre-trained language models such as BERT. We additionally propose a dynamicoracle for our top-down parser, and demonstrate that our best model achieves a new state-of-the-art for RST parsing.     <ref type="table">Table 9</ref>: Running time of the static models during the training. The transition-based model is  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example discourse tree, from the RST Discourse Treebank (elab = elaboration).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of the LSTM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Nuclearity and relation prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Dynamic oracle for top-down approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Confusion matrix of relation prediction over the test set with top-7 relations (elab = Elaboration, cont = Contrast, list = List, back = Background, same = Same, temp = Temporal, eval = Evaluation, other = Other 11 relations).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Impact of the dynamic oracle over documents</cell></row><row><cell>of differing length. Scores (micro-averaged F-1 on la-</cell></row><row><cell>belled attachment decisions) are averaged over three</cell></row><row><cell>runs on the test set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Examples of misclassified relations. pairs (18% of NN) are classified as NS (Nucleus-Satellite). Class imbalance in the training set (NN:NS:SN = 23:61:16) is the main factor that drives the model to favor NS over the other classes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>Feature addition over the test set to find the best configuration for our models. Presented results are the mean and standard deviation of the Full metric (micro-averaged F-score on labelled attachment decisions) over three runs.</figDesc><table><row><cell cols="2">C Model Configuration for Training</cell></row><row><cell>Configuration</cell><cell>Value</cell></row><row><cell>LSTM1, LSTM2, LSTM3</cell><cell>200, 256</cell></row><row><cell>LSTM4</cell><cell>100, 128, 200</cell></row><row><cell>Word embedding</cell><cell>200</cell></row><row><cell>POS embedding</cell><cell>200</cell></row><row><cell>EDU type embedding</cell><cell>100</cell></row><row><cell>Syntax Feature</cell><cell>1200</cell></row><row><cell>? 1</cell><cell>1.0, 1.2</cell></row><row><cell>? 2</cell><cell>0.6, 0.8, 1.0, 1.2</cell></row><row><cell>? (Loss penalty)</cell><cell>0, 0.2, 0.35, 0.4, 0.6, 0.8, 1.0</cell></row><row><cell>? (Dynamic oracle)</cell><cell>0, 0.25, 0.5, 0.65, 0.75, 1.0</cell></row><row><cell>Batch size</cell><cell>2, 4</cell></row><row><cell>Gradient accumulation</cell><cell>2, 4</cell></row><row><cell>Learning rate</cell><cell>0.001</cell></row><row><cell>Dropout probability</cell><cell>0.5</cell></row><row><cell>Infrastructure</cell><cell>1 GPU V100 (16 GB)</cell></row><row><cell>Metrics to evaluate</cell><cell>Full</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>Parameter trials of our LSTM model. Bold indicates the best value after tuning over the development set.</figDesc><table><row><cell>Configuration</cell><cell>Value</cell></row><row><cell>BERT encoder</cell><cell>BERT-Base</cell></row><row><cell>Transformer2</cell><cell>L=2, H=8, FF=2048</cell></row><row><cell>EDU type embedding</cell><cell>100</cell></row><row><cell>? 1</cell><cell>0.5, 1.0, 1.5</cell></row><row><cell>? 2</cell><cell>0.6, 0.8, 1.0, 1.5</cell></row><row><cell>? (Loss penalty)</cell><cell>0, 0.2, 0.35, 0.4, 0.6, 0.8, 1.0</cell></row><row><cell>? (Dynamic oracle)</cell><cell>0, 0.25, 0.5, 0.65, 0.75, 1.0</cell></row><row><cell>Batch size</cell><cell>1, 2</cell></row><row><cell>Gradient accumulation</cell><cell>2, 3, 4, 8</cell></row><row><cell>Learning rate</cell><cell>5e-5</cell></row><row><cell>Dropout probability</cell><cell>0.1, 0.3, 0.4, 0.5</cell></row><row><cell>Infrastructure</cell><cell>4 GPU V100 (16 GB)</cell></row><row><cell>Metrics to evaluate</cell><cell>Full</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Parameter trials of our transformer model. Bold indicates the best value after tuning over the development set.</figDesc><table><row><cell>Method</cell><cell>1 Epoch</cell><cell>Converge at Epoch</cell></row><row><cell>Transition-based</cell><cell>5 mins</cell><cell>60-70</cell></row><row><cell>LSTM (Ours)</cell><cell>1 min 21 secs</cell><cell>60-70</cell></row><row><cell>Transformer (Ours)</cell><cell>1 min</cell><cell>130-150</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Details of individual relations can be found at: http: //www.sfu.ca/rst/index.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">After standardizing evaluation (based on micro-averaged F-1), they found that DPLP achieves the best Full performance, outperforming the deep learning models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We also experimented with predicting the nuclearity and discourse labels separately, but found joint prediction to work better in preliminary experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12"> use paragraph boundary features in their original code, but do not report it in the paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the anonymous reviewers for their helpful feedback and suggestions. In this research, the first author is supported by the Australia Awards Scholarship (AAS), funded by the Department of Foreign Affairs and Trade (DFAT), Australia. This research was undertaken using the LIEF HPC-GPGPU Facility hosted at The University of Melbourne. This facility was established with the assistance of LIEF Grant LE170100200.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<idno>dynamic) ? 71</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Better document-level sentiment analysis from RST discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References Parminder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2212" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-lingual RST discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="292" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spain</forename><surname>Valencia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view and multi-task training of RST discourse parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1903" to="1913" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Building a discourse-tagged corpus in the framework of rhetorical structure theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL &apos;01 Proceedings of the Second SIGdial Workshop on Discourse</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Learning Representations</title>
		<meeting>the 2016 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A lineartime bottom-up discourse parser with constraints and post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Wei Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1048</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="521" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modular and efficient top-down parsing for ambiguous left-recursive grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahmatullah</forename><surname>Hafiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Callaghan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Parsing Technologies</title>
		<meeting>the Tenth International Conference on Parsing Technologies<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="109" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dynamic oracle for arc-eager dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="959" to="976" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Empirical comparison of dependency conversions for RST discourse trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-3616</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue<address><addrLine>Los Angeles</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="128" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hilda: A discourse parser using support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What does BERT learn about the structure of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1356</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squibs and discussions: Memoization in top-down parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="405" to="417" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining intra-and multisentential rhetorical parsing for document-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="486" to="496" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CODRA: A novel discriminative framework for rhetorical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00226</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="435" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Top-down RST parsing utilizing granularity levels in documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2020 : The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved document modelling with a neural discourse parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fajri</forename><surname>Koto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Association Workshop</title>
		<meeting>the Australasian Language Technology Association Workshop<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recursive deep models for discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1220</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2061" to="2069" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discourse parsing with attention-based hierarchical neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1035</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A unified linear-time framework for sentence-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prathyusha</forename><surname>Jwalapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bari</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4190" to="4200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical pointer net parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1007" to="1017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
		<title level="m">Rhetorical structure theory: Toward a functional theory of text organization. Text Interdisciplinary Journal for the Study of Discourse</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="243" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-5010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>Baltimore</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The Theory and Practice of Discourse Parsing and Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">How much progress have we made on RST discourse parsing? a replication study of recent results on the RST-DT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1319" to="1324" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DisSent: Learning sentence representations from explicit discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1442</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4497" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Building a discourse-tagged corpus in the framework of rhetorical structure theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<idno>abs/1704.01444</idno>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Probabilistic top-down parsing and language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<idno type="DOI">10.1162/089120101750300526</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="276" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient probabilistic top-down and left-corner parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.3115/1034678.1034743</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics<address><addrLine>College Park, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="421" to="428" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Next sentence prediction helps implicit discourse relation classification within and across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5789" to="5795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mining discourse markers for unsupervised sentence representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Sileo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Pradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1351</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3477" to="3486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two practical rhetorical structure theory parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><forename type="middle">Antonio</forename><surname>Valenzuela-Esc?rcega</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/N15-3001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A two-stage parsing method for text-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2029</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="184" to="188" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transition-based neural RST parsing with implicit syntax features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="559" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Transition-based neural RST parsing with implicit syntax features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="559" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A top-down neural architecture towards text-level parsing of discourse rhetorical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.569</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6386" to="6395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<idno>static) ? 85.8 72.6 59.5 59.0</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<idno>dynamic) ? 85</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">?&quot; and &quot; ?&quot; denote that the model uses sentence and paragraph boundary features, respectively</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Table 5: Results over the test set calculated using micro-averaged F-1 on RST-Parseval. All metrics (S: Span, N: Nuclearity</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>this evaluation, Zhang et al. (2020) does not report the RST-Parseval result. Also, both Zhang et</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">2020) do not release their code. First, We can see that our model achieves the best results in terms of Relation and Full. Compared to other models without paragraph boundary features, our proposed model also performs best on the Full metric by a comfortable margin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kobayashi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
