<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
							<email>sihengc@andrew.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories 3 Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
							<email>wangyanfeng@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
						</author>
						<title level="a" type="main">Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Action recognition with skeleton data has recently attracted much attention in computer vision. Previous studies are mostly based on fixed skeleton graphs, only capturing local physical dependencies among joints, which may miss implicit joint correlations. To capture richer dependencies, we introduce an encoder-decoder structure, called A-link inference module, to capture action-specific latent dependencies, i.e. actional links, directly from actions. We also extend the existing skeleton graphs to represent higherorder dependencies, i.e. structural links. Combing the two types of links into a generalized skeleton graph, we further propose the actional-structural graph convolution network (AS-GCN), which stacks actional-structural graph convolution and temporal convolution as a basic building block, to learn both spatial and temporal features for action recognition. A future pose prediction head is added in parallel to the recognition head to help capture more detailed action patterns through self-supervision. We validate AS-GCN in action recognition using two skeleton data sets, NTU-RGB+D and Kinetics. The proposed AS-GCN achieves consistently large improvement compared to the state-of-the-art methods. As a side product, AS-GCN also shows promising results for future pose prediction. Our code is available at https://github.com/limaosen0/AS-GCN . 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human action recognition, broadly applicable to video surveillance, human-machine interaction, and virtual reality <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25]</ref>, has recently attracted much attention in computer vision. Skeleton data, representing dynamic 3D joint <ref type="bibr" target="#b0">1</ref> Accepted by CVPR 2019.  <ref type="figure">Figure 1</ref>: Feature learning with generalized skeleton graphs. the actional links and structural links capture dependencies between joints. For the action "walking", actional links denotes that hands and feet are correlated. The semilucent circles on the right bodies are the joint feature maps for recognition, whose areas are the response magnitudes. Compared to ST-GCN, AS-GCN obtains responses on collaborative moving joints (red boxes).</p><p>positions, have been shown to be effective in action representation, robust against sensor noise, and efficient in computation and storage <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>. The skeleton data are usually obtained by either locating 2D or 3D spatial coordinates of joints with depth sensors or using pose estimation algorithms based on videos <ref type="bibr" target="#b2">[3]</ref>. The earliest attempts of skeleton action recognition often encode all the body joint positions in each frame to a feature vector for pattern learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref>. These models rarely explore the internal dependencies between body joints, resulting to miss abundant actional information. To capture joint dependencies, recent methods construct a skeleton graph whose vertices are joints and edges are bones, and apply graph convolutional networks (GCN) to extract correlated features <ref type="bibr" target="#b16">[17]</ref>. The spatio-temporal GCN arXiv:1904.12659v1 [cs.CV] <ref type="bibr" target="#b25">26</ref> Apr 2019 (ST-GCN) is further developed to simultaneously learn spatial and temporal features <ref type="bibr" target="#b28">[29]</ref>. ST-GCN though extracts the features of joints directly connected via bones, structurally distant joints, which may cover key patterns of actions, are largely ignored. For example, while walking, hands and feet are strongly correlated. While ST-GCN tries to aggregate wider-range features with hierarchical GCNs, node features might be weaken during long diffusion <ref type="bibr" target="#b18">[19]</ref>.</p><p>We here attempt to capture richer dependencies among joints by constructing generalized skeleton graphs. In particular, we data-driven infer the actional links (A-links) to capture the latent dependencies between any joints. Similar to <ref type="bibr" target="#b15">[16]</ref>, an A-link inference module (AIM) with an encoder-decoder structure is proposed. We also extend the skeleton graphs to represent higher order relationships as the structural links (S-links). Based on the generalized graphs with the A-links and S-links, we propose an actional-structural graph convolution to capture spatial features. We further propose the actional-structural graph convolution network (AS-GCN), which stacks multiple of actional-structural graph convolutions and temporal convolutions. As a backbone network, AS-GCN adapts various tasks. Here we consider action recognition as the main task and future pose prediction as the side one. The prediction head promotes self-supervision and improve recognition by preserving detailed features. <ref type="figure">Figure 1</ref> presents the characteristics of the AS-GCN model, where we learn the actional links and extend the structural links for action recognition. The feature responses present that we could capture more global joint information than ST-GCN, which only uses the skeleton graph to model the local relations.</p><p>To verify the effectiveness of the proposed AS-GCN, we conduct extensive experiments on two distinct large-scale data sets: NTU-RGB+D <ref type="bibr" target="#b21">[22]</ref> and Kinetics <ref type="bibr" target="#b11">[12]</ref>. The experiments have demonstrated that AS-GCN outperforms the state-of-the-art approaches in action recognition. Besides, AS-GCN accurately predicts future frames, showing that sufficient detailed information is captured. The main contributions in this paper are summarized as follows:</p><p>? We propose the A-link inference module (AIM) to infer actional links which capture action-specific latent dependencies. The actional links are combined with structural links as generalized skeleton graphs; see <ref type="figure">Figure 1</ref>; ? We propose the actional-structural graph convolution network (AS-GCN) to extract useful spatial and temporal information based on the multiple graphs; see <ref type="figure">Figure 2</ref>; ? We introduce an additional future pose prediction head to predict future poses, which also improves the recognition performance by capturing more detailed action patterns; ? The AS-GCN outperforms several state-of-the-art methods on two large-scale data sets; As a side product, AS-GCN is also able to precisely predict the future poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Skeleton data is widely used in action recognition. Numerous algorithms are developed based on two approaches: the hand-crafted-based and the deep-learning-based. The first approach designs algorithms to capture action patterns based on the physical intuitions, such as local occupancy features <ref type="bibr" target="#b27">[28]</ref>, temporal joint covariances <ref type="bibr" target="#b9">[10]</ref> and Lie group curves <ref type="bibr" target="#b26">[27]</ref>. On the other hand, the deep-learning-based approach automatically learns the action faetures from data. Some recurrent-neural-network (RNN)-based models capture the temporal dependencies between consecutive frames, such as bi-RNNs <ref type="bibr" target="#b5">[6]</ref>, deep LSTMs <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20]</ref>, and attention-based model <ref type="bibr" target="#b23">[24]</ref>. Convolutional neural networks (CNN) also achieve remarkable results, such as residual temporal CNN <ref type="bibr" target="#b13">[14]</ref>, information enhancement model <ref type="bibr" target="#b20">[21]</ref> and CNN on action representations <ref type="bibr" target="#b12">[13]</ref>. Recently, with the flexibility to exploit the body joint relations, the graphbased approach draws much attention <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b22">23]</ref>. In this work, we adopt the graph-based approach for action recognition. Different from any previous method, we learn the graphs adaptively from data, which captures useful non-local information about actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>In this section, we cover the background material necessary for the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations</head><p>We consider a skeleton graph as G(V, E), where V is the set of n body joints and E is a set of m bones. Let A ? {0, 1} n?n be the adjacent matrix of the skeleton graph, where A i,j = 1 if the i-th and the j-th joints are connected and 0 otherwise. A fully describes the skeleton structure. Let D ? R n?n be the diagonal degree matrix, where D i,i = j A i,j . To capture more refined location information, we part one root node and its neighbors into three sets, including 1) the root node itself, 2) the centripetal group, which are closer to the body barycenter than root, and 3) the centrifugal group, and A is accordingly parted to be A (root) , A (centripetal) and A (centrifugal) . We denote the partition group set as P = {root, centripetal, centrifugal}. Note that p?P A (p) = A. Let X ? R n?3?T be the 3D joint positions across T frames. Let X t = X :,:,t ? R n?3 be the 3D joint positions at the t-th frame, which slices the t-th frame in the last dimension of X . Let x t i = X i,:,t ? R d be the positions of the i-th joint at the t-th frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatio-Temporal GCN</head><p>Spatio-temporal GCN (ST-GCN) <ref type="bibr" target="#b28">[29]</ref> consists of a series of the ST-GCN blocks. Each block contains a spatial graph convolution followed by a temporal convolution, which alternatingly extracts spatial and temporal features. The last ST-GCN block is connected to a fully-connected classifier to generate final predictions. The key component in ST-GCN is the spatial graph convolution operation, which introduces weighted average of neighboring features for each joint. Let X in ? R n?din be the input features of all joints in one frame, where d in is the input feature dimension, and X out ? R n?dout be the output features obtained from spatial graph convolution, where d out is the dimension of output features. The spatial graph convolution is</p><formula xml:id="formula_0">X out = p?P M (p) st ? A (p) X in W (p) st ,<label>(1)</label></formula><p>where   </p><formula xml:id="formula_1">A (p) = D (p) ? 1 2 A (p) D (p) ? 1 2 ? R n?n is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Actional-Structural GCN</head><p>The generalized graph, named actional-structural graph, is defined as G g (V, E g ), where V is the original set of joints and E g is the set of generalized links. There are two types of links in E g : structural links (S-links), explicitly derived from the body structure, and actional links (A-links), directly inferred from skeleton data. See the illustration of both types in <ref type="figure" target="#fig_2">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Actional Links (A-links)</head><p>Many human actions need far-apart joints to move collaboratively, leading to non-physical dependencies among joints. To capture corresponding dependencies for various actions, we introduce actional links (A-links), which are activated by actions and might exist between arbitrary pair of joints. To automatically infer the A-links from actions, we develop a trainable A-link inference module (AIM), which consists of an encoder and a decoder. The encoder produces A-links by propagating information between joints and links iteratively to learn link features; and the decoder predict future joint positions based on the inferred A-links; see <ref type="figure" target="#fig_5">Figure 4</ref>. We use AIM to warm-up the A-links, which are further adjusted during the training process.</p><p>Encoder. The functionality of an encoder is to estimate the states of the A-links given the 3D joint positions across time; that is,</p><formula xml:id="formula_2">A = encode(X ) ? [0, 1] n?n?C ,<label>(2)</label></formula><p>where C is the number of A-link types. Each element A i,j,c denotes the probability that the i, j-th joints are connected with the c-th type. The basic idea to design the mapping encode(?) is to first exact link features from 3D joint positions and then convert the link features to the linking probabilities. To exact link features, we propagate information between joints and links alternatingly. Let  x i = vec (X i,:,: ) ? R dT be the vector representation of the i-th joint's feature across all the T frames. We initialize the joint feature p (0) i = x i . In the k-th iteration, we propagate information back and forth between joints and links,</p><formula xml:id="formula_3">link features : Q (k+1) i,j = f (k) e (f (k) v (p (k) i ) ? (f (k) v (p (k) j )), joint features : p (k+1) i = F(Q (k+1) i,: ) ? p (k) i ,</formula><p>where f v (?) and f e (?) are both multi-layer perceptrons, ? is vector concatenation, and F(?) is an operation to aggregate link features and obtain the joint feature; such as averaging and elementwise maximization. After propagating for K times, the encoder outputs the linking probabilities as</p><formula xml:id="formula_4">A i,j,: = softmax Q (K) i,j + r ? ? R C ,<label>(3)</label></formula><p>where r is a random vector, whose elements are i.i.d. sampled from Gumbel(0, 1) distribution and ? controls the discretization of A i,j,: . Here we set ? = 0.5. We obtain the linking probabilities A i,j,: in the approximately categorical form by Gumbel softmax <ref type="bibr" target="#b10">[11]</ref>.</p><p>Decoder. The functionality of the decoder to predict the future 3D joint positions conditioned on the A-links inferred by the encoder and previous poses; that is,</p><formula xml:id="formula_5">X t+1 = decode(X t , . . . , X 1 , A) ? R n?3 ,</formula><p>where X t is the 3D joint positions at the t-th frame. The basic idea is to first extract joint features based on the Alinks and then convert joint features to future joint positions. Let x t i ? R d be the features of the ith joint at the t-th frame. The mapping decode(?) works as</p><formula xml:id="formula_6">(a) Q t i,j = C c=1 A i,j,c f (c) e (f (c) v (x t i ) ? f (c) v (x t j )) (b) p t i = F(Q t i,: ) ? x t i (c) S t+1 i = GRU(S t i , p t i ) (d)? t+1 i = f out (S t+1 i ) ? R 3 , where f (c) v (?), f (c) e (?) and f out (?) are MLPs.</formula><p>Step (a) generates link features by weighted averaging on the linking probabilities A i,j,: ; Step (b) aggregates the link features to obtain the corresponding joint features; Step (c) uses a gated recurrent unit (GRU) to update the joint features <ref type="bibr" target="#b4">[5]</ref>; and</p><p>Step (d) predicts the mean of future joint positions. We finally sample the future joint positionsx t+1</p><formula xml:id="formula_7">i ? R 3 from a Gaussian distribution, i.e.x t+1 i ? N (? t+1 i , ? 2 I),</formula><p>where ? 2 denotes the variance and I is an identity matrix.</p><p>We pretrain AIM for a few epoches to warm-up A-links. Mathematically, the cost function of AIM is</p><formula xml:id="formula_8">L AIM (A) = ? n i=1 T t=2 x t i ?? t i 2 2? 2 + C c=1 log A :,:,c A (0) :,:,c , where A<label>(0)</label></formula><p>:,:,c is the prior of A. In experiments, we find the performance boosts when p(A) promotes the sparsity. The intuition behind is that too many links would capture useless dependencies to confuse action pattern learning; however, in <ref type="formula" target="#formula_4">(3)</ref>, we ensure that C c=1 A i,j,c = 1. Since the probability one is allocated to C link types, it is hard to promote sparsity when C is small. To control the sparsity level, we introduce a ghost link with a large probability, indicating that two joints are not connected through any A-link. The ghost link still ensures that the probabilities sum up to one; that is, for ?i, j, :,:,c = P 0 /C for c = 1, 2, ? ? ? , C. In the training of AIM, we only update the probabilities of A-links A i,j,c , where c = 1, ? ? ? , C.</p><formula xml:id="formula_9">A i,j,0 + C c=1 A i,j,c = 1, where A i,j,</formula><p>We accumulate L AIM for multiple samples and minimize it to obtain a warmed-up A. Let A (c) act = A :,:,c ? [0, 1] n?n be the c-th type of linking probability, which represents the topology of the c-th actional graph. We define the actional graph convolution (AGC), which uses the A-links to capture the actional dependencies among joints. In the AGC, we use? (c) act as the graph convolution kernel, where?</p><formula xml:id="formula_10">(c) act = D (c) act ?1 A (c)</formula><p>act . Given the input X in , the AGC is</p><formula xml:id="formula_11">X act = AGC (X in ) (4) = C c=1? (c) act X in W (c) act ? R n?dout , where W (c)</formula><p>act is the trainable weight to capture feature importance. Note that we use the AIM to warm-up A-links in the pretraining process; during the training of action recognition and pose prediction, the A-links are further optimized by forward-passing the encoder of AIM only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Structural Links (S-links)</head><p>As shown in (1), A (p) X in aggregates the 1-hop neighbors' information in skeleton graph; that is, each layer in ST-GCN only diffuse information in a local range. To obtain long-range links, we use the high-order polynomial of A, indicating the S-links. Here we use? L as the graph convolution kernel, where? = D ?1 A is the graph transition matrix and L is the polynomial order.? introduces the degree normalization to avoid the magnitude explosion and has probabilistic intuition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. With the L-order polynomial, we define the structural graph convolution (SGC), which can directly reach the L-hop neighbors to increase the receptive field. The SGC is formulated as</p><formula xml:id="formula_12">X struc = SGC (X in ) (5) = L l=1 p?P M (p,l) struc ?? (p)l X in W (p,l) struc ? R n?dout ,</formula><p>where l is the polynomial order,? (p) is the graph transition matrix for p-th parted graph, M (p,l) struc ? R n?n and W (p,l) struc ? R n?dstruc are the trainable weights to capture edge weights and feature importance; namely, larger weight indicates more important corresponding feature. The weights are introduced for each polynomial order and each individual parted graph. Note that with the degree normalization, the graph transition matrix? (p) provides the nice initialization for edge weights, which stabilizes the learning of M (p,l) struc . When L = 1, the SGC degenerates to the original spatial graph convolution operation. For L &gt; 1, the SGC acts like the Chebyshev filter and is able to approximate the convolution designed in the graph spectral domain <ref type="bibr" target="#b1">[2]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Actional-Structural Graph Convolution Block</head><p>To integrally capture the actional and structural features among arbitrary joints, we combine the AGC and SGC and develop the actional-structural graph convolution (ASGC). In <ref type="formula">(4)</ref> and <ref type="formula">(5)</ref>, we obtain the joint features from AGC and SGC in each time stamp, respectively. We use a convex combination of both as the response of the ASGC. Mathematically, the ASGC operation is formulated as</p><formula xml:id="formula_13">X out = ASGC (X in ) = X struc + ?X act ? R n?dout ,</formula><p>where ? is a hyper-parameter, which trades off the importance between structural features and actional features. A non-linear activation function, such as ReLU(?), can be further introduced after ASGC. Theorem 1. The actional-structural graph convolution is a valid linear operation; that is, when Y 1 = ASGC (X 1 ) and Y 2 = ASGC (X 2 ).</p><p>Then, aY 1 + bY 2 = ASGC (aX 1 + bX 2 ), ?a, b ? R.</p><p>The linearity ensures that ASGC effectively preserves information from both structural and actional aspects; for example, when the response from the action aspect is stronger, it can be effectively reflected through ASGC.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AS-GCN Block?3</head><p>AS-GCN Block?3 To capture the inter-frame action features, we use one layer of temporal convolution (T-CN) along the time axis, which extracts the temporal feature of each joint independently but shares the weights on each joint. Since ASGC and T-CN learns spatial and temporal features, respectively, we concatenate both layers as an actional-structural graph convolution block (AS-GCN block) to extract temporal features from various actions; see <ref type="figure" target="#fig_7">Figure 5</ref>. Note that ASGC is a single operation to extract only spatial information and the AS-GCN block includes a series of operations to extract both spatial and temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Multitasking of AS-GCN</head><p>Backbone network. We stack a series of AS-GCN blocks to be the backbone network, called AS-GCN; see <ref type="figure">Figure 6</ref>. After the multiple spatial and temporal feature aggregations, AS-GCN extracts high-level semantic information across time.</p><p>Action recognition head. To classify actions, we construct a recognition head following the backbone network. We apply the global averaging pooling on the joint and temporal dimensions of the feature maps output by the backbone network, and obtain the feature vector, which is finally fed into a softmax classifier to obtain the predicted class-label?. The loss function for action recognition is the standard cross entropy loss</p><formula xml:id="formula_14">L recog = ?y T log(?),</formula><p>where y is the ground-truth label of the action.</p><p>Future pose prediction head. Most previous works on the analysis of skeleton data focused on the classification task. Here we also consider pose prediction; that is, using AS-GCN to predict future 3D joint positions given by historical skeleton-based actions.</p><p>To predict future poses, we construct a prediction module followed by the backbone network. We use several AS-GCN blocks to decode the high-level feature maps extracted from the historical data and obtain the predicted future 3D joint positionsX ? R n?3?T ; see <ref type="figure" target="#fig_9">Figure 7</ref>. The loss function for future prediction is the standard 2 loss</p><formula xml:id="formula_15">L predict = 1 ndT nd i=1 T t=1 X i,:,t ? X i,:,t 2 2 .</formula><p>(6)</p><p>Joint model. In practice, when we train the recognition head and future prediction head together, recognition performance gets improved. The intuition behind is that the future prediction module promotes self-supervision and avoids overfitting in recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data sets and Model Configuration</head><p>NTU-RGB+D. NTU-RGB+D, containing 56, 880 skeleton action sequences completed by one or two performers and categorized into 60 classes, is one of the largest data sets for skeleton-based action recognition <ref type="bibr" target="#b21">[22]</ref>. It provides the 3D spatial coordinates of 25 joints for each human in an action. For evaluating the models, two protocols are recommended: Cross-Subject and Cross-View. In Cross-Subject, 40, 320 samples performed by 20 subjects are separated into training set, and the rest belong to test set. Cross-View assigns data according to camera views, where training and test set have 37, 920 and 18, 960 samples, respectively.</p><p>Kinetics. Kinetics is a large data set for human action analysis, containing over 240, 000 video clips <ref type="bibr" target="#b11">[12]</ref>. There are 400 types of actions. Due to only RGB videos are provided, we obtain skeleton data by estimating joint locations on certain pixels with OpenPose toolbox <ref type="bibr" target="#b2">[3]</ref>. The toolbox generates 2D pixel coordinates (x, y) and confidence score c for totally 18 joints from the resized videos with resolution of 340 ? 256. We represent each joint as a threeelement feature vector: [x, y, c] T . For the multiple-person cases, we select the body with the highest average joint confidence in each sequence. Therefore, one clip with T frames is transformed into a skeleton sequence with dimension of 18 ? 3 ? T ). Finally, we pad each sequence by repeating the data from the start to totally T = 300. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subject with various links: S-links, A-links and A-with S-links</head><p>(AS-links). We tune the polynomial order in S-links from 1 to 4. Model Setting. We construct the backbone of AS-GCN with 9 AS-GCN blocks, where the features dimensions are 64, 128, 256 in each three blocks. The structure and operations of future pose prediction module are symmetric to the recognition module and we use the residual connection. In the AIM, we set the hidden features dimensions to be 128. The number of A-link types C = 3 and the prior of the ghost link P 0 = 0.95. ? = 0.5. We use PyTorch 0.4.1 and train the model for 100 epochs on 8 GTX-1080Ti GPUs. The batch size is 32. We use the SGD algorithm to train both recognition and prediction heads of AS-GCN, whose learning rate is initially 0.1, decaying by 0.1 every 20 epochs. We use Adam optimizer <ref type="bibr" target="#b14">[15]</ref> to train the A-link inference module with the initial learning rate 0.0005. All hyper-parameters are selected using a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Polynomial order S-links A-links AS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>To analyze each individual component of AS-GCN, we conduct extensive experiments on Cross-Subject benchmark of the NTU-RGB+D data set <ref type="bibr" target="#b21">[22]</ref>.</p><p>Effect of link types. Here we focus on validating the proposed A-links and S-links. In the experiments, we consider three link-type combinations, including S-links, Alinks and AS-links (A-links + S-links), with the original skeleton links. While involving S-links, we respectively set the polynomial order L = 1, 2, 3, 4 in the model. Note that when L = 1, the corresponding S-link is exactly the skeleton itself. <ref type="table" target="#tab_2">Table 1</ref> shows the classification accuracy of action recognition. We see that (1) either S-links with higher polynomial order or A-links can improve the recognition performance; (2) when using both S-links and A-links together, we achieve the best performance; (3) with only Alinks and skeleton graphs, the classification accuracy result reaches 83.2%, which is higher than S-links with polynomial order 1 (81.5%). These results validate the limitation of the original skeleton graph and the effectiveness of the proposed S-link and A-link.</p><p>Visualizing A-links. Various actions may activate different actional dependencies among joints. <ref type="figure" target="#fig_11">Figure 8</ref> shows the inferred A-links for three actions. The A-links with probabilities larger than 0.9 are illustrated as orange lines, where wider lines represent larger linking probabilities. We see that <ref type="bibr" target="#b0">(1)</ref> in Plots (a) and (c), the actions of hand waving and taking a selfie are mainly upper limb actions, where   arms have large movements and interact with the whole bodies, so that many A-links are built between arms and other body parts. (2) In Plot (b), the action of kicking something shows that the kicked leg is highly correlated to the other joints, indicating the body balancing during this action. These results validate that richer information of action patterns is captured by A-links.</p><p>The number and priors of A-links. To select the appropriate C: the number of A-link types; and P 0 : the prior of the ghost links for training the AIM. We test the models with different C and P 0 to obtain the corresponding recognition accuracies, which are presented in <ref type="table" target="#tab_4">Table 2</ref>. We see that when C = 3 and P 0 = 0.95, we could obtain the highest recognition accuracy. The intuition is that too few A-link types cannot capture significant actional relationships and too many causes overfitting. And the sparse A-links would promote the recognition performance.</p><p>Effect of prediction head. To analyze the effect of the prediction head on improving recognition performance, we perform two groups of contrast tests. For the first group, AS-GCNs only employs S-links for action recognition but one has prediction head and the other does not have. In the other group, AS-GCN with/without prediction head additionally employ A-links. The polynomial order of S-links is from 1 to 4. <ref type="table" target="#tab_5">Table 3</ref> shows the classification results with/without prediction heads. We obtain better recognition performance consistently by around 1% when we introduce the prediction head. The intuition behind is that the prediction modules promotes to preserve more detailed information and introduce self-supervision to help recognition module avoid overfitting and achieve higher action  recognition performance. The sparse skeleton actions may sometimes rely on the detailed motions rather than coarse profiles which are easily-confused in some actions classes.</p><p>Feature visualization. To validate how the features of each joint effect on the final performance, we visualize the feature maps of actions in <ref type="figure" target="#fig_12">Figure 9</ref>, where the circle around each joint indicates magnitude of feature responses of this joint in the last AS-GCN block of the recognition module of AS-GCN. Plot (a) shows the feature responses of the action 'hand waving' at different time. At the initial phase of action, namely Frame 15, many joints of upper limb and trunk have approximately comparative responses; however, in the subsequent frames, large responses are distributed on the upper body especially waving arm. Note that other nonfunctional joints are not much neglected, because abundant hidden relationships are built. Plot (b) shows the other two actions, where we are able to capture many long-range dependencies. Plot (c) compares the features between AS-GCN and ST-GCN. ST-GCN does apply multi-layer GCNs to cover the entire spatial domain; however, the feature are weakened during the propagation and distant joints cannot interact effectively, leading to localized feature responses. On the other hand, the proposed AS-GCN could capture </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Cross Subject Cross View Lie Group <ref type="bibr" target="#b26">[27]</ref> 50.1% 52.8% H-RNN <ref type="bibr" target="#b5">[6]</ref> 59.1% 64.0% Deep LSTM <ref type="bibr" target="#b21">[22]</ref> 60.7% 67.3% PA-LSTM <ref type="bibr" target="#b21">[22]</ref> 62.9% 70.3% ST-LSTM+TS <ref type="bibr" target="#b19">[20]</ref> 69.2% 77.7% Temporal Conv <ref type="bibr" target="#b13">[14]</ref> 74.3% 83.1% Visualize CNN <ref type="bibr" target="#b20">[21]</ref> 76.0% 82.6% C-CNN+MTLN <ref type="bibr" target="#b12">[13]</ref> 79.6% 84.8% ST-GCN <ref type="bibr" target="#b28">[29]</ref> 81.5% 88.3% DPRL <ref type="bibr" target="#b25">[26]</ref> 83.5% 89.8% SR-TSL <ref type="bibr" target="#b22">[23]</ref> 84.8% 92.4% HCN <ref type="bibr" target="#b17">[18]</ref> 86.5% 91.1% AS-GCN (Ours) 86.8% 94.2% <ref type="table">Table 5</ref>: Comparison of action recognition performance on Kinetics. We list the top-1 and top-5 classification accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Top-1 Acc Top-5 Acc Feature Enc <ref type="bibr" target="#b7">[8]</ref> 14.9% 25.8% Deep LSTM <ref type="bibr" target="#b21">[22]</ref> 16.4% 35.3% Temporal Conv <ref type="bibr" target="#b13">[14]</ref> 20.3% 40.0% ST-GCN <ref type="bibr" target="#b28">[29]</ref> 30.7% 52.8% AS-GCN (Ours) 34.8% 56.5%</p><p>useful long-range dependencies to recognize the actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparisons to the State-of-the-Art</head><p>We compare AS-GCN on skeleton-based action recognition tasks with the state-of-the-art methods on the data sets of NTU-RGB+D and Kinetics. On NTU-RGB+D, we train AS-GCN on two recommended benchmarks: Cross-Subject and Cross-View, then we respectively obtain the top-1 classification accuracies in the test phase. We compare with covering hand-crafted methods <ref type="bibr" target="#b26">[27]</ref>, RNN/CNNbased deep learning models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref> and recent graph-based methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b22">23]</ref>. Specifically, ST-GCN <ref type="bibr" target="#b28">[29]</ref> combines GCN with temporal CNN to capture spatio-temporal features, and SR-TSL <ref type="bibr" target="#b22">[23]</ref> use gated recurrent unit (GRU) to propagate messages on graphs and use LSTM to learn the temporal features. <ref type="table" target="#tab_6">Table 4</ref> shows the comparison. We see that the proposed AS-GCN outperforms the other methods.</p><p>In the Kinetics dataset, we compare AS-GCN with four state-of-the-art approaches. A hand-crafted based method named Feature Encoding <ref type="bibr" target="#b7">[8]</ref> is presented at first. Then Deep LTSM and Temporal ConvNet <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14]</ref> are implemented as two deep learning models on Kinetics skeletons. Additionally, ST-GCN is also evaluated for Kinetics action recognition. <ref type="table">Table 5</ref> shows the top-1 and top-5 classification performances. We see that AS-GCN outperforms the other com- We present the action "Use a fan" in NTU-RGB+D dataset. Both ground-truth and predicted data are shown.</p><p>petitive methods in both top-1 and top-5 accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Future Pose Prediction</head><p>We evaluate the performance of AS-GCN for future pose prediction. For each action, we take all frames except for the last ten as the input. We attempt to predict the last ten frames. <ref type="figure" target="#fig_13">Figure 10</ref> visualizes the original and predicted action. We sample five frames at regular intervals in ten. The predicted frame provides the future joint position with a low error, especially the characteristic actional body parts, e.g. shoulders and arms. As for the peripheral parts such as legs and feet, the predicted positions have larger error, which is the secondary information of the action pattern. These results show that AS-GCN preserves more detailed features especially for the action-functional joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We propose the actional-structural graph convolution networks (AS-GCN) for skeleton-based action recognition. The A-link inference module captures actional dependencies. We also extend the skeleton graphs to represent higher order relationships. The generalized graphs are fed to AS-GCN block for a better representation of actions. An additional future pose prediction head captures more detailed patterns through self-supervision. We validate AS-GCN in action recognition using two data sets, NTU-RGB+D and Kinetics. The AS-GCN achieves large improvement compared with the previous methods. Moreover, AS-GCN also shows promising results for future pose prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof 1</head><p>The operations in actional graph convolution (AGC) are all linear, as well as the structural graph convolution (SGC). The AGC satisfies</p><formula xml:id="formula_16">AGC(aX 1 + bX 2 ) = C c=1? (c) act (aX 1 + bX 2 ) W (c) act = a C c=1? (c) act X 1 W (c) act + b C c=1? (c) act X 2 W (c) act = aAGC(X 1 ) + bAGC(X 2 ).</formula><p>Similarly, SGC satisfies</p><formula xml:id="formula_17">SGC(aX 1 + bX 2 ) = L =1 p?P M (p, ) struct ?? (p) (aX 1 + bX 2 )W (p, ) struc = aSGC(X 1 ) + bSGC(X 2 ).</formula><p>With both AGC and SGC operations, the actional-structural convolution (ASGC) is formulated as</p><formula xml:id="formula_18">Y 1 = ASGC (X 1 ) = (1 ? ?)SGC (X 1 ) + ?AGC (X 1 ) ,</formula><p>which is a linear summation of AGC and SGC. Therefore, we have</p><formula xml:id="formula_19">ASGC (aX 1 + bX 2 ) = (1 ? ?)SGC (aX 1 + bX 2 ) + ?AGC (aX 1 + bX 2 ) , = (1 ? ?)(aSGC (X 1 ) + bSGC (X 2 )) +?(aAGC (X 1 ) + bAGC (X 2 )), = a((1 ? ?)SGC(X 1 ) + ?AGC(X 1 )) +b((1 ? ?)SGC(X 2 ) + ?AGC(X 2 )) = aASGC(X 1 ) + bASGC(X 2 ) = aY 1 + bY 2 .</formula><p>The ASGC is a linear operation for the input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Model Architectures</head><p>In this section, we show the detailed architectures of the proposed AS-GCN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A-links Inference Module (AIM) Encoder</head><p>Given the 3D joint positions of n joints across T frames, X ? R n?3?T , we first downsample the videos to obtain 50 frames from the valid frames at regular intervals. If T &lt; 50, we pad the sequences to be 50 frames with 0. For any joint v i on the body, where i ? {1, 2, . . . , n}, we represent the joint feature across 50 frames as x i ? R 150 . We set that there are four types of A-links for actional dependencies (including the ghost link). As for link feature aggregation, we use average operation for all links surrounding one joint. The operations in the encoder in AIM are presented in Table 6. The activation functions of MLPs in the encoder are </p><formula xml:id="formula_20">i , p (1) j p (1) i ? p (1) j Q (2) i,j Q (2) i,: ( 1 n n j=1 Q (2) i,j ) ? p (1) i p (2) i 384 elu ?? 128 elu ?? 128 (bn) p (2) i , p (2) j p (2) i ? p (2) j Q (3) i,j Q (3) i,j 256 elu ?? 128 elu ?? 128 (bn) ? 4 A i,j,:</formula><p>exponential linear unit (elu) functions, and 'bn' denotes the batch normalization to the features. ? is the concatenation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder</head><p>We present the detailed configuration of the decoder of AIM. Given the position of joint v i at time t, x t i , the decoder aims to predict the future joint position x t+1 i conditioned on the sourrounding A-links, A i,j,: . The architectures are presented in <ref type="table" target="#tab_8">Table 7</ref>. GRU(?) denotes a GRU unit, whose hid- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Operation Output  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>The backbone network of the AS-GCN extracts the rich spatial and temporal feature of actions with the proposed ASGC and temporal CNN (T-CN). For example, we build AS-GCN on NTU-RGB+D dataset and Cross-Subject benchmark <ref type="bibr" target="#b21">[22]</ref>. There are 25 joints, 3D spatial positions and 300 padded frames for each action. The architecture of the backbone is presented in <ref type="table">Table 8</ref>. There are <ref type="table">Table 8</ref>: The architecture of the backbone network of AS-GCN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In-Shape</head><p>Operation Shape Out-Shape nine ASGC blocks consisting of the backbone of AS-GCN model. The input/output feature maps are 3D tensors, where the three axes represent the joint number, feature dimension and frame number, respectively. The shapes of operations have the consistent dimensions with input and output feature maps, where the first axis is the filter number or output feature dimension, and the other three correspond to the input shape. A and B are the types of A-links and Slinks. For action recognition, we obtain the last feature map whose shape is <ref type="bibr" target="#b24">[25,</ref><ref type="bibr">256,</ref><ref type="bibr">75]</ref> and apply a global average pooling operation on the time and joint axis, i.e. the 1st and 3rd axis. Thus we obtain a semantic feature vector of the action, whose dimension is 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future Action Prediction Head</head><p>The architecture of the future action prediction head of AS-GCN model are presented in <ref type="table" target="#tab_9">Table 9</ref>. We input the output feature map from the backbone network in to the prediction head. The input tensor are calculated by nine ASGC blocks. The first five blocks reduce the frame number to aggregate higher-level action features. The last four blocks work on action regeneration. For the last four blocks, we concatenate the last input frame to each feature map. Fi- </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>An example of the skeleton graph, S-links and A-links for walking. In each plot, the links from "Left Hand" to its neighbors are shown in solid lines. (a) Skeleton links with limited neighboring range; (b) S-links, allowing "Left Hand" to link to the entire arm; (c) A-links, capturing long-range action-specific relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>A-links inference module (AIM). To infer the A-link between two joints, the joint features are concatenated and fed into the encoder-decoder formed AIM. The encoder produces the inferred A-links and the decoder generates the future pose conditioned on the A-links and previous actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>0 is the probability of isolation. Here we set the prior A (0) :,:,0 = P 0 and A (0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>An AS-GCN block consists of ASGC, T-CN, and other operations: batch normalization (BN), ReLU and the residual block. The shapes of data are above the BN and ReLU blocks. The shapes of network parameters are under ASGC and T-CN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 Figure 6 :</head><label>16</label><figDesc>The backbone network of AS-GCN, which includes nine AS-GCN blocks. The feature dimensions are presented.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Future prediction head of AS-GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(b) Kick something (c) Taking a selfie (a) Hand waving</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>A-links in actions. We plot the A-links with probabilities larger than 0.9. The wider line denotes the larger probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Feature responses in the last layer of AS-GCN backbone. The areas of translucid circles indicates the response magnitudes. Plot (a) shows the feature maps of action 'hand waving' in different frames; Plot (b) shows the feature maps of other two actions; Plot (c) compares AS-GCN to ST-GCN on 'hand waving'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>The predicted action samples from prediction module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>3</head><label>3</label><figDesc>den feature dimension is 64. It predicts the future position of all joints conditioned on A-links and previous frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The recognition accuracy on NTU-RGB+D Cross-</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Recognition accuracy with various number of A-link types and different priors of the ghost links.</figDesc><table><row><cell>C</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell cols="6">Acc 84.6% 86.5% 86.8% 85.8% 83.3%</cell></row><row><cell>P0</cell><cell>0.99</cell><cell>0.95</cell><cell>0.50</cell><cell>0.20</cell><cell>0.00</cell></row><row><cell cols="6">Acc 86.0% 86.8% 84.3% 82.7% 81.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The recognition results of models with/without prediction heads on NTU-RGB+D Cross-Subject are listed, where the models use AS-links. We tune the order of S-links from 1 to 4.</figDesc><table><row><cell cols="3">Polynomial order AS-links + Pred</cell></row><row><cell>1</cell><cell>83.2%</cell><cell>84.0%</cell></row><row><cell>2</cell><cell>83.7%</cell><cell>84.3%</cell></row><row><cell>3</cell><cell>84.4%</cell><cell>85.1%</cell></row><row><cell>4</cell><cell>86.1%</cell><cell>86.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of action recognition performance on NTU-RGB+D. The classification accuracies on both Cross-Subject and Cross-View benchmarks are presented.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The architecture of the encoder in AIM</figDesc><table><row><cell cols="2">Input</cell><cell></cell><cell cols="2">Operation</cell><cell>Output</cell></row><row><cell>p (0) i</cell><cell>= x i</cell><cell>150</cell><cell>elu ?? 128</cell><cell>elu ?? 128 (bn)</cell><cell>(1) p i</cell></row><row><cell>p (1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>The architecture of the decoder in AIM</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>The architecture of the prediction head of AS-GCN</figDesc><table><row><cell>model</cell><cell></cell><cell></cell></row><row><cell>In-Shape</cell><cell>Operation Shape</cell><cell>Out-Shape</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We are supported by The High Technology Research and Development Program of China (2015AA015801), NSFC (61521062), and STCSM (18DZ2270700).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Theorem Proof</head><p>Theorem 1 The actional-structural graph convolution is a valid linear operation; that is, when Y 1 = ASGC (X 1 ) and Y 2 = ASGC (X 2 ).</p><p>Then, aY 1 + bY 2 = ASGC (aX 1 + bX 2 ), ?a, b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C: More Future Pose Predictions</head><p>More future action prediction results of different actions are illustrated in <ref type="figure">Figure 11</ref>, which contains the action of 'wipe face', 'throw' and 'nausea or vomiting condition'. As we see, the actions are predicted with very low error.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World-Wide Web Conference (WWW)</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>abs/1611.08097</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast resampling of three-dimensional point clouds via graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vetro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kova?evi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="666" to="681" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoderdecoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Integrating perceptual and cognitive modeling for adaptive and intelligent human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Schoelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="1272" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A string of feature graphs model for recognition of complex activities in natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="2595" to="2602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="2466" to="2472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="2688" to="2697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="786" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018-02" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Skeletonbased action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An end-toend spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017-02" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Approaches and applications of virtual reality and gesture recognition: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Sudha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sriraghav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Abisheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manisha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Ambient Computing &amp; Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep progressive reinforcement learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="5323" to="5332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018-02" />
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Skeletonaided articulated motion generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang ; T-Cn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (ACMMM)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
	<note>25,3,300] ASGC:[64,1,64,1]?(nA+nS. 25,64,300. 64,1,64,7], stride=1 [25,64,300] ASGC:[64,1,64,1]?(nA+nS) [25,64,300] T-CN:[64,1,64,7], stride=1 [25,64,300] ASGC:[64,1,64,1]?(nA+nS) [25,64,300] T-CN:[64,1,64,7], stride=1 [25,64,300] ASGC:[64,1,64,1]?(nA+nS) [25,128,150</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T-Cn ; T-Cn ; T-Cn ; T-Cn ; T-Cn ; T-Cn ; T-Cn ; T-Cn ; T-Cn ; T-Cn</forename></persName>
		</author>
		<idno>stride=1 [25,128,150] ASGC:[128,1,128,1]?(nA+nS) [25,128,150</idno>
		<imprint/>
	</monogr>
	<note>128,1,64,7], stride=2 [25,128,150] ASGC:[128,1,128,1]?(nA+nS) [25,128,150. 128,1,128,7. 128,1,128,7. stride=1 [25,128,150] ASGC:[128,1,128,1]?(nA+nS. 25,256,75. 256,1,128,7. stride=2 [25,256,75] ASGC:[256,1,256,1]?(nA+nS) [25,256,75. 256,1,256,7], stride=1 [25,256,75] ASGC:[256,1,256,1]?(nA+nS. 25,256,75. 256,1,256,7], stride=1 [25,256,75] ASGC:[128,1,256,1]?(nA+nS. 25,128,39. 128,1,128,7. stride=2 [25,128,39] ASGC:[128,1,128,1]?(nA+nS. 25,128,19. 128,1,128,7. stride=2 [25,128,19] ASGC:[128,1,128,1]?(nA+nS) [25,128,10. 128,1,128,7], stride=2 [25,128,10] ASGC:[128,1,128,1]?(nA+nS) [25,128,5] T-CN:[128,1,128,3], stride=2 [25,128,5] ASGC:[128,1,128,1]?(nA+nS) [25,128,1. 128,1,128,5], stride=1 [25,131,1] ASGC:[64,1,131,1]?(nA+nS) [25,64,1] T-CN:[64,1,64,1], stride=1 [25,67,1] ASGC:[32,1,67,1]?(nA+nS. 25,32,1] T-CN:[32,1,32,1], stride=1 [25,35,1] ASGC:[30,1,35,1]?(nA+nS) [25,30,1] T-CN:[30,1,30,1], stride=1 [25,33,1] FC:[30,1,33,1] [25,30,1</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
