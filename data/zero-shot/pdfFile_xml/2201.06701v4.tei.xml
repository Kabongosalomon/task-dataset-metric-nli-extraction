<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Motion In-betweening via Deep ?-Interpolator</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
							<email>boris.oreshkin@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Unity Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Valkanas</surname></persName>
							<email>antonios.valkanas@mail.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F?lix</forename><forename type="middle">G</forename><surname>Harvey</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Unity Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Simon</forename><surname>M?nard</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Unity Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Bocquelet</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Unity Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Coates</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Motion In-betweening via Deep ?-Interpolator</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that the task of synthesizing human motion conditioned on a set of key frames can be solved more accurately and effectively if a deep learning based interpolator operates in the delta mode using the spherical linear interpolator as a baseline. We empirically demonstrate the strength of our approach on publicly available datasets achieving state-ofthe-art performance. We further generalize these results by showing that the ?-regime is viable with respect to the reference of the last known frame (also known as the zero-velocity model). This supports the more general conclusion that operating in the reference frame local to input frames is more accurate and robust than in the global (world) reference frame advocated in previous work. Our code is publicly available at https://github.com/boreshkinai/delta-interpolator.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computer generated imagery (CGI) has enabled the generation of high quality special effects in the movie industry with the movie making process becoming ever more technologically intensive as time progresses <ref type="bibr" target="#b13">(Ji and Waterman 2010)</ref>. Animating 3D characters is integral to the process of creating high quality CGI in films and in a variety of other contexts. Unsurprisingly, 3D computer animation is at the core of modern film, television and video gaming industries <ref type="bibr" target="#b0">(Beane 2012)</ref>. Traditional animation workflows rely on the 3D sketches of the most important key-frames in the produced character motion sequence drawn by experienced artists. For example, one viable animation workflow relies on a senior artist drawing a few 3D key-frames pinpointing the most important aspects of the authored 3D sequence and a few less senior animators working out the details in-between these key-frames. Another viable computer aided animation workflow relies on massive motion capture (MOCAP) databases, enabling an animator to query suitable motion sequences from them based on a variety of inputs, such as a few key-frames, root-motion curve, meta-data tags, etc. The discovered motion fragments can be further blended to create new motion sequences with desired characteristics. Time and effort involved in exploring and traversing such diverse databases to find and blend suitable motion segments can be a significant bottleneck. <ref type="bibr">Copyright ? 2023</ref>, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. <ref type="figure">Figure 1</ref>: The demonstration of the proposed ?-interpolator approach (right, green), Linear interpolator baseline (middle, yellow), ground truth motion (left, white). Positional errors are indicated using red mask. The animation is best viewed in Adobe Reader.</p><p>These tedious processes can be automated to a certain extent by filling the missing in-between frames using an array of techniques including linear interpolation, motion blending trees and character specific MOCAP libraries. However, a more promising avenue to enable data-driven motion generation has been unblocked more recently by the advancement in deep learning algorithms <ref type="bibr" target="#b10">(Harvey et al. 2020;</ref><ref type="bibr" target="#b4">Duan et al. 2022)</ref>. Deep learning models that are trained on massive high quality MOCAP data, when conditioned on a few keyframes, are capable of producing reasonably high quality and relatively long animation sequences, and thus save time for animators. This data driven approach leverages existing high quality MOCAP footage and is able to re-purpose it towards creating new animation sequences, providing a simple yet powerful user interface. In this new type of AI assisted workflow, the deep learning model fulfils a few critical functions. First, the model compresses a MOCAP dataset compactly, encoding it in its weights by developing an internal latent space representation of motion. Second, it exposes comprehensive inputs, enabling a familiar yet flexible and powerful user interface by accepting key-frame conditioning from the animator. Finally, the deep neural network queries its latent space with the information provided by the user and outputs a highly believable motion sequence matching natural motion statistics that is ideally blended into user-provided key-frames. It is clear that deep learning will continue to expand its reach and strive for domination in the field of 3D computer animation.</p><p>The extent and velocity of this process depends significantly on the progress in improving accuracy, robustness and flexibility of deep motion models, which is the focus of our current paper. We propose a configuration of the deep motion in-betweening interpolator that has better accuracy than state-of-the-art approaches and is more robust to the out-of-distribution changes due to the local nature of the deep neural network inputs and outputs. Additionally, we contribute to democratizing the animation authoring by making our model code publicly available 1 . A sample animation of our approach in action is shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Summary of Contributions</head><p>1. We propose a novel deep ?-interpolator motion synthesis architecture. We make our model code public. It is worth noting that the most recent SOTA works in this domain still have no publicly released implementations <ref type="bibr" target="#b10">(Harvey et al. 2020;</ref><ref type="bibr" target="#b4">Duan et al. 2022</ref>). 2. Recent study advocates a global coordinate system for deep motion synthesis <ref type="bibr" target="#b4">(Duan et al. 2022)</ref>. We provide empirical evidence of the advantage of the local motion synthesis operation by achieving SOTA results on two public datasets (LaFAN1 and Anidance). 3. We make our processed data public along with the dataloaders and evaluation scripts, unlike other works, lowering the entry barrier to this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Conditional motion synthesis, and more specifically motion completion, is the process of filling the missing frames in a temporal sequence given a set of key-frames. Early work focused on generating missing in-between frames by integrating key-frame information into space-time models <ref type="bibr" target="#b27">(Witkin and Kass 1988;</ref><ref type="bibr" target="#b19">Ngo and Marks 1993)</ref>. The next generation of in-betweening works developed a more sophisticated probabilistic approach to the constrained motion synthesis, formulating it as a maximum a posteriori (MAP) problem <ref type="bibr" target="#b1">(Chai and Hodgins 2007)</ref> or a Gaussian process <ref type="bibr" target="#b7">(Grochow et al. 2004;</ref><ref type="bibr" target="#b26">Wang, Fleet, and Hertzmann 2007)</ref>. Over the past decade, deep learning has become the modus operandi, significantly outperforming probabilistic approaches. Due to the temporal nature of the task, RNNs have dominated the field compared to other neural network architectures <ref type="bibr" target="#b12">(Holden, Saito, and Komura 2016;</ref><ref type="bibr" target="#b9">Harvey and Pal 2018)</ref>. The latest RNN work by <ref type="bibr" target="#b10">Harvey et al. (2020)</ref> focuses on augmenting a Long Short Term Memory (LSTM) based architecture with time-to-arrival embeddings and a scheduled target noise vector, allowing the system to be robust to target distortions. <ref type="bibr" target="#b5">Geleijn et al. (2021)</ref> propose a lightweight and simplified recurrent architecture that maintains good performance, while 1 https://github.com/boreshkinai/delta-interpolator significantly reducing the computational complexity. Beyond RNN models, motion has been modelled by 1D temporal convolutions <ref type="bibr" target="#b11">(Holden, Saito, and Komura 2015)</ref>. More recent work has relied on Transformer based models predicting the entire motion in one pass unlike auto-regressive recurrent models <ref type="bibr" target="#b2">(Devlin et al. 2019;</ref><ref type="bibr" target="#b4">Duan et al. 2022)</ref>. Motion in-betweening is an instance of conditional motion generation, in which the conditioning signal has the form of key-frames. Therefore, methods using other types of conditioning to generate motion are related to ours. These range from audio conditioned human motion generation such as creating dances for a musical piece <ref type="bibr" target="#b14">(Kao and Su 2020;</ref><ref type="bibr" target="#b18">Li et al. 2021b)</ref>, to semantically conditioned motion <ref type="bibr" target="#b8">(Guo et al. 2020;</ref><ref type="bibr" target="#b20">Petrovich, Black, and Varol 2021)</ref>.</p><p>While being similar to our setting, motion prediction works by <ref type="bibr" target="#b19">(Martinez, Black, and Romero 2017;</ref><ref type="bibr" target="#b25">Wang et al. 2022)</ref> are not directly applicable. First, they typically rely on dense key-frame conditioning prior to the beginning of the missing frames <ref type="bibr" target="#b25">(Wang et al. 2022)</ref> and do not support conditioning on a future frame. Additionally, to reduce the error they sometimes rely on costly Inverse Kinematics (IK) projections to preserve limb lengths <ref type="bibr" target="#b25">(Wang et al. 2022)</ref> or allow limb dislocations <ref type="bibr" target="#b15">(Kaufmann et al. 2020)</ref>. In a high quality animation authoring setting the computational cost of IK or the error that can result from skipping IK are unsustainable. Furthermore, in classical animation workflows we have a sparse key-frame distribution across a sequence with gaps of regular intervals rather than contiguous blocks of key-frames followed by a large gap as in motion prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Statement and Background</head><p>For a skeleton with J joints, we define the tensor x t ? R J?9 fully specifying the pose at time t. It contains the concatenation of the global positions p t ? R J?3 as well as rotations r t ? R J?6 . By default, the rotation of the root joint is defined in the global coordinate system, whereas the rest of the joints specify rotations relative to their parent joints, unless explicitly stated otherwise.</p><p>For positions, we use a standard 3D Euclidean representation. For joint rotations, we use the robust ortho6D representation proposed by <ref type="bibr" target="#b28">Zhou et al. (2019)</ref>. This addresses the continuity issues of quaternion and Euler representations.</p><p>Suppose a unit vector is defined as ? ? u ? u/ u 2 and the vector cross product is u ? v = u v sin(?) ? ? n (? is the angle between u and v in the plane containing them and ? ? n is the normal to the plane). Given the ortho6D rotation r t,j ? R 6 , the rotation matrix of joint j at time t, R t,j , is:</p><formula xml:id="formula_0">R x t,j = ? ??? ? r t,j [: 3], R z t,j = ? ????????? ? R x t,j ? r t,j [4 :], R y t,j = R z t,j ? R x t,j , R t,j = R x t,j R y t,j R z t,j .</formula><p>The model observes a subset of samples in the time window T = {1, . . . , T } of total length T , defined as X ? {x t : t ? T in }, T in ? T . The model solves the in-betweening task on the subset T out = T \ T in of missing samples Y ? {x t : t ? T out } by predicting global positions and rotations of the root joint and local rotations of the other joints. This is sufficient to fully specify the pose by computing global positions and rotations of all joints after the Forward Kinematics (FK) pass. The forward kinematics pass operates on a skeleton defined by the offset vectors o j = [o x,j , o y,j , o z,j ] specifying the displacement of each joint with respect to its parent joint when joint j rotation is zero (the norm of the offset vector is the bone length, which we consider to be constant). Provided with the local offset vectors and rotation matrices of all joints, the global rigid transform of any joint j is computed following the tree recursion from its parent joint p(j):</p><formula xml:id="formula_1">G t,j = G t,p(j) R t,j o j 0 1 .</formula><p>The global transform matrix G t,j of joint j contains its global rotation matrix, G t,j [1 : 3, 1 : 3], and its 3D global position, G t,j [1 : 3, 4].</p><p>In this paper we consider the variation of the in-betweening problem defined in the LaFAN1 benchmark <ref type="bibr" target="#b9">(Harvey and Pal 2018)</ref>: A neural network is provided with 10 past poses and one future pose, the task being to fill the frames in between. We also consider the in-filling version of the problem as posed in <ref type="figure" target="#fig_3">Figure 4</ref> of <ref type="bibr" target="#b3">Duan et al. (2021)</ref> using the Anidance benchmark, where key-frames are distributed throughout the sequence in regular intervals of one key-frame every 6 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>In this section we present the proposed ?-interpolator approach and its ingredients. We first introduce the spherical linear interpolator (SLERP) serving as the foundation for the proposed delta principle. We further discuss the details of the proposed neural network architecture, including the composition of its inputs and outputs as well as the mathematical details of the neural architecture. A high level block diagram is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. We close the section with the detailed analysis of the architectural novelty of the proposed method with respect to the most closely related work by <ref type="bibr" target="#b4">Duan et al. (2022)</ref> and the description of loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The SLERP Interpolator Baseline</head><p>Common tool-chains in the animation industry often offer the linear interpolator and blending curves as a tool for motion inbetweening. The curves require a significant amount of manual tweaking to make high quality motion transitions. The linear interpolator generates inbetween root motion via linear interpolation and inbetween local rotations via spherical linear interpolation (SLERP) in the quaternion space (Shoemake 1985). We denote Y the SLERP interpolator output computed based on X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The ?-Interpolator</head><p>We propose the ?-interpolator approach in which the output of the neural network f ? with parameters ? is defined as a delta w.r.t. the prediction of the linear interpolator, and its input is defined as a delta w.r.t. a reference X ref ? R 9 . The latter can be defined, for example, as the concatenation of the 3D coordinate and 6D rotation of the root joint of the last history frame. The ?-solution, ?Y: belongs to the space local to the reference frame implicitly defined by the prediction of the linear interpolator, Y. Additionally, the input to f ? is localized with respect to the reference frame X ref defined based on the information available in X. The intuition behind the proposed ?-interpolator is two-fold. First, if the linear interpolator provides a good initial prediction Y, solving the ?-interpolation task should be easier, requiring less training data and resulting in better generalization accuracy. Second, since the output is defined in the reference frame relative to the linear interpolator, it provides the opportunity for defining the input relative to a reference X ref without loss of information. 2 As a result, the ?-interpolator neural network does not have to deal with any absolute domain shifts (either at the input or at the output). This improves its ability to cope with any global reference frame shifts and out-of-distribution domain shifts arising between training and inference scenarios. As an example, suppose the test data is defined in a reference frame that is shifted with respect to the original training reference frame by an arbitrary constant delta ?p. In our experience, even relatively small ?p, comparable to the span of the skeleton, is enough to significantly degrade performance. In contrast, it is easy to show that the ?-interpolator defined in equations (1-2) is completely insensitive to it. First, the domain shift in X will be compensated by that in X ref and the input to the neural network will essentially be the same with or without ?p. Moreover, the domain shift will be added to the output of the ?-interpolator in equation <ref type="formula" target="#formula_3">(2)</ref> via Y, resulting in the correct reference shift presented at the output.</p><formula xml:id="formula_2">?Y ? f ? (X ? X ref ),<label>(1)</label></formula><formula xml:id="formula_3">Y = Y + ?Y,<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Neural Network</head><p>In this section we describe the details of the proposed neural architecture, graphically depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. We start by defining some standard notions from the Transformer literature. We use the Transformer approach with a multi-head self-attention encoder for the key-frames and a multi-head cross-attention encoder for the missing frames. Following the original Transformer formulation <ref type="bibr" target="#b24">(Vaswani et al. 2017)</ref>, the attention unit is defined as:</p><formula xml:id="formula_4">ATT(Q, K, V) = SOFTMAX(QK T / ? d)V,<label>(3)</label></formula><p>where d is model-wide hidden dimension width. The multihead attention unit is defined as:</p><formula xml:id="formula_5">h i = ATT(QW Q i , KW K i , VW V i ), MHA(Q, K, V) = CONCAT(h 1 , . . . , h h )W O .</formula><p>Next, we describe the details of the proposed architecture.</p><p>Inputs As mentioned previously, the input of the network is localized w.r.t. X ref . We define X ref ? R |Tin|?J?9 by (i) setting all its position components along time and joint axes to the root position at the last available frame; and (ii) setting all its rotation components along time and joint axes to the root rotation at the last available frame. Furthermore, f ? accepts (i) e K 0 : the key-frame embedding initialized as the concatenation of X ? X ref with the learnable input position encoding, linearly projected into model width d; as well as (ii) e M 0 : the missing frame template initialized as the learnable output position encoding, linearly projected into model width d. Input and output position embeddings are shared and consist of a learnable vector of size 32 for each possible frame index t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Key-frame encoder</head><p>The key-frame encoder is described by the following recursion over ? (0, R] encoder residual 2 Note that an approach that predicts absolute global positions would necessarily incur information loss if its input was made local.</p><p>blocks taking e K 0 as input:</p><formula xml:id="formula_6">e K = MHA (e K ?1 , e K ?1 , e K ?1 ), e K = RELU(LAYERNORM (e K + e K ?1 )), e K = MLP (e K ).</formula><p>The purpose of the key-frame encoder is to create a deep representation of key-frames that can be further cross-correlated with the templates of missing frames in the missing frame encoder. Using a separate encoder for key frames is computationally efficient. Typically, the number of key frames is low and they encapsulate all the information to infer the missing in-between frames. Therefore, processing them in the same self-attention block as the missing frames, as proposed by <ref type="bibr" target="#b4">Duan et al. (2022)</ref>, is unnecessary from the information processing standpoint and cost-ineffective from the computational standpoint. See Section 4.4 for an extended discussion.</p><p>Missing frame encoder The missing frame encoder iterates over R residual blocks, cross-processing the key-and missing frame representations. At each level , it accepts the key-frame encodings e K created by the key-frame encoder and the missing frame encodings from the previous level, e M ?1 . The first level = 1 accepts the missing frame template,</p><formula xml:id="formula_7">e M 0 . e M = MHA (e M ?1 , e K , e K ), e M = RELU(LAYERNORM (e M + e M ?1 )), e M = MLP (e M ).</formula><p>Obviously, the missing frame encoder is where the known key-frames meet the unknown missing in-between frames. Note that the common self-attention block in <ref type="bibr" target="#b4">(Duan et al. 2022</ref>) accepts both key-frames and missing frames, crosspolluting their representations. Our architecture implements a different information processing flow, emphasizing the cause and effect relationship between inputs and outputs in the inbetweening problem. Hence, in the case of our architecture design, the information flows in a directed fashion from the key-frames that are provided by the user as a conditioning signal towards the missing frames that are inferred at the output.</p><p>Decoder The decoder is an MLP that maps the representations of key-frames and missing frames, e K L and e M L , into the predictions of full poses of all frames in the sequence:</p><formula xml:id="formula_8">?Y, ?X = MLP D (e M L ), MLP D (e K L )</formula><p>. Note that the MHA and MLP blocks are shared between the key-frame and missing frame encoder blocks; similarly, the same decoder is used to output key-frame reconstructions and missing frame predictions.</p><p>Outputs The ?-predictions ?Y, ?X supplied by the decoder contain a J ? 6 rotation prediction (global rotation of the root plus local rotations of all other joints in their parent-relative coordinate systems) and a 1 ? 3 global root joint position prediction. These are further subjected to the skeleton forward kinematics pass to derive the full pose missing frame predictions Y ? R |Tout|?J?9 and the key-frame Autoencoder <ref type="formula">(</ref> 4.4 Architectural novelty with respect to <ref type="bibr" target="#b4">(Duan et al. 2022)</ref>.</p><p>The architecture in <ref type="bibr" target="#b4">(Duan et al. 2022</ref>) is based on a single selfattention module; key-frames concatenated with the linear interpolator predictions are used as input to the self-attention; the key-frames and the missing frames are marked with the key-frame encoding so that the neural network knows which frames to predict. We experimentally confirm in Section 5 several important observations. First, when we use self-attention for the key-frames and cross-attention for the missing frames, the key-frame encoding is not needed. Second, we show that MHA and MLP blocks can be shared in the self-attention and cross-attention blocks. Weight sharing is important for reducing the model size. Third, the architecture of <ref type="bibr" target="#b4">Duan et al. (2022)</ref> is not operational without the interpolator providing input initialization. Therefore, using this architecture in contexts such as motion prediction is not viable. In our case, neither X ref nor Y has to be based on a linear interpolator. We show that our approach can use the last frame reference both for X ref and Y, while the missing frame input is initialized with zeros. This makes our approach significantly more general (e.g., also suitable for motion prediction applications). Finally, processing key-frames via self-attention and missing frames via cross-attention is more computationally efficient. Indeed, for a problem with n k key-frames and n in inbetween frames, attention complexity (i.e., the complexity of equation (3)) of <ref type="bibr" target="#b4">(Duan et al. 2022</ref>) scales as (n k + n in ) 2 , whereas our approach's attention complexity scales as n 2 k +n k n in . The ratio of the two quantities is 1 + n in /n k . Suppose n in = 30 and n k = 11, we have that the attention complexity of our approach is asymptotically 1 + 30/11 = 3.7 times smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Losses</head><p>The neural network is supervised using a combination of quaternion and position losses, similar to <ref type="bibr" target="#b4">(Duan et al. 2022)</ref>:</p><formula xml:id="formula_9">L TOT = L quat + L pos .</formula><p>The position loss is L1 loss defined both on missing and keyframes. In the case of missing frames we call it predictive position loss. In the case of key-frames we call it the reconstruction position loss, as then the network acts as an auto encoder. X pos and Y pos are position only components of X and Y, respectively (e.g., X pos ? {p t : t ? T in }).</p><formula xml:id="formula_10">L pos = Y pos ? Y pos 1 + X pos ? X pos 1 .<label>(4)</label></formula><p>Rotational output is supervised with the L1 loss based on quaternion representations X quat ? R B?|Tin|?4 and Y quat ? R B?|Tout|?4 derived from the ortho6D predictions of global rotations contained in X and Y:</p><formula xml:id="formula_11">L quat = Y quat ? Y quat 1 + X quat ? X quat 1 . (5)</formula><p>The L1 norm here is taken over the last tensor dimension whereas the two leading dimensions are average pooled. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section we present our empirical results. The section starts with the description of the datasets, as well as the benchmark and metrics used for the qualitative evaluation of our model. We then carefully describe the details of training and evaluation setups as well as the computational budget used to produce our results. Our key results are presented in <ref type="table" target="#tab_0">Tables 1  and 2</ref>, demonstrating the state-of-the-art performance of our method relative to existing work. Finally, we conclude this section by describing the ablation studies we conducted. The results of ablations confirm the significance and flexibility of our ?-interpolator approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Benchmarking</head><p>We empirically evaluate our method on the well-established motion synthesis benchmarks LaFAN1 <ref type="bibr" target="#b10">(Harvey et al. 2020)</ref> and Anidance <ref type="bibr" target="#b23">(Tang, Mao, and Jia 2018)</ref>. LaFAN1 consists of 496,672 motion frames captured in a production-grade MOCAP studio. The footage is captured at 30 frames per second for a total of over 4.5 hours of content. There are 77 sequences in total with 15 action categories that include a diverse set of activities such as walking, dancing, and fighting. Each sequence contains actions performed by one of 5 subjects (MOCAP actors). Sequences with subject 5 are used as the test set. We build the training and test sets following Harvey et al. <ref type="formula" target="#formula_3">(2020)</ref>, sampling smaller sliding windows from the original long sequences. We sample our training windows from sequences acted by subjects 1-4 and retrieve windows of 50 frames with an offset of 20 frames. We repeat a similar exercise for the test set, sampling sequences of subject 5 at every 40 frames, yielding 2232 test windows of 65 frames. We also extract the same statistics used for data normalization as outlined in <ref type="bibr" target="#b10">(Harvey et al. 2020</ref>), applying the same data normalization procedures. The Anidance dataset was originally proposed as a music-to-dance generation dataset <ref type="bibr" target="#b23">(Tang, Mao, and Jia 2018)</ref>. Anidance contains 61 dance sequences from 4 genres shot at 25 frames per second. In total, the dataset is composed of 101,390 frames of global positional coordinates of the skeletal joints (no joint rotations are provided). <ref type="bibr" target="#b4">Duan et al. (2022)</ref> discarded the audio features and used the dataset as a motion completion benchmark following pre-processing steps similar to LaFAN1 with windows and offset of size 128 and 64 frames respectively for both train and test splits. We report our key empirical results in Tables 1, 2, demonstrating state-of-the-art performance of our method. Our results are based on 8 different random seed runs of the algorithm and metric values averaged over the 10 last optimization epochs. Following previous work, we consider L2Q (the global quaternion squared loss (6)), L2P (the global position squared loss <ref type="formula" target="#formula_13">(7)</ref>) and NPSS (the normalized power spectrum similarity score) <ref type="bibr" target="#b6">(Gopalakrishnan et al. 2019)</ref>. <ref type="table" target="#tab_1">Table 2</ref> does not contain L2Q and NPSS metrics because the Anidance dataset does not contain angular information.</p><formula xml:id="formula_12">L2Q = 1 |D| 1 |T out | s?D t?Tout || q s t ? q s t || 2 ,<label>(6)</label></formula><formula xml:id="formula_13">L2P = 1 |D| 1 |T out | s?D t?Tout || p s t ? p s t || 2 ,<label>(7)</label></formula><p>where q t ? R J?4 represents the quaternion vector of all skeletal joints at time t, p t ? R J?3 denotes the global position vectors, superscript s denotes sequences in the test dataset D. We measure the metrics for scenarios with |T out | ? {5, 15, 30} missing in-between frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training and Hyperparameters</head><p>The training loop is implemented in PyTorch <ref type="bibr" target="#b20">(Paszke et al. 2019)</ref> using the Adam optimizer <ref type="bibr" target="#b16">(Kingma and Ba 2015)</ref>. The learning rate is warmed to 0.0002 during the first 50 epochs using a linear schedule and it steps down by a factor of 10 at epoch 250; training is continued until epoch 300. Most 3D geometry operations are handled by pytorch3d <ref type="bibr" target="#b21">(Ravi et al. 2020)</ref>. Batch sampling. Following previous work, we split the entire dataset into windows of maximum length T max (see supplementary for the details of building the datasets). To construct each batch of size 64, we set the number of the start key-frames to be 10 for LaFAN1, 1 for Anidance and the number of the end key-frames to be 1 for both datasets. We then sample the number of in-between frames from the range <ref type="bibr">[5,</ref><ref type="bibr">39]</ref> without replacement. We employ weighted sampling, and the weight associated with the number of in-between frames n in is set to be inversely proportional to it, w nin = 1/n in . This weighting prevents overfitting on the windows with a large number of in-between frames. Since Anidance has a periodic keyframe distribution, once the transition length has been sampled we provide every 6-th frame as an input frame along with the first and last frames of the sampled window and mask all other frames.</p><p>Hyperparameters and compute requirements are discussed in detail in the supplementary. The neural network encoder has 6 residual blocks R of width 1024 with 8 MHA heads and 3 MLP layers each. The decoder's MLP has one hidden layer. Each row in our empirical tables is based on 8 different random seed runs of the algorithm and metric values averaged over 10 last epochs and 8 different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>?-regime ablation The ablation of the ?-regime is presented in <ref type="table" target="#tab_2">Table 3</ref>, which compares our proposed approach (?-Interpolator) against a few variants, applying different ? configurations at the input and output of the neural network. We explored three ? configurations and one configuration with no delta-regime at either input or output. Configuration "I" indicates that the output is ? w.r.t. to the linear interpolator. Configuration "L" indicates for the input that it is in the ?-mode w.r.t X ref and for the output that it is in the ?-mode w.r.t. the Zero-Velocity model (last history frame). Finally, configuration "No" indicates no ?-mode (i.e., at the output the neural network predicts global root position directly and at the input X ref is not subtracted).</p><p>Our proposed ?-Interpolator corresponds to the configuration (Last I) in the first row of <ref type="table" target="#tab_2">Table 3</ref>. The first alternative in the ablation study (Last Last) is different in that instead of relying on SLERP as a baseline, it uses the last known frame as a baseline, both at the input and at the output, making the neural network operation completely local w.r.t. the reference frame implied by the last known frame. We can see that this leads to a small but consistent deterioration of metrics compared to the proposed ?-interpolator. It is worth pointing out, however, that this variant does not at all rely on the SLERP interpolator. Therefore, it is viable to achieve very impressive results simply using ? interpolation with respect to the last known frame. We noticed that the SLERP interpolator may be computationally quite demanding and hard to optimize on a GPU. Therefore, in applications where computation is a bottleneck, the (Last Last) configuration may be attractive. The third row of <ref type="table" target="#tab_2">Table 3</ref> is the configuration that does not rely on ? interpolation at all. We can see that it has noticeably degraded performance. Still, compared to existing methods in   <ref type="table" target="#tab_0">Table 1</ref> performance is very competitive, validating our neural network architecture design. Comparing the first and the third rows in <ref type="table" target="#tab_2">Table 3</ref> proves the effectiveness of the proposed ?-Interpolator approach. Rows 4-5 ablate the use of the ?mode at the input of the deep neural network. We can see that (i) the input ?-mode helps when the output ?-mode is set to the last frame; (ii) the positive impact of the input ?-mode is more pronounced when the transition length is longer; and (iii) ?-mode at the input has positive effect both for the last frame (Last) and the Interpolator (I) configurations of the output ?-mode. Qualitatively, the robustness of the proposed ?interpolator is demonstrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. Following the training protocol of <ref type="bibr" target="#b10">Harvey et al. (2020)</ref>, the neural network is trained with the data normalized via XZ-center and Y-rotate operations. Normally, the same operations would have to be applied at inference time to ensure proper operation of all networks operating with global inputs and outputs, including <ref type="bibr" target="#b10">(Harvey et al. 2020;</ref><ref type="bibr" target="#b4">Duan et al. 2022</ref>) and our approach with disabled ?-mode, otherwise their failure is unavoidable, which we can see clearly in <ref type="figure" target="#fig_2">Fig. 3</ref>. In contrast, the ?-interpolator demonstrates robust out-of-distribution operation, insensitive to the distribution shift induced in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion of Findings</head><p>Our results reveal SoTA performance of the ?-interpolator, defining both input and output of its deep neural network in the local coordinate frame derived from key-frames. This is opposite to the findings of <ref type="bibr" target="#b4">Duan et al. (2022)</ref>, who advocate the use of the global reference frame for neural interpolation. Our approach produces more accurate results and makes the neural network robust w.r.t. out-of-distribution domain shifts due to its local nature. We believe this is an important contribution to the methodology of developing robust and accurate in-betweening algorithms.</p><p>Our ablation studies demonstrate that our approach without ?-regime is significantly worse (although competitive against <ref type="bibr" target="#b4">(Duan et al. 2022)</ref>), confirming the importance of the proposed ?-interpolation approach. We additionally show that the last known frame can be used in lieu of SLERP as a reference, resulting in only slightly worse performance. This obviates the dependency on SLERP while doing neural in-betweening, further validates the general ?-interpolation idea laid out in equations (1) and (2), and makes our approach suitable for forward motion prediction applications.</p><p>The ablation studies (see supplementary for ablation of our proposed reconstruction loss) imply that different input and output referencing methods have a noticeable effect, opening up a direction for future research. In our view, answering the questions "What is a simple and more optimal X ref ?"; and "What is a simple and more optimal Y?" has the potential to improve predictive accuracy without inflating computational costs. We believe this is pointing the community towards (i) looking for more optimal auxiliary reconstruction losses, (ii) smarter sampling of inputs, or (iii) employing data augmentation in conjunction with reconstruction as potentially promising ways to further improve generalization results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Ablation of Proposed Reconstruction Loss</head><p>Recall that the position and quaternion losses are:</p><formula xml:id="formula_14">L pos = Y pos ? Y pos 1 + X pos ? X pos 1 ,<label>(8)</label></formula><formula xml:id="formula_15">L quat = Y quat ? Y quat 1 + X quat ? X quat 1 . (9)</formula><p>The ablation study of the reconstruction loss terms is presented in <ref type="table" target="#tab_3">Table 4</ref>. The second row in this table shows the generalization result with the reconstruction loss term removed. The reconstruction term includes two parts: X pos ? X pos 1 from (8) and X quat ? X quat 1 from (9). Interestingly, these terms do not directly penalize the errors on the missing frames, but rather only the reconstruction loss on the key frames. Also, the key frames are provided as inputs and it might be logical to think that the task of reconstructing them should be trivial. However, we can clearly see that penalizing the reconstruction error on the key frames provides positive regularizing effect showing in improved metrics measured on the test missing frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameter Settings</head><p>The training loop is implemented in PyTorch <ref type="bibr" target="#b20">(Paszke et al. 2019)</ref> using Adam optimizer <ref type="bibr" target="#b16">(Kingma and Ba 2015)</ref>. The learning rate is warmed to 0.0002 during the first 50 epochs using a linear schedule and it steps down by a factor of 10 at epoch 250; training is continued until epoch 300. Most 3D geometry operations (transformations across quaternions, ortho6d and rotation matrix representations, quaternion arithmetic) are handled by pytorch3d <ref type="bibr" target="#b21">(Ravi et al. 2020)</ref>.</p><p>Following previous work, we split the entire dataset into windows of maximum length T max (see Appendix E for the details of building the dataset). To construct each batch (batch size is 64), we set the number of the start key-frames to be 10 and the number of the end key-frames to be 1. We then sample the number of in-between frames from the range <ref type="bibr">[5,</ref><ref type="bibr">39]</ref> without replacement. We employ weighted sampling, and the weight associated with the number of in-between frames n in is set to be inversely proportional to it, w nin = 1/n in . This weighting prevents overfitting on the windows with a large number of in-between frames. Shorter windows are sampled more often as they are more abundant and hence harder to overfit. Indeed, the number of unique non-overlapping sequences of a given total length 10+1+n in is approximately inversely proportional to n in . Finally, given the total sampled sequence length 1+10+n in , the sequence start index is sampled uniformly at random in the range [0, T max ? (1 + 10 + n in )].</p><p>The neural network encoder has 6 residual blocks R of width 1024 with 8 MHA heads and 3 MLP layers each. The decoder's MLP has one hidden layer. All hyperparameter settings are provided in <ref type="table" target="#tab_4">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Compute Requirements</head><p>Each row in our empirical tables is based on 8 different random seed runs of the algorithm and metric values averaged over 10 last optimization epochs and the 8 different random seed runs. One training run takes approximately 8 hours on a single NVIDIA M40 GPU on Dell PowerEdge C4130 server equipped with 2 x Intel Xeon E5-2660v3 CPUs. It takes roughly 4 days on the aforementioned server to reproduce the LaFAN1 results presented in the main paper (6 rows x 8 runs x 8 hours / 4 GPUs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D LaFAN1 Data Preparation</head><p>Since the authors of <ref type="bibr" target="#b10">(Harvey et al. 2020</ref>) did not release a PyTorch compatible data loader we implement it on our own in accordance with their description of the sliding window technique and data normalization. To ensure that our approach is correct we implement the same basic baselines as the authors, also in PyTorch, including a zero-velocity model and spherical linear interpolator (SLERP). We then validate our implementation by obtaining the same values for the metrics on the zero-velocity and SLERP baselines as the ones reported in <ref type="bibr" target="#b10">(Harvey et al. 2020)</ref>. Note that our results in the main paper report zero-velocity and SLERP metrics based on our own implementation of the data loader and the algorithms.</p><p>As previously noted, the LaFAN1 benchmark 3 <ref type="bibr" target="#b10">(Harvey et al. 2020</ref>) was provided to us in raw BVH format. To make the data usable in the PyTorch and Unity pipeline we implemented, it was necessary to first convert it to a set of regular CSV files (one for each BVH animation) and to assign the animations to a Unity engine avatar for integrating our model in the engine as shown in <ref type="figure">Figure 1</ref> of the main paper. This section outlines the steps taken for this conversion:   the Unity engine operates with left handed coordinates. The different handedness of the coordinate systems introduces a flip of the sign along the Z-axis (which we deal with by exporting negative Z-axis coordinates from Blender hence introducing another axis flip thus restoring the original coordinate orientation). However, this also changes the exported joint angular rotations which are challenging to fix and are hence left as is besides applying a quaternion discontinuity fix similar to <ref type="bibr" target="#b10">(Harvey et al. 2020)</ref>.</p><p>To validate that our approach produces high fidelity conver-sions from BVH to CSV we re-implement and compare the zero velocity and linear interpolation baselines and evaluate them on the same tasks as <ref type="bibr" target="#b10">(Harvey et al. 2020</ref>). Since these dummy models have zero parameters if our pre-processing does not compromise data integrity we should obtain very similar values to what was reported in <ref type="bibr" target="#b10">(Harvey et al. 2020</ref>).</p><p>As we can see in <ref type="table" target="#tab_5">Table 6</ref> the benchmark models perform within ? 1% thus validating our pre-processing pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Anidance Data Preparation</head><p>The Anidance benchamark 5 was originally created for musicconditioned motion generation. <ref type="bibr" target="#b4">Duan et al. (2022)</ref> extracted the human motion data and formulated a dataset similar to LaFAN1 that contains only global joint coordinates. In it's raw form the dataset is provided as a .json file. We took the following pre-processing steps:</p><p>1. Convert the .json file into a Pandas dataframe with each column corresponding to a joint coordinate. Each joint has 3 coordinates (x, y, z) and we have 24 joints in the Anidance skeleton so we have a total of 72 columns. Each row in the dataframe corresponds to one time step of the 25 FPS recording. 2. Using the same data pipeline as with the LaFAN1 data we created windows of size 128 frames with an offset of 64 frames for all sequences. Note that unlike LaFAN1 we do not perform XZ-centering, rotation along the Yaxis or any other other pre-processing besides calculating the necessary statistics for the standardization of the L2P metric.  3. After corresponding with <ref type="bibr" target="#b4">Duan et al. (2022)</ref> we were able to replicate the data split. However, since the data pipeline for the code is not public we had to re-implement it from scratch. We recover the exact same number of frames (101,390) and number of test dance sequences (323) but we obtain slightly more training sequences (1172 vs. 1117) than reported by <ref type="bibr" target="#b4">Duan et al. (2022)</ref>. The difference is smaller than 5%. 4. Our test data are identical, however since the L2P metric is standardized based on the training data this creates small discrepancies. To validate that the discrepancies are relatively minor we compare the L2P metric for zeroparameter deterministic models, namely zero-velocity and linear interpolation with respect to the reference values of <ref type="bibr" target="#b4">Duan et al. (2022)</ref> in <ref type="table" target="#tab_6">Table 7</ref>. As we can see, the L2P we recover for these baseline models is often identical with reported reference figures and in the worst case within 5% and in general appear not to be biased in either the direction of overestimating or underestimating the error.</p><p>Our results indicate improvements in the range of ?30% error reduction therefore our experimental setup is valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Animations</head><p>A non-animated figure for comparison of tracer lines of the predicted joint positions by our model vs. SLERP and the ground truth is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Additional demo videos 6 show long-horizon animation authoring using our approach with random-in-the-wild keyframe distributions outside the training set when we plug-in our model to the Unity editor.</p><p>For an objective evaluation of real world use cases of our model we strongly encourage viewing these videos. Our model is able to consistently run at over 200 frames per second inside the Unity Editor.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Proposed ?-interpolator architecture. Full-pose information from key-frames is referenced with respect to the root reference of the last frame in the past context, X ref , concatenated with positional encoding and linearly projected into model width d. Self-attention block encodes key-frame representations. Cross-attention block combines key-frame encodings with zero-initialized and positionally encoded templates of missing frames. Six encoder blocks are followed by a decoder MLP that produces ?-predictions of root position and joint rotations. Final prediction combines SLERP interpolator output with the ?-predictions of the neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>rec (rec. loss) (Harvey et al. 2020) 0.21 0.48 0.83 0.32 0.85 1.82 0.0025 0.0304 0.1608 TG complete (rec. &amp; adv. loss) (Harvey et al. 2020) 0.17 0.42 0.69 0.23 0.65 1.28 0.0020 0.0258 0.1328 SSMCT (local) (Duan et al. 2022reconstructions X ? R |Tin|?J?9 containing J ? 6 global rotations and J ? 3 global positions of all joints. Y = FK( Y + ?Y); X = FK( X + ?X).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Robustness of the ?-interpolator (right, green), w.r.t. the out-of-distribution operation. Ground truth motion (left, white), SLERP interpolator (middle-left, yellow), proposed model with both input and output delta modes disabled (middle-right, blue). All models are fed with the data that are not subjected to the input normalization applied during training. Positional errors are indicated using red mask. ?interpolator is not affected by distribution shift. The animation is best viewed in Adobe Reader.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of human motion prediction. (Left): Our model. (Center): Linear interpolation. (Right): Ground Truth. Tracer lines represent evolution of joint locations over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Key Empirical Results on LaFAN1 dataset. Lower score is better. .10 1.51 1.52 3.69 6.60 0.0053 0.0522 0.2318 SLERP Interpolation 0.22 0.62 0.98 0.37 1.25 2.32 0.0023 0.0391 0.2013</figDesc><table><row><cell>L2Q</cell><cell>L2P</cell><cell>NPSS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on Anidance dataset. Lower score is better.</figDesc><table><row><cell>L2P</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation of the ?-interpolation regime based on LaFAN1 dataset. Lower score is better.</figDesc><table><row><cell cols="2">?-mode</cell><cell></cell><cell>L2Q</cell><cell></cell><cell></cell><cell>L2P</cell><cell></cell><cell></cell><cell>NPSS</cell></row><row><cell cols="2">Input Output</cell><cell>5</cell><cell>15</cell><cell>30</cell><cell>5</cell><cell>15</cell><cell>30</cell><cell>5</cell><cell>15</cell><cell>30</cell></row><row><cell>Last</cell><cell>I</cell><cell cols="9">0.11 0.32 0.57 0.13 0.47 1.00 0.0014 0.0217 0.1217</cell></row><row><cell>Last</cell><cell>Last</cell><cell cols="9">0.12 0.33 0.58 0.14 0.49 1.01 0.0015 0.0221 0.1217</cell></row><row><cell>No</cell><cell>No</cell><cell cols="9">0.15 0.35 0.62 0.22 0.56 1.17 0.0017 0.0238 0.1300</cell></row><row><cell>No</cell><cell>I</cell><cell cols="9">0.11 0.32 0.59 0.13 0.48 1.09 0.0014 0.0221 0.1252</cell></row><row><cell>No</cell><cell>Last</cell><cell cols="9">0.12 0.33 0.59 0.14 0.51 1.12 0.0015 0.0227 0.1245</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation of the loss terms based on LaFAN1 dataset. Lower score is better.</figDesc><table><row><cell></cell><cell cols="2">L2Q</cell><cell></cell><cell></cell><cell>L2P</cell><cell></cell><cell></cell><cell>NPSS</cell><cell></cell></row><row><cell>Reconstruction Loss</cell><cell>5</cell><cell>15</cell><cell>30</cell><cell>5</cell><cell>15</cell><cell>30</cell><cell>5</cell><cell>15</cell><cell>30</cell></row><row><cell></cell><cell cols="9">0.11 0.32 0.57 0.13 0.47 1.00 0.0014 0.0217 0.1217</cell></row><row><cell></cell><cell cols="9">0.13 0.34 0.59 0.15 0.50 1.03 0.0015 0.0228 0.1247</cell></row><row><cell>Hyperparameter</cell><cell>Value</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Epochs</cell><cell>300</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Batch size</cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Optimizer</cell><cell cols="2">Adam</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Learning rate, max</cell><cell>2e-4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Warmup period, epochs</cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Scheduler step size</cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Scheduler gamma</cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dropout</cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Losses</cell><cell>L1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Width (d)</cell><cell>1024</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MHA heads (h)</cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Residual Blocks (R)</cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Layers, encoder MLP (L E )</cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Layers, decoder MLP (L D )</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Embedding dimensionality</cell><cell>32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Augmentation</cell><cell cols="2">NONE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters of our model on both benchmarks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of zero parameter models to verify data integrity of LaFAN1 benchmark.</figDesc><table><row><cell>L2Q</cell><cell>L2P</cell><cell>NPSS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison of zero parameter models to verify data integrity of Anidance benchmark.</figDesc><table><row><cell></cell><cell></cell><cell>L2P</cell><cell></cell></row><row><cell>Length</cell><cell>5</cell><cell>15</cell><cell>30</cell></row><row><cell>Zero Velocity (Duan et al. 2022)</cell><cell cols="3">2.34 5.12 6.73</cell></row><row><cell>Zero Velocity (ours)</cell><cell cols="3">2.44 5.15 6.89</cell></row><row><cell cols="4">LERP Interpolation (Duan et al. 2022)0.94 3.24 4.68</cell></row><row><cell>LERP Interpolation (ours)</cell><cell cols="3">0.94 3.06 4.84</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">ConclusionsThis paper solves the prominent 3D animation task of motion completion. We propose the in-betweening algorithm in which a deep neural network acts in the ?-regime predicting the residual with respect to a linear SLERP interpolator. We empirically demonstrate that this mode of operation leads to more accurate neural in-betweening results on the publicly available LaFAN1 and Anidance benchmarks. Additionally, the ?-regime implies stronger robustness with respect to outof-distribution shifts as both the input and the output of the network can be defined in the reference frame local to the SLERP interpolator baseline. Moreover, we show that the last known frame can be used in lieu of the SLERP interpolator, further simplifying the implementation of the algorithm at a small accuracy cost.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Accessible at: https://github.com/ubisoft/ubisoft-laforgeanimation-dataset 4 Available for download at: https://www.blender.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Accesible at: https://github.com/Music-to-dance-motionsynthesis/dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Accessible at:https://storage.googleapis.com/delta-interpolator/ additional-demo-videos.zip</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Download the original LAFAN1 source and follow the steps outlined in the repository (see footnote 5) to extract 77 BVH files and verify data integrity. 2. To import the animations to the Unity engine we first need to convert them from BVH to FBX files. This is done in an open source, free computer graphics software called Blender 4 . 3. Create a new Unity project and import all the animations as assets. 4. We disable compression, unit conversion, keyframe reduction and enable baking of axis conversion. These settings were chosen to maximize the animation quality. 5. To make the animations viewable in Unity we select the clip group and add an avatar to the FBX animation clips. 6. We create a dataset from the clip group with the following parameters set: Bone positions (world space), Bone rotations (local space), root reference (local space) and timestamps. We do not enable root motion. 7. Finally, we use a proprietary animation baker component to the clip, link it to our skeleton characterization and select 30 FPS output. The baker provides CSV files for each of the 77 animations with time stamps.</p><p>While our process results in high fidelity copies of the BHV animations in CSV format there are some small sources of error. Specifically, steps 2 and 3 introduce most of the error since Blender has a right handed coordinate system whereas</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">3D Animation Essentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Constraint-based motion optimization using a statistical dynamic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="8" to="18" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. N. American Chapter of the Assoc. Computational Linguistics: Human Language Tech</title>
		<meeting>Conf. N. American Chapter of the Assoc. Computational Linguistics: Human Language Tech<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00776</idno>
		<title level="m">Single-Shot Motion Completion with Transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Unified Framework for Real Time Motion Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="4459" to="4467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lightweight Quaternion Transition Generation with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radziszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Van Straaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Galvan Debarba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Virtual Reality 3D User Interfaces</title>
		<imprint>
			<publisher>VRW</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="579" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A neural temporal model for human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognit. (CVPR)</title>
		<meeting>Conf. Comput. Vision Pattern Recognit. (CVPR)<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12116" to="12125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Style-based inverse kinematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grochow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="522" to="531" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action2Motion: Conditioned Generation of 3D Human Motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2021" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent transition networks for character locomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia Technical Briefs</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust Motion In-Betweening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yurick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="60" to="72" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning an inverse rig mapping for character animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIG-GRAPH/Eurographics Symposium on Comput. Animation</title>
		<meeting>SIG-GRAPH/Eurographics Symposium on Comput. Animation<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="165" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Deep Learning Framework for Character Motion Synthesis and Editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Production technology and trends in movie content: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Waterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Department of Telecommunications, Indiana University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporally Guided Musicto-Body-Movement Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional Autoencoders for Human Motion Infilling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pece</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. 3D Vision (3DV)</title>
		<meeting>IEEE Int. Conf. 3D Vision (3DV)<address><addrLine>Fukuoka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Task-Generic Hierarchical Human Motion Prior using VAEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. 3D Vision (3DV)</title>
		<meeting>IEEE Int. Conf. 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="771" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">AI Choreographer: Music Conditioned 3D Dance Generation with AIST++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vision (ICCV)</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13381" to="13392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On Human Motion Prediction Using Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Honolulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usa</forename><surname>Hi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Graphics and Interactive Techniques</title>
		<meeting>Conf. Comput. Graphics and Interactive Techniques<address><addrLine>Anaheim, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="343" to="350" />
		</imprint>
	</monogr>
	<note>Proc. Conf. Comput. Vision Pattern Recognit. (CVPR)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canada</forename><surname>Vancouver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Petrovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Int. Conf. Comput. Vision (ICCV)</title>
		<meeting>IEEE/CVF Int. Conf. Comput. Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10985" to="10995" />
		</imprint>
	</monogr>
	<note>Proc. Neural Info. Proc. Syst. (NeurIPS)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08501</idno>
		<title level="m">Accelerating 3D Deep Learning with PyTorch3D</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Animating Rotation with Quaternion Curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shoemake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="254" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AniDance: Real-Time Dance Motion Synthesize to the Song</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia Conf</title>
		<meeting>ACM Multimedia Conf<address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1237" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Info. Proc. Syst. (NeurIPS)</title>
		<meeting>Neural Info. . Syst. (NeurIPS)<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Velocityto-velocity human motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page">108424</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="298" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spacetime constraints. ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="159" to="168" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the Continuity of Rotation Representations in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jingwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jimei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recognit. (CVPR)<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
