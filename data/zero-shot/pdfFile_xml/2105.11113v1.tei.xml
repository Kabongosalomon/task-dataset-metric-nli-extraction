<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Class Queue for Large Scale Face Recognition In the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Xi</surname></persName>
							<email>xiteng01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zhang</surname></persName>
							<email>zhanggang03@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haocheng</forename><surname>Feng</surname></persName>
							<email>fenghaocheng@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
							<email>hanjunyu@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
							<email>liujingtuo@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<email>dingerrui@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Class Queue for Large Scale Face Recognition In the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning discriminative representation using largescale face datasets in the wild is crucial for real-world applications, yet it remains challenging. The difficulties lie in many aspects and this work focus on computing resource constraint and long-tailed class distribution. Recently, classification-based representation learning with deep neural networks and well-designed losses have demonstrated good recognition performance. However, the computing and memory cost linearly scales up to the number of identities (classes) in the training set, and the learning process suffers from unbalanced classes. In this work, we propose a dynamic class queue (DCQ) to tackle these two problems. Specifically, for each iteration during training, a subset of classes for recognition are dynamically selected and their class weights are dynamically generated on-the-fly which are stored in a queue. Since only a subset of classes is selected for each iteration, the computing requirement is reduced. By using a single server without model parallel, we empirically verify in large-scale datasets that 10% of classes are sufficient to achieve similar performance as using all classes. Moreover, the class weights are dynamically generated in a few-shot manner and therefore suitable for tail classes with only a few instances. We show clear improvement over a strong baseline in the largest public dataset Megaface Challenge2 (MF2) which has 672K identities and over 88% of them have less than 10 instances. Code is available at https://github.com/ bilylee/DCQ * Corresponding to Bi Li and Teng Xi. Equal contribution. " Class Queue enter leave " FC Layer ! weight generation "? SGD update all classes vs. subset SGD-update vs. feedforward generation (a) FC-based (b) DCQ-based</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, face recognition has witnessed great progress along with the development of deep neural networks and large-scale datasets. The size of the public training set has <ref type="bibr">Figure 1</ref>. High-level comparison between the fully connected layer (FC layer) used in classification and the proposed DCQ module. Generally, there are two differences. 1) For the FC layer, all classes in the training dataset are included in the FC layer (each row in the FC layer represents a class weight). While for DCQ, only a subset is used. 2) The class weights in the FC layer are randomly initialized and then updated via SGD. In contrast, DCQ gets the class weights in a few-shot manner based on another instance x with the same identity as input x. been steadily increasing from CASIA-Webface <ref type="bibr" target="#b26">[27]</ref> (10K identity, 0.5M instance) ? MS-Celeb-1M <ref type="bibr" target="#b6">[7]</ref> (100K identity, 5M instance) ? MF2 <ref type="bibr" target="#b18">[19]</ref> (672K identity, 4.7M instance). For commercial applications, the training dataset easily scales up to millions of identities and it is a matter of time to reach billions of identities.</p><p>More data brings better performance. However, the training difficulty accumulates along with the growth of the training data. First of all, it simply needs more computing resources. For classification-based methods, where each identity is taken as a class and the feature extractor is learned through the classification task, the memory consumption of the fully connected layer W ? R D?C linearly scales up to the number of identities C in the training set. So is the cost to compute the matrix multiplication between the FC layer and the input feature. Secondly, for data gathered in the real world, the class distribution is typically long-tailed, that is, some classes have abundant instances (head classes) while most classes have few instances (tail classes). For example, the MF2 dataset contains images gathered from Flickr, and over 88% of identities have less than 10 images. As witnessed by various studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>, long-tailed classification is itself a challenging problem.</p><p>To tackle the computing resource constraint, one option is to adopt pairwise-based methods which have the benefits of being class-number agnostic and therefore can be potentially extended to datasets with an arbitrary number of identities. However, the pair sampling mechanism is critical for this method to achieve good performance <ref type="bibr" target="#b20">[21]</ref> and it takes a much longer time to converge. Another option is to dynamically reduce the number of classes used during training. Zhang et al. <ref type="bibr" target="#b29">[30]</ref> propose to use a hashing forest to partition the space of class weights into small cells. Given a training sample x, they walk through the forest to find the closest cell and use classes within as the FC Layer. Concurrent with our work, An et al. <ref type="bibr" target="#b0">[1]</ref> demonstrate that randomly sampling classes from all classes can also achieve matching performance as using all classes. These methods can largely reduce the computational cost by using a subset of classes for computing the matrix multiplication and the softmax function. However, they still require all class weights to be stored in the memory. <ref type="bibr" target="#b29">[30]</ref> uses a parameter server to overcome this problem while <ref type="bibr" target="#b0">[1]</ref> uses model parallel 1 to distribute the class weights into several GPUs. Moreover, since the class weights W are updated by SGD, they also need to store all optimization-related stats of the class weights such as the momentum in memory.</p><p>As for long-tailed class distribution, a simple solution would be removing tail classes such that the class distribution is balanced. However, the full potential of the training data is undermined in this way. Another option is to use class-based sampling during training, i.e. sampling classes with equal probability <ref type="bibr" target="#b2">[3]</ref>. However, as demonstrated in our experiment, this is not necessary and even harmful for our method. Zhong et al. <ref type="bibr" target="#b32">[33]</ref> propose a two-stage training mechanism where the model is firstly trained with only the head classes and then finetuned with both head and tail classes. As noted by the authors, the second stage needs careful manual tuning to achieve good performance. It is of interest to design methods that require only single-stage training.</p><p>In this work, we propose a single-stage method that is computationally efficient and has low memory consumption. The key innovation is to design a dynamic class queue. The meaning of "dynamic" is two-fold. First, the class subset used for classification is dynamically selected. The computational cost is reduced since only a part of the classes are involved in the computation. Second, the class weights are dynamically generated on-the-fly instead of learning via SGD. Since the class weight is dynamically generated at each iteration, it does not require storing all class weights or optimization-related stats in the memory. These class weights are stored in a queue where the queue size can be 10x smaller than all classes in our experiments. Importantly, the class weights are dynamically generated in a few-shot way which is friendly to tail classes and therefore helpful for training long-tailed datasets. For a visual illustration of the proposed method, please refer to <ref type="figure" target="#fig_2">Figure 1</ref>.</p><p>We empirically verify that the proposed method is effective and efficient. By using a single server and less than 9GB memory per GPU card without model parallel, the proposed method uses 10% of classes while still achieving similar performance as the baseline which uses all classes in the large-scale dataset (MS1MV2 <ref type="bibr" target="#b4">[5]</ref>). The proposed method is most useful in the real-world long-tailed dataset. We demonstrate this on the MF2 <ref type="bibr" target="#b18">[19]</ref> dataset which outperforms a strong baseline by 0.75% (absolute change) in identification and 1.72% in verification tasks with only 10% classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Pairwise-loss based. A straightforward way to learn the representation is by sampling pair or triplet samples <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25]</ref> and minimize the pairwise losses. The simplest form of pairwise loss is verification loss which samples two images at a time and minimizes the L 2 distance when the two images come from the same identity, otherwise enforces a margin. FaceNet <ref type="bibr" target="#b20">[21]</ref> adopts triplet loss which samples three images at a time, an anchor image A, a positive image P and a negative image N , and the relative distance d(A, P ) ? d(A, N ) should be minimized below a margin. Our work can be interpreted in the pairwise-loss-based framework. The difference is, instead of using only negatives from the current batch, we build a class queue that is much larger than the batch and is dynamically updated by the samples in the batch.</p><p>Classification based. Recently, classification based representation learning have been actively exploited in face recognition <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5]</ref>. To ensure intra-class compactness, <ref type="bibr" target="#b25">[26]</ref> proposes the center loss to penalize the distance between the image feature and the corresponding class center. For inter-class dispersion, the feature extractor is also constrained by the entropy loss. SphereFace <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> notice that the entropy loss can be reinterpreted in a geometric way by l 2 normalizing the weights of the last FC layer and therefore projecting the distance metric to angular space. An angular margin is then introduced to enforce inter-class dispersion. CosFace <ref type="bibr" target="#b23">[24]</ref> further normalizes both the weight and the features to remove radial variations and introduces a cosine margin. Compared to SphereFace <ref type="bibr" target="#b14">[15]</ref>, CosFace is more robust during training since it overcomes the optimization difficulty in the angular space. ArcFace <ref type="bibr" target="#b4">[5]</ref> also notices the training difficulty of SphereFace and pro- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backward computation</head><p>Reference Image Sample Image <ref type="figure">Figure 2</ref>. The framework of the proposed method. Given two images of the same identities, the reference image passes through the weight generator to get the class weight and update the class queue. The sample image is embedded by the feature extractor, then classified by the class queue and supervised by the classification loss. During backward pass, the gradients update the feature extractor. The weight generator is updated by the moving average of the feature extractor.</p><p>poses an additive angular margin. Momentum-based model update. Feature drift has been one of the key challenges to maintain a memory(queue) that works across the batch. Recently, He et al. <ref type="bibr" target="#b7">[8]</ref> propose a momentum-based method to keep the features consistent in the memory and largely reduced the gap between supervised and unsupervised learning. Du et al. <ref type="bibr" target="#b5">[6]</ref> notice the shallow face problem in face recognition and propose to use the momentum-based update to keep the class weights different from sample features. Their method bears similarities with our work in that class weights are generated in a feedforward manner based on a support image. Nevertheless, the main goal of this work is to design methods for training with large-scale datasets in the wild, therefore we focus on the long-tailed problem and propose to use a subset of classes for large-scale training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first introduce the preliminaries (Sec. 3.1) and introduce the difficulties faced in training large-scale datasets (Sec. 3.2), then introduce the proposed dynamic class queue in detail (Sec. 3.3). For a visual illustration of the proposed method, please refer to <ref type="figure">Figure.</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Classification-based Representation Learning. Given a training dataset T {(x i , y i ), i ? {1, 2, ? ? ? , N }}, where N = |T | is number of images in T , x i ? R H?W ?3 is a face image and y i ? Y = {1, 2, .., C} is the corresponding face identity encoded via one-hot vector. C is the total number of identities which can be over millions in large-scale datasets. To learn a feature extractor ? ? (?) with learnable parameters ?, images are first encoded by the feature extractor f = ? ? (x) ? R D . Then, they are classified by a linear layer (fully connected layer) with weight W ? R D?C , that</p><formula xml:id="formula_0">is? = W T ? ? (x).</formula><p>The feature extractor is trained by minimizing the loss:</p><formula xml:id="formula_1">arg min ?,W 1 N N i=1 L(W T ? ? (x i ), y i ) where L(?, y)</formula><p>measures the discrepancy between the predicted value? and the groundtruth y. Cross entropy loss is typically used for classification task. However, it is found that a variant of cross entropy loss which first projecting the feature f and the linear classifier W into the spherical space and add large cosine margin, the model can learn more discriminative features for face recognition. Specifically, in this work, we adopt the loss function used in CosFace <ref type="bibr" target="#b23">[24]</ref>, that is:</p><formula xml:id="formula_2">L = ? ln e s(cos(?y)?m) e s(cos(?y)?m) + j?Y/{y} e s cos(?j )<label>(1)</label></formula><p>where</p><formula xml:id="formula_3">cos(?) = ( W ||W || ) T f ||f || ? R C , cos(? j )</formula><p>is the jth value. Y/{y} are all classes excluding the groundtruth y. s and m are hyperparameters, s controls the scale and m is the cosine margin. Note that the proposed method is not limited to CosFace and can be applied to any other loss functions such as ArcFace <ref type="bibr" target="#b4">[5]</ref>.</p><p>The loss is minimized via stochastic gradient descent (SGD). During evaluation, the FC layer is removed and only the feature extractor is used. The learned representation can be used for either face verification by thresholding the distance between the test and the reference face, or face identification by searching the nearest neighbor in a face database.</p><p>Feedforward weight generation The class weights w in the FC layer are randomly initialized and iteratively updated via SGD. This is beneficial for classes with sufficient training samples. However, it struggles with tail classes. In this work, we would like to generate the class weights in a feedforward manner and on-the-fly at each iteration. Moreover, it should be friendly to tail classes with few instances. We tackle this problem by following the few-shot learning or meta-learning methodology. Given a support image (or reference image) x from the same identity as the query image x, it learns a weight generator function g(?) such that w = g(x ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Difficulties for large-scale Datasets</head><p>Hardware constraints. In the above classificationbased framework, the size of the classifier weight W ? R D?C linearly increases with the number of identities in the training dataset C. This can be problematic for largescale face recognition with a large number of identities. The FC layer easily exceeds the memory limit of the GPU and solely computes the? = W T ? ? (x) will dominate the computation cost.</p><p>Learning Difficulties. Even if the computing and memory resource is unlimited, the learning process faces intrinsic difficulties. To illustrate, we use cross entropy loss as an example. The reasoning holds for other cross entropy based variants such as CosFace and ArcFace, et al.</p><p>During training, given a sample x, we can get its predictions? = W T ? ? (x) ? R C . The predictions are then normalized into probabilities by a softmax function:</p><formula xml:id="formula_4">p i = e? i j e? j which has properties C i=1 p i = 1 and p i ? 0.</formula><p>Denote the probability of the groundtruth as p + and others as p ? j . Without loss of generality, we have p =</p><formula xml:id="formula_5">[p + , p ? 1 , p ? 2 , ..., p ? C?1 ]. Correspondingly, the FC layer can be represented as W = [w + , w ? 1 , w ? 2 , ..., w ? C?1 ].</formula><p>The cross entropy loss of the predictions is then L CE = ? log p + . Notice that even though only p + is used in the cross entropy loss, the predictions of other classes are implicitly contained in the denominator of p + due to the softmax function.</p><p>Then, the gradients of the cross entropy loss L with respect to the feature f and the weights in the FC layer are:</p><formula xml:id="formula_6">?L ?f = ?(1 ? p + )w + + i p ? i w ? i (2) ?L ?w + = ?(1 ? p + )f (3) ?L ?w ? j = p ? j f<label>(4)</label></formula><p>Intuitively, these gradients show the "pull" and "push" forces caused by the positive and negative class weights and samples. For example, the feature f is updated by the total forces of positive and negative classes. The positive class weights w + are pulled to the feature f while the negative class weights w ? are pushed away from f . The probabilities p indicates the strength of these forces. Interestingly, the gradient of the feature is balanced since (1 ? p + ) = i p ? i . Now lets focus on the update of a single class weight w. Through the training process, it is pushed away by samples from other classes and pulled close to its instances, that is,</p><formula xml:id="formula_7">w = w 0 + a?C + (1 ? p + a )f a ? b?C ? p ? b f b<label>(5)</label></formula><p>Where C + represents all instances belong to this class accoutered during training and C ? represents instances of other classes. w 0 is the random initialization. Learning rates are omitted. This can be problematic for large-scale datasets in the wild. For large-scale datasets, the classes are typically longtailed. That is, some classes have abundant instances while most classes contain only few instances. Therefore, for tail classes with only few instances, their class weights are dominated by push-away updates and cannot represents their instances. <ref type="figure">Figure 3</ref>. The illustration of the learning difficulty.</p><formula xml:id="formula_8">1 ? # ) ! # , # $ /#" $ /</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic Class Queue</head><p>In this work, we design a dynamic class queue module to tackle all the above difficulties: 1) computational and 2) memory cost of using all classes and 3) long-tailed class distribution. Two crucial points of the proposed method are: First, the classes used for classification are dynamically selected. Second, the class weights are dynamically generated on-the-fly instead of randomly initialized and then updated via SGD. Since only a subset of classes is used at each iteration, the computational cost is largely reduced. Moreover, the class weights are dynamically generated at each iteration and therefore do not require storing all class weights in the memory. Finally, the class weights are generated in a few-shot manner and friendly to tail classes.</p><p>Dynamic Class Selection. The key idea behind dynamic class selection is that since the model is optimized via stochastic gradient descent and the training process proceeds in a batch-wise way, therefore not all classes are needed in one batch. Let X B = {x 1 , x 2 , ..., x B } be a batch of images during training, and Y B be their corresponding identity labels. Let Y S be a set of classes used for classification, it is clear that Y B ? Y S ? Y. Accordingly, we can rewrite eq. 1 by substituting all classes Y with a subset Y S : </p><formula xml:id="formula_9">L S = ? ln</formula><p>Now, the question is how to design the class set Y S ? A trivial solution will be setting Y S = Y, that is, using all classes in the training set. However, to overcome the scaling problem, we would like a class set that is much smaller than all classes, i.e., |Y S | |Y|. Since the classes of images in the batch Y B change for every batch, it means Y S should be dynamically updated every iteration during training. We call Y S the dynamic class set.</p><p>Concurrent with our work, An et al. <ref type="bibr" target="#b0">[1]</ref> indicates that random sampling a subset of classes performs as well as using all classes. Let Y t S be the dynamic class set at iteration t, by using random sampling we have</p><formula xml:id="formula_11">Y t S = Y t B ? Sample(Y/Y t B , K)</formula><p>. Further more,we notice that instead of using totally different classes at each iteration, we can build a queue with size B +K and update B element of the queue at each iteration. That is,</p><formula xml:id="formula_12">Y t S = Y t B ? Y t?1 B ? Y t?1 S /Y oldest B .<label>(7)</label></formula><p>where Y oldest B is the oldest batch in the queue (i.e., first in first out). Since Y t B is randomly sampled at each batch, therefore the classes in the queue can be seen as a random sampling from all classes.</p><p>The queue-based dynamic class selection is important in our work because the class weights are dynamically generated. By reusing the K class weights in the queue (only B class weights are updated), the computation for getting all the selected class weights is greatly reduced. However, two problems need to be solved to make it work. One is that duplicate classes may exist in the queue. We overcome this problem by setting the logits to ?? for duplicate classes (Refer to Algorithm 1 for details). Another problem with reusing the class weights is that these weights are generated at earlier iterations and may have feature drift. We will tackle this problem in the following subsection.</p><p>Dynamic Class Generation. There are various methods in the meta-learning literature to design the mapping function, we will adopt the simplest one in this work which already demonstrates good performance empirically. Note that the design of the mapping function is orthogonal to other components in this work and we expect better performance with the development in the meta-learning field. Specifically, we adopt the siamese network and therefore g(?) = ? ? (?) where ? ? (?) is the feature extractor. We have also tried prototypical network which takes the mean of several support samples as the class weight, i.e. w = 1 n n i=1 ? ? (x i ), however it does not demonstrate better performance. We emphasize that class weight generated via meta learning is friendly to tail classes as it is agnostic to the number of samples in the class.</p><p>One problem with the current class weight generation method is that since we reuse the K class weights in the queue which are generated in earlier iterations during training, they may have different feature distribution compared to the recently generated class weights. Therefore, the model can easily discriminate between the positive classes (which are generated in the current batch) and the negative classes (which are reused in the queue) and the learned feature is not discriminative. To overcome this problem, we need to bridge the feature gap between class weights generated at different time steps. Recently, He et al. <ref type="bibr" target="#b7">[8]</ref> proposes to mitigate the feature drift problem by taking the moving average of the neural network parameters. We adopt this method in this work. Specifically, at one iteration, the feature extractor is first updated via SGD, we then take the moving average of parameters of the feature extractor as the weight generator. That is,</p><formula xml:id="formula_13">? t = ?? t?1 + (1 ? ?)? t<label>(8)</label></formula><p>where ? t is the parameter of the weight generator at time t and ? t is the parameters of the feature extractor updated by SGD, ? is the momentum hyperparameter.  <ref type="figure">Figure 4</ref>. The effects of the queue size. The x-axis is the queue size 2048 ? 2 x . The y-axis is the verification accuracies on six benchmarks. The red dotted line is the CosFace baseline using all classes. Models are trained on MS1MV2 with ResNet50 backbone. Experiments are repeated 3 times and we report the mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Training Dataset. To evaluate different aspects of the proposed method, two large-scale datasets are used respectively. For training under clean dataset with balanced classes, we use MS1MV2 <ref type="bibr" target="#b4">[5]</ref>. It is a cleaned version of MS-1M-Celeb <ref type="bibr" target="#b6">[7]</ref>. This dataset contains images of celebrities which consists of 85K identities and 5.8M images. It has balanced classes, each identity has about 100 images on average. For training with the long-tailed dataset, we use the training set of Megaface Challenge 2 (MF2) <ref type="bibr" target="#b18">[19]</ref>. This is one of the largest datasets publicly available. It has 672K identities and 4.7M images, each identity has 7 images on average. Unlike MS1MV2, the images of MF2 are collected from Flickr and most of them are common people. In MF2, the classes are long-tail distributed, 88.42% of identities have less than 10 images.</p><p>Training Settings. Two ResNet <ref type="bibr" target="#b9">[10]</ref> backbones with different depth are used for feature extraction: ResNet50 and ResNet101. For fair comparison, we adopt the backbone improvements (remove the first pooling operation, change ReLU <ref type="bibr" target="#b17">[18]</ref> to PReLU <ref type="bibr" target="#b8">[9]</ref>, etc.) proposed in <ref type="bibr" target="#b4">[5]</ref>. The models are trained using SGD with momentum 0.9 and weight decay 0.0001. The batch size is 512. For the CosFace baseline, the initial learning rate is 0. Hyperparameters. The feature dimension of all models is 512. For our baseline model, we use the same hyperparameters as the ones proposed in the CosFace <ref type="bibr" target="#b23">[24]</ref>. The scale and the margin in the loss function are 64 and 0.35. For our DCQ model, we find that smaller scale and margin result in better performance (not the case for our baseline model), therefore we use scale 50 and margin 0.3. The momentum ? in Equation 8 is set to 0.999. Data Preprocessing. We use MTCNN <ref type="bibr" target="#b27">[28]</ref> to locate 5 facial landmarks and use them to normalize the face to a canonical position. The normalized face is then cropped and resized to 112x112. During training, the RGB values are normalized to the range (-1, 1) by subtracting 127.5 and then dividing by 128. For training on MS1MV2, we use only flipping augmentation with probability 50%. For MF2, besides flipping, we also use monochrome augmentation with probability 20%.</p><p>Testing Settings. During testing, following <ref type="bibr" target="#b4">[5]</ref> features of the original image and the flipped image are averaged as the final testing feature. Cosine distance of the features is used. A single crop is used for all tests. We tests our models on standard verification benchmarks including LFW <ref type="bibr" target="#b11">[12]</ref>, CFP-FF, CFP-FP <ref type="bibr" target="#b21">[22]</ref>, AGEDB-30 <ref type="bibr" target="#b16">[17]</ref>, CALFW <ref type="bibr" target="#b31">[32]</ref> and CPLFW <ref type="bibr" target="#b30">[31]</ref>. Moreover, the model is evaluated on the test set of the Megaface Challenge 2. The test set includes two tasks: identification and verification. For the identification task, it has 1M distractors (disjoint with the MF2 training set) in the gallery set and 100K images from FaceScrub <ref type="bibr" target="#b19">[20]</ref> as the probe set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Exploratory Experiments</head><p>Dynamic Class Queue works as well as using all classes. One of the most important questions in this work is how does DCQ perform compared to baselines using all classes during training? To answer this question, we compare DCQ with baselines by training on the MS1MV2 dataset. Although MS1MV2 has identities as large as 85K, it is still possible to fit the whole FC layer into GPU memory and therefore suitable for training our baselines without compromise. Moreover, this dataset is class-balanced and allows us to focus on the approximation ability of the dynamic class queue without considering the long-tail effect (which will be discussed in MF2 based experiments).</p><p>For fast experimentation, we use ResNet50 as the backbone. The performance of DCQ with different queue sizes is shown in <ref type="figure">Figure 4</ref>. As can be seen, the recognition performance steadily improves when the queue size increases from 2048 to 16384. Notice that when the queue size is 8192, it has about 10% of all classes (85,742) and already performs as well as the CosFace baseline using all classes. By further increasing the queue size, DCQ is able to outperform the baseline method in LFW, AGEDB-30, and CALFW while performs slightly worse (within 0.3%) in CFP-FP and CPLFW. This is likely due to the different requirements of intra-and inter-variance in different benchmarks. We leave more in-depth investigations for future works. Nevertheless, DCQ achieves a good balance between accuracy and efficiency. As we will show, the benefits of weight generation outweigh the drawbacks of partial classes and DCQ achieves superior results in the long-tailed dataset.</p><p>Dynamic Class Queue is hardware friendly. One major benefit of the proposed method is in terms of computational cost and memory consumption. We show the GPU memory used in one card and seconds per batch in <ref type="table" target="#tab_2">Table 1</ref>. Both methods are trained on the MF2 dataset. During training, the CosFace has 642,962 classes and the DCQ has queue size 65,536. As we will show, using 10% of classes can outperform the baseline in the MF2 dataset. ResNet50 is used as the backbone and batch size is 512. Data loading time is removed. Code runs on a single server with 8 V100 32G GPU cards. DCQ works well for the long-tailed dataset. We have just verified the effectiveness of DCQ on a clean and balanced dataset, MS1MV2. Now, we will let DCQ go through the real test: MF2. MF2 is by far the public dataset with the largest number of identities: 672K, which has about 8x more identities than MS1MV2 (85K). Moreover, it is a long-tailed dataset, over 80% identities have less than 10 images. It is representative of datasets used in real applications. Models are trained with ResNet50 backbone and then tested on the test set of the Megaface Challenge2. We compare with two baselines, CosFace-h9 is trained only on the head classes with instances ? 9, which has 100K classes and 2.2M images. CosFace-all is trained with all classes. Our DCQ model uses 10% of all classes. The results are shown in <ref type="table" target="#tab_3">Table 2</ref>. Notice that simply removing all tail classes degrades the final performance. On the other hand, by adopting the proposed DCQ method, with only 10% of all classes, it is able to outperform the baseline 0.75% in identification and 1.72% in verification. This demonstrates the effectiveness of DCQ in the long-tailed dataset. Moreover, the improvement is consistent in LFW, AGEDB-30, and CPLFW benchmarks.</p><p>Momentum update is very important. The central design of this work is to maintain a queue that contains class weights generated on-the-fly at different iterations during training. To investigate the importance of smoothing the feature drift, we use different momentums ? to train our DCQ model with the ResNet50 backbone. Intuitively, when ? is as small as 0, there is no feature smoothing between different iterations. On the other hand, when ? is as large as 1, the feature extractor will remain unchanged and cant reflect the current feature distribution. Since the feature drift problem is more evident when the queue is large, we use queue size 65,536 and train the model on MS1MV2.</p><p>The performance on LFW, CFP-FP, AGEDB-30, and CPLFW with different momentums are presented in <ref type="figure" target="#fig_4">Figure 5</ref>. As can be seen, as the moving average ? increases from 0.9 (the right side of the <ref type="figure">figure)</ref>, the accuracies are consistently improved on all datasets and saturate at 0.999 (the left side of the <ref type="figure">figure)</ref>. This demonstrates the importance of momentum update for mitigating the feature drift. Note that the momentum has different impacts on different datasets. For LFW, the influence is marginal while for other datasets the improvement can be over 6%. This implies that LFW is a relatively easy testing benchmark compared to other datasets and does not require very strong features.</p><p>Instance-based is better than class-based sampling for DCQ. For long-tailed datasets, one common practice is to adopt class-based sampling, that is, all classes are sampled with equal probability. This is in contrast to instancebased sampling where all images are sampled equally and therefore tail classes with more images are more likely to be sampled. Interestingly, the proposed DCQ model performs better with instance-based sampling even in longtailed dataset as shown in <ref type="table" target="#tab_4">Table 3</ref>. One possible explanation is that class-balanced sampling is good for learning the classifier (e.g. the FC layer) since all classes are equally sampled without bias. However, it is bad for representation learning which requires diversity in the training data, and increasing the probability of sampling tail classes just means the same data are repeated several times. Since the DCQ model does not need to learn the classifier which is directly generated, it is beneficial to use instance-based sampling. DCQ converges as fast as CosFace in the balanced dataset and faster in the long-tailed dataset. Since the class queue is randomly assembled and changes after each iteration, one concern is about the convergence speed of DCQ. To investigate this problem, we train ResNet50 backbones with CosFace and DCQ using the same training settings (optimizer, number of epochs, and learning rate schedule). These settings are adopted following the best practice of training CosFace, we do no tune them for DCQ. For DCQ, we use 10% classes. The performance of each epoch is shown in <ref type="figure">Figure.</ref> As can be seen, DCQ converges as fast as CosFace in MS1MV2 (balanced dataset) and faster in MF2 (long-tailed dataset). This is beneficial for the longtailed dataset in the wild.  Train: MF2, Test: MF2 Ver.</p><p>CosFace DCQ <ref type="figure">Figure 6</ref>. Convergence comparisons. The x-axis is the epoch. The y-axis is the verification accuracy. The left is trained on MS1MV2 and tested on AGEDB-30 while the right is trained on MF2 and tested on the verfication benchmark of MF2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">State-of-the-art Comparisons</head><p>In this section, we compare DCQ with state-of-the-art methods to give readers an idea of how good the proposed method is in the face recognition field. Yet we emphasize that achieving the best recognition performance is not the focus of this work. Instead, we aim for a new paradigm for face recognition in the wild with large-scale datasets. The proposed method is general enough to easily combine with other methods (ArcFace loss, Curriculum Learning, etc.) to further boost the performance. Without bells and whistles, the proposed method is already competitive with other stateof-the-art methods.</p><p>Comparisons on MS1MV2. We train models on the MS1MV2 dataset and test them on standard benchmarks. Following most SOTA methods, we use ResNet101 as the backbone. The results are shown in <ref type="table" target="#tab_6">Table 4</ref>. As can be seen, our method performs on par with most SOTA methods. In particular, Partial FC is a recently proposed method that reduces computations by randomly sampling classes at each iteration and bears similarities with our method. The major difference is that in PartialFC, class weights are learned via SGD while we directly generate them via one-shot learning. We achieve a similar performance as the partial FC. This implies that directly generates class weights on-the-fly works as well as learning them via SGD. The extra benefit of DCQ is that it requires less GPU memory during training since Partial FC needs to store all class weights in the GPU through model parallel. Comparisons on MF2. We train models on the MF2 dataset and test them on the test set of MF2. For a fair comparison with other methods, ResNet50 is used as the backbone. The results are shown in <ref type="table" target="#tab_7">Table 5</ref>. As can be seen, the proposed DCQ outperforms most SOTA methods. NRA+CD achieves better performance than our method with multi-stage training and careful noise reduction in the MF2 dataset. In fact, the NRA+CD is orthogonal to our method and should further improve the performance when combined together. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we propose a new framework for training with large-scale datasets in the wild. Specifically, two challenging problems, i.e., hardware constraints and long-tailed classes, are solved simultaneously in a unified way via the dynamic class queue. In essence, the proposed method dynamically selects classes for classification which reduces the computing cost and dynamically generates class weights which saves memory and overcomes the long-tail issue. The DCQ model is empirically validated on two large-scale datasets where it achieves similar performance as using all classes in the balanced dataset and better performance in the long-tailed dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Pseudocode of DCQ. bmm: batch matrix multiplication; mm: matrix multiplication; cat: concatenate. # f &amp; g: feature extractor &amp; weight generator network # w_queue: class weights queue of length K (CxK) # l_queue: class label queue of length K (1xK) # a: momentum # s &amp; m: scale and margin in CosFace loss g.params = f.params # initialize for (x_t, x_w, y) in loader: # x_t: test sample, x_w: reference sample # y: class labels of x_t t = f.forward(x_t) # test features: NxC w = g.forward(x_w) # class weights: NxC w = w.detach() # no gradient to class weights # positive and negative logits l_pos = bmm(t.view(N,1,C), w.view(N,C,1)) # NX1 l_neg = mm(q.view(N,C), w_queue.view(C,K)) # NxK # mute duplicate responses in the queue l_diff = y -l_queue # N x K l_neg = l_neg.masked_fill(l_diff == 0, -1e9) # CosFace Loss l_pos = l_pos -m # add margin logits = cat([l_pos, l_neg], dim=1) logits * = s # add scale labels = zeros(N) # positives are the 0-th loss = CrossEntropyLoss(logits, labels) # SGD update and momentum update loss.backward() update(f.params) g.params = a * g.params+(1-a) * f.params # update queue, first in first out enqueue(w_queue, w) enqueue(l_queue, y) dequeue(w_queue) dequeue(l_queue)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>e s(cos(?y)?m) e s(cos(?y)?m) + j?Y S /{y} e s cos(?j )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>The effects of momentum update with different momentum ?. The x-axis is 1 -? and in log scale. The y-axis is the verification accuracies. The DCQ model is trained on MS1MV2 with ResNet50 backbone. The queue size 65,536.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5</head><label>5</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1. For DCQ, it is 0.06. Both methods follow the same learning rate schedule. For MS1MV2, the learning rate is decreased 10x at epoch 8, 16, and 18. The total epoch is 20. For MF2, the learning rate is decreased at epoch 25, 35, 38, for a total of 40 epochs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Time and memory comparison.</figDesc><table><row><cell>model</cell><cell>Classes</cell><cell cols="2">Time (Sec/batch) Memory (MB)</cell></row><row><cell cols="2">CosFace 642,962</cell><cell>0.291</cell><cell>18,621</cell></row><row><cell>DCQ</cell><cell>65,536</cell><cell>0.245</cell><cell>8,435</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison on training with long-tailed dataset: MF2</figDesc><table><row><cell>model</cell><cell>MF2 Id. (%)</cell><cell>MF2 Ver.(%)</cell><cell>LFW</cell><cell cols="2">AGEDB-30 CPLFW</cell></row><row><cell>CosFace-h9</cell><cell>77.10</cell><cell>86.75</cell><cell>99.58</cell><cell>90.35</cell><cell>88.60</cell></row><row><cell>CosFace-all</cell><cell>77.24</cell><cell>87.56</cell><cell>99.60</cell><cell>90.27</cell><cell>88.52</cell></row><row><cell>DCQ</cell><cell>77.99</cell><cell>89.28</cell><cell>99.58</cell><cell>91.07</cell><cell>90.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparisons between instance-and class-sampling,</figDesc><table><row><cell>model</cell><cell cols="2">MF2 Id.(%) MF2 Ver.(%)</cell><cell>LFW</cell><cell cols="2">AGEDB-30 CPLFW</cell></row><row><cell>DCQ-cls</cell><cell>76.78</cell><cell>88.44</cell><cell>99.51</cell><cell>90.55</cell><cell>89.42</cell></row><row><cell>DCQ-ins</cell><cell>77.99</cell><cell>89.28</cell><cell>99.58</cell><cell>91.07</cell><cell>90.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>SOTA comparisons on models trained with MS1MV2.</figDesc><table><row><cell>model</cell><cell>LFW</cell><cell cols="4">CFP-FP AGEDB-30 CALFW CPLFW</cell></row><row><cell>CosFace[24]</cell><cell>99.43</cell><cell>-</cell><cell>-</cell><cell>90.57</cell><cell>84.00</cell></row><row><cell>ArcFace[5]</cell><cell>99.82</cell><cell>98.27</cell><cell>-</cell><cell>95.45</cell><cell>92.08</cell></row><row><cell>GroupFace[14]</cell><cell>99.85</cell><cell>98.63</cell><cell>98.28</cell><cell>96.20</cell><cell>93.17</cell></row><row><cell cols="2">CurricularFace[13] 99.80</cell><cell>98.37</cell><cell>98.32</cell><cell>96.20</cell><cell>93.13</cell></row><row><cell>PartialFC-r1.0[1]</cell><cell>99.83</cell><cell>98.51</cell><cell>98.03</cell><cell>96.20</cell><cell>93.10</cell></row><row><cell>PartialFC-r0.1[1]</cell><cell>99.82</cell><cell>98.60</cell><cell>98.13</cell><cell>96.12</cell><cell>92.90</cell></row><row><cell>CosFace(ours)</cell><cell>99.78</cell><cell>98.38</cell><cell>98.22</cell><cell>96.20</cell><cell>93.15</cell></row><row><cell>DCQ(ours)</cell><cell>99.80</cell><cell>98.44</cell><cell>98.23</cell><cell>96.07</cell><cell>92.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>SOTA comparisons on models trained with MF2.</figDesc><table><row><cell>model</cell><cell cols="2">MF2 Id.(%) MF2 Ver.(%)</cell></row><row><cell>3DiVi</cell><cell>57.05</cell><cell>66.46</cell></row><row><cell>NEC</cell><cell>62.12</cell><cell>66.85</cell></row><row><cell>RangeLoss</cell><cell>69.54</cell><cell>82.67</cell></row><row><cell>SphereFace</cell><cell>71.17</cell><cell>84.22</cell></row><row><cell>GRCCV</cell><cell>75.77</cell><cell>74.84</cell></row><row><cell>Yang Sun</cell><cell>75.79</cell><cell>84.03</cell></row><row><cell>CosFace[24]</cell><cell>74.11</cell><cell>86.77</cell></row><row><cell>NRA+CD[33]</cell><cell>80.02</cell><cell>89.93</cell></row><row><cell>CosFace(Ours)</cell><cell>77.24</cell><cell>87.56</cell></row><row><cell>DCQ(Ours)</cell><cell>77.99</cell><cell>89.28</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It splits the model into parts and places them in different GPUs to meet memory constraints.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Partial fc: Training 10 million identities on a single machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of predictive modeling on imbalanced domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paula</forename><surname>Branco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu?s</forename><surname>Torgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><forename type="middle">P</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority oversampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-Siamese Training for Shallow Face Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Curricularface: adaptive curriculum learning loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5901" to="5910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Groupface: Learning latent groups and constructing group-based representations for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Cheol</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongju</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5621" to="5630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Agedb: the first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Level playing field for million scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Nech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7044" to="7053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A data-driven approach to cleaning large face datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Wei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="343" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with longtailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Accelerated training for massive classification via dynamic class selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno>AAAI. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Beijing University of Posts and Telecommunications</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-age LFW: A database for studying cross-age face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<idno>abs/1708.08197</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unequaltraining for deep face recognition with long-tailed noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianteng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohai</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7812" to="7821" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
