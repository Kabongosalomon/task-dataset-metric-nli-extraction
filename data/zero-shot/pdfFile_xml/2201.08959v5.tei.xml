<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-shot Object Counting with Similarity-Aware Feature Enhancement Train on Base Classes Test on Novel Classes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>You</surname></persName>
							<email>zhiyuanyou@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
							<email>luxin@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Le</surname></persName>
							<email>lexinyi@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>University</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sensetime</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tencent</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Few-shot Object Counting with Similarity-Aware Feature Enhancement Train on Base Classes Test on Novel Classes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head><p>Count: 113</p><p>Net Net <ref type="figure">Figure 1</ref>. Illustration of few-shot object counting, where we would like to find how many exemplar objects described by a few support images occur in the query image. Besides the objects included in the training phase, we also expect the model to handle novel classes at the test stage without retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>This work studies the problem of few-shot object counting, which counts the number of exemplar objects (i.e., described by one or several support images) occurring in the query image. The major challenge lies in that the target objects can be densely packed in the query image, making it hard to recognize every single one. To tackle the obstacle, we propose a novel learning block, equipped with a similarity comparison module and a feature enhancement module. Concretely, given a support image and a query image, we first derive a score map by comparing their projected features at every spatial position. The score maps regarding all support images are collected together and normalized across both the exemplar dimension and the spatial dimensions, producing a reliable similarity map. We then enhance the query feature with the support features by employing the developed point-wise similarities as the weighting coefficients. Such a design encourages the model to inspect the query image by focusing more on the regions akin to the support images, leading to much clearer boundaries between different objects. Extensive experiments on various benchmarks and training setups suggest that we surpass the state-of-the-art methods by a sufficiently large margin. For instance, on a recent large-scale FSC-147 dataset, we surpass the state-of-the-art method by improving the mean absolute error from 22.08 to 14.32 (35%?). Code has been released here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Illustration of few-shot object counting, where we would like to find how many exemplar objects described by a few support images occur in the query image. Besides the objects included in the training phase, we also expect the model to handle novel classes at the test stage without retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>This work studies the problem of few-shot object counting, which counts the number of exemplar objects (i.e., described by one or several support images) occurring in the query image. The major challenge lies in that the target objects can be densely packed in the query image, making it hard to recognize every single one. To tackle the obstacle, we propose a novel learning block, equipped with a similarity comparison module and a feature enhancement module. Concretely, given a support image and a query image, we first derive a score map by comparing their projected features at every spatial position. The score maps regarding all support images are collected together and normalized across both the exemplar dimension and the spatial dimensions, producing a reliable similarity map. We then enhance the query feature with the support features by employing the developed point-wise similarities as the weighting coefficients. Such a design encourages the model to inspect the query image by focusing more on the regions akin to the support images, leading to much clearer boundaries between different objects. Extensive experiments on various benchmarks and training setups suggest that we surpass the state-of-the-art methods by a sufficiently large margin. For instance, on a recent large-scale FSC-147 dataset, we surpass the state-of-the-art method by improving the mean absolute error from 22.08 to <ref type="bibr">14.32 (35%?)</ref>. Code has been released here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object counting <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, which aims at investigating how many times a certain object occurs in the query image, has received growing attention due to its practical usage <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b50">51]</ref>. Most existing studies assume that the object to count at the test stage is covered by the training data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>. As a result, each learned model can only handle a specific object class, greatly limiting its application.</p><p>To alleviate the generalization problem, few-shot object counting (FSC) is recently introduced <ref type="bibr" target="#b23">[24]</ref>. Instead of predefining a common object that is shared by all training images, FSC allows users to customize the object of their own interests with a few support images, as shown in <ref type="figure">Fig. 1</ref>. In this way, we can use a single model to unify the counting of various objects, and even adapt the model to novel classes (i.e., unseen in the training phase) without any retraining. <ref type="bibr">Figure 2</ref>. Concept comparison between our method and existing alternatives. (a) Feature-based approach <ref type="bibr" target="#b19">[20]</ref>, where the query feature is concatenated with the pooled support feature for regression. (b) Similarity-based approach <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref>, where a similarity map is developed from raw features for regression. (c) Our proposed similarity-aware feature enhancement block, consisting of a similarity comparison module (SCM) and a feature enhancement module (FEM). Concretely, the reliable feature similarity developed by SCM is exploited as the guidance of FEM to enhance the query feature with the support feature. The details of SCM and FEM can be found in Sec. 3.2 and <ref type="figure">Fig. 3</ref>. comparison unreliable. The other is similarity-based <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref>, as shown in <ref type="figure">Fig. 2b</ref>, where a similarity map is developed from raw features as the regression object. Nevertheless, the similarity is far less informative than feature, making it hard to identify clear boundaries between objects (see <ref type="figure">Fig. 5</ref>). Accordingly, the counting performance heavily deteriorates when the target objects are densely packed in the query image, like the shoal of fish in <ref type="figure">Fig. 1</ref>.</p><p>In this work, we propose a Similarity-Aware Feature Enhancement block for object Counting (SAFECount). As discussed above, feature is more informative while similarity better captures the support-query relationship. Our novel block adequately integrates both of the advantages by exploiting similarity as a guidance to enhance the features for regression. Intuitively, the enhanced feature not only carries the rich semantics extracted from the image, but also gets aware of which regions within the query image are similar to the exemplar object. Specifically, we come up with a similarity comparison module (SCM) and a feature enhancement module (FEM), as illustrated in <ref type="figure">Fig. 2c</ref>. On one hand, different from the naive feature comparison in <ref type="figure">Fig. 2b</ref>, our SCM learns a feature projection, then performs a comparison on the projected features to derive a score map. This design helps select from features the information that is most appropriate for object counting. After the comparison, we derive a reliable similarity map by collecting the score maps with respect to all support images (i.e., few-shot) and normalizing them along both the exemplar dimension and the spatial dimensions. On the other hand, the FEM takes the point-wise similarities as the weighting coefficients, and fuses the support features into the query feature. Such a fusion is able to make the enhanced query feature focus more on the regions akin to the exemplar object defined by support images, facilitating more precise counting.</p><p>Experimental results on a very recent large-scale FSC dataset, FSC-147 <ref type="bibr" target="#b23">[24]</ref>, and a car counting dataset, CARPK <ref type="bibr" target="#b9">[10]</ref>, demonstrate our substantial improvement over state-of-the-art methods. Through visualizing the intermediate similarity map and the final predicted density map, we find that our SAFECount substantially benefits from the clear boundaries learned between objects, even when they are densely packed in the query image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Class-specific object counting counts objects of a specific class, such as people <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>, animals <ref type="bibr" target="#b0">[1]</ref>, cars <ref type="bibr" target="#b9">[10]</ref>, among which crowd counting has been widely explored. For this purpose, traditional methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38]</ref> count the number of people occurring in an image through person detection. However, object detection is not particularly designed for the counting task and hence shows unsatisfying performance when the crowd is thick. To address this issue, recent work <ref type="bibr" target="#b36">[37]</ref> employs a deep model to predict the density map from the crowd image, where the sum over the density map gives the counting result <ref type="bibr" target="#b14">[15]</ref>. Based on this thought, many attempts have been made to handle more complicated cases <ref type="bibr">[2, 18, 23, 27-29, 42, 44, 47, 48, 51]</ref>. Some recent studies <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36]</ref> propose effective loss functions that help predict the position of each person precisely. However, all of these methods can only count objects regarding a particular class (e.g., person), making them hard to generalize. There are also some approaches targeting counting objects of multiple classes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref>. In particular, Stahl et al. <ref type="bibr" target="#b31">[32]</ref> propose to divide the query image into regions and regress the counting results with the inclusion-exclusion principle. Laradji et al. <ref type="bibr" target="#b12">[13]</ref> formulate counting as a segmentation problem for better localization. Michel et al. <ref type="bibr" target="#b20">[21]</ref> detect target objects and regress multi-class density maps simultaneously. Xu et al. <ref type="bibr" target="#b42">[43]</ref> mitigate the mutual interference across various classes by proposing categoryattention module. Nevertheless, they still can not handle the object classes beyond the training data.</p><p>Few-shot object counting (FSC) has recently been proposed <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b45">46]</ref> and presents a much stronger generalization ability. Instead of pre-knowing the type of object to count, FSC allows users to describe the exemplar object of their own interests with one or several support images. This setting makes the model highly flexible in that it does not require the test object to be covered by the training samples. In other words, a well-learned model could easily make inferences on novel classes (i.e., unseen in the training phase) as long as the support images are provided. To help the model dynamically get adapted to an arbitrary class, a great choice is to compare the object and the query image in feature space <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b45">46]</ref>. GMN <ref type="bibr" target="#b19">[20]</ref> pools the support feature, and concatenates the pooling result onto the query feature, then learns a regression head for pointwise feature comparison. However, the comparison built on concatenation is not as reliable as the similarity <ref type="bibr" target="#b45">[46]</ref>. Instead, CFOCNet <ref type="bibr" target="#b45">[46]</ref> first performs feature comparison with dot production, and then regresses the density map from the similarity map derived before. FamNet <ref type="bibr" target="#b23">[24]</ref> further improves the reliability of the similarity map through multi-scale augmentation and test-time adaptation. But similarities are far less informative than features, hence regressing from the similarity map fails to identify clear boundaries between the densely packed objects. In this work, we propose a similarityaware feature enhancement block, which integrates the advantages of both features and similarities. Few-shot learning has received popular attention in the past few years thanks to its high data efficiency <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49]</ref>. The rationale behind this is to adapt a well-trained model to novel test data (i.e., having a domain gap to the training data) with a few support samples. In the field of image classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">40]</ref>, MAML <ref type="bibr" target="#b6">[7]</ref> proposes to fit parameters to novel classes at the test stage using a few steps of gradient descent. FRN <ref type="bibr" target="#b39">[40]</ref> formulates few-shot classification as a reconstruction problem. As for object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">41]</ref>, Fan et al. <ref type="bibr" target="#b5">[6]</ref> exploit the similarity between the input image and the support images to detect novel objects. Wu et al. <ref type="bibr" target="#b40">[41]</ref> create multi-scale positive samples as the object pyramid for prediction refinement. When the case comes to semantic segmentation <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b48">49]</ref>, CANet <ref type="bibr" target="#b48">[49]</ref> iteratively refines the segmentation results by comparing the query feature and the support feature. Yang et al. <ref type="bibr" target="#b44">[45]</ref> aim to alleviate the problem of feature undermining and enhance the embedding of novel classes. In this work, we explore the usage of few-shot learning on the object counting task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Few-shot object counting (FSC) <ref type="bibr" target="#b23">[24]</ref> aims to count the number of exemplar objects occurring in a query image with only a few support images describing the exemplar object. In FSC, object classes are divided into base classes C b and novel classes C n , where C b and C n have no intersection. For each query image from C b , both a few support images and the ground-truth density map are provided. While, for query images from C n , only a few support images are available. FSC aims to count exemplar objects from C n using only a few support images by leveraging the generalization knowledge from C b . If we denote the number of support images for one query image as K, the task is called K-shot FSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Similarity-Aware Feature Enhancement</head><p>Overview. <ref type="figure">Fig. 3</ref> illustrates the core block in our framework, termed as the similarity-aware feature enhancement block. We respectively denote the support feature and the query feature as f S ? R K?C?H S ?W S and f Q ? R C?H Q ?W Q , where K is the number of support images. The similarity comparison module (SCM) first projects f S and f Q to a comparison space, then compares these projected features at every spatial position, deriving a score map, R 0 . Then, R 0 is normalized along both the exemplar dimension and the spatial dimensions, resulting in a reliable similarity map, R. The following feature enhancement module (FEM) first obtains the similarity-weighted feature, f R , by weighting f S with R, and then manages to fuse f R into f Q , producing the enhanced feature, f ? Q . By doing so, the features regarding the regions similar to the support images are "highlighted", which could help the model get distinguishable borders between densely packed objects. Finally, the density map is regressed from f ? Q . Similarity Comparison Module (SCM). As discussed above, similarity can better characterize how a particular image region is alike the exemplar object. However, we find that the conventional feature comparison approach (i.e., using the vanilla dot production) used in prior arts <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref> is not adapted to fit the FSC task. By contrast, our proposed SCM develops a reliable similarity map from the input features with the following three steps.</p><p>Step-1: Learnable Feature Projection. Before performing feature comparison, f S and f Q are first projected to a comparison space via a 1 ? 1 convolutional layer. This projection asks the model to automatically select suitable information from the features. We also add a shared layer normalization after the projection to make these two features subject to the same distribution as much as possible.</p><p>Step-2: Feature Comparison. The point-wise feature comparison is realized with convolution. In particular, we convolve the projected f Q with the projected f S as kernels, which gives us the score map, R 0 ? R K?1?H Q ?W Q , as</p><formula xml:id="formula_0">R 0 = conv(g(f Q ), kernel = g(f S )),<label>(1)</label></formula><p>where g(?) denotes the feature projection described in Step-1, i.e., a 1 ? 1 convolutional layer followed layer normalization.</p><p>Step-3: Score Normalization. The values of the score map, R 0 , are normalized to a proper range to avoid some unusual (e.g., too large) entries from dominating the learning. ? <ref type="figure">Figure 3</ref>. Illustration of the similarity-aware feature enhancement block under the 3-shot case. Given features, f S , f Q , that are extracted from the support images and the query image respectively, the similarity comparison module (SCM) first develops a score map, R0, by comparing the projected features, and then produces a similarity map, R, via normalizing R0 along both the exemplar dimension and the spatial dimensions. Here, feature projection is implemented with a 1 ? 1 convolution. The following feature enhancement module (FEM) weights f S with R to derive a similarity-weighted feature, f R , and manages to fuse f R into f Q as a feature enhancement. Such a block can be stacked for multiple times in the training framework.</p><p>Here, we propose Exemplar Normalization (ENorm) and Spatial Normalization (SNorm). On the one hand, ENorm normalizes R 0 along the exemplar dimension as</p><formula xml:id="formula_1">R EN = softmax dim=0 ( R 0 ? H S W S C ),<label>(2)</label></formula><p>where softmax dim (?) is the softmax layer operated along a specific dimension. On the other hand, R 0 is also normalized along the spatial dimensions (i.e., the height and width) with SNorm, as</p><formula xml:id="formula_2">R SN = exp(R 0 / ? H S W S C) max dim=(2,3) (exp(R 0 / ? H S W S C)) ,<label>(3)</label></formula><p>where max dim (?) finds the maximum value from the given dimensions. After SNorm, the score value of the most support-relevant position would be 1, and others would be among [0, 1]. Finally, the similarity map, R, is obtained from R EN and R SN with</p><formula xml:id="formula_3">R = R EN ? R SN ? R K?1?H Q ?W Q ,<label>(4)</label></formula><p>where ? is the element-wise multiplication. The studies of the effect of ENorm and SNorm can be found in Sec. 4.4. Feature Enhancement Module (FEM). Recall that, compared to similarity, feature is more informative in representing the image yet less accurate in capturing the supportquery relationship. To take sufficient advantages of both, we propose to use the similarity developed by SCM as the guidance for feature enhancement. Specifically, our FEM integrates the support feature, f S , into the query feature, f Q , with similarity values in R as the weighting coefficients. In this way, the model can inspect the query image by paying more attention to the regions that are akin to the support images. This module consists of the following two steps.</p><p>Step-1: Weighted Feature Aggregation. In this step, we aggregate the support feature, f S , by taking the pointwise similarity, R, into account. Namely, the feature point corresponding to a higher similarity score should have larger voice to the final enhancement. Such a weighted aggregation is implemented with convolution, which outputs the similarity-weighted feature,</p><formula xml:id="formula_4">f ? R = conv(R, kernel = flip(f S )) ? R K?C?H Q ?W Q ,<label>(5)</label></formula><formula xml:id="formula_5">f R = sum dim=0 (f ? R ) ? R C?H Q ?W Q ,<label>(6)</label></formula><p>where sum dim (?) accumulates the input tensor along specific dimensions, flip(?) denotes the flipping operation, which flips the input tensor both horizontally and vertically. Flipping helps f ? R preserve the spatial structure of f S . The intuitive illustration and the performance improvement of flipping can be found in Supplementary Material.</p><p>Step-2: Learnable Feature Fusion. The similarityweighted feature, f R , is fused into the query feature, f Q , via an efficient network. It contains a convolutional block and a layer normalization, as shown in <ref type="figure">Fig. 3</ref>. Finally, we obtain the enhanced feature, f ? Q , with</p><formula xml:id="formula_6">f ? Q = layer_norm(f Q + h(f R )) ? R C?H Q ?W Q ,<label>(7)</label></formula><p>where h(?) is implemented with two convolutional layers. Comparison with Attention. A classical attention module <ref type="bibr" target="#b34">[35]</ref> involved with query, key, and value (denoted as q, k, v) is represented as similarity(q, k)v. The key idea is employing the similarity values between q and k as weighting coefficients to aggregate v as an information aggregation. Our SAFECount is similar with the similarityguided aggregation of existing attention modules. However, existing attention modules omit the spatial information as they need to flatten a feature map (C?H ?W ) to a collection of feature vectors (C ?HW ). Instead, in all processes of our SCM and FEM, the feature maps are designed to maintain their spatial structure (C ? H ? W ), which plays a vital role in learning clear boundaries between objects. The ablation study in Sec. 4.4 confirms our significant advantage over the classical attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Framework</head><p>Sec. 3.2 describes the core block of our approach, SAFECount. In practice, such a block should work together with a feature extractor, which feeds input features into the block, and a regression head, which receives the enhanced feature for object counting. Moreover, it is worth mentioning that our SAFECount allows stacking itself for continuous performance improvement. In this part, we will introduce these assistant modules, whose detailed structures are included in Supplementary Material. Feature Extractor. When introducing our SAFECount block, we start with the support feature, f S , and the query feature, f Q , which are assumed to be well prepared. Specifically, we use a fixed ResNet-18 <ref type="bibr" target="#b8">[9]</ref> pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref> as the feature extractor. In particular, given a query image, we resize the outputs of the first three stages of ResNet-18 to the same size, H Q ?W Q , and concatenate them along the channel dimension as the query feature. Besides, given a support image, which is usually cropped from a large image so as to contain the exemplar object only, the support feature is obtained by applying ROI pooling <ref type="bibr" target="#b25">[26]</ref> on the feature extracted from its parent before cropping. Here, the ROI pooling size is the size of f S , i.e., H S ? W S . Regression Head. After getting the enhanced feature, f ? Q , we convert it to a density map, D ? R H?W , with a regression head. Following existing methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b45">46]</ref>, the regression head is implemented with a sequence of convolutional layers, followed by Leaky ReLU activation and bi-linear upsampling. Multi-block Architecture. Recall that our proposed SAFE-Count block enhances the input query feature, f Q , with the support features, f S . The enhanced feature, f ? Q , is with exactly the same shape as f Q . As a result, it can be iteratively enhanced simply by stacking more blocks. The ablation study on the number of blocks can be found in Sec. 4.4, where we verify that adding one block is already enough to boost the performance substantially. Objective Function. Most counting datasets are annotated with the coordinates of the target objects within the query image <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b50">51]</ref>. However, directly regressing the coordinates is hard <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b50">51]</ref>. Following prior work <ref type="bibr" target="#b23">[24]</ref>, we generate the ground-truth density map, D GT ? R H?W , from the labeled coordinates, using Gaussian smoothing with adaptive window size. Our model is trained with the MSE loss as</p><formula xml:id="formula_7">L = 1 H ? W ||D ? D GT || 2 2 .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Metrics and Datasets</head><p>Metrics. We choose Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to measure the performance of counting methods following <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref>:</p><formula xml:id="formula_8">M AE = 1 N Q N Q i=1 |C i ? C i GT |, RM SE = 1 N Q N Q i=1 (C i ? C i GT ) 2 ,<label>(9)</label></formula><p>where N Q is the number of query images, C i and C i GT are the predicted and ground-truth count of the i th query image, respectively. FSC-147. FSC-147 <ref type="bibr" target="#b23">[24]</ref> is a multi-class, 3-shot FSC dataset with 147 classes and 6135 images. Each image has 3 support images to describe the target objects. Note that the training classes share no intersection with the validation classes and test classes. The training set contains 89 classes, while validation set and test set both contain another disjoint 29 classes. The number of objects per image varies extremely from 7 to 3701 with an average of 56. Cross-validation of FSC-147. In original FSC-147 <ref type="bibr" target="#b23">[24]</ref>, the dataset split and the shot number are both fixed, while other few-shot tasks including classification <ref type="bibr" target="#b39">[40]</ref>, detection <ref type="bibr" target="#b5">[6]</ref>, and segmentation <ref type="bibr" target="#b44">[45]</ref> all contain multiple dataset splits and shot numbers. Therefore, we propose to evaluate FSC methods with multiple dataset splits and shot numbers by incorporating FSC-147 and cross-validation. Specifically, we split all images in FSC-147 to 4 folds, whose class indices, class number, and image number are shown in Tab. 1. The class indices ranging from 0 to 146 are obtained by sorting the class names of all 147 classes. Note that these 4 folds share no common classes. When fold-i (i = 0, 1, 2, 3) serves as the test set, the remaining 3 folds form the training set. Also, we evaluate FSC methods in both 1-shot and 3-shot cases. For 3-shot case, the original three support images in FSC-147 are used. For 1-shot case, we randomly sample one from the original three support images. CARPK. A car counting dataset, CARPK <ref type="bibr" target="#b9">[10]</ref>, is used to test our model's ability of cross-dataset generality. CARPK contains 1448 images and nearly 90, 000 cars from a drone perspective. These images are collected in various scenes of <ref type="table">Table 1</ref>. Statistics of the four fold splits from FSC-147 <ref type="bibr" target="#b23">[24]</ref>. <ref type="table" target="#tab_0">Indices #Classes #Images   0  0-35  36  2033  1  36-72  37  1761  2  73-109  37  1239  3</ref> 110-146 37 1113  <ref type="bibr" target="#b11">[12]</ref> for 200 epochs with batch size 8. The learning rate is set as 2e-5 initially, and it is dropped by 0.25 every 80 epochs. FSC-147. Quantitative results on FSC-147 are given in Tab. 2. Our method is compared with GMN <ref type="bibr" target="#b19">[20]</ref>, MAML <ref type="bibr" target="#b6">[7]</ref>, FamNet <ref type="bibr" target="#b23">[24]</ref>, and CFOCNet <ref type="bibr" target="#b45">[46]</ref>. Our approach outperforms all counterparts with a quite large margin. For example, we surpass FamNet+ by 8.47 MAE and 21.87 RMSE on validation set, 7.76 MAE and 14.00 RMSE on test set. Note that FamNet+ needs test-time adaptation for novel classes, while our SAFECount needs no test-time adaptation. These significant advantages demonstrate the effectiveness of our method. In <ref type="figure">Fig. 4</ref>, we show some qualitative results of SAFECount. Compared with Famnet+ <ref type="bibr" target="#b23">[24]</ref>, our SAFECount has much stronger ability to separate each independent object within densely packed objects, thus helps obtain an accurate count. Especially, for densely packed green peas <ref type="figure">(Fig. 4b)</ref>, we not only exactly predict the object count, but also localize target objects with such a high precision that every single object could be clearly distinguished. Cross-validation of FSC-147. The dataset split and shot number are both fixed in FSC-147 benchmark, which could not provide a comprehensive evaluation. Therefore, we incorporate FSC-147 with cross-validation to evaluate FSC methods with 4 dataset splits and 2 shot numbers. Our approach is compared with FSC baselines including GMN <ref type="bibr" target="#b19">[20]</ref> and FamNet <ref type="bibr" target="#b23">[24]</ref>. These baselines are trained and evaluated by ourselves with the official code. The cross-validation results are shown in Tab. 3, where fold-i (i = 0, 1, 2, 3) indicates the test set. Under all dataset splits and shot numbers, our method significantly outperforms baseline methods with both MAE and RMSE. Averagely, we outperform FamNet by 8.82 MAE and 20.18 RMSE in 1-shot case, 9.67 MAE and 21.74 RMSE in 3-shot case. Moreover, from 1-shot case to 3-shot case, our approach gains more performance improvement than two baseline methods, reflecting the superior ability of our SAFECount to utilize multiple support images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fold Class</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Cross-dataset Generalization</head><p>Following FamNet <ref type="bibr" target="#b23">[24]</ref>, we test our model's generalization ability on the car counting dataset, CARPK <ref type="bibr" target="#b9">[10]</ref>. The models are first pre-trained on FSC-147 <ref type="bibr" target="#b23">[24]</ref> (the "car" category is excluded), then fine-tuned on CARPK. The results are shown in Tab. 4. For the models pre-trained on FSC-147, we significantly surpass FamNet by 42.23% in MAE and 45.85% in RMSE. When it comes to the finetuning scenario, our method still consistently outperforms all baselines. For instance, we surpass GMN [20] by 28.74% in MAE and 28.89% in RMSE. Therefore, our SAFECount has much better ability in cross-dataset generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>To verify the effectiveness of the proposed modules and the selection of hyper-parameters, we implement extensive ablation studies on FSC-147 <ref type="bibr" target="#b23">[24]</ref>. SCM and FEM. To demonstrate the effectiveness of the proposed SCM and FEM modules, we conduct diagnostic experiments. The substitute of FEM is to concatenate the similarity map and query feature together, then recover the feature dimension through a 1 ? 1 convolutional layer. To invalidate SCM, we replace the score normalization to a naive maximum normalization (dividing the maximum value). The results are shown in Tab. 5a. Both SCM and FEM are necessary for our SAFECount. Specifically, when we replace FEM, the performance drops remarkably by 9.05 MAE in test set. This reflects that FEM is of vital significance in our SAFECount, since the core of our insight, i.e. feature enhancement, is completed in FEM. Besides, when we remove SCM, the performance also drops by 1.81 MAE in test set. This indicates that SCM derives a similarity map with a proper value range, promoting the performance. Score Normalization. We conduct ablation experiments regarding ENorm and SNorm in Tab. 5b. A naive maximum normalization (dividing the maximum value) serves as the baseline when both normalization methods are removed. Even if without score normalization, we still stably outperform all baselines in Tab. 2. Adding either ENorm or SNorm improves the performance greatly (? 4 MAE in test set), indicating the significance of score normalization. ENorm together with SNorm brings the best performance, reflecting <ref type="table">Table 3</ref>. Counting performance with cross-validation setting on FSC-147 dataset <ref type="bibr" target="#b23">[24]</ref>. Fold-i (i = 0, 1, 2, 3) indicates the test set. ? stands for the averaged improvement of the 3-shot case over the 1-shot case.  <ref type="figure">Figure 4</ref>. Qualitative results on the FSC-147 dataset <ref type="bibr" target="#b23">[24]</ref> under the 3-shot case. From left to right: support images, query image overlaid by the ground-truth density map, predicted density map by FamNet+ <ref type="bibr" target="#b23">[24]</ref>, and our prediction. The numbers bellow are the counting results. <ref type="table">Table 4</ref>. Cross-dataset generalization on the car counting dataset CARPK <ref type="bibr" target="#b9">[10]</ref>. The models are first pre-trained on FSC-147 <ref type="bibr" target="#b23">[24]</ref> (the "car" category is excluded), then fine-tuned on CARPK. Here we explore the influence of the block number. As shown in Tab. 5c, only 1-block SAFECount has achieved state-of-the-art performance by a large margin, which illustrates the effectiveness of our designed SAFECount architecture. Furthermore, the performance gets improved gradually when the block number increases from 1 to 4, and decreased slightly when the block number is added to 5. As proven in <ref type="bibr" target="#b8">[9]</ref>, too many blocks could hinder the training process, decreasing the performance. Finally, we set the block number as 4 for FSC-147.</p><p>Regressing from Similarity Map v.s. Enhanced Feature.</p><p>The density map can be regressed from either the enhanced feature or the similarity map. We compare these two choices in Tab. 5d. Raw Similarity is similar to FamNet <ref type="bibr" target="#b23">[24]</ref> (without test-time adaptation), predicting the density map directly from the raw similarity. The rest 3 methods follow our design, where i-block Similarity and i-block Feature mean that the density map is regressed from the similarity map and enhanced feature of the i th block, respectively. Obviously, 1block Feature and 4-block Feature significantly outperform Raw Similarity and 4-block Similarity, respectively. The reason may be that the enhanced feature contains rich semantics and can filter out some erroneous high similarity values, i.e. the high similarity values that do not correspond to target objects, as proven in <ref type="bibr" target="#b33">[34]</ref>. Comparison with Attention. In Tab. 5e, when the similarity derivation in SCM and the feature aggregation in FEM are replaced by an vanilla attention <ref type="bibr" target="#b34">[35]</ref>, the performance drops dramatically. As stated in Sec. 3.2, our method could better utilize the spatial structure of features than vanilla attention, which helps find more accurate boundaries between objects and brings substantial improvement.  <ref type="figure">Figure 5</ref>. Visualization of the similarity maps developed by FamNet+ <ref type="bibr" target="#b23">[24]</ref> and our SAFECount. Benefiting from the proposed SAFECount block, our approach recognizes much clearer boundaries between densely packed objects.</p><p>Kernel Flipping in FEM. The kernel flipping in FEM could help the similarity-weighted feature, f R , inherit the spatial structure from the support feature, f S (see Supplementary Material for details). The effectiveness of adding the flipping is proven by Tab. 5f. Adding the flipping could improve the performance stably (? 1 MAE), reflecting that preserving the spatial structure of f R benefits the counting performance.</p><p>Training v.s. Freezing Backbone. The comparison results are provided in Tab. 5g. The frozen backbone significantly surpasses the trainable backbone. Considering that the testing classes are different from training classes in FSC-147 <ref type="bibr" target="#b23">[24]</ref>, training backbone will lead the backbone to extract more relevant features to training classes, which decreases the performance in the validation and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization</head><p>We visualize and compare the intermediate similarity map in FamNet <ref type="bibr" target="#b23">[24]</ref> and SAFECount in <ref type="figure">Fig. 5</ref>, which intuitively explains why SAFECount surpasses FamNet substantially.</p><p>Here the similarity map in SAFECount means the one in the last block. In FamNet, the similarity map is derived by direct comparison between the raw features of the query image and support images. However, the similarity map is far less informative than features, making it hard to identify clear boundaries within densely packed objects. In contrast, we weigh the support feature based on the similarity values, then integrate the similarity-weighted feature into the query feature. This design encodes the support-query relationship into features, while keeping the rich semantics extracted from the image. Also, our similarity comparison module is learnable. Benefiting from these, our SAFECount gets clear boundaries between densely packed objects in the similarity map, which is beneficial to regress an accurate count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, to tackle the challenging few-shot object counting task, we propose the similarity-aware feature enhancement block, composed of a similarity comparison module (SCM) and a feature enhancement module (FEM). Our SCM compares the support feature and the query feature to derive a score map. Then the score map is normalized across both the exemplar and spatial dimensions, producing a reliable similarity map. The FEM views these similarity values as weighting coefficients to integrate the support features into the query feature. By doing so, the model will pay more attention to the regions similar to support images, bringing distinguishable borders within densely packed objects. Extensive experiments on various benchmarks and training settings demonstrate that we achieve state-of-the-art performance by a considerably large margin.</p><p>This part describes the detailed architecture of our SAFECount block and other assistant modules, followed by the training configurations. Feature Extractor. We select ResNet-18 <ref type="bibr" target="#b8">[9]</ref> pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref> as the feature extractor. <ref type="bibr" target="#b0">1</ref> Given a query image, Q ? R 3?512?512 , we resize the outputs of the first three residual stages of ResNet-18 to the same size, 128 ? 128, and concatenate them along the channel dimension. Afterward, a 1 ? 1 convolutional layer is applied to reduce the channel dimension to 256, resulting in the query feature, f Q ? R 256?128?128 . The size of ROI pooling <ref type="bibr" target="#b25">[26]</ref> is set as 3 ? 3, so the support feature, f S , has the shape of K ? 256 ? 3 ? 3 in the K-shot case. The backbone is frozen during training, while the 1 ? 1 convolutional layer is not. Similarity Comparison Module (SCM). Our SCM is implemented with three steps: learnable feature projection, feature comparison, and score normalization. The feature comparison is implemented by convoluting the query feature, f Q , with the support feature, f S , as kernels, deriving a score map, R 0 . This process is illustrated intuitively in <ref type="figure" target="#fig_0">Fig. 6a</ref>. Other components in SCM have been detailed in the paper. The SCM finally outputs a similarity map, R ? R K?1?128?128 . Feature Enhancement Module (FEM). The FEM is composed of two steps: weighted feature aggregation and learnable feature fusion. The weighted feature aggregation treats the values in the similarity map, R, as weighting coefficients to integrate f S , producing the similarity-weighted feature, f R . This process is realized by the convolution, as shown  in <ref type="figure" target="#fig_0">Fig. 6b</ref>. Besides, before serving as the convolutional kernels, f S is flipped both horizontally and vertically. As illustrated in <ref type="figure" target="#fig_1">Fig. 7</ref>, the flipping helps f R inherit the spatial structure from f S . In <ref type="figure" target="#fig_1">Fig. 7</ref>, R is a unit impulse function, meaning that only one position has the maximum similarity with f S , while other positions have no similarity with f S at all. Therefore, f R should have a sub-part exactly the same with f S in the position corresponding to the maximum similarity, while the others should be zero vectors. The weighted feature aggregation constructs f R following the above insights via flipping and convolution. The learnable feature fusion is completed by a 2-layer convolutional network, skip connection, and layer normalization. The architecture of the convolutional network is shown in Tab. 6a. Other components in FEM have been detailed in the paper. Eventually, the FEM produces the enhanced feature, f ? Q ? R 256?128?128 . Regress Head. The regress head regresses the density map, D ? R 512?512 , from the enhanced feature, f ? Q . The regression head is composed of a sequence of convolutional  <ref type="bibr" target="#b23">[24]</ref>. The sizes of the query image, the query feature, and the support feature are selected as 512 ? 512, 128 ? 128, and 3 ? 3, respectively. The SAFECount block number is set as 4. The model is trained with Adam optimizer <ref type="bibr" target="#b11">[12]</ref> for 200 epochs with batch size 8. The hyper-parameter ? in Adam optimizer is set as 4e-11, much smaller than the default 1e-8, considering the small norm of the losses and the gradients. The learning rate is set as 2e-5 initially, and it is dropped by 0.25 after every 80 epochs. Data augmentation methods including random horizontal flipping, color jittering, and random gamma transformation are adopted.</p><formula xml:id="formula_9">, ? " ? " , Kernel , 1? $ ? $ Scalar Slide Kernel , ? $ ? $ , ? $ ? $ , 1? $ ? $ A Channel of , 1? $ ? $ A Channel of (lip( ), 1? " ? " , Kernel Slide Kernel Scalar (lip( ), ? " ? " (a) (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Studies</head><p>This part conducts comprehensive ablation studies on the components of our approach.</p><p>Loss Function. The loss function described in the paper is MSE loss. Actually, we also implement experiments with another SSIM term as follows,</p><formula xml:id="formula_10">L = MSE(D, D GT ) ? ?SSIM(D, D GT ),<label>(10)</label></formula><p>where SSIM(?) is the structural similarity function <ref type="bibr" target="#b38">[39]</ref>, which measures the local pattern consistence between the predicted density map and the ground-truth, ? is the weight term. The results with different ? are shown in Tab. 7a. Adding the SSIM term promotes the performance of MAE but with the sacrifice of RMSE. Compared with MAE, RMSE relies more heavily on the prediction of the samples with extremely large count. Therefore, we speculate that the SSIM term is beneficial to some samples, but may harm the samples with extremely large count. We finally decide not to add the SSIM term, because the performance drop of RMSE is too large.</p><p>Size of ROI Pooling. To study the influence of the ROI pooling size, we conduct experiments with different ROI pooling sizes. The results are shown in Tab. 7b. The performance is the worst with the ROI pooling size as 1 ? 1, i.e. pooling to a support vector, since pooling to a support vector fully omits the spatial information of the support image. Adding the ROI pooling size to 3 ? 3 brings stable improvement. However, further increasing the ROI pooling size to 5 ? 5 decreases the performance slightly. This may be because too large ROI pooling size would slightly hinder the accurate localization of target objects. Accordingly, we select the ROI pooling size as 3 ? 3 for FSC-147.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Results</head><p>This part presents more experimental results, including the quantitative evaluation on various class-specific counting datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b50">51]</ref>, as well as some visual samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Class-specific Object Counting</head><p>Our method is designed to be a general class-agnostic FSC approach. Nonetheless, we still evaluate our method on class-specific counting tasks to further testify its superiority. Class-specific Counting Datasets. We select five classspecific counting datasets including two car counting datasets: CARPK <ref type="bibr" target="#b9">[10]</ref> and PUCPR+ <ref type="bibr" target="#b9">[10]</ref> and three crowd counting datasets: ShanghaiTech (PartA and PartB) <ref type="bibr" target="#b50">[51]</ref>, UCSD <ref type="bibr" target="#b2">[3]</ref>, and Mall <ref type="bibr" target="#b3">[4]</ref>. The details of these datasets are given in Tab. 8.  1 Detectors provided by the benchmark <ref type="bibr" target="#b9">[10]</ref>. <ref type="bibr" target="#b1">2</ref> Single-class car counting methods. <ref type="bibr" target="#b2">3</ref> Single-class crowd counting methods. <ref type="bibr" target="#b3">4</ref> Multi-class counting methods (classes for training and test must be the same). <ref type="bibr" target="#b4">5</ref> Few-shot counting methods. ? trained and evaluated by ourselves with the official code.</p><p>FSC methods with a large margin, and achieves competitive performance on par with specific crowd counting methods. It is emphasized that, our method is not tailored to the specific crowd counting task, while the compared methods are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. More Qualitative Results</head><p>Qualitative Results on FSC-147 <ref type="bibr" target="#b23">[24]</ref>. The qualitative results of FSC-147 are shown in <ref type="figure">Fig. 8, Fig. 9</ref>, and <ref type="figure">Fig. 10a</ref>. For each class, the images from top to down are the query image and the predicted density map. The objects circled by the red rectangles are the support images. The texts below the density map describe the counting results. Our SAFECount could successfully count objects of all categories with various densities and scales, demonstrating strong generalization ability and robustness. Specifically, for both objects with extremely high density (e.g., Legos in <ref type="figure">Fig. 9</ref>) and objects with quite sparse density (e.g., Prawn Crackers in <ref type="figure">Fig. 9</ref>), both small objects (e.g., Birds in <ref type="figure">Fig.   8</ref>) and large objects (e.g., Horses in <ref type="figure">Fig. 8</ref>), both round objects (e.g., Apples in <ref type="figure">Fig. 8</ref>) and square objects (e.g., Stamps in <ref type="figure">Fig. 9</ref>), both vertical strip objects (e.g., Skis in <ref type="figure">Fig. 9</ref>) and horizontal strip objects (e.g., Shirts in <ref type="figure">Fig. 9</ref>), our approach could precisely count objects of interest with high localization accuracy.</p><p>Qualitative Results on Class-specific Object Counting. Our method is evaluated on two car counting datasets and three crowd counting datasets. For each dataset, five support images are randomly sampled from the training set and fixed for both training and test, as shown in <ref type="figure">Fig. 10b</ref>. The qualitative results on CARPK <ref type="bibr" target="#b9">[10]</ref>, PUCPR+ <ref type="bibr" target="#b9">[10]</ref>, UCSD <ref type="bibr" target="#b2">[3]</ref>, Mall <ref type="bibr" target="#b3">[4]</ref>, and ShanghaiTech <ref type="bibr" target="#b50">[51]</ref> are shown in <ref type="figure">Fig. 10c-h.</ref> (1) Car Counting: Our approach could localize and count cars with different angles and scales successfully. Especially, in the cases that some cars are in the deep shadows (e.g., the 7 th , 11 th examples in <ref type="figure">Fig. 10c</ref>, the 11 st example in <ref type="figure">Fig. 10d</ref>) or partly hidden under the trees (e.g., the 3 rd , 10 th examples in <ref type="figure">Fig. 10c</ref>, the 5 th , 12 nd examples in <ref type="figure">Fig. 10d</ref>), our method still accurately localizes these cars, indicating the superiority of our approach.</p><p>(2) Crowd Counting: In the cases of UCSD and Mall where the crowd density is relatively sparse, our approach could count the number of persons precisely with extremely small error. For ShanghaiTech PartA, if the persons in the crowd are distinguishable (e.g., the 1 st , 11 th examples in <ref type="figure">Fig. 10g</ref>), our model could localize each person precisely. If the persons are too crowded to distinguish (e.g., the 2 nd , 5 th examples in <ref type="figure">Fig. 10g</ref>), our method could predict an accurate density estimate for crowds. For ShanghaiTech PartB where most persons are distinguishable, our approach successfully localizes and counts persons, indicating that our approach is capable of crowd counting with various crowd densities.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 6 .</head><label>6</label><figDesc>(a) Illustration of the feature comparison in SCM under the 1-shot case, where the feature projection is omitted. (b) Illustration of the weighted feature aggregation in FEM under the 1-shot case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 .</head><label>7</label><figDesc>Illustration of kernel flipping in FEM, which helps f R inherit the spatial structure from f S . The convolution is implemented with the same padding strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Qualitative results on unseen classes in FSC-147 (from Kiwis to Tree Logs). There are only 2 images of Prawn Crackers in FSC-147. (a) Qualitative results on unseen classes (Watches) in FSC-147. (b) Support images of class-specific datasets. (c-h) Qualitative results on class-specific datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results on FSC-147 dataset<ref type="bibr" target="#b23">[24]</ref>, where we surpass other competitors by a sufficiently large margin. The sizes of the query image, the query feature map, and the support feature map, H ? W , H Q ? W Q , and H S ? W S , are selected as 512 ? 512, 128 ? 128, and 3 ? 3, respectively. The dimension of the projected features are set as 256. The multi-block number is set as 4. The model is trained with Adam optimizer</figDesc><table><row><cell>Method</cell><cell cols="2">Val Set</cell><cell cols="2">Test Set</cell></row><row><cell></cell><cell>MAE</cell><cell>RMSE</cell><cell>MAE</cell><cell>RMSE</cell></row><row><cell>GMN [20]</cell><cell>29.66</cell><cell>89.81</cell><cell>26.52</cell><cell>124.57</cell></row><row><cell>MAML [7]</cell><cell>25.54</cell><cell>79.44</cell><cell>24.90</cell><cell>112.68</cell></row><row><cell>FamNet [24]</cell><cell>24.32</cell><cell>70.94</cell><cell>22.56</cell><cell>101.54</cell></row><row><cell>FamNet+ [24]</cell><cell>23.75</cell><cell>69.07</cell><cell>22.08</cell><cell>99.54</cell></row><row><cell>CFOCNet [46]</cell><cell>21.19</cell><cell>61.41</cell><cell>22.10</cell><cell>112.71</cell></row><row><cell>SAFECount (ours)</cell><cell>15.28</cell><cell>47.20</cell><cell>14.32</cell><cell>85.54</cell></row><row><cell cols="5">4.2. Class-agnostic Few-shot Object Counting</cell></row></table><note>Our method is evaluated on the FSC dataset FSC-147 [24] under the original setting and the cross-validation setting. Setup.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Ablation studies on (a) the effect of the similarity comparison module (SCM) and feature enhancement module (FEM), (b) the score normalization in SCM, (c) the stacked number of our SAFECount block, and (d) the place to regress density map, (e) comparison with attention, (f) kernel flipping, (g) training or freezing backbone.Feat. 16.23 55.34 16.46 92.62 4-block Simi. 19.74 64.30 18.70 99.34 4-block Feat. 15.28 47.20 14.32 85.54</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(c) Number of Block</cell></row><row><cell cols="3">(a) SCM &amp; FEM SCM FEM Val Set</cell><cell>Test Set</cell><cell cols="3">(b) Score Normalization ENorm SNorm Val Set Test Set</cell><cell># Block</cell><cell>Val Set MAE RMSE MAE RMSE Test Set</cell><cell>(d) Similarity Map v.s. Enhanced Feature Val Set Test Set</cell></row><row><cell></cell><cell cols="3">MAE RMSE MAE RMSE</cell><cell></cell><cell cols="2">MAE RMSE MAE RMSE</cell><cell>1</cell><cell>16.23 55.34 16.46 92.62</cell><cell>MAE RMSE MAE RMSE</cell></row><row><cell>?</cell><cell cols="3">? 21.35 62.13 22.10 99.89</cell><cell>?</cell><cell cols="2">? 17.55 58.66 16.13 96.90</cell><cell>2</cell><cell>16.04 54.53 15.36 87.35</cell><cell>Raw Simi.</cell><cell>24.36 74.61 23.65 108.77</cell></row><row><cell>? ? ?</cell><cell cols="8">? 21.45 59.15 23.37 98.01 ? 17.55 58.66 16.13 96.90 ? 15.28 47.20 14.32 85.54 1-block (e) Vanilla Attention [35] v.s. SAFECount ? ? 16.55 51.87 15.14 85.65 ? ? 16.58 51.26 16.40 93.97 ? ? 15.28 47.20 14.32 85.54 3 15.78 53.39 14.74 88.22 4 15.28 47.20 14.32 85.54 5 15.67 50.73 15.54 96.10 (f) Kernel Flipping in FEM (g) Training v.s. Freezing Backbone</cell></row><row><cell></cell><cell></cell><cell cols="2">Val Set</cell><cell>Test Set</cell><cell>Kernel Flipping</cell><cell>Val Set</cell><cell></cell><cell>Test Set</cell><cell>Freezing Backbone</cell><cell>Val Set</cell><cell>Test Set</cell></row><row><cell></cell><cell></cell><cell cols="3">MAE RMSE MAE RMSE</cell><cell></cell><cell cols="3">MAE RMSE MAE RMSE</cell><cell>MAE RMSE MAE RMSE</cell></row><row><cell cols="5">Vanilla Attention [35] 20.45 55.22 20.21 93.47</cell><cell>?</cell><cell cols="3">16.78 57.47 15.35 93.59</cell><cell>?</cell><cell>25.24 65.23 26.00 103.83</cell></row><row><cell></cell><cell>SAFECount</cell><cell cols="3">15.28 47.20 14.32 85.54</cell><cell>?</cell><cell cols="3">15.28 47.20 14.32 85.54</cell><cell>?</cell><cell>15.28 47.20 14.32 85.54</cell></row><row><cell></cell><cell>Support</cell><cell cols="2">Query</cell><cell>FamNet+</cell><cell>Ours</cell><cell cols="2">Support</cell><cell>Query</cell><cell>FamNet+</cell><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Network architectures of (a) the learnable feature fusion in FEM, where the skip connection and the layer normalization are omitted, and (b) the regress head. The enhanced feature derived by one block, f ? Q , could serve as the input to the next block by taking the place of the query feature, f Q , forming a multiblock architecture. As for another input of the next block, the support feature, f S , there are two choices. If the support image is cropped from the query image, f S is updated by ROI pooling on newly obtained f ? Q . If not, f S does not change in different blocks.</figDesc><table><row><cell></cell><cell cols="6">(a) Architecture of the learnable feature fusion</cell></row><row><cell cols="2">layer</cell><cell></cell><cell>kernel</cell><cell>in</cell><cell>out</cell><cell>activation</cell></row><row><cell cols="2">Conv</cell><cell cols="2">3 ? 3</cell><cell>256</cell><cell>1024</cell><cell>Leaky ReLU</cell></row><row><cell cols="2">Conv</cell><cell cols="2">3 ? 3</cell><cell>1024</cell><cell>256</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="5">(b) Architecture of the regress head</cell></row><row><cell>layer</cell><cell>kernel</cell><cell></cell><cell>in</cell><cell>out</cell><cell>activation</cell><cell>followed by</cell></row><row><cell>Conv</cell><cell cols="2">5 ? 5</cell><cell>256</cell><cell>128</cell><cell>Leaky ReLU</cell><cell>2?Upsample</cell></row><row><cell>Conv</cell><cell cols="2">3 ? 3</cell><cell>128</cell><cell>64</cell><cell>Leaky ReLU</cell><cell>2?Upsample</cell></row><row><cell>Conv</cell><cell cols="2">1 ? 1</cell><cell>64</cell><cell>32</cell><cell>Leaky ReLU</cell><cell>-</cell></row><row><cell>Conv</cell><cell cols="2">1 ? 1</cell><cell>32</cell><cell>1</cell><cell>ReLU</cell><cell>-</cell></row><row><cell cols="7">layers, followed by the Leaky ReLU activation and bi-linear</cell></row><row><cell cols="6">upsampling, as shown in Tab. 6b.</cell></row><row><cell cols="6">Multi-block Architecture. Training Configuration on FSC-147</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Ablation studies regarding (a) loss weight term ? in Eq. (10), (b) size of ROI pooling.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(a) ? in Eq. (10)</cell><cell></cell></row><row><cell>?</cell><cell cols="2">Val Set</cell><cell cols="2">Test Set</cell></row><row><cell></cell><cell>MAE</cell><cell>RMSE</cell><cell>MAE</cell><cell cols="2">RMSE</cell></row><row><cell>1e-3</cell><cell>15.15</cell><cell>52.02</cell><cell>15.42</cell><cell cols="2">95.77</cell></row><row><cell>1e-4</cell><cell>14.18</cell><cell>53.65</cell><cell>13.55</cell><cell cols="2">89.69</cell></row><row><cell>1e-5</cell><cell>15.11</cell><cell>56.05</cell><cell>14.63</cell><cell cols="2">93.41</cell></row><row><cell>0</cell><cell>15.28</cell><cell>47.20</cell><cell>14.32</cell><cell cols="2">85.54</cell></row><row><cell></cell><cell cols="3">(b) Size of ROI Pooling</cell><cell></cell></row><row><cell cols="2">Size of ROI Pooling</cell><cell cols="2">Val Set</cell><cell cols="2">Test Set</cell></row><row><cell></cell><cell></cell><cell>MAE</cell><cell>RMSE</cell><cell>MAE</cell><cell>RMSE</cell></row><row><cell>1 ? 1</cell><cell></cell><cell>15.83</cell><cell>54.65</cell><cell>16.13</cell><cell>95.52</cell></row><row><cell>3 ? 3</cell><cell></cell><cell>15.28</cell><cell>47.20</cell><cell>14.32</cell><cell>85.54</cell></row><row><cell>5 ? 5</cell><cell></cell><cell>15.57</cell><cell>53.79</cell><cell>15.18</cell><cell>89.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Class-specific counting datasets.Training Configuration on Class-specific Counting. The size of the support feature is set as 1 ? 1. The block number is set as 2. Data augmentation methods including random flip, color jitter, random rotation, and random grayscale are used to prevent over-fitting and improve the generalization ability. Other setups are the same as FSC-147. Car Counting. Car counting tasks are conducted on CARPK<ref type="bibr" target="#b9">[10]</ref> and PUCPR+<ref type="bibr" target="#b9">[10]</ref>. 5 support images are randomly sampled from the training set and fixed for both training and test. Our method is compared with 4 categories of baselines: object detectors, single-class car counting methods, multi-class counting methods, and FSC methods. Note that multi-class counting methods could only count classes in training set, while FSC methods can count unseen classes. The quantitative results are shown in Tab. 9a. Our approach surpasses all multi-class counting methods and FSC methods with a large margin, and achieves comparable performance with single-class car counting methods. Crowd Counting. Crowd counting tasks are implemented on UCSD<ref type="bibr" target="#b2">[3]</ref>, Mall<ref type="bibr" target="#b3">[4]</ref>, and ShanghaiTech<ref type="bibr" target="#b50">[51]</ref>. We randomly sample 5 support images from the training set and fixed them for both training and test. 3 kinds of competitors are included: single-class crowd counting methods, multiclass counting methods, and FSC methods. The results of MAE are reported in Tab. 9b. For UCSD and Mall where the crowd is relatively sparse, our approach surpasses all counterpart methods stably. For ShanghaiTech, our approach outperforms all multi-class counting methods and</figDesc><table><row><cell>Type</cell><cell>Dataset</cell><cell>#Images</cell><cell>#Objects</cell></row><row><cell>Car</cell><cell>CARPK [10] PUCPR+ [10]</cell><cell>1448 125</cell><cell>89,777 16,916</cell></row><row><cell></cell><cell>PartA [51]</cell><cell>482</cell><cell>241,677</cell></row><row><cell>Crowd</cell><cell>PartB [51] UCSD [3]</cell><cell>716 2000</cell><cell>88,488 49,885</cell></row><row><cell></cell><cell>Mall [4]</cell><cell>2000</cell><cell>62,325</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Counting performance on class-specific datasets, including CARPK<ref type="bibr" target="#b9">[10]</ref>, PUCPR+<ref type="bibr" target="#b9">[10]</ref>, UCSD<ref type="bibr" target="#b2">[3]</ref>, Mall<ref type="bibr" target="#b3">[4]</ref>, and ShanghaiTech (Part A &amp; Part B)<ref type="bibr" target="#b50">[51]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(a) Car Counting</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell cols="2">CARPK</cell><cell cols="2">PUCPR+</cell></row><row><cell></cell><cell></cell><cell>MAE</cell><cell>RMSE</cell><cell>MAE</cell><cell>RMSE</cell></row><row><cell></cell><cell>YOLO [25]</cell><cell>48.89</cell><cell>57.55</cell><cell>156.00</cell><cell>200.42</cell></row><row><cell>1</cell><cell>F-RCNN [26] S-RPN [10]</cell><cell>47.45 24.32</cell><cell>57.39 37.62</cell><cell>111.40 39.88</cell><cell>149.35 47.67</cell></row><row><cell></cell><cell>RetinaNet [17]</cell><cell>16.62</cell><cell>22.30</cell><cell>24.58</cell><cell>33.12</cell></row><row><cell>2</cell><cell>LPN [10] HLCNN [11]</cell><cell>23.80 2.12</cell><cell>36.79 3.02</cell><cell>22.76 2.52</cell><cell>34.46 3.40</cell></row><row><cell></cell><cell>One Look [22]</cell><cell>59.46</cell><cell>66.84</cell><cell>21.88</cell><cell>36.73</cell></row><row><cell>4</cell><cell>IEP Count [32]</cell><cell>51.83</cell><cell>-</cell><cell>15.17</cell><cell>-</cell></row><row><cell></cell><cell>PDEM [8]</cell><cell>6.77</cell><cell>8.52</cell><cell>7.16</cell><cell>12.00</cell></row><row><cell></cell><cell>GMN [20]</cell><cell>7.48</cell><cell>9.90</cell><cell>-</cell><cell>-</cell></row><row><cell>5</cell><cell>FamNet [24]</cell><cell>18.19</cell><cell>33.66</cell><cell>14.68  ?</cell><cell>19.38  ?</cell></row><row><cell></cell><cell>Ours</cell><cell>5.33</cell><cell>7.04</cell><cell>2.42</cell><cell>3.55</cell></row><row><cell></cell><cell cols="3">(b) Crowd Counting (MAE)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell>UCSD</cell><cell>Mall</cell><cell>PartA</cell><cell>PartB</cell></row><row><cell></cell><cell>Crowd CNN [48]</cell><cell>1.60</cell><cell>-</cell><cell>181.8</cell><cell>32.0</cell></row><row><cell></cell><cell>MCNN [51]</cell><cell>1.07</cell><cell>-</cell><cell>110.2</cell><cell>26.4</cell></row><row><cell></cell><cell>Switch-CNN [2]</cell><cell>1.62</cell><cell></cell><cell>90.4</cell><cell>21.6</cell></row><row><cell>3</cell><cell>CP-CNN [30] CRSNet [16]</cell><cell>-1.16</cell><cell>--</cell><cell>73.6 68.2</cell><cell>20.1 10.6</cell></row><row><cell></cell><cell>RPNet [47]</cell><cell>-</cell><cell>-</cell><cell>61.2</cell><cell>8.1</cell></row><row><cell></cell><cell>GLF [36]</cell><cell>-</cell><cell>-</cell><cell>61.3</cell><cell>7.3</cell></row><row><cell>4</cell><cell>LC-FCN8 [13] LC-PSPNet [13]</cell><cell>1.51 1.01</cell><cell>2.42 2.00</cell><cell>--</cell><cell>13.14 21.61</cell></row><row><cell></cell><cell>GMN [20]</cell><cell>-</cell><cell>-</cell><cell>95.8</cell><cell>-</cell></row><row><cell>5</cell><cell>FamNet [24]</cell><cell>2.70  ?</cell><cell>2.64  ?</cell><cell>159.11  ?</cell><cell>24.90  ?</cell></row><row><cell></cell><cell>Ours</cell><cell>0.98</cell><cell>1.69</cell><cell>73.70</cell><cell>9.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Qualitative results on unseen classes in FSC-147 (from Ants to Keyboard Keys). There are only 2 images of Flowers in FSC-147.</figDesc><table><row><cell>Ants Bottle Caps Carrom Board Pieces Comic Books Eggs Flower Pots Green Peas Milk Cartons Pills Red Beans Seagulls Skateboards Figure 8. Kiwis Strawberries</cell><cell>Apples Bullets Cashew Nuts Crab Cakes Elephants Flowers Horses Legos Nail Polishes Polka Dots Sauce Bottles Shallots Skis Sunglasses</cell><cell>Birds Camels Chairs Deer Finger Foods Fresh Cuts Hot Air Balloons Marbles Oysters Potato Chips Sausages Sheep Stamps Toilet Paper Rolls</cell><cell>Books Candy Pieces Chicken Wings Donuts Flamingos Grapes Keyboard Keys Markers Peaches Prawn Crackers Sea Shells Shirts Sticky Notes Tree Logs</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">different parking lots. The training set contains 3 scenes, while another scene is used for test.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We borrow the checkpoint here.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Counting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak Babu</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang-Sheng John</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brit. Mach. Vis. Conf., page 3</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fewshot object detection with attention-rpn and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Precise detection in densely packed scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dronebased object counting by spatially regularized regional proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Ru</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An accurate car counting in aerial images based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Kilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serkan</forename><surname>Ozturk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Ambient Intelligence and Humanized Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Where are the blobs: Counting by localization with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Issam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pedestrian detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Seemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="878" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1091" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DecideNet: Counting varying density crowds through attention guided detection and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-modal collaborative representation learning and a large-scale rgbt benchmark for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4823" to="4833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Class-agnostic counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Class-aware object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Schenkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Middelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. Appl. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A large contextual dataset for classification, detection and counting of cars with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>T Nathan Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Konjevod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wesam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kofi</forename><surname>Sakla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boakye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Iterative crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="270" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to count everything</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udbhav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Top-down feedback for crowd counting convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babu</forename><surname>Deepak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Assoc. Adv. Artif. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crowd counting with deep negative correlation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenglin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyan</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5382" to="5390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generating highquality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1861" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating highquality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1861" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking counting and localization in crowds: A purely point-based framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3365" to="3374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Divide and count: Generic object counting by image divisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Stahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Silvia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan C Van</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2325" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A generalized loss function for crowd counting and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="1974" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Residual regression with semantic prior for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4036" to="4045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic adaptation of a generic pedestrian detector to a specific traffic scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3401" to="3408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Few-shot classification with feature map reconstruction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Wertheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiscale positive sample refinement for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="456" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatiotemporal modeling for crowd counting in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5151" to="5159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dilated-scale-aware category-attention convnet for multi-class object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingkang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Perspective-guided convolution networks for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="952" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mining latent classes for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Class-agnostic few-shot object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo-Diao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Ting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Winston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Chin</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. Appl. Comput. Vis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="870" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reverse perspective network for perspectiveaware object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">CANet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5217" to="5226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cross-view crossscene multi-view crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="557" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
