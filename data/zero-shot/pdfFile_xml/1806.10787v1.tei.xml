<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How To Extract Fashion Trends From Social Media? A Robust Object Detector With Support For Unsupervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Gabale</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Infilect Technologies Pvt. Ltd. Bangalore</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><forename type="middle">Prabhu</forename><surname>Subramanian</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Infilect Technologies Pvt. Ltd</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How To Extract Fashion Trends From Social Media? A Robust Object Detector With Support For Unsupervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Social Media Fashion</term>
					<term>Fashion Object Detection</term>
					<term>Unsupervised Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the proliferation of social media, fashion inspired from celebrities, reputed designers as well as fashion influencers has shortned the cycle of fashion design and manufacturing. However, with the explosion of fashion related content and large number of user generated fashion photos, it is an arduous task for fashion designers to wade through social media photos and create a digest of trending fashion. Designers do not just wish to have fashion related photos at one place but seek search functionalities that can let them search photos with natural language queries such as 'red dress', 'vintage handbags', etc in order to spot the trends. This necessitates deep parsing of fashion photos on social media to localize and classify multiple fashion items from a given fashion photo. While object detection competitions such as MSCOCO have thousands of samples for each of the object categories, it is quite difficult to get large labeled datasets for fast fashion items. Moreover, state-of-the-art object detectors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref> do not have any functionality to ingest large amount of unlabeled data available on social media in order to fine tune object detectors with labeled datasets. In this work, we show application of a generic object detector <ref type="bibr" target="#b10">[11]</ref>, that can be pretrained in an unsupervised manner, on 24 categories from recently released Open Images V4 dataset. We first train the base architecture of the object detector using unsupervisd learning on 60K unlabeled photos from 24 categories gathered from social media, and then subsequently fine tune it on 8.2K labeled photos from Open Images V4 dataset. On 300 ? 300 image inputs, we achieve 72.7% mAP on a test dataset of 2.4K photos while performing 11% to 17% better as compared to the state-of-the-art object detectors. We show that this improvement is due to our choice of architecture that lets us do unsupervised learning and that performs significantly better in identifying small objects. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Fashion designers today actively seek inspirations from social media photos to formulate innovative designs before they are put into production. In last several years, social media is flooded with fashion inspirations from celebrities, reputed designers as well as fashion influencers. However, it has only made the task of manually parsing and extracting intelligence from these photos arduous for fashion designer. Fashion designers today wish to have an easy to use tool with several search functionalities that can let them search photos with natural language queries such as 'red dress', 'flora handbags', 'vintage tees', etc. The main bottleneck in creating this digest is the identification of fashion photos and fashion items inside these photos. This necessitates deep parsing of fashion photos on social media to localize and classify multiple fashion items from a given fashion photo.</p><p>Image object detection involves identifying bounding boxes encapsulating objects and classifying each bounding box to recognize the underlying object category. Recently there has been mounting interest in the research community to detect multiple objects in an image using Single Shot Detection techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. These techniques effectively combine region proposal and classification into a single step by foregoing the candidate box proposal (or region proposal) module employed by several two-step detection techniques <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>While these techniques have made some progress, they fundamentally lack two key features to solve problems such as detecting fashion objects from fashion photos in the wild: (1) ability to ingest large amount of unlabeled dataset and (2) ability to maintain detection accuracy in the absence of large labeled datasets for small and big object alike. These two problems are especially relevant to parse fashion photos on social media since social media has large amount of unlabeled fashion photos, i.e., fashion photos for which classes and boxes are absent. Moreover, there are no labeled datasets that have several thousands of labeled photos for each of several tens of fashion categories. It is not easy to get such large dataset labeled owing expertise, cost, and timing constraints.</p><p>In this work, we apply a convolution-deconvolution based object detector to extract fashion objects from fashion photos. Specifically,</p><p>? We apply an end-to-end trainable convolutiondeconvolution based single shot detection framework ? We compare our object detector with state-of-the-art object detectors. We take object detection models pretrained on PASCAL VOC dataset and then fine tune them on fashion datasets. We show that our model performs 11% to 17% better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CDSSD ARCHITECTURE</head><p>We apply the technique proposed in CDSSD <ref type="bibr" target="#b10">[11]</ref> to detect fashion objects from fashion photos. In this section, we first give a primer on SSD architecture that is popularly used for object detection. We then explain why this architecture is inadequate for us to process social media fashion photos. We then briefly explain how CDSSD architecture extends SSD and discuss how we benefit by applying CDSSD architecture to extract fashion objects from fashion photos in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SSD</head><p>The SSD network is a convolutional architecture that utilizes different layers to predict presence of multiple objects in an image.</p><p>To recognize objects at different scales, SSD utilizes predictions on different feature maps, each from a different layer, of a single network. Instead of processing the image at different sizes, these feature maps are processed by a fixed-size collection of bounding boxes customized for each layer. The boxes are applied on each feature map. Each default box is then evaluated for the presence of object class instances. For feature map f of size m ? n with p channels, K default-sized bounding boxes are applied on each of m ? n cells. Subsequently, C filters of size 3 ? 3 ? p are applied for each cell and for a given bounding box to produce individual scores to predict each of C classes, and 4 additional filters are applied to produce offsets (center co-ordinate, height, width) to position the box on the underlying cell in order to encapsulate the object (as shown in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>). Note that, for a given feature map f , the default boxes are scaled with a scaling factor f scal e with respect m and n and thus, they are customized to have different aspect ratios. Hence, bounding boxes on initial stage feature maps cover a smaller receptive field to identify objects at a smaller scale, whereas bounding boxes on later stage feature maps cover larger receptive fields to identify objects with larger scale. By utilizing predictions for all the default boxes with different scales and aspect ratios from all locations of many feature maps, SSD attempts a diverse set of predictions, covering various input object sizes and shapes. While SSD technique is useful to detect fashion objects from fashion photos, it has two main limitations.</p><p>? It does not have any provision to input unlabeled fashion photos, and train the feature maps in an unsupervised fashion. Thus, although social media has several hundreds of thousands of fashion photos, these photos can not be directly applied unless they are labeled with classes and bounding boxes. ? It is known to perform poorly to identify small sized objects. This is especially a concern for fashion photos since object categories such as earrings or clutches are small in size and appear in all sorts of sizes and shapes. Furthermore, since tops and tees or such fashion pairs look quite similar, extracting fashion objects from photos in the wild is necessarily a harder problem than extracting objects such as planes vs people that are relatively easy to distinguish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CDSSD Architecture</head><p>As mentioned in <ref type="bibr" target="#b10">[11]</ref>, CDSSD facilitates unsupervised training of the underlying network architecture. For the purpose of this work, we use ResNet 101 architecture <ref type="bibr" target="#b4">[5]</ref> and construct a convolutiondeconvolution based auto-encoder (shown in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>). The deconvolution block produces an image of the same dimension as input.</p><p>We use an input image of 300 ? 300 ? 3, with 7 meta-layers of convolution and pooling and 7 meta-layers of deconvolution with learned upsampling. Given an image dataset, we first pretrain the architecture several thousands of fashion photos in the wild. After pretraining, we fine tune the same network by applying supervised object detection. It is well-known that feature maps in different layers of a network have different receptive fields and hence they learn different set of features; initial layers especially learn generic features whereas final layers learn semantically rich features. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref> observe that the initial layers of a deep network lack strong semantic information and respond to only high-level features of an image. Furthermore, the improvement in acquiring semantic information across consecutive feature maps is only marginal, especially in initial layers of a network. CDSSD exploits these observations and fuses generic and semantic features to enrich feature maps. Furthermore, CDSSD combines all the four feature maps of a given meta layer as shown in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>. Similar to SSD, CDSS then applies a set of K default-boxes and (C + 4) ?m ?n ?K filters on the resulting feature map to predict detection of objects. Note that, since 6th and 7th meta-layers have higher reception field and contain richer semantic information, they are quite capable of detecting bigger size and scale objects <ref type="bibr" target="#b1">[2]</ref>.</p><p>This feature especially helps us to locate and classify small size objects such as earrings, sandals, hats, boots, wrist-watches, sunglasses from photos in the wild. Similar to CDSSD, to compute different aspect ratios for each cell, we take a statistical approach and compute a cumulative distribution of aspect ratios of the ground truth boxes in a given dataset. We then divide the distribution into B bins and pick the average value of a bin as one of the aspect ratio, thus resulting in B aspect ratios. For each b i ? B, for a feature map with size m ? n and scale of f scal e , we then set height to be m ? b i ? f scal e and width to be n ? b i ? f scal e . With optimized aspect ratios that fit the underlying dataset and different scales for different layers, we apply appropriate default boxes at box-pooled locations in each feature map, covering different object sizes and shapes.</p><p>We trained CDSSD architecture on 60K social media photos in an unsupervised fashion, and 8.2 photos in a supervised photos. After training, we applied the model on 2.4K photos for evaluation. Furthermore, we applied the model to more than 500K fashion photos, and extracted fashion objects and monthwise trends of different fashion styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>Our experiments are governed to answer the following key question: can we achieve better results in identifying fashion items from photos in the wild by employing unsupervised learning and confluence of feature maps from convolution and deconvolution blocks? Towards answering this question, we compare our approach with prior work on two state-of-the-art techniques: SSD <ref type="bibr" target="#b6">[7]</ref> and YOLO <ref type="bibr" target="#b8">[9]</ref>. Note that, SSD and YOLO do not employ unsupervised learning and do not consider confluence contextual and semantic features from convolution and deconvolution blocks. We take object detection    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We consider recently released Open Images V4 dataset that has photos and bounding boxes for the following object categories. Object Categories: sandal, high-heels, boot, jeans, shorts, swimwear, brasseire, shirt, coat, suit, miniskirt, jacket, dress, sunhat, cowboy-hat, umbrella, glasses, belt, earrings, handbag, watch, backpack, suitcase, briefcase.</p><p>We totally consider 8.2K photos with on an average 600 instances for each object category. In Addition, we consider an unlabeled dataset of 60K photos collected from public APIs of social media networks, Facebook and Instagram. These photos contain even distribution of the above categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>The configuration of our network architecture is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. We keep the dropout layers during unsupervised training and remove them while training for object detection. We train our models on Azure GPU instances that have NVIDIA K80 GPUs with 12GB of memory. We use batch size of 16, momentum as 0.9 and weight decay 0.0005. Similar to SSD <ref type="bibr" target="#b6">[7]</ref>, we match a default box to target ground truth boxes, if Jaccard overlap is larger than a threshold (e.g. 0.5). We compute the target ground truth box for each layer of the  network by scaling it with respect to the feature map and original image sizes. We minimize the joint localization loss (i.e., smooth L1) and confidence loss (i.e., softmax-cross-entropy). Note that after the matching step, most of the default boxes are negatives. Hence, to avoid the imbalance between the positive and negative training examples, we sort the negative boxes using the joint loss for each default box and then pick the top ones to maintain a 2:1 negative to positive ratio. We found 2:1 ratio leads to faster optimization as compared to the ratio of 3:1 as mentioned in the original SSD paper.</p><p>We further make the model robust to different input object sizes and shapes by invoking extensive augmentation. Specifically, we sample a patch from a ground truth box so that the minimum Jaccard overlap with the objects is 0.5, 0.7, or 0.9. Furthermore, we randomly sample a patch between [0.5, 1] of the original image size, and the aspect ratio is between <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Also, we randomly flip each patch horizontally with probability of 0.5, apply different transformations such as gaussian blur, emboss, edge prominence, random black-out of 20% of pixels, and color (hue, saturation, contrast) distortions. We apply 3 ? 3 box pooling for layer 3 and 4, 2 ? 2 box pooling for layer 5, and no box pooling for layer 6 and 7. We apply nonmaximum suppression (NMS) to post-process the predictions to get final detection results.</p><p>We train the entire network with learning rate at 10 ?3 for 25K batches, and then with learning rate of 10 ?4 for 60K batches to execute unsupervised pretraining on the underlying train dataset During object detection training, we again fine-tune the entire network with learning rate of 2 ? 10 ?3 for 30K iterations, and 60K iterations with learning rate of 10 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Comparison of MAP for different models on the test fashion set are shown in Tab. 1. Although 2.4K is a smaller dataset for MAP evaluation, we clearly get an indication that CDSSD performs better due to the facility of pretraining using unsupervised learning. Note that the same 8.2K labeled dataset is used to fine-tune all the networks. Per category results are shown in Tab. 2. These results corroborrate that that by adding unsupervised pretraining and confluence of feature maps, CDSSD consistently outperforms YOLO and SSD by 11% to 17% points for several object categories. CDSSD especially shows significant improvement for small objects such as boots, high-heels, hand-bags. CDSSD detects majority objects with high confidence with less localization error and less confusion for similar object categories Finally, CDSSD achieves high-precision at high-recall range and outperforms YOLO and SSD (Tab. 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fashion Trends</head><p>Using the object detection technique, we could identify several fashion trends on social media platform. We mainly did analysis of Indian social media trends. We could predict the use usage of palazzos in 2015 before they became famous in India in 2016. By parsing celebrity photos, we could predict 'green' as the dominant color for hand-bags for 2018. Parsing photos published by fashion weeks and fashion shows, we could identify light gray color as the dominant color for long dresses. Furthermore, we could identify plum as the complementary color for tops that are paired with black jeans. This level of trend analysis would not have been possible without deeply parsing photos and extracting each fashion object along with its class and bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>We design an end-to-end framework using convolutiondeconvolution deep networks to improve the state-of-the-art of single shot object detection techniques. Using a combination of unsupervised learning and confluence of feature maps with different receptive fields, we demonstrate substantial improvement in mAP for different objects in PASCAL VOC and MS COCO datasets while reducing the bounding box requirement by 8 times, thus improving inference time by 10%. We believe that our work will inspire extensions to region proposal based detection techniques as well as other genres of objection detection towards </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>CDSSD combines information from convolution and deconvolution feature maps</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Results of fashion detectionfinding more effective and efficient ways to combine feature maps of convolution and deconvolution blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of single-shot detection techniques on 2.4K fashion photos, trained on 8.2K fashion photos</figDesc><table><row><cell>method</cell><cell>network</cell><cell cols="3">mAP number of default boxes fps</cell><cell>lib</cell></row><row><cell cols="3">YOLOv2_352 [9] DarkNet-19 56.7</cell><cell>98</cell><cell cols="2">81 DarkNet</cell></row><row><cell>DSSD321 [2]</cell><cell cols="2">ResNet-101 63.6</cell><cell>43688</cell><cell>9.5</cell><cell>Caffe</cell></row><row><cell>Stairnet [12]</cell><cell>VGGNet</cell><cell>64.8</cell><cell>8732</cell><cell cols="2">30 PyTorch</cell></row><row><cell>CDSSD300</cell><cell cols="2">ResNet-101 72.7</cell><cell>1182</cell><cell>51</cell><cell>TF</cell></row><row><cell cols="3">models for each of these techniques, pretrained on PASCAL VOC</cell><cell></cell><cell></cell></row><row><cell cols="3">dataset, and then fine tune them on 8.2K labeled photos from 24</cell><cell></cell><cell></cell></row><row><cell>categories.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>mAP comparison of single-shot detection techniques on fashio dataset</figDesc><table><row><cell>method</cell><cell>jeans</cell><cell cols="2">boot high-heels</cell><cell>shorts</cell><cell>sandals</cell><cell cols="2">briefcase coat</cell><cell>shirt</cell><cell cols="2">brasseire swimwear</cell><cell>suit</cell><cell>miniskirt</cell></row><row><cell>YOLO [7]</cell><cell>68.5</cell><cell>72.2</cell><cell>74.8</cell><cell>61.9</cell><cell>47.6</cell><cell>76.7</cell><cell>76.8</cell><cell>73.5</cell><cell>58.1</cell><cell>60.0</cell><cell>72.4</cell><cell>78.9</cell></row><row><cell>DSSD321 [2]</cell><cell>67.6</cell><cell>73.3</cell><cell>75.4</cell><cell>64.6</cell><cell>46.8</cell><cell>74.7</cell><cell>71.5</cell><cell>66.9</cell><cell>59.5</cell><cell>78.3</cell><cell>73.2</cell><cell>75.4</cell></row><row><cell>StairNet [12]</cell><cell>67.0</cell><cell>75.4</cell><cell>74.2</cell><cell>64.2</cell><cell>51.3</cell><cell>77.6</cell><cell>78.0</cell><cell>72.0</cell><cell>58.9</cell><cell>71.8</cell><cell>68.4</cell><cell>70.2</cell></row><row><cell>CDSSD300</cell><cell>77.4</cell><cell>73.9</cell><cell>78.2</cell><cell>89.5</cell><cell>84.7</cell><cell>80.2</cell><cell>80.3</cell><cell>78.7</cell><cell>73.4</cell><cell>69.9</cell><cell>76.7</cell><cell>83.3</cell></row><row><cell>method</cell><cell cols="2">jacket dress</cell><cell>sun-hat</cell><cell cols="2">cowboy-hat umbrella</cell><cell>glasses</cell><cell cols="3">belt earrings hand-bag</cell><cell>watch</cell><cell>backpack</cell><cell>suitcase</cell></row><row><cell>YOLO [7]</cell><cell>60.1</cell><cell>63.4</cell><cell>75.7</cell><cell>70.5</cell><cell>62.6</cell><cell>60.2</cell><cell>63.8</cell><cell>69.3</cell><cell>66.6</cell><cell>72.1</cell><cell>68.9</cell><cell>72.5</cell></row><row><cell>DSSD321 [2]</cell><cell>62.3</cell><cell>74.5</cell><cell>76.2</cell><cell>76.6</cell><cell>78.1</cell><cell>53.3</cell><cell>79.6</cell><cell>75.7</cell><cell>72.2</cell><cell>73.9</cell><cell>76.3</cell><cell>78.4</cell></row><row><cell>StairNet [12]</cell><cell>65.0</cell><cell>69.6</cell><cell>56.3</cell><cell>74.2</cell><cell>62.6</cell><cell>73.2</cell><cell>75.5</cell><cell>61.8</cell><cell>66.7</cell><cell>52.1</cell><cell>70.3</cell><cell>71.9</cell></row><row><cell>CDSSD300</cell><cell>74.5</cell><cell>79.4</cell><cell>77.1</cell><cell>75.2</cell><cell>82.6</cell><cell>66.4</cell><cell>76.1</cell><cell>71.4</cell><cell>74.7</cell><cell>77.9</cell><cell>80.8</cell><cell>82.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">: mAP at recall greater than 0.7</cell></row><row><cell>method</cell><cell>data</cell><cell></cell><cell></cell><cell>recall</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell><cell>mAP@70%</cell></row><row><cell>YOLO</cell><cell>07+12</cell><cell>81.9</cell><cell>73.7</cell><cell>33.4</cell><cell>40.2</cell></row><row><cell>SSD</cell><cell cols="3">07+12 84.6.3 78.7.5</cell><cell>33.6</cell><cell>45.7</cell></row><row><cell cols="2">CDSSD 07+12</cell><cell>98.4</cell><cell>90.1</cell><cell>56.6</cell><cell>62.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">DSSD : Deconvolutional Single Shot Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>abs/1701.06659</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. 2017. Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (1) (LNCS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9905</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Understanding the Effective Receptive Field in Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>abs/1701.04128</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CDSSD: Refreshing Single Shot Object Detection Using A Conv-Deconv Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabale</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sawant</forename><surname>Uma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD, To Appear</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">StairNet: Top-Down Semantic Aggregation for Accurate One Shot Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<idno>abs/1709.05788</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Soonmin Hwang, and In So Kweon</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Object Detectors Emerge in Deep Scene CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?gata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1412.6856</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
