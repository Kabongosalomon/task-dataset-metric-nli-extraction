<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Bagchi</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jazib</forename><surname>Mahmood</surname></persName>
							<email>jazib.mahmood@research.iiit.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kiran Sarvadevabhatla</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVIT</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">CVIT, IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dolton Fernandes CVIT, IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">CVIT, IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State of the art architectures for untrimmed video Temporal Action Localization (TAL) have only considered RGB and Flow modalities, leaving the information-rich audio modality totally unexploited. Audio fusion has been explored for the related but arguably easier problem of trimmed (clip-level) action recognition. However, TAL poses a unique set of challenges. In this paper, we propose simple but effective fusion-based approaches for TAL. To the best of our knowledge, our work is the first to jointly consider audio and video modalities for supervised TAL. We experimentally show that our schemes consistently improve performance for state of the art video-only TAL approaches. Specifically, they help achieve new state of the art performance on large-scale benchmark datasets -ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14 (57.18 mAP@0.5). Our experiments include ablations involving multiple fusion schemes, modality combinations and TAL architectures. Our code, models and associated data are available at https://github.com/skelemoa/tal-hmo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the boom in online video production, video understanding has become one of the most heavily researched domains. Temporal Action Localization (TAL) is one of the most interesting and challenging problems in the domain. The objective of TAL is to identify the category (class label) of activities present in a long, untrimmed, real world video and their temporal boundaries (start and end time). Apart from inheriting the challenges from the related problem of trimmed (clip-level) video action recognition, TAL also requires accurate temporal segmentation, i.e. to precisely locate the start time and end time of action categories present in a given video.</p><p>TAL is an active area of research and several approaches have been proposed to tackle the problem <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b22">22]</ref>. For the most part, existing approaches depend solely on the visual modality (RGB, Optical Flow). An important and obvious source of additional informationthe audio modality -has been overlooked. This is surprising since audio has been shown to be immensely useful for other video-based tasks such as object localization <ref type="bibr" target="#b2">[3]</ref>, action recognition <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b35">35]</ref> and egocentric action recognition <ref type="bibr" target="#b18">[18]</ref>.</p><p>Analyzing the untrimmed videos, it is evident that the audio track provides crucial complementary information regarding the action classes and their temporal extents. Action class segments in untrimmed videos are often characterized by signature audio transitions as the activity progresses (e.g. the rolling of a ball in a bowling alley culminating in striking of pins, an aquatic diving event culminating with the sound of splash in the water). Depending on the activity, the associated audio features can supplement and complement their video counterparts if the two feature sequences are fused judiciously <ref type="figure" target="#fig_0">(Figure 1</ref>).</p><p>Motivated by observations mentioned above, we make the following contributions:</p><p>? We propose simple but effective fusion approaches to combine audio and video modalities for TAL (Section 3). Our work is the first to jointly process audio and video modalities for supervised TAL.</p><p>? We show that our fusion schemes can be readily plugged into existing state-of-the-art video based TAL pipelines (Section 3).</p><p>? To determine the efficacy of our fusional approaches, we perform comparative evaluation on large-scale benchmark datasets -ActivityNet and THUMOS14.  Our results (Section 6) show that the proposed schemes consistently boost performance for state of the art TAL approaches, resulting in an improved mAP of 52.73 for ActivityNet-1.3 and 57.18 mAP for THU-MOS14.</p><p>? Our experiments include ablations involving multiple fusion schemes, modality combinations and TAL architectures.</p><p>Our code, models and additional data can be found at https://github.com/skelemoa/tal-hmo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Temporal Action Localization: A popular technique for Temporal Action Localization is inspired from the proposalbased approach for object detection <ref type="bibr" target="#b11">[12]</ref>. In this approach, a set of so-called proposals are generated and subsequently refined to produce the final class and boundary predictions. Many recent approaches employ this proposal-based formulation <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b45">45]</ref>. Specifically, this is the case for state-ofthe-art approaches we consider in this paper -G-TAD <ref type="bibr" target="#b47">[47]</ref>, PGCN <ref type="bibr" target="#b50">[50]</ref> and MUSES baseline <ref type="bibr" target="#b27">[27]</ref>. Both G-TAD <ref type="bibr" target="#b47">[47]</ref> and PGCN <ref type="bibr" target="#b50">[50]</ref> use graph convolutional networks and the concept of edges to share context and background information between proposals. MUSES baseline <ref type="bibr" target="#b27">[27]</ref> on the other hand, achieves state of the art results on the benchmark datasets by employing a temporal aggregation module, originally intended to account for the frequent camera changes in their new multi-shot dataset.</p><p>The proposal generation schemes in the literature are either anchor-based <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">28]</ref> or generate a boundary probabilty sequence <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b39">39]</ref>. Past work in this domain also includes end to end techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b25">25]</ref> which combine the two stages. Frame-level techniques which require merging steps to generate boundary predictions also exist <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b49">49]</ref>. We augment the proposal-based state of the art approaches designed solely for visual modality by incorporating audio into their architectures.</p><p>Audio-only based Localization: Speaker diarization <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b51">51]</ref> involves localization of speaker boundaries and grouping segments that belong to the same speaker. The DCASE Challenge <ref type="bibr" target="#b33">[33]</ref> examines sound event detection in domestic environments as one of the challenge tasks <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">36]</ref>. In our action localization setting, note that audio modality is unrestricted. It is not confined to speech or labelled sound events which is the case for audio-only localization. Audio-visual localization: This task is essentially localization of specific events of interest across modalities. Given a temporal segment of one modality (auditory or visual), we would like to localize the temporal segment of the associated content in the other modality. This is different from our task of temporal action localization (TAL) which focuses on predicting the class-labels and temporal segment boundaries of all actions present in a given video. Fusion approaches for TAL: Fusion of multiple modalities is an effective technique for video understanding tasks due to its ability to incorporate all the information available in videos. The fusion schemes present in the literature can be divided into 3 broad categories -early fusion, mid fusion and late fusion.</p><p>Late fusion combines the representations closer to the output end from each individual modality stream. When per-modality predictions are fused, this technique is also referred to as decision level fusion. Decision level fusion is used in I3D <ref type="bibr" target="#b5">[6]</ref> which serves as a feature extractor for the current state-of-the-art in TAL. However, unlike the popular classification setting, decision level fusion is challenging for TAL since predictions often differ in relative temporal extents. PGCN <ref type="bibr" target="#b50">[50]</ref>, introduced earlier, solves this problem by performing Non Maximal Suppression on the combined pool of proposals from the two modalities (RGB, Optical Flow). MUSES baseline <ref type="bibr" target="#b27">[27]</ref> fuses the RGB and Flow predictions. Mid fusion combines mid-level feature representations from each individual modality stream. Feichtenhofer et. al. <ref type="bibr" target="#b8">[9]</ref> found that fusing RGB and Optical Flow ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Untrimmed Audio</head><p>Untrimmed Video <ref type="figure">Figure 2</ref>. An illustrative overview of our fusion schemes (Section 3). streams at the last convolutional layer yields good visual modality features. The resulting mid-level features have been successfully employed by well performing TAL approaches <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b20">20]</ref>. In particular, they are utilized by G-TAD <ref type="bibr" target="#b50">[50]</ref> to obtain feature representations for each temporal proposal. Early fusion involves fusing the modalities at the input level. In the few papers that compare different fusion schemes <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b40">40]</ref>, early fusion has been shown to be generally an inferior choice. Apart from within (visual) modality fusion mentioned above, audio-visual fusion specifically has been shown to benefit (trimmed) clip-level action recognition <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b18">18]</ref> and audio-visual event localization <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b46">46]</ref>. The audio modality has also been shown to be beneficial for the weakly supervised version of TAL <ref type="bibr" target="#b19">[19]</ref> wherein the boundary labels for activity instances are absent. However, the lack of labels is a fundamental performance bottleneck compared to the supervised approach.</p><p>In our work, we introduce two mid-level fusion schemes along with decision level fusion to combine Audio, RGB, Flow modalities for state of the art supervised TAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED FUSION SCHEMES</head><p>Processing stages in video-only TAL: Temporal Action Localization can be formulated as the task of predicting start and end times (t s , t e ) and action label a for each action in an untrimmed RGB video V ? R F ?3?H?W , where F is the number of frames, H and W represent the frame height and width. Despite the architectural differences, state of the art TAL approaches typically consist of three stages: feature extraction, proposal generation and proposal refinement.</p><p>The feature extraction stage transforms a video into a sequence of feature vectors corresponding to each visual modality (RGB and Flow). Specifically, the feature extractor operates on fixed size snippets S ? R L?C?H?W and produces a feature vector f ? R dv . Here, C is the number of channels and L is the number of frames in the snippet. This results in the feature vector sequence F v ? R Lv?dv mentioned above where L v is the number of snippets. This stage is shown as the component shaded green (containing E v ) in <ref type="figure">Figure 2</ref>.</p><p>The proposal generation stage processes the feature sequence mentioned above to generate action proposals. Each candidate action proposal is associated with temporal extent (start and end time) relative to the input video, and a confidence score. In some approaches, each proposal is also associated with an activity class label.</p><p>The proposal refinement stage takes the feature sequence corresponding to each candidate proposal as input and refines the boundary predictions and confidence scores. Some approaches modify the class label predictions as well. The final proposals are generally obtained by applying non maximal suppression to weed out the redundancies arising from highly overlapping proposals. Also, note that some approaches do not treat proposal generation and refinement as two different stages. To accommodate this variety, we depict the processing associated with proposal generation and refinement as a single module titled 'PG/R' in Figure 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Incorporating Audio</head><p>As with video-only TAL approaches, the first stage of audio modality processing consists of extracting a sequence of features from audio snippets (refer to the blue shaded module termed E a in <ref type="figure">Figure 2</ref>). This results in the 'audio' feature vector sequence F a ? R La?da mentioned above where L a is the number of audio snippets and d a is the length of feature vector for an audio snippet.</p><p>Our objective is to incorporate audio as seamlessly as possible into existing video-only TAL architectures. To enable flexible integration, we present two schemes -proposal fusion and encoding fusion. We describe these schemes next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A Proposal fusion</head><p>This is a decision fusion approach and as suggested by its name, the basic idea is to merge proposals from the audio and video modalities (see 'Proposal Fusion' in Figure 2). To begin with, audio proposals are obtained in a manner similar to the procedure used to obtain video proposals. As mentioned earlier, it is straightforward to fuse action class predictions from each modality by simply averaging the probabilities. However, TAL proposals consist of regression scores for action boundaries, where averaging across modality does not make much sense since it is likely to introduce error into the predictions. This makes the fusion task challenging in the TAL setting.</p><p>To solve this problem while adhering to our objective of leaving the existing video-only pipeline untouched, we re purpose the corresponding module from the pipelines. Specifically, we use Non-Maximal Suppression (NMS) for iteratively choosing the best proposals which minimally overlap with other proposals. In some architectures (e.g. PGCN <ref type="bibr" target="#b50">[50]</ref>), NMS is applied to separate proposals from RGB and Flow components which contribute together as part of the visual modality. We extend this, by initially pooling visual modality proposals with audio proposals, and then apply NMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">B Encoding fusion</head><p>Instead of the late fusion of per-modality proposals described above, an alternative is to utilize the combined set of audio F a and video feature sequences F v to generate a single, unified set of proposals. However, since the encoded representation dimensions d a , d v and the number of sequence elements L a , L v can be unequal, standard dimension-wise concatenation techniques are not applicable. To tackle this issue, we explore four approaches to make the sequence lengths equal for feature fusion (depicted as purple block titled 'Encoding Fusion' in <ref type="figure">Figure 2</ref>).</p><p>For the first two approaches, we revisit the feature extraction phase and extract audio features at the frame rate used for videos. As a result, we obtain a paired sequence of audio and video snippets (i.e. L a = L v ).</p><p>? Concatenation (Concat): The paired sequences are concatenated along the feature dimension.</p><p>? Residual Multimodal Attention (RMAttn): To refine each modality's representation using features from other modality, we employ a residual multimodal attention mechanism <ref type="bibr" target="#b40">[40]</ref> as shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>We define the attention function f att and it can be adaptively learned from the visual feature map v t and audio feature vector a t . At each time step t, the visual context vector v att t is computed by:</p><formula xml:id="formula_0">v att t = f att (a t , v t ) = k i=1 w i t t i t ,<label>(1)</label></formula><p>where w t is an attention weight vector corresponding to the probability distribution over k visual regions that are attended by its audio counterpart. The attention weights can be computed based on MLP with a Softmax activation function:</p><formula xml:id="formula_1">w t = Sof tmax(x t ),<label>(2)</label></formula><p>x</p><formula xml:id="formula_2">t = W f ?(W v U v (v t ) + (W a U a (a t ))1 T ),<label>(3)</label></formula><p>where U v and U a , implemented by a dense layer with nonlinearity, are two transformation functions that project audio and visual features to the same di-</p><formula xml:id="formula_3">mension d, W v ? R k?d , W a ? R k?d , W f ? R 1?k</formula><p>are parameters, the entries in 1 T ? R k are all 1, ?() is the hyperbolic tangent function, and w t ? R k is the computed attention map.</p><p>The other two encoding fusion approaches:</p><p>? Duplicate and Trim (DupTrim): Suppose L v &lt; L a and k = La Lv . We first duplicate each visual feature to obtain a sequence of length kL v . We then trim both the audio and visual feature sequences to a common length L m = min(L a , kL v ). A similar procedure is followed for the other case (L a &lt; L v ).</p><p>? Average and Trim (AvgTrim): Suppose L v &lt; L a and k = La Lv . We group audio features into subsequences of length k = k . We then form a new feature sequence of length L a by averaging each k -sized group. Following a procedure similar to 'DupTrim' above, we trim the modality sequences to a common length, i.e. L m = min(L a , L v ).</p><p>For the above approaches involving trimming, the resulting audio and video sequences are concatenated along the feature dimension to obtain the fused multimodal feature sequence.</p><p>For all the approaches, the resulting fused representation is processed by a 'PG/R' (proposal generation and refinement) module to obtain the final predictions, similar to its usage mentioned earlier (see <ref type="figure">Figure 2</ref>). Apart from the fusion schemes, we also note that each scheme involves additional choices. In our experiments, we perform comparative evaluation for all the resulting combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">IMPLEMENTATION DETAILS TAL video-only architectures:</head><p>To determine the utility of audio modality and to ensure our method is easily applicable to any video-only TAL approach, we do not change the architectures and hyperparameters (e.g. snippet length, frame rate, optimizers) for the baselines. The feature extraction components of the baselines are summarized in <ref type="table" target="#tab_1">Table 1</ref>. Audio extraction: For audio, we use VGGish <ref type="bibr" target="#b14">[15]</ref>, a state of the art approach for audio feature extraction. We use a sampling rate of 16kHz to extract the audio signal and extract 128-D features from 1.2s long snippets. For experiments involving attentional fusion and simple concatenation, we extract features by centering the 1.2s window about the snippets used for video feature extraction so as to maintain the same feature sequence length for audio and video modalities. Windows shorter than 1.2s (a few starting and ending ones) are padded with zeros at the end to specify that no more information is present and to match the 1.2s window requirement. Although the opposite (i.e. changing sampling rate for video, keeping the audio setup unchanged) is possible, we prefer the former since the video-only architecture and (video) data processing can be used as specified originally, without worrying about the consequences of such a change on the existing video-only architectural setup and hyperparameter choices. Proposal Generation and Refinement (PG/R): For proposal generation, we consider state of the art architectures GTAD <ref type="bibr" target="#b47">[47]</ref>, BMN <ref type="bibr" target="#b24">[24]</ref> and BSN <ref type="bibr" target="#b26">[26]</ref>. Similarly, for proposal refinement, we consider proposals generated from BMN and BSN, refined in PGCN <ref type="bibr" target="#b50">[50]</ref> and MUSES <ref type="bibr" target="#b27">[27]</ref> in our experiments with audio. Optimization: We train all the architectures except PGCN and GTAD with their original setting. We use a batch size of 256 for PGCN and 16 for GTAD. For training, we use 4 GeForce 2080Ti 11GB GPUs. The entire codebase is based on Pytorch library except for VGGish <ref type="bibr" target="#b14">[15]</ref> which is based on Keras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>To compare our results with the SOTA architectures in which we incorporate audio, we evaluate our models on two benchmark datasets for temporal action localization.</p><p>Thumos14 <ref type="bibr" target="#b17">[17]</ref> contains 1010 untrimmed videos for validation and 1574 for testing. Of these, 200 validation and 213 testing videos contain temporal annotations spanning 20 activity categories. Following the standard setup <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b50">50]</ref>, we use the 200 validation videos for training and the 213 testing videos for evaluation.</p><p>ActivityNet-1.3 <ref type="bibr" target="#b4">[5]</ref> contains 20k untrimmed videos with 200 action classes between its training, validation and testing sets. Once again, following the standard setup <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b50">50]</ref> , we train on 10024 videos and test on the 4926 videos from the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Protocol</head><p>We label a temporal (proposal) prediction (with associated start and end time) as correct if (i) its Intersection Over Union (IOU) with ground-truth exceeds a pre-determined threshold (ii) proposal's label matches the ground truth counterpart. Following standard protocols, we evaluate the mean Average Precision (mAP) scores at IOU thresholds from 0.3 to 0.7 with a step of 0.1 for THUMOS14 <ref type="bibr" target="#b17">[17]</ref> and {0.5, 0.75, 0.95} for ActivityNet-1.3 <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RESULTS</head><p>The mAP@0.5 results of the best fusion approach for each video-only baseline can be seen in <ref type="table" target="#tab_1">Table 1</ref>. It can be seen that incorporating audio consistently improves performance across all approaches. In particular, this incorporation results in a new state-of-the-art result on both the benchmark TAL datasets. In terms of fusion approaches, the clear dominance of encoding fusion scheme (Section 3.3) can be seen. The Residual Multimodal Attention mechanism from this scheme enables best performance for the relatively larger ActivityNet-1.3 dataset. Similarly, our mechanism of resampling the audio modality followed by concatenation of per-modality features enables best performance for the THUMOS14 dataset. A fuller comparison of the existing best video-only and best audio-visual results obtained via our work can be seen in <ref type="table" target="#tab_2">Tables 2,3</ref>. The results once again reinforce the utility of audio for TAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Ablations</head><p>To analyse the utility of the fusion schemes proposed in Section 3, we compared their performances with audioonly and video-only methods for the best performing approach in each dataset. Looking at <ref type="table" target="#tab_2">Tables 2 and 3</ref>, it is readily evident that fusing audio and video gives the best results. Specifically, RM-Attention fusion enables best result for ActivityNet-1.3 while simple concat works best for Thumos14. The reason for simple concat's superior performance for Thumos14 can be explained by the fact that audio content is less informative regarding the action boundaries in Thumos14 compared to ActivityNet-1.3. This is also evident from the audio-only baselines -compare the second  rows of <ref type="table" target="#tab_2">Tables 2,3</ref>. We hypothesize that RM-Attention is more effective at fusing the modalities than filtering out noise when audio modality is uninformative. In contrast, for simple concat, the separate, non modulated contribution of audio and visual features makes the fusion scheme less susceptible to noise in any one modality. DupTrim seems to perform better than AvgTrim, while both are inferior to simple concatenation. This indicates that preserving the ideal frame rate for each modality may not be that crucial to performance and it is probably better to extract features at the same rate for each modality rather than artificially making them equal after extraction. Among the fusion schemes, proposal fusion performs the worst for both ActivityNet-1.3 and Thumos14. This is to be expected considering the fact that it just selects the best proposal out from the audio and visual streams.</p><p>The performances of the audio-only baselines for each dataset suggests that audio information present in ActivityNet-1.3 is much more indicative of activity boundaries compared to that in Thumos14. This is also consistent with the degree of improvement due to fusion for both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Class-wise analysis</head><p>To examine the effect of audio at the action class level, we plot the change in Average Precision (AP) relative to the video-only score for the best performing setup. <ref type="figure" target="#fig_3">Figure 4</ref> depicts the plot for ActivityNet-1.3, sorted in decreasing order of AP improvement. The majority of action classes show positive AP improvement. This correlates with observations made in the context of aggregate performance ( <ref type="table" target="#tab_1">Table 1</ref>, <ref type="table" target="#tab_2">Table 3</ref>). The action classes which benefit the most from audio (e.g. 'Playing Ten Pins', 'Curling', 'Blow-drying hair') tend to have signature audio transitions marking the beginning and end of the action. The classes at the opposite end (e.g. 'Painting', 'Doing nails', 'Putting in conact lenses') are characterized by very little association between visual and audio modalities. For these classes, we empirically observed that ambient background sounds are present which induce noisy features. However, the gating mechanism enabled by Residual Multimodal Attention ensures that the effect of such noise from the modalities is appropriately mitigated. This can be seen from the smaller magnitude of drop in AP. <ref type="figure" target="#fig_4">Figure 5</ref> depicts the sorted AP improvement plot for the relatively smaller Thumos14 dataset. Similar qualitative trends as for ActivityNet-1.3 mentioned earlier can be seen, i.e. signature audio transitions characterizing largest AP improvement classes and weak inter-modality associations characterizing least AP improvement classes. However, as mentioned in previous section, the relatively weak association between audio and video modalities in Thumos14 causes the % of categories which are negatively impacted by audio inclusion to be greater compared to ActivityNet-1.3.   <ref type="table" target="#tab_1">Table 1)</ref> with inclusion of audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Instance-wise analysis</head><p>Modifying the approach used by Alwassel et al. <ref type="bibr" target="#b0">[1]</ref> for their approach (DETAD), we analyze two salient attributes of data to analyse the effect of adding audio. These attributes are (i) coverage -the proportion of untrimmed video that the ground truth instance spans (ii) length (temporal duration)</p><p>To measure coverage, we normalize duration of an action instance relative to the duration of the video. Thus, larger the coverage, larger the extent the instance occupies in the video. Note that coverage lies between 0 and 1. We group the resulting coverage values into five buckets: Extra Small (XS: (0, 0.2]), Small (S: (0.2, 0.4]), Medium (M: (0.4, 0.6]), Large (L:(0.6, 0.8]), and Extra Large (XL: (0.8, 1.0]). The length is measured as the instance duration in seconds.</p><p>We create five different length groups: Extra Small (XS: (0, 30]), Small (S: <ref type="bibr" target="#b30">(30,</ref><ref type="bibr">60]</ref>), Medium (M: (60, 120]), Long (L: (120, 180]), and Extra Long (XL: &gt; 180). <ref type="figure" target="#fig_5">Figures 6 and 7</ref>, we see that most action instances fall in Extra Small buckets. Also, the distributions of coverage and length of the ground truth instances are skewed towards the left (shorter extents).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From the numbers below the bucket labels on x-axis in</head><p>The change in mAP due to inclusion of audio can be viewed in <ref type="figure" target="#fig_5">Figures 6 and 7</ref> on a per-bucket basis. The overall gain in performance for both dataset is well explained by the overwhelmingly large proportion of the total instances showing improvement due to audio : 63.3% by coverage and 79.81% by length for ActivityNet-1.3 and 95.5% by coverage and length for Thumos14.  <ref type="table" target="#tab_1">(Table 1</ref>) classified by instance length and coverage, with inclusion of audio. The numbers below X-labels represent the percentage of each type of instance class in the dataset From the figures, we see that audio fusion enables consistent improvements for XS and M instances for both datasets while for XL instances, the mAP decreases or remained unchanged. This can be attributed in part to the fact that the shorter instances have an audio 'signature' for the action that spans the majority of the instance which assists detection. For the longer action instances, the actioncharacteristic audio spans a small section of the instance which might not aid detection as much. The change in distribution of possible prediction outcomes with inclusion of audio can be viewed in <ref type="figure" target="#fig_7">Figure 8</ref>. The false positive errors except Background error have increased. However, their relative frequency is smaller. The large decrease in number of Background errors more than mitigates the combined increase in other error subcategories, explaining the overall improvement in performance. The trends in false positive errors also suggest that audio information is most useful in discriminating between activity instances and the background in untrimmed videos. In addition, we observe that the number of true positive predictions (prediction with highest IoU , with IoU &gt; ? and correctly predicted label, where ? is the IoU threshold) increase for both THUMOS14 and ActivityNet-1.3, with the inclusion of audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">False Positive analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we have presented multiple simple but effective fusion schemes for incorporating audio into existing video-only TAL approaches. To the best of our knowledge, our multimodal effort is the first of its kind for fully supervised TAL. An advantage of our schemes is that they can be readily incorporated into a variety of video-only TAL architectures -a capability we expect to be available for future approaches as well. Experimental results on two large-scale benchmark datasets demonstrate consistent gains due to our fusion approach over video-only methods and state-of-theart performance. Our analysis also sheds light on the impact of audio availability on overall as well as per-class performance. Going ahead, we plan to expand and improve the proposed families of fusion schemes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An example illustrating a scenario where audio modality can help improve peformance over video-only temporal action localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Residual Multimodal Attention mechanism with videoonly and audio features as a form of encoding fusion(Section 3.3).indicates tensor addition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>[ActivityNet-1.3] Relative change in per-class AP of the best multimodal setup (Table 1)with inclusion of audio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>[THUMOS14] Relative change in per-class AP of the best multimodal setup (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>[ActivityNet-1.3] Relative change in average mAP of the best multimodal setup (Table 1) classified by instance length and coverage, with inclusion of audio. The numbers below X-labels represent the percentage of each type of instance class in the dataset Figure 7. [THUMOS14] Relative change in average mAP of the best multimodal setup</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Following [ 1 ]</head><label>1</label><figDesc>, we consider the following error subcategories within false positive predictions:?Double Detection error: IoU &gt; ? and correct label but not the highest IoU. ? localization error: 0.1 &lt; IoU &lt; ?, correct label ? Confusion error: 0.1 &lt; IoU &lt; ?, wrong label ? Wrong Label error: IoU &gt; ?, wrong label ? Background error: IoU &lt; 0.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Change in number of True Positive (TP) predictions and False Positive (FP) errors of each type of the best multimodal setup (Table 1) for each dataset with the inclusion of audio. The dashed lines are added to distinguish vey close values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Architectural pipeline components for top-performing TAL approaches. To reduce clutter, only mAP@0.5 is reported. The '+Audio' group refers to the fusion configuration corresponding to the best results (Section 3). From our best run using the official code from<ref type="bibr" target="#b27">[27]</ref>. In the paper, reported mAP is 56.90<ref type="bibr" target="#b1">2</ref> The result is obtained by repeating the evaluation on the set of videos currently available.</figDesc><table><row><cell>+Audio</cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>[ActivityNet-1.3] mAP for GTAD[47] + TSP[2] of video-</cell></row><row><cell>only, audio-only and audio-visual fusional approaches.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">TSP: temporally-sensitive pretraining of video encoders for localization tasks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mean teacher with data augmentation for dcase 2019 task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Delphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Poulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Plapous</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Orange Labs Lannion, France</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional recurrent neural networks for weakly labeled semisupervised sound event detection in domestic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janek</forename><surname>Ebbers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhold</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">CTAP: complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<idno>abs/1807.04821</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">TURN TAP: temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<idno>abs/1703.06189</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cross-domain sound event detection: from synthesized audio to real audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Peng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal transformer networks with latent interaction for audio-visual event localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plakal</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rethinking Fusion Baselines for Multi-modal Human Action Recognition: 19th Pacific-Rim Conference on Multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongda</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, Part III</title>
		<meeting>Part III<address><addrLine>Hefei, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Epic-fusion: Audio-visual temporal binding for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-attentional audio-visual fusion for weaklysupervised action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Tae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoungwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrack</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep concept-wise temporal convolutional networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning salient boundary feature for anchor-free temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3320" to="3329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Guided learning convolution system for dcase 2019 task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese Academy of Sciences</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Institute of Computing Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">BMN: boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<idno>abs/1907.09702</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<idno>abs/1710.06236</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">BSN: boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1806.02964</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-shot temporal event localization: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="12596" to="12606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/1811.11524</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multimodal keyless attention fusion for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><forename type="middle">De</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2018</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Convolutionaugmented transformer for semi-supervised sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Komatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Takeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Temporal activity detection in untrimmed videos with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gir?-I-Nieto</surname></persName>
		</author>
		<idno>abs/1608.08128</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobutaka</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noboru</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohei</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annamaria</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Imoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuma</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Komatsu</surname></persName>
		</author>
		<title level="m">Proceedings of the Fifth Workshop on Detection and Classification of Acoustic Scenes and Events</title>
		<meeting>the Fifth Workshop on Detection and Classification of Acoustic Scenes and Events</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning sight from sound: Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hodgepodge: Sound event detection based on ensemble of semi-supervised learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fujitsu Research and Development Center</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">CDC: convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/1703.01515</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Action temporal localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/1601.02129</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">BSN++: complementary boundary regressor with scale-balanced relation modeling for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno>abs/2009.07641</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Audio-visual event localization in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Philip Andrew Mansfield, and Ignacio Lopz Moreno. Speaker diarization with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlton</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-stream multi-class fusion of deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Cuhk &amp; ethz &amp; siat submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">R-C3D: region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno>abs/1703.07814</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-modal relation-aware networks for audio-visual event localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3893" to="3901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Breaking winner-takes-all: Iterative-winners-out networks for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fully supervised speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aonan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1704.06228</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Positive sample propagation along the audiovisual event line</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
