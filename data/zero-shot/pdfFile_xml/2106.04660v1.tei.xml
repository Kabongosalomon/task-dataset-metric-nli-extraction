<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequential End-to-End Intent and Slot Label Classification and Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nihal</forename><surname>Potdar</surname></persName>
							<email>nihal.potdar@waterloo.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><forename type="middle">R</forename><surname>Avila</surname></persName>
							<email>anderson.avila@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sequential End-to-End Intent and Slot Label Classification and Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: spoken language understanding</term>
					<term>human- computer interaction</term>
					<term>low-latency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-computer interaction (HCI) is significantly impacted by delayed responses from a spoken dialogue system. Hence, endto-end (e2e) spoken language understanding (SLU) solutions have recently been proposed to decrease latency. Such approaches allow for the extraction of semantic information directly from the speech signal, thus bypassing the need for a transcript from an automatic speech recognition (ASR) system. In this paper, we propose a compact e2e SLU architecture for streaming scenarios, where chunks of the speech signal are processed continuously to predict intent and slot values. Our model is based on a 3D convolutional neural network (3D-CNN) and a unidirectional long short-term memory (LSTM). We compare the performance of two alignment-free losses: the connectionist temporal classification (CTC) method and its adapted version, namely connectionist temporal localization (CTL). The latter performs not only the classification but also localization of sequential audio events. The proposed solution is evaluated on the Fluent Speech Command dataset and results show our model ability to process incoming speech signal, reaching accuracy as high as 98.97 % for CTC and 98.78 % for CTL on single-label classification, and as high as 95.69 % for CTC and 95.28 % for CTL on two-label prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Spoken language understanding (SLU) aims at extracting structured semantic representations, such as intent and slots, from the speech signal <ref type="bibr" target="#b0">[1]</ref>. These representations are crucial to enable speech as the primary mode of human-computer interaction (HCI) <ref type="bibr" target="#b1">[2]</ref>. Traditional SLU solutions rely on the text transcription generated by an automatic speech recognition (ASR) module, followed by a natural language understanding (NLU) system, responsible for extracting semantics from the ASR output <ref type="bibr" target="#b2">[3]</ref>. As described in <ref type="bibr" target="#b3">[4]</ref>, in such scenarios the ASR typically operates on chunks of the incoming speech signal and outputs the transcript for each segment. The NLU then waits until all the speech segments are transcribed before processing the ASR output. This has significant latency implications. Another issue is that each module is trained and optimized separately. While the ASR optimization aims at minimizing word error rate, the NLU is often optimized on clean text with the assumption of error-less transcriptions from the ASR <ref type="bibr" target="#b4">[5]</ref>. Besides, this approach provides a cumulative error that propagates from each module, adding up to the overall SLU error.</p><p>Recently, we have witnessed an increasing interest in reducing the latency of the SLU task. Low-latency leads to more naturalness while interacting with a computer system and can ultimately improve the user experience (UX) <ref type="bibr" target="#b5">[6]</ref>. To this end, a handful of studies have specifically addressed the problem. In <ref type="bibr" target="#b3">[4]</ref>, the authors proposed a recurrent neural network (RNN) for processing the output of an ASR system in an online fashion. Their streaming SLU solution is based on an online NLU that processes word sequences of arbitrary length and incrementally provides multiple intent predictions. Similarly, the authors in <ref type="bibr" target="#b6">[7]</ref> propose an RNN-based model that jointly performs online intent detection and slot filling as input word embeddings arrive. Results show that the joint training model provides high accuracy for intent detection and language modeling with a small degradation on slot filling compared to the independent training models. Although these approaches show reasonable performance, they rely on the strong assumption of error-less transcriptions from the ASR as their NLU system is often trained on clean text. Moreover, they can not be considered end-to-end (e2e) solutions as their models are based on the ASR transcription.</p><p>To mitigate this, other studies have proposed the extraction of semantic information directly from audio. For example, several e2e SLU encoder-decoder architectures are investigated in <ref type="bibr" target="#b7">[8]</ref>. The authors showed that better performance is achieved when an e2e SLU solution that performs domain, intent, and argument prediction is jointly trained with an e2e ASR model that learns to generate transcripts from the same input speech. Another recent study introduces the Fluent Speech Command (FSC) dataset <ref type="bibr" target="#b8">[9]</ref>. The authors present a pre-training strategy for e2e SLU models. Their approach is based on using ASR targets, such as words and phonemes, that are used to pre-train the initial layers of their final model. These classifiers once trained are discarded and the embeddings from the pre-trained layers are used as features for the SLU task. Improved performance on large and small SLU training sets was achieved with the proposed pre-training approach. Similarly, in <ref type="bibr" target="#b9">[10]</ref>, the authors also proposed to fine-tune the lower layers of an end-toend CNN-RNN based model that learns to predict graphemes. This pre-trained acoustic model is optimized with the connectionist temporal classification (CTC) loss and then combined with a semantic model to predict intents.</p><p>The aforementioned research efforts have been either on developing online NLU or non-streamable e2e SLU. In the light of that, investigating a complete end-to-end low-latency streaming SLU solution is necessary. In this paper, we propose a compact e2e streamable SLU solution that (1) eliminates the need for an ASR module with (2) an online architecture that provides intent and slot predictions while processing incoming speech signals. To achieve that, a 3-dimensional convolutional neural network (3D-CNN) combined with a unidirectional long-short term memory (LSTM) is explored. We compare two alignmentfree loss functions: the CTC method and its adaptation, namely the connectionist temporal localization (CTL) function. Both methods will be discussed in section 2. We use the FSC dataset to perform our experiments and results show our model achieving accuracy as high as 98.97 % for single intent+slot classifi- cation and 95.69 % for multiple intent+slot detection.</p><p>The remainder of this paper is organized as follows. In Section 2, we discuss sequential labeling. Section 3 presents the proposed architecture. Section 4 describes our experimental setup and Section 5 presents our experimental evaluation. Section 6 gives the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Sequential Labeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Streaming Spoken Language Understanding</head><p>In a streaming e2e SLU scenario, given the input X = [x1, x2, ..., xn] of acoustic features of length N and the corresponding sequence of semantic outputs Y = [y1, y2, ..., yu] of length U , the precise alignment of X and Y is not known and typically N &gt; U . Unlikely ASR, for e2e SLU the gap between input and output length is higher as the semantic label prediction is conditioned to a larger input context. The period of silence in the audio tends to even increase this gap. The goal is to learn the distribution P (Y |X, ?). However, different from the non-streaming scenario, predictions are made for a given timestep, t, with the model incrementally predicting multiple output intents (or blank symbols for silence), while accounting for the context dependency from previous predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Connectionist Temporal Classification</head><p>The CTC method was first motivated to train RNNs of unsegmented data <ref type="bibr" target="#b10">[11]</ref>. Previous to CTC, training RNNs required prior segmentation of the input sequence. For that, each input segment was labeled and the RNN was trained to predict an output for each segment at a time. This required a post-processing step to consolidate the output predictions into a sequence of labels <ref type="bibr" target="#b10">[11]</ref>. With CTC, however, prior segmentation is no longer needed as the method allows a sequence-to-sequence mapping free of alignment. For acoustic modeling, for example, CTC automatically learns the alignments between the input sequence of acoustic frames and the respective sequence of output labels <ref type="bibr" target="#b11">[12]</ref>. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, CTC defines the so-called naive alignment by matching the input and output length adding the blank tokens (&lt;b&gt;), and repeating output predictions. After this, blank tokens are removed and repeated predictions are collapsed.</p><p>Formally, the conditional probability of a single alignment (or path), ?, is the product of the probabilities of observing ?t at time t and can be represented as</p><formula xml:id="formula_0">P (?|X, ?) = T t=1 P (?t|X, ?)<label>(1)</label></formula><p>where ?t represents a given label. Because P (?|X) defines mutually exclusive paths, the conditional probability for a sequence output is given by the sum of the probabilities of all paths corresponding to it: <ref type="figure">Figure 2</ref>: Streaming intent + slot approach versus a nonstreaming one. Note that slot has a higher prediction rate than intent (3 times faster in our experiments) as it depends on less temporal speech context.</p><formula xml:id="formula_1">P (Y |X, ?) = ??A X,Y T t=1 P (?t|X, ?)<label>(2)</label></formula><p>where AX,Y is the set of all valid alignments. The CTC loss is then defined as</p><formula xml:id="formula_2">LCT C (X, Y ) = ?log ??A X,Y T t=1 P (?t|X, ?)<label>(3)</label></formula><p>CTC considers no dependency between previous time steps which allows for frame-wise gradient propagation, but limits the possibility of learning sequential dependencies <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Connectionist Temporal Localization</head><p>CTL was first introduced in <ref type="bibr" target="#b13">[14]</ref> for classification and localization of sound event occurrences in an audio stream. CTC has been previously applied for sound event detection (SED), however, it was found that the method presented the so-called "peak clustering," especially for long events <ref type="bibr" target="#b14">[15]</ref>. In such cases, onset and offset labels will result in a peak of the frame-probabilities. During training, because the adjacent onset and offset labels of long events occur next to each other, CTC may interpret them as the existence of boundaries instead of the existence of an event. Because the CTC loss function focus on predicting sequential labels in the correct order regardless of any temporal constraints, the recurrent layer will derive onset and offset labels next to each other as it minimizes memory effort <ref type="bibr" target="#b13">[14]</ref>. Moreover, the model detects event boundaries which leads to high frame-probabilities surrounding onset and offset events and remains inactive for the period that the event is on, even when changes in the acoustic features are observed.</p><p>Three improvements are proposed to overcome the peak clustering issue <ref type="bibr" target="#b13">[14]</ref>. First, boundary probabilities are attained from network event probabilities using a "rectified delta" operator. This assures that the network predicts frame-wise probabilities of events and not of the event boundaries, which leads to different predictions for different acoustic features <ref type="bibr" target="#b13">[14]</ref>. The event boundaries are calculated then as follows,</p><formula xml:id="formula_3">zt(?) = max[0, yt(E) ? yt?1(E)] zt(?) = max[0, yt?1(E) ? yt(E)]<label>(4)</label></formula><p>where yt(E) is the probability of the event E at frame t, whereas zt(?) and zt(?) represent, respectively, onset and offset labels of event E at frame t.</p><p>Second, the boundary probabilities at each frame are considered mutually independent, which allows the overlap of sound events. The independence assumption eliminates the need for a black symbol used by CTC to emit nothing at a frame, as well as to separate repetition of the same label <ref type="bibr" target="#b13">[14]</ref>.</p><p>The third modification implies that consecutive repeating labels are no longer collapsed. With these modifications, multiple labels can be emitted at the same frame, which can not be achieved with the standard CTC <ref type="bibr" target="#b13">[14]</ref>. Thus the probability of emitting multiple labels at frame t is attained as follow,</p><formula xml:id="formula_4">pt(l1, ..., l k ) = k i=1 zt(li) ? l ? [1 ? zt(li)]<label>(5)</label></formula><p>with pt(l1, ..., l k ), being the probability of emitting the sequence of labels L = (l1, ..., l k ) at frame t without the need of temporal alignment. Thus, given the frame-probabilities of events yt(E) the probability of emitting the first i labels of L can be represented with the recurrence formula bellow <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_5">?t(i) = i j=0 ? (t?1) (i ? j)pt(li?j+1, ..., li)<label>(6)</label></formula><p>where ?t(i) represents the probability of emitting the first i labels at frame t and j is the number of labels emitted at that particular frame, i.e. zero, one or more labels. Initial values of ?0(i) are set to 1 for i = 0. We refer the reader to <ref type="bibr" target="#b13">[14]</ref> for more details regarding the CTL loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Neural Network Architecture</head><p>The proposed model is depicted in <ref type="figure" target="#fig_1">Figure 3</ref> and is divided into three main parts. The first part focuses on learning feature representation from the speech signal. After extracting Melfilterbank features, a stacking operation is performed. Specifically, we stacked 8 input frames with stride 3. The motivation for this is twofold. First, it aims at providing a larger set of context frames to the upper layers which help to alleviate the work of CTC and CTL as it reduces the number of output paths to be explored. Second, it minimizes latency as the frame rate is reduced. After the stacking step, a 3D-CNN is also applied. Adopted in other speech-related studies <ref type="bibr" target="#b16">[16]</ref>[17] <ref type="bibr" target="#b19">[18]</ref>, our motivation is to learn better representation by processing time, frequency, and channel dimensions from the stacked Melfilterbank features. Similar to the approach taken in <ref type="bibr" target="#b17">[17]</ref>, no padding was used and two 3D convolutional layers are used. Both layers have kernel size of 5x5x1 and stride of 2x2x1. 16 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Intent</head><p>RNN+Pre-training <ref type="bibr" target="#b8">[9]</ref> 98.80 CNN+Segment pooling <ref type="bibr" target="#b0">[1]</ref> 97.80 CNN+GRU(SotA) <ref type="bibr" target="#b22">[21]</ref> 99.10 3D-CNN+LSTM+CE 99.26 kernels are used in the first layer, followed by 32 in the second layer. As depicted in <ref type="figure" target="#fig_1">Figure 3</ref>, the temporal dynamics are preserved. Three long short-term memory (LSTM) layers are adopted to capture temporal context from the speech representation. Time reduction is applied only on the second and third LSTM layers along with a projection operation. The time reduction is performed by concatenating the hidden states of the LSTM by a factor of 4. While it results in fewer time steps, the feature dimension increases by the same factor. The feature dimension is controlled with a projection layer. Time reduction (or 'time convolution <ref type="bibr" target="#b20">[19]</ref>) is used to reduce the sequence length of encoded activation, also meant to minimize the gap between the length of input and output sequences. Note that the softmax from the second LSTM layer is used to predict slots, whereas the prediction of intents is based on the softmax from the third LSTM layer. Moreover, because intents and slots likely carry out related information, the output of the second layer (i.e. the prediction of slots) is used as additional input to the third layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Two datasets are used in our experiments. To pretrain our models, we relied on the LibriSpeech corpus, which was introduced in <ref type="bibr" target="#b21">[20]</ref>. Librispeech contains about 1000 hours of speech sampled at 16 kHz. Only the clean speech containing 360 hours of speech was used. The second dataset adopted was the Fluent Speech Commands dataset, which comprises single-channel audio clips sampled at 16 kHz. The data was collected using crowdsourcing, with participants requested to cite random phrases for each intent twice. It contains about 19 hours of speech, providing a total of 30.043 utterances cited by 97 different speakers. The data is split in such a way that the training set contains 14.7 hours of data, totaling 23,132 utterances from 77 speakers. Validation and test sets comprise 1.9 and 2.4 hours of speech, leading to 3,118 utterances from 10 speakers and 3,793 utterances from other 10 speakers, respectively. The dataset comprises a total of 31 unique intent labels resulted in a combination of three slots per audio: action, object, and location. The latter can be either "none", "kitchen", "bedroom", "washroom", "English", "Chinese", "Korean", or "German". More details about the dataset can be found in <ref type="bibr" target="#b8">[9]</ref>. To simulate multi-intent scenarios, an additional version of the FSC dataset was generated. Namely FSC-M2, it is the result of concatenating two utterances from the same speaker into a single sentence. In this version, all the possible intent combinations were evenly distributed in the dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Features</head><p>In this work, audio signals are sampled at 16 kHz. As acoustic features, 80-dimensional log Mel-Filterbank features are adopted <ref type="bibr" target="#b23">[22]</ref>. To extract the Mel features, the audio signal is processed in frames of 320 samples (i.e., 20-ms window length), with a step size of 160 samples (that is, 10-ms hop-size). Global Cepstral Mean and Variance Normalization (CMVN) is applied, as a process that is commonly used with the aim to increase the robustness of ASR systems while mitigating the mismatch between training and testing data <ref type="bibr" target="#b24">[23]</ref>[24]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Settings</head><p>Our network was trained on mini-batches of 64 samples over a total of 200 epochs. In our experiments, we adopted learning rate of 0.0001 and dropout of 0.1. The optimizer used was Adam with weight decay of 0.2. We explored three strategies to optimize our model for the streaming scenario, including (1) using pure CTC or pure CTL as loss function; (2) using CTC or CTL jointly with CE. In the case of CTL, we also attempted to combine it with the multiple instance learning (MIL) technique <ref type="bibr" target="#b13">[14]</ref>; and finally (3) adding a pretrained ASR to our model, optimized with the first layer with the CTC loss on character prediction. Note that CE and the MIL were only used during training. For the CE, we applied softmax to the last timestep and average the CTC and CE losses with a fixed weight of 0.6 and 0.4, respectively. Because the MIL is based on frame-wise probabilities just as CTL, their combination is straight-forward and consists of a simple weighted average of the two losses. The main difference is that the MIL aggregates the frame-probabilities into recording-level probabilities with a linear softmax pooling function <ref type="bibr" target="#b13">[14]</ref>. Note that to ensure the streaming ability of our model, at the testing time only CTC or CTL are used. The third strategy consisted of pretraining an ASR model with the CTC loss. This procedure aimed at leveraging pre-trained embeddings by learning better representations from a large corpus. The model was trained to predict characters and only the first layer of our neural network was used. After training the ASR for 150 epochs, its weights were frozen and used with the entire recurrent neural network. In our experiments, we defined intent as the combination of action and object, which led to a total of 15 different intent labels. Location was defined as slot, which led to a total of 8 different slot labels. Our model was evaluated on 3 tasks: intent prediction; slot prediction; and in-tent+slot prediction. While intent predictions are attained using the outputs of the third layer, slot predictions rely on the output of the second layer, as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head><p>The performance of the proposed architecture is investigated in three different experiments. In the first one, we evaluate our model in a non-streaming scenario. The model is based on the pre-trained ASR and the architecture presented in Section3.</p><p>Only CE loss is used for optimizing our model. This experiment aims to compare our network with non-streamable e2e SLU solutions proposed in the literature. Results are reported in <ref type="table" target="#tab_0">Table 1</ref> and it shows our model achieving high accuracy in the three tasks and outperforming the other three baselines.</p><p>In the second experiment, we tackle a more challenging setting and our model is evaluated under the streaming regime. As described in <ref type="table" target="#tab_2">Table 2</ref>, the model is trained with a single label (i.e. intent, slot, or intent+slot) and tested on one (FSC-M1-Tst) and two labels (FSC-M2-Tst). Results show that using the CE loss during training is a key strategy to boost the performance of both CTC and CTL. The use of a pretrained ASR is also crucial for achieving high accuracy. Also, combining CTL with MIL yields better accuracy overall. Identifying intent and slot correctly at the same time is difficult and provides lower accuracy when compared to the single task (i.e. single intent or single slot). Detecting 2 intents have a detrimental impact on the performance of our model. To overcome this, in the last experiment, our model is trained on two labels and tested on one and two labels as well. Results are presented on <ref type="table" target="#tab_3">Table 3</ref>. We can observe that training on two labels benefits the performance of our model. Although CTC and CTL present comparable results, one has to consider that CTL has the potential for performing localization, whereas CTC is limited to predicting a sequence. Moreover, CTL allows for overlap events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we evaluated a compact spoken language understanding (SLU) model optimized with two alignment-free losses: the connectionist temporal classification (CTC) and the connectionist temporal localization (CTL). These losses allow the streaming capability on SLU, enabling the prediction of semantics based on incoming speech. In our first experiments, we showed that our model can achieve accuracy as high as 99.26 % on non-streaming settings while predicting intent and slot. For streaming scenarios, the proposed model can achieve accuracy of 98.97 % for CTC and 98.78 % for CTL on single label prediction. We also showed that our model is able to perform sequential labeling without compromising performance when it is presented with multiple utterances and targets during training and accuracy is as high as 95.69 % for CTC and 95.28 % for CTL on two label prediction. As future work, we plan to investigate the capability of our model to precisely locate and decode the semantic span within an utterance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>CTC alignment for X = [x1, x2, ..., xn], output Yasr = [ t,u,r,n, ,o,n, ,t,h,e, ,c,a,r] and Y slu = [activate, car].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Diagram depicting the proposed end-to-end unidirectional RNN for mapping a sequence of speech frames into a sequence of slots+intents without predefined alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on FSC for single-intent classification. Performance is reported in terms of accuracy (%) for 31 intent targets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on multi-intent classification. The training set consists of 1 label and the testing set contains utterances with 1 and 2 labels, referred to as FSC-M1-Tst and FSC-M2-Tst.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">FSC-M1-Tst</cell><cell></cell><cell cols="2">FSC-M2-Tst</cell></row><row><cell>Model</cell><cell>Intent</cell><cell>Slot</cell><cell>Intent+Slot</cell><cell>Intent</cell><cell>Slot</cell><cell>Intent+Slot</cell></row><row><cell>CTC</cell><cell cols="2">27.70 54.41</cell><cell>14.50</cell><cell cols="2">29.92 24.83</cell><cell>5.30</cell></row><row><cell>+ Joint CE</cell><cell cols="2">98.25 99.34</cell><cell>97.73</cell><cell cols="2">53.00 60.55</cell><cell>41.31</cell></row><row><cell cols="3">+ Pretrained ASR 99.15 99.76</cell><cell>98.97</cell><cell cols="2">56.04 64.21</cell><cell>44.30</cell></row><row><cell>CTL</cell><cell cols="2">12.76 55.54</cell><cell>3.60</cell><cell>0.14</cell><cell>25.05</cell><cell>0.05</cell></row><row><cell>+ Joint CE</cell><cell cols="2">89.05 95.35</cell><cell>85.68</cell><cell cols="2">48.37 80.14</cell><cell>41.28</cell></row><row><cell cols="3">+ Pretrained ASR 97.25 99.12</cell><cell>96.41</cell><cell cols="2">44.50 81.22</cell><cell>39.29</cell></row><row><cell>CTL + MIL</cell><cell cols="2">98.54 98.39</cell><cell>96.99</cell><cell cols="2">78.21 86.00</cell><cell>69.10</cell></row><row><cell cols="3">+ Pretrained ASR 99.18 99.57</cell><cell>98.78</cell><cell cols="2">66.17 96.77</cell><cell>64.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on multi-intent classification. The training set consists of 2 labels and the testing set contains utterances with 1 and 2 labels, referred to as FSC-M1-Tst and FSC-M2-Tst.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">FSC-M1-Tst</cell><cell></cell><cell cols="2">FSC-M2-Tst</cell></row><row><cell>Model</cell><cell>Intent</cell><cell>Slot</cell><cell>Intent+Slot</cell><cell>Intent</cell><cell>Slot</cell><cell>Intent+Slot</cell></row><row><cell>CTC</cell><cell>22.83</cell><cell>0.00</cell><cell>0.00</cell><cell cols="2">22.43 25.03</cell><cell>4.24</cell></row><row><cell>+ Joint CE</cell><cell cols="2">97.49 99.23</cell><cell>96.88</cell><cell cols="2">95.16 97.77</cell><cell>93.73</cell></row><row><cell cols="3">+ Pretrained ASR 98.54 99.39</cell><cell>98.15</cell><cell cols="2">96.45 98.47</cell><cell>95.69</cell></row><row><cell>CTL</cell><cell cols="2">11.83 55.54</cell><cell>2.66</cell><cell>1.17</cell><cell>26.58</cell><cell>0.20</cell></row><row><cell>+ Joint CE</cell><cell cols="2">86.79 94.93</cell><cell>83.15</cell><cell cols="2">76.45 88.43</cell><cell>69.31</cell></row><row><cell cols="3">+ Pretrained ASR 95.38 99.10</cell><cell>94.67</cell><cell cols="2">87.58 97.33</cell><cell>85.50</cell></row><row><cell>CTL + MIL</cell><cell cols="2">96.70 98.89</cell><cell>95.75</cell><cell cols="2">93.14 96.83</cell><cell>90.30</cell></row><row><cell cols="3">+ Pretrained ASR 98.65 99.68</cell><cell>98.36</cell><cell cols="2">95.78 99.50</cell><cell>95.28</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A low latency asr-free end to end spoken language understanding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mhiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Myer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04884</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Streaming end-to-end speech recognition for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6381" to="6385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spoken language understanding of human-machine conversations for language learning applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ubale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Signal Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Incremental online spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10287</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5754" to="5758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Voice interfaces in everyday life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porcheron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharples</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 2018 CHI conference on human factors in computing systems</title>
		<meeting>the 2018 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Joint online spoken language understanding and language modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01462</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From audio to semantics: Approaches to end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="720" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Speech model pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spoken language understanding without speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bangalore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6189" to="6193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An empirical exploration of ctc acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2623" to="2627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep learning for nlp and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Whitaker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">84</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Connectionist temporal localization for sound event detection with sequential labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="745" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A first attempt at polyphonic sound event detection using connectionist temporal classification</title>
	</analytic>
	<monogr>
		<title level="m">2017 ieee international conference on acoustics, speech and signal processing</title>
		<imprint/>
	</monogr>
	<note>icassp</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2986" to="2990" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning spectro-temporal features with 3d cnns for speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Englebienne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Evers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="383" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3-d cnn models for far-field multichannel speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="5499" to="5503" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Speech emotion recognition from 3d log-mel spectrograms with deep learning network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="125" to="868" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="193" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving end-to-end speech-to-intent classification with reptile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Gorinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01994</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Espnet: End-to-end speech processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00015</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On real-time mean-andvariance normalization of speech recognition features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Macho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nadeu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE international conference on acoustics speech and signal processing proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="I" to="I" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The pytorch-kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6465" to="6469" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
