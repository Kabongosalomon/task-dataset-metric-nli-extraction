<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OccamNets: Mitigating Dataset Bias by Favoring Simpler Hypotheses</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
							<email>ckanan@cs.rochester.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rochester Institute of Technology</orgName>
								<address>
									<settlement>Rochester</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<settlement>Rochester</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>995x</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<settlement>San Jose</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OccamNets: Mitigating Dataset Bias by Favoring Simpler Hypotheses</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dataset bias and spurious correlations can significantly impair generalization in deep neural networks. Many prior efforts have addressed this problem using either alternative loss functions or sampling strategies that focus on rare patterns. We propose a new direction: modifying the network architecture to impose inductive biases that make the network robust to dataset bias. Specifically, we propose OccamNets, which are biased to favor simpler solutions by design. OccamNets have two inductive biases. First, they are biased to use as little network depth as needed for an individual example. Second, they are biased toward using fewer image locations for prediction. While OccamNets are biased toward simpler hypotheses, they can learn more complex hypotheses if necessary. In experiments, OccamNets outperform or rival state-of-theart methods run on architectures that do not incorporate these inductive biases. Furthermore, we demonstrate that when the state-of-the-art debiasing methods are combined with OccamNets 4 results further improve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Dataset Bias and Bias Mitigation. When trained with empirical risk minimization (ERM), deep networks have been shown to exploit dataset biases in the training data resulting in poor test generalization <ref type="bibr" target="#b23">[23,</ref>45,<ref type="bibr" target="#b30">62,</ref><ref type="bibr" target="#b38">70]</ref>. Existing works for mitigating this problem have focused on these approaches: 1) focusing on rare data patterns through re-sampling <ref type="bibr" target="#b7">[8,</ref>40], 2) loss re-weighting <ref type="bibr" target="#b14">[15,</ref>57], 3) adversarial debiasing <ref type="bibr" target="#b20">[20,</ref>34], 4) model ensembling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>, 5) minority/counterfactual sample generation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref>35]  and 6) invariant/robust risk minimization <ref type="bibr" target="#b4">[5,</ref>36,56]. Most of these methods require bias variables, e.g., sub-groups within a category, to be annotated <ref type="bibr" target="#b20">[20,</ref>34,40,57,<ref type="bibr" target="#b30">62]</ref>. Some recent methods have also attempted to detect and mitigate biases without these variables by training separate biasamplified models for de-biasing the main model <ref type="bibr" target="#b12">[13,</ref>47,<ref type="bibr" target="#b26">58,</ref><ref type="bibr" target="#b39">71]</ref>. This paper is the first to explore architectural inductive biases for combating dataset bias.</p><p>Early Exit Networks. OccamNet is a multi-exit architecture designed to encourage later layers to focus on samples that earlier layers find difficult. Multi-exit networks have been studied in past work to speed up average inference time by minimizing the amount of compute needed for individual examples <ref type="bibr" target="#b9">[10,</ref>31,<ref type="bibr" target="#b34">66,</ref>74], but their impact on bias-resilience has not been studied. In [59], a unified framework for studying early exit mechanisms was proposed, which included commonly used training paradigms [26,38,<ref type="bibr" target="#b32">64,</ref>73]  and biological plausibility [46,49,50]. During inference, multi-exit networks choose the earliest exit based on either a learned criterion <ref type="bibr" target="#b9">[10]</ref> or through a heuristic, e.g., exit if the confidence score is sufficiently high <ref type="bibr" target="#b19">[19]</ref>, exit if there is low entropy [66], or exit if there is agreement among multiple exits [79]. Recently,<ref type="bibr" target="#b19">[19]</ref> proposed early exit networks for long-tailed datasets; however, they used a class-balanced loss and did not study robustness to hidden covariates, whereas, OccamNets generalize to these hidden variables without oracle bias labels during training.</p><p>Exit Modules and Spatial Maps. OccamNets are biased toward using fewer spatial locations for prediction, which we enable by using spatial activation maps <ref type="bibr" target="#b24">[24,</ref>44,54]. While most recent convolutional neural networks (CNNs) use global average pooling followed by a linear classification layer <ref type="bibr" target="#b25">[25,</ref>29,32], alternative pooling methods have been proposed, including spatial attention <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">21,</ref>30,75]   and dynamic pooling [30,33,37]. However, these methods have not been explored for their ability to combat bias mitigation, with existing bias mitigation methods adopting conventional architectures that use global average pooling instead. For</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Frustra fit per plura quod potest fieri per pauciora</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>William of Occam, Summa Totius Logicae (1323 CE)</head><p>Spurious correlations and dataset bias greatly impair generalization in deep neural networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b30">62]</ref>. This problem has been heavily studied. The most common approaches are re-sampling strategies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr">57]</ref>, altering optimization to mitigate bias [55], adversarial unlearning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr">53,</ref><ref type="bibr">77]</ref>, learning invariant representations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">67]</ref>, and ensembling with bias-amplified models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">47]</ref>. Here, we propose a new approach: incorporating architectural inductive biases that combat dataset bias. <ref type="figure">Fig. 1</ref>: OccamNets focus on architectural inductive biases, which is an orthogonal direction to tackling dataset biases compared to the existing works.</p><p>In a typical feedforward network, each layer can be considered as computing a function of the previous layer, with each additional layer making the hypothesis more complex. Given a system trained to predict multiple categories, with some being highly biased, this means the network uses the same level of complexity across all of the examples, even when some examples should be classified with simpler hypotheses (e.g., less depth). Likewise, pooling in networks is typically uniform in nature, so every location is used for prediction, rather than only the minimum amount of information. In other words, typical networks violate Occam's razor. Consider the Biased MNIST dataset <ref type="bibr" target="#b30">[62]</ref>, where the task is to recognize a digit while remaining invariant to multiple spuriously correlated factors, which include colors, textures, and contextual biases. The most complex hypothesis would exploit every factor during classification, including the digit's color, texture, or background context. A simple hypothesis would instead be to focus on the digit's shape and to ignore these spuriously correlated factors that work very well during training but do not generalize. We argue that a network should be capable of adapting its hypothesis space for each example, rather than always resorting to the most complex hypothesis, which would help it to ignore extraneous variables that hinder generalization.</p><p>Here, we propose convolutional OccamNets which have architectural inductive biases that favor using the minimal amount of network depth and the minimal number of image locations during inference for a given example. The first inductive bias is implemented using early exiting, which has been previously studied for speeding up inference. The network is trained such that later layers focus on examples earlier layers find hard, with a bias toward exiting early. The second inductive bias replaces global average pooling before a classification layer with a function that is regularized to favor pooling with fewer image locations from class activation maps (CAMs). We hypothesize this would be especially useful for combating background and contextual biases <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">63]</ref>. OccamNets are complementary to existing approaches and can be combined with them.</p><p>In this paper, we demonstrate that architectural inductive biases are effective at mitigating dataset bias. Our specific contributions are:</p><p>-We introduce the OccamNet architecture, which has architectural inductive biases for favoring simpler solutions to help overcome dataset biases. Oc-camNets do not require the biases to be explicitly specified during training, unlike many state-of-the-art debiasing algorithms. -In experiments using biased vision datasets, we demonstrate that Occam-Nets greatly outperform architectures that do not use the proposed inductive biases. Moreover, we show that OccamNets outperform or rival existing debiasing methods that use conventional network architectures. -We combine OccamNets with four recent debiasing methods, which all show improved results compared to using them with conventional architectures.</p><p>OccamNets, each exit produces a class activation map, which is biased toward using fewer visual locations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OccamNets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">OccamNet Architecture for Image Classification</head><p>OccamNets have two inductive biases: a) they prefer exiting as early as possible, and b) they prefer using fewer visual regions for predictions. Following Occam's principles, we implement these inductive biases using simple, intuitive ideas: early exit is based on whether or not a sample is correctly predicted during training and visual constraint is based on suppressing regions that have low confidence towards the ground truth class. We implement these ideas in a CNN. Recent CNN architectures, such as ResNets <ref type="bibr" target="#b25">[25]</ref> and DenseNets [32], consist of multiple blocks of convolutional layers. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, these inductive biases are enabled by attaching an exit module E j to block B j of the CNN, as the blocks serve as natural endpoints for attaching them. Below, we describe how we implement these two inductive biases in OccamNets.</p><p>In an OccamNet, each exit module E j takes in feature maps produced by the backbone network and processes them with F j , which consists of two convolutional layers, producing feature maps used by the following components:</p><p>Suppressed CAM Predictors (C j ). Each C j consists of a single convolutional layer, taking in the feature maps from F j to yield class activation maps, c. The maps provide location wise predictions of classes. Following Occam's principles, the usage of visual regions in these CAMs is suppressed through the CAM suppression loss L CS described in Sec. 3.2.</p><p>Output Predictors (O j ). The output predictor applies global average pooling on the suppressed CAMs predicted by C j to obtain the output prediction vector,? j ? R n Y , where n Y is the total number of classes. The entire network is trained with the output prediction loss L O , which is a weighted sum of cross entropy losses between the ground truth y and the predictions? j from each of the exits. Specifically, the weighting scheme is formulated to encourage the deeper layers to focus on the samples that the shallower layers find difficult. The detailed training procedure is described in Sec. 3.3.</p><p>Exit Decision Gates (G j ). During inference, OccamNet needs to decide whether or not to terminate the execution at E j on a per-sample basis. For this, each E j consists of an exit decision gate, G j that yields an exit decision score g j , which is interpreted as the probability that the sample can exit from E j . G j is realized via a ReLU layer followed by a sigmoid layer, taking in representations from F j . The gates are trained via exit decision gate loss, L G which is based on whether or not O j made correct predictions. The loss and the training procedure are elaborated further in Sec. 3.4.</p><p>The total loss used to train OccamNets is given by:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training the Suppressed CAMs</head><p>To constrain the usage of visual regions, OccamNets regularize the CAMs so that only some of the cells exhibit confidence towards the ground truth class, whereas rest of the cells exhibit inconfidence i.e., have uniform prediction scores for all the classes. Specifically, let c y ? R h?w be the CAM where each cell encodes the score for the ground truth class. Then, we apply regularization on the locations that obtain softmax scores lower than the average softmax score for the ground truth class. That is, let c y be the softmax score averaged over all the cells in c y , then the cells at location l, c l ? R n Y are regularized if the softmax score for the ground truth class, c l y is less than c y . The CAM suppression loss is:</p><formula xml:id="formula_0">L CS = hw l=1 1(c l y &lt; c y ) KLD(c l , 1 n Y 1),<label>(1)</label></formula><p>where, KLD(c l , 1 n Y 1) is the KL-divergence loss with respect to a uniform class distribution and 1(c l y &lt; c y ) ensures that the loss is applied only if the ground truth class scores lower than c y . The loss weight for L CS is ? CS , which is set to 0.1 for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training the Output Predictors</head><p>The prediction vectors? obtained by performing global average pooling on the suppressed CAMs c are used to compute the output prediction losses. Specifically, we train a bias-amplified first exit E 0 , using a loss weight of: W 0 = p ?0 0 , where, p 0 is the softmax score for the ground truth class. Here, ? 0 &gt; 0 encourages E 0 to amplify biases i.e., it provides higher loss weights for the samples that already have high scores for the ground truth class. This encourages E 0 to focus on the samples that it already finds easy to classify correctly. For all the experiments, we set ? 0 = 3 to sufficiently amplify the biases. The subsequent exits are then encouraged to focus on samples that the preceding exits find difficult. For this, the loss weights are defined as:</p><formula xml:id="formula_1">W j = (1 ? g j?1 + ?), if j &gt; 0,<label>(2)</label></formula><p>where, g j?1 is the exit decision score predicted by (j ? 1) th exit decision gate and ? = 0.1 is a small offset to ensure that all the samples receive a minimal, non-zero loss weight. For the samples where g j?1 is low, the weight loss for j th exit, W j becomes high. The total output loss is then:</p><formula xml:id="formula_2">L O = n E ?1 j=0 W j CE(? j , y),<label>(3)</label></formula><p>where, CE(? j , y) is the cross-entropy loss and n E is the total number of exits.</p><p>Note that E j 's are 0-indexed and the first bias-amplified exit E 0 is not used during inference. Furthermore, during training, we prevent the gradients of E 0 from passing through B 0 (.) to avoid degrading the representations available for the deeper blocks and exits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training the Exit Decision Gates</head><p>Each exit decision gate G j (.) yields an exit probability score? j = G j (.). During inference, samples with? j ? 0.5 exit from E j and samples with? j &lt; 0.5 continue to the next block B j+1 , if available. During training, all the samples use the entire network depth and g j is used to weigh losses as described in Sec. 3.3. Now, we specify the exit decision gate loss used to train G j :</p><formula xml:id="formula_3">L G = k?{0,1} 1(g j = k)BCE(g j ,? j ) 1(g j = k) ,<label>(4)</label></formula><p>where g j is the ground truth value for the j th gate, which is set to 1 if the predicted class y ? is the same as the ground truth class y and 0 otherwise. That is, G j is trained to exit if the sample is correctly predicted at depth j, else it is trained to continue onto the next block. Furthermore, the denominator:</p><p>1(g j = k) balances out the contributions from the samples with g = 1 and g = 0 to avoid biasing one decision over the other. With this setup, sufficiently parameterized models that obtain 100% training accuracy will result in a trivial solution where g j is always set to 1 i.e., the exit will learn that all the samples can exit. To avoid this issue, we stop computing g j once E j 's mean-per-class training accuracy reaches a predefined threshold ? acc,j . During training, we stop the gradients from G from passing through F j (.) and B(.), since this improved the training stability and overall accuracy in the preliminary experiments. The loss weight ? G is set to 1 in all the experiments. Biased MNIST <ref type="bibr" target="#b30">[62]</ref>. As shown in <ref type="figure" target="#fig_1">Fig. 3a</ref>, Biased MNIST requires classifying MNIST digits while remaining robust to multiple sources of biases, including color, texture, scale, and contextual biases. This is more challenging than the widely used Colored MNIST dataset <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">34,</ref><ref type="bibr">40]</ref>, where the only source of bias is the spuriously correlated color. In our work, we build on the version created in <ref type="bibr" target="#b30">[62]</ref>. We use 160 ? 160 images with 5 ? 5 grids of cells, where the target digit is placed in one of the grid cells and is spuriously correlated with: a) digit size/scale (number of cells a digit occupies), b) digit color, c) type of background texture, d) background texture color, e) co-occurring letters, and f) colors of the co-occurring letters. Following <ref type="bibr" target="#b30">[62]</ref>, we denote the probability with which each digit co-occurs with its biased property in the training set by p bias . For instance, if p bias = 0.95, then 95% of the digit 1s are red, 95% of digit 1s co-occur with letter 'a' (not necessarily colored red) and so on. We set p bias to 0.95 for all the experiments. The validation and test sets are unbiased. Biased MNIST has 10 classes and 50K train, 10K validation, and 10K test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>COCO-on-Places <ref type="bibr" target="#b2">[3]</ref>. As shown in <ref type="figure" target="#fig_1">Fig. 3b</ref>, COCO-on-Places puts COCO objects [41] on spuriously correlated Places backgrounds <ref type="bibr">[78]</ref>. For instance, buses mostly appear in front of balloons and birds in front of trees. The dataset provides three different test sets: a) biased backgrounds (in-distribution), which reflects the object-background correlations present in the train set, b) unseen backgrounds (non-systematic shift), where the objects are placed on backgrounds that are absent from the train set and c) seen, but unbiased backgrounds (systematic shift) where the objects are placed on backgrounds that were not spuriously correlated with the objects in the train set. Results in <ref type="bibr" target="#b2">[3]</ref> show it is difficult to maintain high accuracy on both the in-distribution and shifted-distribution test sets. Apart from that, COCO-on-Places also includes an anomaly detection task, where anomalous samples from unseen object class need to be distinguished from the in-distribution samples. COCO-on-Places has 9 classes with 7200 train, 900 validation, and 900 test images.</p><p>Biased Action Recognition (BAR) <ref type="bibr">[47]</ref>. BAR reflects real world challenges where bias attributes are not explicitly labeled for debiasing algorithms, with the test set containing additional correlations not seen during training. The dataset consists of correlated action-background pairs, where the train set consists of selected action-background pairs, e.g., climbing on a rock, whereas the evaluation set consists of differently correlated action-background pairs, e.g., climbing on snowy slopes (see <ref type="figure" target="#fig_1">Fig. 3c</ref>). The background is not labeled for the debiasing algorithms, making it a challenging benchmark. BAR has 6 classes with 1941 train and 654 test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison Methods</head><p>We compare OccamNets with four state-of-the-art bias mitigation methods, apart from the vanilla empirical risk minimization procedure:</p><p>-Empirical Risk Minimization (ERM) is the default method used by most deep learning models and it often leads to dataset bias exploitation since it minimizes the train loss without any debiasing procedure. -Spectral Decoupling (SD) [51] applies regularization to model outputs to help decouple features. This can help the model focus more on the signal. -Group Upweighting (Up Wt) balances the loss contributions from the majority and the minority groups by multiplying the loss by 1 n ? g , where n g is the number of samples in group g and ? is a hyper-parameter.</p><formula xml:id="formula_4">-Group DRO (gDRO) [55]</formula><p>is an instance of a broader family of distributionally robust optimization techniques <ref type="bibr" target="#b18">[18,</ref><ref type="bibr">48,</ref><ref type="bibr">52]</ref>, that optimizes for the difficult groups in the dataset. -Predictive Group Invariance (PGI) <ref type="bibr" target="#b2">[3]</ref> is another grouping method, that encourages matched predictive distributions across easy and hard groups within each class. It penalizes the KL-divergence between predictive distributions from within-class groups.</p><p>Dataset Sub-groups. For debiasing, Up Wt, gDRO, and PGI require additional labels for covariates (sub-group labels). Past work has focused on these labels being supplied by an oracle; however, having access to all relevant sub-group labels is often impractical for large datasets. Some recent efforts have attempted to infer these sub-groups. Just train twice (JTT) [42] uses a bias-prone ERM model by training for a few epochs to identify the difficult groups. Environment inference for invariant learning (EIIL) <ref type="bibr" target="#b13">[14]</ref> learns sub-group assignments that maximize the invariant risk minimization objective <ref type="bibr" target="#b4">[5]</ref>. Unfortunately, inferred sub-groups perform worse in general than when they are supplied by an oracle <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">42]</ref>. For the methods that require them, which excludes OccamNets, we use oracle group labels (i.e., for Biased MNIST and COCO-on-Places). Inferred group labels are used for BAR, as oracle labels are not available.</p><p>For Biased MNIST, all the samples having the same class and the same value for all of the spurious factors are placed in a single group. For COCO-on-Places, objects placed on spuriously correlated backgrounds form the majority group, while the rest form the minority group. BAR does not specify oracle group labels, so we adopt the JTT method. Specifically, we train an ERM model for single epoch, reserving 20% of the samples with the highest losses as the difficult group and the rest as the easy group. We chose JTT over EIL for its simplicity. OccamNets, of course do not require such group labels to be specified.</p><p>Architectures. ResNet-18 is used as the standard baseline architecture for our studies. We compare it with an OccamNet version of ResNet-18, i.e., OccamResNet-18. To create this architecture, we add early exit modules to each of ResNet-18's convolutional blocks. To keep the number of parameters in OccamResNet-18 comparable to ResNet-18, we reduce the feature map width from 64 to 48. Assuming 1000 output classes, ResNet-18 has 12M parameters compared to 8M in OccamResNet-18. Further details are provided in Sec. A.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Metrics and Model Selection</head><p>We report the means and standard deviations of test set accuracies computed across five different runs for all the datasets. For Biased MNIST, we report the unbiased test set accuracy (i.e., p bias = 0.1) alongside the majority and minority group accuracies for each bias variable. For COCO-on-Places, unless otherwise specified, we report accuracy on the most challenging test split: with seen, but unbiased backgrounds. We also report the average precision score to measure the ability to distinguish 100 anomalous samples from the in-distribution samples for the anomaly detection task of COCO-on-Places. For BAR, we report the overall test accuracies. We use unbiased validation set of Biased MNIST and validation set with unbiased backgrounds for COCO-on-Places for hyperparameter tuning. The hyperparameter search grid and selected values are specified in Sec. A.8. <ref type="table">Table 1</ref>: Unbiased test set accuracies comparing OccamResNet to the more conventional ResNet architectures without early exits and constrained class activation maps. We format the first, second and third best results.  <ref type="table" target="#tab_1">Table 2</ref>, where we provide unbiased accuracy along with any improvement or impairment of performance when OccamResNet-18 is used instead of ResNet-18. All methods benefit from using the OccamResNet architecture compared to ResNet-18, with gains of 10.6% -28.2% for Biased MNIST, 0.9% -7.8% for COCO-on-Places, and 1.0% -14.2% for BAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of the Proposed Inductive Biases</head><p>In this section, we analyze the impacts of each of the proposed modifications in OccamNets and their success in achieving the desired behavior.</p><p>Analysis of Early Exits. OccamResNet has four exits, the first exit is used for bias amplification and the rest are used to potentially exit early during inference. To analyze the usage of the earlier exits, we plot the percentage of  <ref type="figure" target="#fig_3">Fig. 5</ref>. For ResNets that were run with ERM and PGI, we show Grad-CAM visualizations <ref type="bibr" target="#b28">[60]</ref>, whereas for OccamResNets, we directly visualize the CAM heatmaps obtained from the earliest exit used for each sample. As shown in the figure, OccamResNet generally prefers smaller regions that include the target object. On the other hand, comparison methods tend to focus on larger visual regions that include irrelevant object/background cues leading to lower accuracies.</p><p>Ablations. To study the importance of the introduced inductive biases, we perform ablations on Biased MNIST and COCO-on-Places. First, to examine if We hypothesize that since the earlier exits suffice for a large number of samples in Biased MNIST (as indicated in <ref type="figure">Fig. 4a</ref>), the later exits may not receive sufficient training signal with the weighted output prediction losses. Finally, we ran experiments without the CAM suppression loss by setting ? CS to 0. This caused large accuracy drops of 16.8% on Biased MNIST and 6.3% on COCOon-Places. These experiments demonstrate that both inductive biases are vital for OccamNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Robustness to Different Types of Shifts</head><p>A robust system must handle different types of bias shifts. To test this ability, we examine the robustness to each bias variable in Biased MNIST and we also compare the methods on the differently shifted test splits of COCO-on-Places.</p><p>In <ref type="table">Table 4</ref>, we compare how ResNet and OccamResNet are affected by the different bias variables in Biased MNIST. For this, we present majority and minority group accuracies for each variable. Bias variables with large differences between the majority and the minority groups, i.e., large majority/minority group discrepancy (MMD) are the most challenging spurious factors. Occam-ResNets, with and without PGI, improve on both majority and minority group accuracies across all the bias variables. OccamResNets are especially good at ignoring the distracting letters and their colors, obtaining MMD values between 0-1%. ResNets, on the other hand are susceptible to those spurious factors, obtaining MMD values between 7.9-20.7%. Among all the variables, digit scale and digit color are the most challenging ones and OccamResNets mitigate their exploitation to some extent.</p><p>Next, we show accuracies for base method and PGI on both ResNet-18 and OccamResNet-18 on all of the test splits of COCO-on-Places in <ref type="table" target="#tab_4">Table 5</ref>. The different test splits have different kinds of object-background combinations, and ideally the method should work well on all three test splits. PGI run on ResNet-18 improves on the split with seen, but non-spurious backgrounds but incurs a large accuracy drop of 7.4% on the in-distribution test set, with biased back- <ref type="table">Table 4</ref>: Accuracies on majority (maj)/minority (min) groups for each bias variable in Biased MNIST (p bias = 0.95). We embolden the results with the lowest differences between the groups.  grounds. On the other hand, OccamResNet-18 shows only 0.9% drop on the biased backgrounds, while showing 2.6% accuracy gains on the split with unseen backgrounds and 7.8% accuracy gains on the split with seen, but non-spurious backgrounds. It further obtains 2.2% gains on the average precision metric for the anomaly detection task. PGI run on OccamResNet exhibits a lower drop of 2.1% on the in-distribution split as compared to the PGI run on ResNet, while obtaining larger gains on rest of the splits. These results exhibit that OccamNets obtain high in-distribution and shifted-distribution accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture+Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation on Other Architectures</head><p>To examine if the proposed inductive biases improve bias-resilience in other architectures too, we created </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Do OccamNets Work on Less Biased Datasets?</head><p>To examine if OccamNets also work well on datasets with less bias, we train ResNet-18 and OccamResNet-18 on 100 classes of the ImageNet dataset <ref type="bibr" target="#b16">[16]</ref>. OccamResNet-18 obtains competitive numbers compared to the standard ResNet-18 (OccamResNet-18: 92.1, vs. ResNet-18: 92.6, top-5 accuracies). However, as described in rest of this paper, OccamResNet-18 achieves this with improved resistance to bias (e.g., if the test distributions were to change in future). Additionally, it also reduces the computations with 47.6% of the samples exiting from E 1 and 13.3% of samples exiting from E 2 . As such, OccamNets have the potential to be the de facto network choice for visual recognition tasks regardless of the degree of bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Relation to Mixed Capacity Models. Recent studies have shown that sufficiently simple models, e.g., models with a limited number of parameters <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">27]</ref>   <ref type="bibr" target="#b36">68]</ref> and use extra information at test time to choose the best hypothesis. The techniques include training a set of models with dissimilar input gradients <ref type="bibr" target="#b36">[68]</ref> and training multiple prediction heads that disagree on a target distribution <ref type="bibr">[39]</ref>. An interesting extension of OccamNets could be making diverse predictions through the multiple exits and through CAMs that focus on different visual regions. This could help avoid discarding complex features in favor of simpler ones <ref type="bibr" target="#b29">[61]</ref>. This may also help with under-specified tasks, where there are equally viable ways of making the predictions <ref type="bibr">[39]</ref>.</p><p>Other Architectures and Tasks. We tested OccamNets implemented with CNNs; however, they may be beneficial to other architectures as well. The ability to exit dynamically could be used with transformers, graph neural networks, and feed-forward networks more generally. There is some evidence already for this on natural language inference tasks, where early exits improved robustness in a transformer architecture <ref type="bibr">[79]</ref>. While the spatial bias is more vision specific, it could be readily integrated into recent non-CNN approaches for image classification <ref type="bibr" target="#b17">[17,</ref><ref type="bibr">43,</ref><ref type="bibr" target="#b37">69</ref>,76].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Conclusion</head><p>In summary, the proposed OccamNets have architectural inductive biases favoring simpler solutions. The experiments show improvements over state-of-the-art bias mitigation techniques. Furthermore, existing methods tend to do better with OccamNets as compared to the standard architectures. <ref type="bibr">26</ref>. Hettinger, C., Christensen, T., Ehlert, B., Humpherys, J., Jarvis, T., Wade, S.:</p><p>Forward thinking: Building and training neural networks one layer at a time. arXiv preprint arXiv:1706.02480 (2017) 27. Hooker, S., Moorosi, N., Clark, G., Bengio, S., Denton, E.: Characterising bias in compressed models. arXiv preprint arXiv:2010.03058 (2020) 28. Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., <ref type="table">Tan</ref>    <ref type="table">Table A7</ref>: Accuracy on the three splits of COCO-on-Places, alongside average precision for the anomaly detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>In-Distribution Now, as shown in <ref type="table" target="#tab_11">Table A8</ref>, methods run on OccamResNet show gains in terms of the overall test set accuracies over the methods run on ResNet. The perclass standard deviations are larger (1.8-16.2%) as compared to the standard deviations for the overall test set accuracies (0.7-2.4%). That is, across the five different experiments run with different random seeds, the same methods run on the same architectures end up favoring different classes. We hypothesize that despite starting off from the same initial conditions i.e., the same pre-trained parameters, the randomness in the mini-batches drive the models to favor certain classes over the others. Tuning the optimizer e.g., switching to SGD, lowering the learning rates or increasing the weight decay can potentially help mitigate the unstable behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Early Exit Statistics</head><p>To examine the efficiency and robustness of each exit for all of the datasets, we present the exit %, accuracy on the exited samples and overall exit-wise accuracies on all the samples for OccamResNet-18 in <ref type="table" target="#tab_12">Table A9</ref>  the overall accuracy is 42.4%, indicating need for improvement in terms of training the earlier exit gates. We believe that tuning the training thresholds more comprehensively can potentially close this gap. Finally, for BAR, more than half i.e., 55.9% of the samples exit from E 3 . The accuracies on the samples exited from E 1 and E 2 : 55.0% and 65.3% are higher than the overall accuracies computed on all the samples i.e., 47.4% and 52.3% respectively. This again shows the ability to exit early whenever appropriate and the ability to utilize the full network depth only for the remaining samples.   Exit Statistics on differently shifted distributions.</p><p>In general, we find that earlier exits are triggered more often for in-distribution (easier) test samples as compared to shifted distribution (more difficult) test samples. As shown in <ref type="table" target="#tab_13">Table A10</ref>, for BiasedMNIST, when p bias is increased from 0.5 (easy) to 0.95 (hard), exit% of the earliest exit: E 1 drops from 67.3% to 53.6%. Similarly, as shown in <ref type="table" target="#tab_14">Table A11</ref>, for COCO-on-Places, E 1 's exit% on in-distribution (easy) split is 54.6%, whereas it is 39.4% on unbiased backgrounds (hard) split. However, for the test split with unseen backgrounds, E 1 's exit%: 13.1% is lower than the exit% of 39.4% obtained for the test split with unbiased backgrounds, despite the latter being more difficult. OccamNet failed to trigger earlier exits even though E 1 was accurate for 51.7% of the samples E 2 was comparable with E 3 in terms of overall accuracy. We hypothesize that the earlier exits failed to trigger since they had not been trained to exit when the backgrounds are out-of-distribution. Thus, one area for improvement is to enable the ability to exit confidently in spite of the presence of previously unseen factors. Note that this analysis was performed on a single run, so may have small differences with the multi-run averages presented elsewhere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Using comparable # of parameters</head><p>In the main paper, we compared OccamResNet-18 with 8M parameters (feature width = 48) and ResNet-18 with 12M parameters (feature width = 64). To examine if the lower number number of parameters is helping e.g., due to implicit regularization, we test an OccamResNet-18 with 12M parameters by setting the feature width to 58. As shown in <ref type="table" target="#tab_1">Table A12</ref>, OccamResNet-18 with 12M parameters shows small improvements over OccamResNet-18 with 8M parameters in all the datasets. A more thorough analysis of model sizes and their impacts on accuracy is an interesting study and we leave this to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Sample Complexity</head><p>It is desirable to have models that generalize despite being trained with a limited number of samples i.e., with reduced sample complexity. This is especially true for biased datasets, where reducing the train set size can amplify biases <ref type="bibr">[72]</ref>. To study the ability to generalize when only a subset of the training data is available,  To gauge the robustness of models, it is important to examine their behaviors across varying levels of biases. For this, we present the unbiased accuracies obtained by training separate models on training sets with p bias ? {0.75, 0.9, 0.95, 0.99}. As shown in <ref type="figure">fig. A7</ref>, all of the methods obtain similar accuracies at p bias = 0.75, where bias is not severe. OccamResNet+PGI outperform rest of the methods at p bias = {0.9, 0.95, 0.99}. The gap between OccamRes-Net+PGI and other methods are especially drastic for p bias = 0.99, indicating that when OccamResNet is trained to have similar prediction distributions across groups, it is capable of tackling highly biased training distributions too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Evaluation on Other Architectures</head><p>Apart from ResNet, we also tested the proposed inductive biases on EfficientNet and MobileNet. The results are presented in <ref type="table" target="#tab_2">Table A13</ref>. For both Biased MNIST and COCO-on-Places, Occam variants outperform the standard architectures, showing the efficacy of the proposed modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 OccamNet Implementation Details</head><p>In OccamNet, each exit module E j takes in feature maps produced by the corresponding block B j of the backbone network. E j consists of two 3?3 convolutional layers (F j ) for the initial pre-processing of the feature maps. F j consists of convolutional layers with the number of channels set to: max( dj 4 , d min ), where d j is the number of channels in the feature maps produced by B j , and d min is set to 32 for OccamResNet and OccamMobileNet and 16 for OccamEfficientNet. Feature maps from F j are fed into the CAM predictor C j and the exit gate G j . C j is a 1 ? 1 convolutional layer with the number of output channels set to the number of classes, n Y . G j consists of a 16-dimensional hidden ReLU layer followed by a sigmoid layer that predicts the exit probability.</p><p>Exit Details. For convenience, we specify the exit locations with reference to PyTorch 1.7.1 implementations of the architectures. For ResNet, the residual layers that yield the same number of output channels are grouped together and we refer to each of those groups as a 'block'. ResNet-18 consists of 4 blocks and we attach an exit to each of the blocks. For OccamResNet-18, with feature width of 58, the exit-wise input dimensions are: E 0 : 58, E 1 : 116, E 2 : 232 and E 3 : 464. Similarly, EfficientNet-B2 consists of 9 blocks and we attach the exits to the 3 rd , 5 th , 7 th and 9 th blocks. We decrease the width multiplier of 1.1 in the standard architecture to 0.88 in OccamEfficientNet-B2 to create a model with comparable number of parameters of 9M for both. The input dimensions of the corresponding exits are: E 0 : 24, E 1 : 72, E 2 : 168 and E 3 : 1120. Finally, MobileNetv3-large consists of 17 blocks, and the exits are attached to the 2 nd , 7 th , 13 th and the 17 th blocks. We decrease the width multiplier from a value of 1 in MobileNet-v3-large to 0.95 in OccamMobileNet-v3-large, so that both models have 5.5M parameters. The input dimensions of the corresponding exits are: E 0 : 16, E 1 : 40, E 2 : 104, E 3 : 912.</p><p>Modifications for COCO-on-Places. For COCO-on-Places, the images are small (64 ? 64), so for ResNet-18 and OccamResNet-18, we replace the first convolutional layer (kernel size=7, padding=3, stride=2), with a smaller layer (kernel size=3, padding=1 and stride=1) and also remove the initial max pooling layer. For the standard and Occam variants of EfficientNet-B2 and MobileNet-v3, we scale up the image size to 224 ? 224, which improved the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Costs and Training Durations</head><p>OccamResNet18 incurs additional multiply-accumulate (Mac) operations, requiring 2.11 GMacs compared to 1.82 GMacs required by ResNet18. While OccamResNet is slower than ERM on ResNet, it is faster than PGI run on ResNet. Average training durations per epoch for ERM on ResNet, PGI on ResNet and OccamResNet are: 10, A.10 Augmentations Biased MNIST. We do not perform any augmentation. COCO-on-Places. Following <ref type="bibr" target="#b2">[3]</ref>, we apply random cropping by padding the original images by 8 pixels on all the sides (reflection padding) and taking 64 ? 64 random crops. We also apply random horizontal flips.</p><p>BAR. We apply random resized crops using a scale range of 0.7 to 1.0 and selecting aspect ratios between 1.0 to <ref type="bibr">4 3</ref> . We also apply random horizontal flips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.11 Model Calibration</head><p>In <ref type="figure" target="#fig_4">Fig. A8</ref> and A9 we show the reliability diagrams for ERM model (leftmost column) and for each exit (E1 ? E3) for OccamResNet for COCO-on-Places and Biased MNIST respectively. In terms of model calibration, OccamNet reduces the expected calibration error (ECE) to some extent, yet there is a large room for improvement, which is an interesting direction to pursue.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>OccamNets are multi-exit architectures capable of exiting early through the exit decision gates. The exits yield class activation maps that are trained to use a constrained set of visual regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>For each dataset, the first two columns show bias-aligned (majority) samples, and the last column shows bias-conflicting (minority) samples. For BAR, the train set does not contain any bias-conflicting samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V., et al.: Searching for mobilenetv3. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1314-1324 (2019) 29. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 (2017) 30. Hu, J., Shen, L., Albanie, S., Sun, G., Vedaldi, A.: Gather-excite: Exploiting feature context in convolutional neural networks. Advances in neural information processing systems 31 (2018) 31. Hu, T.K.,Chen, T., Wang, H., Wang, Z.: Triple wins: Boosting accuracy, robustness and efficiency together by enabling input-adaptive inference. arXiv preprint arXiv:2002.10025 (2020) 32. Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4700-4708 (2017) 33. Jaderberg, M., Simonyan, K., Zisserman, A., et al.: Spatial transformer networks. Advances in neural information processing systems 28 (2015) 34. Kim, B., Kim, H., Kim, K., Kim, S., Kim, J.: Learning not to learn: Training deep neural networks with biased data. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. pp. 9012-9020. Computer Vision Foundation / IEEE (2019). https://doi.org/10.1109/CVPR.2019.00922 35. Kim, E., Lee, J., Choo, J.: Biaswap: Removing dataset bias with bias-tailored swapping augmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14992-15001 (2021) 36. Krueger, D., Caballero, E., Jacobsen, J.H., Zhang, A., Binas, J., Zhang, D., Le Priol, R., Courville, A.: Out-of-distribution generalization via risk extrapolation (rex). In: International Conference on Machine Learning. pp. 5815-5826. PMLR (2021) 37. Lee, C.Y., Gallagher, P.W., Tu, Z.: Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree. In: Artificial intelligence and statistics. pp. 464-472. PMLR (2016) 38. Lee, C.Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z.: Deeply-supervised nets. In: Artificial intelligence and statistics. pp. 562-570. PMLR (2015) 39. Lee, Y., Yao, H., Finn, C.: Diversify and disambiguate: Learning from underspecified data. arXiv preprint arXiv:2202.03418 (2022) 40. Li, Y., Vasconcelos, N.: REPAIR: removing representation bias by dataset resampling. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. pp. 9572-9581. Computer Vision Foundation / IEEE (2019). https://doi.org/10.1109/CVPR.2019.00980 41. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll?r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference on computer vision. pp. 740-755. Springer (2014) 42. Liu, E.Z., Haghgoo, B., Chen, A.S., Raghunathan, A., Koh, P.W., Sagawa, S., Liang, P., Finn, C.: Just train twice: Improving group robustness without training group information. In: International Conference on Machine Learning. pp. 6781-6792. PMLR (2021) 43. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10012-10022 (2021) 44. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3431-3440 (2015) 45. Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., Galstyan, A.: A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR) 54(6), 1-35 (2021) 46. Mostafa, H., Ramesh, V., Cauwenberghs, G.: Deep supervised learning using local errors. Frontiers in neuroscience p. 608 (2018) 47. Nam, J., Cha, H., Ahn, S., Lee, J., Shin, J.: Learning from failure: Training debiased classifier from biased classifier. In: Advances in Neural Information Processing Systems (2020) 48. Namkoong, H., Duchi, J.C.: Stochastic gradient methods for distributionally robust optimization with f-divergences. In: Lee, D.D., Sugiyama, M., von Luxburg, U., Guyon, I., Garnett, R. (eds.) Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain. pp. 2208-2216 (2016) 49. N?kland, A.: Direct feedback alignment provides learning in deep neural networks. Advances in neural information processing systems 29 (2016) 50. N?kland, A., Eidnes, L.H.: Training neural networks with local error signals. In: International conference on machine learning. pp. 4839-4850. PMLR (2019) 51. Pezeshki, M., Kaba, S.O., Bengio, Y., Courville, A., Precup, D., Lajoie, G.: Gradient starvation: A learning proclivity in neural networks. arXiv preprint arXiv:2011.09468 (2020) 52. Rahimian, H., Mehrotra, S.: Distributionally robust optimization: A review. arXiv preprint arXiv:1908.05659 (2019) 53. Ramakrishnan, S., Agrawal, A., Lee, S.: Overcoming language priors in visual question answering with adversarial regularization. In: Bengio, S., Wallach, H.M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (eds.) Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr?al, Canada. pp. 1548-1558 (2018) 54. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. pp. 234-241. Springer (2015) 55. Sagawa, S., Koh, P.W., Hashimoto, T.B., Liang, P.: Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. CoRR abs/1911.08731 (2019), http://arxiv.org/abs/1911.08731 56. Sagawa, S., Koh, P.W., Hashimoto, T.B., Liang, P.: Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731 (2019) 57. Sagawa, S., Raghunathan, A., Koh, P.W., Liang, P.: An investigation of why overparameterization exacerbates spurious correlations. In: Proceedings of the 37th 72. Utama, P.A., Moosavi, N.S., Gurevych, I.: Towards debiasing NLU models from unknown biases. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 7597-7610. Association for Computational Linguistics, Online (2020). https://doi.org/10.18653/v1/2020.emnlpmain.613, https://www.aclweb.org/anthology/2020.emnlp-main.613 73. Venkataramani, S., Raghunathan, A., Liu, J., Shoaib, M.: Scalable-effort classifiers for energy-efficient machine learning. In: Proceedings of the 52nd Annual Design Automation Conference. pp. 1-6 (2015) 74. Wo lczyk, M., W?jcik, B., Ba lazy, K., Podolak, I., Tabor, J.,?mieja, M., Trzcinski, T.: Zero time waste: Recycling predictions in early exit neural networks. Advances in Neural Information Processing Systems 34 (2021) 75. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. In: International conference on machine learning. pp. 2048-2057. PMLR (2015) 76. Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., Yan, S.: Metaformer is actually what you need for vision. arXiv preprint arXiv:2111.11418 (2021) 77. Zhang, B.H., Lemoine, B., Mitchell, M.: Mitigating unwanted biases with adversarial learning. In: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. pp. 335-340 (2018) 78. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence 40(6), 1452-1464 (2017) 79. Zhou, W., Xu, C., Ge, T., McAuley, J., Xu, K., Wei, F.: Bert loses patience: Fast and robust inference with early exit. Advances in Neural Information Processing Systems 33, 18330-18341 (2020)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>A. 5</head><label>5</label><figDesc>Robustness to Varying Levels of Bias in Biased MNIST Fig. A7: Unbiased accuracies at varying bias levels (p bias ) in Biased MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. A8 :</head><label>A8</label><figDesc>Reliability diagrams for the classifier trained with ERM (leftmost column) versus exit gate calibrations for E1 ? E3 (right hand columns) on COCOon-Places (unbiased backgrounds test split).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. A9 :</head><label>A9</label><figDesc>Reliability diagrams for the classifier trained with ERM (leftmost column) versus exit gate calibrations for E1 ? E3 (right hand columns) on Biased MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Overall Results OccamNets vs. ERM and Recent Bias Mitigation Methods. To examine how OccamNets fare against ERM and state-of-the-art bias mitigation methods, we run the comparison methods on ResNet and compare the results with OccamResNet. Results are given in Table 1. OccamResNet outperforms stateof-the-art methods on Biased MNIST and COCO-on-Places and rivals PGI on BAR, demonstrating that architectural inductive biases alone can help mitigate dataset bias. The gap between OccamResNet and other methods is large on Biased MNIST (16.4 -46.0% absolute difference). For COCO-on-Places, PGI rivals OccamResNet, and clearly outperforms all other methods, in terms of accuracy on the test split with seen, but unbiased backgrounds. OccamResNet's results are impressive considering that Up Wt, gDRO, and PGI all had access to the bias group variables, unlike OccamNet, ERM, and SD. Combining OccamNets with Recent Bias Mitigation Methods. Because OccamNets are a new network architecture, we used OccamResNet-18 with each of the baseline methods instead of ResNet-18. These results are shown in</figDesc><table><row><cell cols="3">Architecture+Method Biased MNIST COCO-on-Places</cell><cell>BAR</cell></row><row><cell></cell><cell cols="2">Results on Standard ResNet-18</cell><cell></cell></row><row><cell>ResNet+ERM</cell><cell>36.8 ? 0.7</cell><cell>35.6 ? 1.0</cell><cell>51.3 ?1.9</cell></row><row><cell>ResNet+SD [51]</cell><cell>37.1 ? 1.0</cell><cell>35.4 ? 0.5</cell><cell>51.3 ?2.3</cell></row><row><cell>ResNet+Up Wt</cell><cell>37.7 ? 1.6</cell><cell>35.2 ? 0.4</cell><cell>51.1 ?1.9</cell></row><row><cell>ResNet+gDRO [56]</cell><cell>19.2 ? 0.9</cell><cell>35.3 ?0.1</cell><cell>38.7 ?2.2</cell></row><row><cell>ResNet+PGI [3]</cell><cell>48.6 ? 0.7</cell><cell>42.7 ? 0.6</cell><cell>53.6 ?0.9</cell></row><row><cell></cell><cell cols="2">Results on OccamResNet-18</cell><cell></cell></row><row><cell>OccamResNet</cell><cell>65.0 ?1.0</cell><cell>43.4 ? 1.0</cell><cell>52.6 ?1.9</cell></row><row><cell cols="2">5 Results and Analysis</cell><cell></cell><cell></cell></row><row><cell>5.1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Unbiased accuracies alongside improvement/ impairment when the comparison methods are run on OccamResNet instead of ResNet. Percentage of samples exited (Exit%) from each exit (barring E 0 ).samples that exited from each exit inFig. 4. For Biased MNIST dataset, a large portion of the samples, i.e., 59.8% exit from the shallowest exit of E 1 and only 13.3% exit from the final exit E 3 . For COCO-on-Places and BAR, 50.4% and 44.1% samples exit before E 3 , with 49.6% and 55.9% samples using the full depth respectively. These results show that the OccamNets favor exiting early, but that they do use the full network depth if necessary.Fig. 5: Original image, and Grad-CAM visualizations for ERM and PGI on ResNet, and CAM visualizations on OccamResNet. The visualizations are for the ground truth. CAM Visualizations. To compare the localization capabilities of Occam-ResNets to ResNets, we present CAM visualizations in</figDesc><table><row><cell>Architecture+Method</cell><cell cols="3">Biased MNIST COCO-on-Places</cell><cell>BAR</cell></row><row><cell>OccamResNet</cell><cell>65.0 (+28.2)</cell><cell>43.4 (+7.8)</cell><cell></cell><cell>52.6 (+1.3)</cell></row><row><cell>OccamResNet+SD [51]</cell><cell>55.2 (+18.1)</cell><cell>39.4 (+4.0)</cell><cell></cell><cell>52.3 (+1.0)</cell></row><row><cell>OccamResNet+Up Wt</cell><cell>65.7 (+28.0)</cell><cell>42.9 (+7.7)</cell><cell></cell><cell>52.2 (+1.1)</cell></row><row><cell>OccamResNet+gDRO [56]</cell><cell>29.8 (+10.6)</cell><cell>40.7 (+5.4)</cell><cell></cell><cell>52.9 (+14.2)</cell></row><row><cell>OccamResNet+PGI [3]</cell><cell>69.6 (+21.0)</cell><cell>43.6 (+0.9)</cell><cell></cell><cell>55.9 (+2.3)</cell></row><row><cell>(a) Biased MNIST</cell><cell cols="2">(b) COCO-on-Places</cell><cell cols="2">(c) BAR</cell></row><row><cell>Fig. 4:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation Studies on OccamResNet the multi-exit setup is helpful, we train networks with single exit attached to the end of the network. This caused accuracy drops of 29.1% on Biased MNIST and 8.4% on COCO-on-Places, indicating that the multi-exit setup is critical. To examine if the weighted output prediction losses are helpful or harmful, we set all the loss weights (W</figDesc><table><row><cell>Ablation Description</cell><cell cols="2">Biased MNIST COCO-on-Places</cell></row><row><cell>Using only one exit at the end</cell><cell>35.9</cell><cell>35.0</cell></row><row><cell>Weighing all the samples equally i.e., Wj = 1</cell><cell>66.3</cell><cell>40.8</cell></row><row><cell>Without the CAM suppression loss i.e., ?CS = 0</cell><cell>48.2</cell><cell>37.1</cell></row><row><cell>Full OccamResNet</cell><cell>65.0</cell><cell>43.4</cell></row></table><note>j ) to 1. This resulted in an accuracy drop of 2.6% on COCO-on-Places. However, it improved accuracy on Biased MNIST by 1.3%.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Architecture+Method</cell><cell>Biased Backgrounds</cell><cell>Unseen Backgrounds</cell><cell>Seen, but Non-Spurious Backgrounds</cell><cell>Anomaly Detection</cell></row><row><cell></cell><cell cols="3">Results on Standard ResNet-18</cell><cell></cell></row><row><cell>ResNet+ERM</cell><cell>84.9 ? 0.5</cell><cell>53.2 ?0.7</cell><cell>35.6 ?1.0</cell><cell>20.1 ?1.5</cell></row><row><cell>ResNet+PGI [3]</cell><cell>77.5 ?0.6</cell><cell>52.8 ?0.7</cell><cell>42.7 ?0.6</cell><cell>20.6 ?2.1</cell></row><row><cell></cell><cell cols="2">Results on OccamResNet-18</cell><cell></cell><cell></cell></row><row><cell>OccamResNet</cell><cell>84.0 ?1.0</cell><cell>55.8 ?1.2</cell><cell>43.4 ?1.0</cell><cell>22.3 ?2.8</cell></row><row><cell>OccamResNet+PGI [3]</cell><cell>82.8 ?0.6</cell><cell>55.3 ?1.3</cell><cell>43.6 ? 0.6</cell><cell>21.6 ?1.6</cell></row></table><note>Accuracies on all three test splits of COCO-on-Places, alongside mean average precision for the anomaly detection task.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>OccamEfficientNet-B2 and OccamMobileNet-v3 by modifying EfficientNet-B2 [65] and MobileNet-v3 [28,29]. OccamNet variants outperform standard architectures on both Biased MNIST (OccamEfficientNet-B2: 59.2 vs. EfficientNet-B2: 34.4 and OccamMobileNet-v3: 49.9 vs. MobileNet-v3: 40.4) and COCO-on-Places (OccamEfficientNet-B2: 39.2 vs. EfficientNet-B2: 34.2 and OccamMobileNet-v3: 40.1 vs. MobileNet-v3: 34.9). The gains show that the proposed modifications improve robustness in other commonly used architectures too. We provide the detailed results in Sec. A.6.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>InTable A6, we present the unbiased test accuracies and majority/minority group accuracies for each bias variable. Methods run on Occam ResNet-18 lower the majority/minority discrepancy (MMD) compared to the methods run on ResNet-18 for all of the variables, indicating that OccamNets lower the tendencies to latch onto all of the spurious factors. OccamResNet-18 is especially robust to letter, letter color and texture color biases as shown by the low MMD values of 0 ? 2.8% compared to the larger MMD values obtained by ResNet-18.</figDesc><table><row><cell>A Appendix</cell></row><row><cell>A.1 Detailed Results on Each Dataset</cell></row><row><cell>Biased MNIST.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A6 :</head><label>A6</label><figDesc>Accuracies on majority (maj)/minority (min) groups for each bias variable in Biased MNIST (p bias = 0.95).</figDesc><table><row><cell>Architecture+Method</cell><cell>Digit Scale</cell><cell>Digit Color</cell><cell>Texture</cell><cell>Texture Color</cell><cell>Letter</cell><cell>Letter Color</cell></row><row><cell>Test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Acc.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>maj/min maj/min maj/min maj/min maj/min maj/min</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>. For Biased MNIST, the earliest exits E 1 and E 2 have high exit percentages of 59.8% and 26.9% respectively, alongside high accuracies on the exited samples: 68.1% and 64.8% respectively. These results show that OccamResNet has learned to identify and trigger earlier exits whenever appropriate. For COCO-on-Places, we observe large accuracies of 50.8% and 50.2% on the 13.9% and 49.6% samples exited from E 2 and E 3 respectively. The large percentage of samples exiting from E 3 shows that OccamResNet is capable of using the full network depth whenever needed. The accuracy on the samples that exited from E 1 is however low: 31.3%, even though</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A8 :</head><label>A8</label><figDesc>Overall and per-class accuracies on BAR</figDesc><table><row><cell>Methods</cell><cell>Overall Climbing Diving Fishing</cell><cell>Pole Vaulting</cell><cell>Racing Throwing</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A9 :</head><label>A9</label><figDesc>Percentage samples exited: (Exit %), accuracy (Acc.) on exited samples and accuracy on all the samples for each exit (E j ).</figDesc><table><row><cell></cell><cell></cell><cell>Biased MNIST</cell><cell cols="2">COCO-on-Places</cell><cell>BAR</cell></row><row><cell></cell><cell>E0</cell><cell cols="2">E1 E2 E3 E0</cell><cell>E1 E2 E3 E0</cell><cell>E1 E2 E3</cell></row><row><cell>Exit%</cell><cell cols="4">0.0 59.8 26.9 13.3 0.0 36.5 13.9 49.6 0.0 23.7 20.3 55.9</cell></row><row><cell cols="5">Acc. (exited) N/A 68.1 64.8 52.1 N/A 31.3 50.8 50.2 N/A 55.0 65.3 46.6</cell></row><row><cell>Acc. (all)</cell><cell cols="4">12.7 65.1 65.5 65.5 10.0 42.4 43.4 41.4 26.5 47.4 52.3 52.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table A10 :</head><label>A10</label><figDesc>Exit comparison on models trained with different levels of biases on Biased MNIST. We present the percentage samples exited: (Exit %) and accuracy on all the samples for each exit (E j ).</figDesc><table><row><cell></cell><cell></cell><cell>p bias = 0.5</cell><cell>p bias = 0.95</cell></row><row><cell></cell><cell>E0</cell><cell>E1 E2 E3 E0</cell><cell>E1 E2 E3</cell></row><row><cell>Exit%</cell><cell cols="3">0.0 67.3 23.8 8.9 0.0 53.6 34.2 12.2</cell></row><row><cell cols="4">Acc. (all) N/A 98.2 98.1 98.1 N/A 63.3 63.3 62.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A11 :</head><label>A11</label><figDesc>Exit comparison on different test splits of COCO-on-Places. We present the percentage samples exited: (Exit %), accuracy (Acc.) on exited samples and accuracy on all the samples for each exit (E j ).</figDesc><table><row><cell></cell><cell></cell><cell>In-Distribution</cell><cell cols="2">Unseen Backgrounds Unbiased Backgrounds</cell></row><row><cell></cell><cell>E0</cell><cell cols="2">E1 E2 E3 E0</cell><cell>E1 E2 E3 E0</cell><cell>E1 E2 E3</cell></row><row><cell>Exit%</cell><cell cols="4">0.0 54.6 12.4 33.0 0.0 13.1 9.7 77.2 0.0 39.4 14.5 46.1</cell></row><row><cell cols="5">Acc. (exited) N/A 94.7 86.5 66.9 N/A 58.9 67.8 52.3 N/A 27.5 55.4 49.2</cell></row><row><cell>Acc. (all)</cell><cell cols="4">67.2 81.1 84.0 85.3 20.9 51.7 55.1 55.4 11.2 40.1 41.6 39.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table A12 :</head><label>A12</label><figDesc>We train on OccamResNet-18-width-58 with 12M parameters (feature width set to<ref type="bibr" target="#b26">58)</ref> to make the number of parameters comparable to ResNet-18 (12M parameters, feature width=48).Fig. A6: Unbiased accuracies obtained when trained with the indicated percent of training data. we train ResNet and OccamResNet on 1%, 5%, 10%, 25%, 50%, 100% of Biased MNIST's train set. As shown in Fig. A6, OccamResNet (without PGI) trained on only 25% of the data outperforms ResNet+ERM and ResNet+PGI trained on 100% of the data showing increased sample complexity. When trained on only 10% of the training set, OccamResNet+PGI outperforms rest of the methods by large margins of 11.5 ? 16.2% showing that OccamResNet with group labels show the greatest efficacy in the low-shot data regime. When only 1% of the training data is available, all the methods obtain chance-level accuracies (i.e., near 10%) indicating lack of enough sufficient training samples for classification. For the rest, methods run on OccamResNet-18 outperform the methods run on ResNet-18, showing improved sample complexity.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table A13 :</head><label>A13</label><figDesc>Unbiased test set accuracies comparing the standard and Occam variants of ResNet-18, EfficientNet-B2 and MobileNetv3 architectures, run without additional debiasing procedures.</figDesc><table><row><cell>Architecture</cell><cell>Number of Parameters</cell><cell cols="2">Biased MNIST COCO-on-Places</cell></row><row><cell>ResNet-18</cell><cell>12M</cell><cell>36.8</cell><cell>35.6</cell></row><row><cell>OccamResNet-18</cell><cell>12M</cell><cell>65.9</cell><cell>43.8</cell></row><row><cell>EfficientNet-B2</cell><cell>9M</cell><cell>34.4</cell><cell>34.2</cell></row><row><cell>OccamEfficientNet-B2</cell><cell>9M</cell><cell>59.2</cell><cell>39.2</cell></row><row><cell>MobileNet-v3</cell><cell>5.5M</cell><cell>40.4</cell><cell>34.9</cell></row><row><cell>OccamMobileNet-v3</cell><cell>5.5M</cell><cell>49.9</cell><cell>40.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table A14 :</head><label>A14</label><figDesc>Hyperparameters and other settings used for each method on all of the datasets.We find that gDRO on Biased MNIST and ResNet+gDRO on BAR obtain accuracies lower than ResNet+ERM. To alleviate this issue, we tried to tune the hyperparameters by lowering the learning rates to {10 ?4 , 10 ?5 } and increasing the weight decays to {0.1, 0.01, 10 ?3 } as suggested in [56], yet gDRO obtained low accuracies. We believe the challenge stems from the large number of dataset groups in Biased MNIST and the small training set size of BAR. While optimizing gDRO on such conditions still remains a challenge, gDRO run on Occam-ResNet showed accuracy gains of 10.6% on Biased MNIST and 14.2% on BAR over gDRO run on ResNet, indicating that OccamNets also offer better training process.</figDesc><table><row><cell>Datasets/ Methods</cell><cell>Setting</cell><cell>Biased MNIST</cell><cell>COCO on Places</cell><cell>BAR</cell></row><row><cell cols="2">Common to all the methods Optimizer</cell><cell>Adam</cell><cell>SGD</cell><cell>Adam</cell></row><row><cell></cell><cell>Learning Rate (LR)</cell><cell>10 ?3</cell><cell>0.1</cell><cell>5e-4</cell></row><row><cell></cell><cell cols="3">LR Decay Milestones [50,70] [100,120,140]</cell><cell>-</cell></row><row><cell></cell><cell>LR Decay Gamma</cell><cell>0.1</cell><cell>0.1</cell><cell>-</cell></row><row><cell></cell><cell>Weight Decay</cell><cell>5 ? 10 ?4</cell><cell cols="2">5 ? 10 ?4 5 ? 10 ?4</cell></row><row><cell></cell><cell>Momentum</cell><cell>-</cell><cell>0.9</cell><cell>-</cell></row><row><cell></cell><cell>Batch Size</cell><cell>128</cell><cell>64</cell><cell>128</cell></row><row><cell></cell><cell>Epochs</cell><cell>90</cell><cell>150</cell><cell>150</cell></row><row><cell>Spectral Decoupling (SD) on ResNet-18 [51]</cell><cell>Output Decay (?)</cell><cell>? = 0.1</cell><cell>? min. = 10 ?3 ? maj. = 0.1</cell><cell>? = 0.1</cell></row><row><cell>Spectral Decoupling (SD) on OccamResNet-18 [51]</cell><cell>Output Decay (?)</cell><cell cols="2">? = 10 ?3 ? min. = 10 ?3 ? maj. = 0.1</cell><cell>? = 10 ?3</cell></row><row><cell>Up Wt</cell><cell>Exponentiation Factor (?)</cell><cell>2</cell><cell>1</cell><cell>1</cell></row><row><cell>Group DRO (gDRO) on ResNet-18 [56]</cell><cell>Step size</cell><cell>10 ?3</cell><cell>10 ?3</cell><cell>0.01</cell></row><row><cell></cell><cell>Exponentiation Factor (?)</cell><cell>0.5</cell><cell>1</cell><cell>0.01</cell></row><row><cell>Group DRO (gDRO) on OccamResNet-18 [56]</cell><cell>Step size</cell><cell>10 ?3</cell><cell>10 ?4</cell><cell>0.01</cell></row><row><cell></cell><cell>Exponentiation Factor (?)</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>Predictive Group Invariance (PGI) on ResNet-18 [3]</cell><cell>Invariance Loss Weight</cell><cell>100</cell><cell>50</cell><cell>10</cell></row><row><cell>Predictive Group Invariance (PGI) on OccamResNet-18 [3]</cell><cell>Invariance Loss Weight</cell><cell>50</cell><cell>1</cell><cell>50</cell></row><row><cell>OccamNets</cell><cell>Threshold for E 0 (? acc,0 )</cell><cell>0.5</cell><cell>0.5</cell><cell>0.1</cell></row><row><cell></cell><cell>CAM Suppression Loss Weight (? CS )</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell cols="3">A.9 Issues Training with GroupDRO (gDRO)</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/erobic/occam-nets-v1 arXiv:2204.02426v4 [cs.LG] 12 Jul 2022</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported in part by NSF awards #1909696 and #2047556. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies or endorsements of any sponsor.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We tested OccamNets implemented with CNNs; however, they may be beneficial to other architectures as well. The ability to exit dynamically could be used with transformers, graph neural networks, and feed-forward networks more generally.</p><p>There is some evidence already for this on natural language inference tasks, where early exits improved robustness in a transformer architecture <ref type="bibr">[79]</ref>. It would be interesting to evaluate multiple existing early exit mechanisms <ref type="bibr" target="#b27">[59]</ref> for their abilities to discard spurious correlations. Furthermore, adapting the early exit ideas to non-classification tasks e.g., regression may require small changes e.g., exiting based on continuous error, which can be explored in future works.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bias-resilient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pfefferbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pohl</surname></persName>
		</author>
		<idno>ArXiv abs/1910.03676</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t just assume; look and answer: Overcoming priors for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00522</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00522" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="4971" to="4980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Systematic generalisation with group invariant predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Seijen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00636</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00636" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Invariant risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<editor>Lee, D.D., Sugiyama, M., von Luxburg, U., Guyon, I., Garnett, R.</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="4349" to="4357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">RUBi: Reducing Unimodal Biases for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cad?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="839" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Counterfactual samples synthesizing for robust visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10800" to="10809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to stop while learning to predict</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An empirical study of invariant risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Uncertainty and Robustness in Deep Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Don&apos;t take the easy way out: Ensemble based methods for avoiding known dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1418</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1418" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4069" to="4082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to model and ignore dataset bias with mixed capacity ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.272</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.findings-emnlp.272" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3031" to="3045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Environment inference for invariant learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2189" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/CVPR.2019.00949</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00949" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<title level="m">Distributionally robust losses against mixture covariate shifts. Under review</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Elf: An earlyexiting framework for long-tailed classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhamnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11979</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial regularization for visual question answering: Strengths, shortcomings, and side effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-1801</idno>
		<ptr target="https://doi.org/10.18653/v1/W19-1801" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Shortcomings in Vision and Language</title>
		<meeting>the Second Workshop on Shortcomings in Vision and Language<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07624</idno>
		<title level="m">Attention mechanisms in computer vision: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adasyn: Adaptive synthetic sampling approach for imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international joint conference on neural networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1322" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8346" to="8356" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.01300</idno>
		<title level="m">Learning from others&apos; mistakes: Avoiding dataset biases without modeling them</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Why should we add early exits to neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scarpiniti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baccarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="954" to="966" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradcam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The pitfalls of simplicity bias in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tamuly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An investigation of critical issues in bias mitigation techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1943" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Don&apos;t judge an object by its context: Learning to overcome contextual bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01108</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.01108" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11067" to="11075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Branchynet: Fast inference via early exiting from deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teerapittayanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcdanel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2464" to="2469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unshuffling data for improved generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11894</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Evading the simplicity bias: Training a diverse set of models discovers solutions with superior ood generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05612</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2011.5995347</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2011.5995347" />
	</analytic>
	<monogr>
		<title level="m">The 24th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards debiasing NLU models from unknown biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Utama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.613</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.613" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 7597-7610. Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 7597-7610. Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnet+sd</surname></persName>
		</author>
		<idno>51] 37.1 83.4/32.0 76.9/32.7 76.7/32.7 42.3/36.6 48.3/35.8 48.9/35.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Occamresnet+gdro</surname></persName>
		</author>
		<idno>56] 29.8 72.8/25.0 69.5/25.3 45.8/28.0 39.4/28.8 29.7/29.8 36.1/29.1</idno>
		<imprint/>
	</monogr>
	<note>OccamResNet+PGI [3] 69.6 95.4/66.7 97.0/66.5 88.6/67.4 71.4/69.4 69.6/69.6 70.5/69.5</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">As discussed in 5.1, methods run on OccamResNet-18 show improvements over the methods run on ResNet-18 on the shifted test splits and the anomaly detection task. Furthermore, while PGI run on ResNet-18 shows a large drop of 7.4% on the in-distribution test split, methods (barring gDRO) run on OccamResNet-18 show smaller drops of 0.1 ? 2.1%, indicating robustness to distributions consisting of the same or different biases as compared to the train distribution. BAR. First of all, BAR consists of only 1941 samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coco-On-Places</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Table A7, we present the accuracies on each of the test splits of COCO-on-Places, alongside the average precision for the anomaly detection task</title>
		<imprint/>
	</monogr>
	<note>ResNet-18 and OccamResNet-18 on 100 classes of ImageNet (obtaining 92.6% and 92.1% top-5 accuracies respectively) before training on BAR. Without the pre-trained weights, BAR obtains 15-20% lower test set accuracies for both ResNet and OccamResNet as compared to the results with pre-trained weights. Results on ResNet-18</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Occamresnet+gdro</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">14 secs for COCO-on-Places and 40, 67, 60 secs for BiasedMNIST on a Titan RTX</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The search range for the invariance penalty loss, i.e., the KLD loss between different groups from the same class is: {1, 10, 50, 100, 500, 1000}. OccamNets. For OccamNets, we recommend tuning the accuracy threshold of the first exit: (? acc,0 ) on a validation set, but fixing rest of the hyperparameters, based on the following observations: -Bias Amplification Factor (? 0 ) and Weight offset (?): We tuned ? 0 and ? on COCO-on-Places, but fixed the values for rest of the datasets. We observe that ? 0 ? 3 ensures sufficient bias amplification, so recommend using ? 0 = 3. Furthermore, ? = 0.1 ensures non-zero losses in all the datasets, so we recommend using this default value. -Accuracy Thresholds (? acc ): We use arithmetic progression for the meanper-class accuracy thresholds ? acc , with the difference ?? acc,j , set to 0.1, i.e., the threshold is increased by 0.1 every subsequent exit. We search for the initial training threshold ? acc,0 ? {0.1, 0.3, 0.5}. BMNIST and COCO were relatively insensitive to ? acc,0 , with absolute differences of only: 1-2% in accuracy and 1-4% in exit%. For BAR and ImageNet, we decreased ? acc</title>
	</analytic>
	<monogr>
		<title level="m">Hyperparameters and Other Settings In Table A14, we present the details about optimizers, training epochs and other hyperparameters for each method on each dataset. The hyperparameter search grids for OccamResNet-18 and all of the comparison methods are shown below. For each dataset, we tune ResNet-18 and OccamResNet-18 separately. Spectral Decoupling (SD)</title>
		<imprint/>
	</monogr>
	<note>Group adjustment hyperparameter, i.e., 1, 2, 3}. Group DRO (gDRO). Again, we search for the group adjustment hyperparameter, i.e., ? ? {0.5, 1, 2, 3}. Group weight step size, which is used to control the group-wise loss weights is selected by searching from these values: {0.1, 0.01, 10 ?3 , 10 ?4 }. Predictive Group Invariance (PGI). 0 to 0.1 since higher values increase exit% of E 1 , which decreases the overall accuracy. So, we recommend tuning ? acc,0 . -Normalization term: The balancing/normalization in equation 4 can be generalized as ( 1 searched for ? in {0.5, 1.0}. With ? = 1, only</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
