<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Novel Plug-in Module for Fine-Grained Visual Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yung</forename><surname>Chou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">National Taiwan Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Cheng-Hung</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">National Taiwan Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Wen-Chung</forename><surname>Kao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">National Taiwan Normal University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Novel Plug-in Module for Fine-Grained Visual Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolutional Neural Network</term>
					<term>Vision Trans- former</term>
					<term>Fine-grained Visual Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual classification can be divided into coarsegrained and fine-grained classification. Coarse-grained classification represents categories with a large degree of dissimilarity, such as the classification of cats and dogs, while fine-grained classification represents classifications with a large degree of similarity, such as cat species, bird species, and the makes or models of vehicles. Unlike coarse grained visual classification, fine-grained visual classification often requires professional experts to label data, which makes data more expensive. To meet this challenge, many approaches propose to automatically find the most discriminative regions and use local features to provide more precise features. These approaches only require image-level annotations, thereby reducing the cost of annotation. However, most of these methods require two-or multi-stage architectures and cannot be trained end-to-end. Therefore, we propose a novel plug-in module that can be integrated to many common backbones, including CNN-based or Transformer-based networks to provide strongly discriminative regions. The plugin module can output pixel-level feature maps and fuse filtered features to enhance fine-grained visual classification. Experimental results show that the proposed plugin module outperforms state-ofthe-art approaches and significantly improves the accuracy to 92.77% and 92.83% on CUB200-2011 and NABirds, respectively. We have released our source code in Github https://github.com/ chou141253/FGVC-PIM.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Visual classification can be divided into coarse-grained and fine-grained classification. Coarse-grained classification represents the classification of categories with a large degree of dissimilarity, such as the classification of cats and dogs, while fine-grained classification represents classifications with a large degree of similarity, such as bird species <ref type="bibr" target="#b0">[1]</ref> [2], dog species <ref type="bibr" target="#b2">[3]</ref>, and the makes or models of vehicles <ref type="bibr" target="#b3">[4]</ref>. "Finegrained" refers to more fine-grained divisions under common species classification. The challenges of fine-grained visual classification task is threefold. First, there is a lot of variation in the same category. Taking birds as an example, photos of the same bird from different angles can vary greatly in color and shape, as shown in <ref type="figure" target="#fig_0">Fig.1</ref>. The second point is that objects of different subcategories may be very similar. As shown in <ref type="figure" target="#fig_0">Fig.1</ref>, the textures of the three birds are very similar. The third point is that unlike coarse grained classification, finegrained classification often requires professional experts to label data, which makes data more expensive. For the above Corresponding author: Cheng-Hung Lin (email: brucelin@ntnu.edu.tw). reasons, frameworks that perform very well on coarse-grained classification tasks such as ResNet <ref type="bibr" target="#b4">[5]</ref>, EfficientNet <ref type="bibr" target="#b5">[6]</ref>, Vision Transformer (ViT) <ref type="bibr" target="#b6">[7]</ref> will be very limited for fine-grained classification.</p><p>To find strong discriminative regions to enhance the finegrained visual classification, the proposed approaches can be classified into three categories. The first type of method finds regions through Region Proposal Network (RPN) <ref type="bibr" target="#b7">[8]</ref>, such as NTS-Net <ref type="bibr" target="#b8">[9]</ref>, FDL <ref type="bibr" target="#b9">[10]</ref>, and StackedLSTM <ref type="bibr" target="#b10">[11]</ref>. The second type of method strengthens the feature map through the attention mechanism, such as CAL <ref type="bibr" target="#b11">[12]</ref>, MA-CNN <ref type="bibr" target="#b12">[13]</ref>, MAMC <ref type="bibr" target="#b13">[14]</ref>, API-Net <ref type="bibr" target="#b14">[15]</ref>, and WS-DAN <ref type="bibr" target="#b15">[16]</ref>. The third type of method uses the strength of the attention-map in the selfattention mechanism as the judgment of discriminative regions, such as TransFG <ref type="bibr" target="#b16">[17]</ref> and FFVT <ref type="bibr" target="#b17">[18]</ref>. The first two approaches are mainly based on convolutional neural networks (CNN), such as ResNet <ref type="bibr" target="#b4">[5]</ref>, DenseNet <ref type="bibr" target="#b18">[19]</ref>, and EfficientNet <ref type="bibr" target="#b5">[6]</ref>, while the third is based on ViT to implement fine-grained visual classification.</p><p>After finding these regions, the above approaches re-input the original image and feature maps to the network by cropping and resizing, or use an attention mechanism to strengthen the relationship between feature maps. The disadvantage of these approaches is that most of them require two-stage or multi-stage complex architectures and cannot be trained end-to-end. Furthermore, these localization methods tend to generate large prediction regions and are less able to focus on subtle local features. On the other hand, the Vit-based method directly uses the attention-map in the self-attention mechanism as the basis for selecting regions, avoiding the feedback-based architecture and achieving efficient end-toend training. But this method is difficult to generalize to convolutional neural networks or other architectures, which means that its scalability is limited.</p><p>In order to better understand the relationship between feature maps and object positions, we explore the relationship between the object detection models and the FGVC methods. From Faster-RCNN <ref type="bibr" target="#b7">[8]</ref>, YOLO <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b20">[21]</ref>, and RetinaNet <ref type="bibr" target="#b21">[22]</ref>, we can find that feature maps can be rich in location and category information. Segmentation methods such as Mask-RCNN <ref type="bibr" target="#b22">[23]</ref> give pixel-level predictions, i.e. fine-grained predictions. Although these methods demonstrate that feature maps are rich in semantics, they are learned by relying on human-annotated region information, unlike the FGVC task, which only uses image-level annotation for training. Therefore, we discuss Weakly Supervised Object Detection(WSOD) methods, such as WSDDN <ref type="bibr" target="#b23">[24]</ref>, OICR <ref type="bibr" target="#b24">[25]</ref>, and WCCN <ref type="bibr" target="#b25">[26]</ref>, etc. The concept of the WSOD method is that the response of feature maps reflects the location of objects, and we can use these feature maps to complete bounding box prediction through an weakly supervised multi-stage architecture and loss function design.</p><p>Based on the above analysis, we propose a plug-in module that can be added to many common backbones, which can be implemented on CNN-based or Transformer-based architectures. In addition, the plugin module outputs pixel-level feature maps and fuses the filtered features.</p><p>To better detect key features of various sizes, we add the Feature Pyramid Network (FPN) <ref type="bibr" target="#b26">[27]</ref> to the backbone network to mix spatial features of different scales. This architecture is very common in object detection tasks, and this approach can improve the quality of local representation.</p><p>The idea of the plugin module is composed of three operations, including division, competition, and combination. Consider the example in <ref type="figure" target="#fig_1">Fig.2</ref>, the patches obtained by dividing the background are almost solid colours. This kind of patch will appear widely in different categories of data. If we use these patches for training, we can expect that the predicted distribution of the test images will be very flat. On the contrary, if we use the patches which contain discriminative objects for training, the predicted results would be more distinguishable.</p><p>The reason is that nearly identical training data but labeled as different classes can lead to difficulty in converging on the sample space during the backward propagation. This phenomenon gives us the idea that the predicted class score can be used to divide the feature map into object candidate regions and background candidate regions. Divided object features of different scales will be fused for classification prediction, while the correction of the background area is aimed at a flat probability distribution. In this way, we can remove background noise and focus on important regions. This paper has the following two main contributions.</p><p>? We propose a novel plug-in network that can be applied to various models. This network integrates novel The probability distribution of background patches will be flat, while the probability distribution of some objects will be shaped.</p><p>background segmentation and feature fusion techniques, which can effectively improve the accuracy of finegrained visual classification. ? The proposed network outperforms state-of-the-art networks and improves the accuracy to 92.77% (+0.97%) and 92.83% (+1.83%) on CUB200-2011 <ref type="bibr" target="#b0">[1]</ref> and NABirds <ref type="bibr" target="#b1">[2]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we will introduce the proposed architecture for coarse-grained image recognition tasks, which are mainly divided into convolution methods and self-attention mechanisms. And then, we will introduce the object detection task and discuss the ideas we will use. Finally, we will analyze several fine-grained visual classification models that have performed well recently, and compare these methods in experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Backbones</head><p>CNN has been quite successful in image recognition. Since the accuracy of AlexNet <ref type="bibr" target="#b27">[28]</ref> on ImageNet <ref type="bibr" target="#b28">[29]</ref> has greatly surpassed traditional methods, various convolutional architectures have been proposed to improve the accuracy of recognition. For example, ResNet <ref type="bibr" target="#b4">[5]</ref> and DenseNet <ref type="bibr" target="#b18">[19]</ref> have performed quite well on various recognition tasks using deep network structures with shortcut connections, and their pre-trained models on ImageNet have also been successfully transferred to various computer vision tasks. EfficientNet <ref type="bibr" target="#b5">[6]</ref> further discusses the depth, width and input resolution of convolutional networks to optimize the network structure. The method successfully maintains high accuracy with a small number of parameters. Since the publication of Transformer <ref type="bibr" target="#b29">[30]</ref>, the self-attention mechanism has been widely used in the field of natural language processing and computer vision, and its performance is quite good. For example, Vit <ref type="bibr" target="#b6">[7]</ref> achieved 90.7% accuracy on ImageNet, which convert the patch of the image into a token through linear transformation, The model architecture and training process of these methods are summarized in <ref type="figure" target="#fig_2">Fig.3</ref>. The light blue square (block 1 ? blockn) in the middle represents the blocks of the main backbone network (e.g. ResNet <ref type="bibr" target="#b4">[5]</ref>, ViT <ref type="bibr" target="#b6">[7]</ref>). The transfer direction is shown by the arrow. The light red solid square on the right side of the backbone network represents the feature map of the input image after passing through the convolutional network. This feature map can be used to complete object positioning or re-cutting. We call this process the localization module. Finally, the integrated information will be sent to the second-stage model, which is represented by a dotted square. The second-stage model may be a specially designed structure, or it may be the original network. In this way, local features can be used more accurately to complete the identification. The light purple solid square on the left side of the backbone network represents the attention-map generated by the Transformer, and the subsequent process is the same as the previous process based on convolution-based models. and complete the global information exchange through the self-attention mechanism. However, its disadvantage is that there is no hierarchical expression ability for the features of local regions. In SwinTransformer <ref type="bibr" target="#b30">[31]</ref>, the extraction of local region features at different scales is done through a multilayer self-attention structure. Because of the success of the above architectures, we will use these models as the backbone network to test the capabilities of the proposed modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object detection</head><p>The goal of object detection is to find the location and classification of objects, so the overall architecture and idea will be very similar to the fine-grained visual classification task. The difference is that the purpose of the fine-grained visual classification is not whether the regions of the object are found, but whether the regions with distinguishable features are found, and whether these features can be more effectively used for identification.</p><p>First of all, we explore the supervised object detection networks. For example, Faster-RCNN <ref type="bibr" target="#b7">[8]</ref> predicts whether each pixel position on the feature map is an object through Region Proposal Network (RPN), and then predicts the category of the object region. YOLO <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b20">[21]</ref> and RetinaNet <ref type="bibr" target="#b21">[22]</ref> complete the prediction of position and category through an overall network. The above methods all learn the region of the object from the manual annotation, and complete the identification for the region of the object.</p><p>Let us explain this process in another angle. In the training phase, the network needs to pay attention to these regions with objects, and then complete the classification prediction of the region. The whole process is very close to the method proposed by FGVC, but the FGVC method we intend to propose does not use the target location information as training data. Instead, we extend to weakly supervised learning methods in object detection that do not rely on manually annotating object regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Weakly supervised object detection</head><p>Weakly Supervised Object Detection (WSOD) relies on meaningful feature maps from deep neural networks. B. Zhou et al. <ref type="bibr" target="#b31">[32]</ref> observed that by learning object class labels, the representation of objects in space can be learned, i.e., virtual labeling of object locations can be done through information on feature maps. For example, WSDDN <ref type="bibr" target="#b23">[24]</ref> uses the pretrained model as the feature extractor, and completes the prediction of the features of the candidate regions through a two-stream network, and then uses the prediction result to filter the regions. That is, the identification is completed by the quality of the prediction of each region.</p><p>OICR <ref type="bibr" target="#b24">[25]</ref> continues the former method and adds a multistage classifier to complete more accurate positioning. WCCN <ref type="bibr" target="#b25">[26]</ref> completes the division of candidate regions through Class Activation Map (CAM) <ref type="bibr" target="#b32">[33]</ref> at first, the principle of CAM is that map prediction score on the previous features map to generate the class-specific heat map, and then use the secondstage model to screen better candidate regions. Methods such as ACoL <ref type="bibr" target="#b33">[34]</ref> and SPG <ref type="bibr" target="#b35">[35]</ref> are also based on CAM to complete localization. DANet <ref type="bibr" target="#b36">[36]</ref> learns more diverse features by comparing the fusion category and uses CAM to locate more complete objects. CASD <ref type="bibr" target="#b37">[37]</ref> trains OICR <ref type="bibr" target="#b24">[25]</ref> by means of self-distillation, and MIST <ref type="bibr" target="#b38">[38]</ref> perform self-training by generating virtual labels. The above methods show that category labels can provide features rich in object location and locate more class-discriminating regions. However, the task with WSOD is still not the same as with FGVC. The goal of the FGVC task is no longer to detect complete objects, but to find the most critical regions and exploit these regions for better recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fine-grained visual classification</head><p>The main purpose of FGVC is to locate highly discriminatory regions. Many methods such as NTS-Net <ref type="bibr" target="#b8">[9]</ref>, FDL <ref type="bibr" target="#b9">[10]</ref>, StackedLSTM <ref type="bibr" target="#b10">[11]</ref> find strong discriminative preselected boxes through Region Proposal Network (RPN), then resize the preselected boxes to a fixed size, and use these local strong features to identify, that is to say, these methods will actually divide the identification task into two stages.</p><p>Mix+ <ref type="bibr" target="#b39">[39]</ref> uses the attention map to find the regions with strong discrimination and uses this map to complete the mixed reinforcement. CCFR <ref type="bibr" target="#b40">[40]</ref> recognizes the global features through these regional features, and learn better local region features by triplet loss and scale-separated NMS.</p><p>Attention mechanisms are also widely used in FGVC tasks, such as MACN <ref type="bibr" target="#b12">[13]</ref>, WS-DAN <ref type="bibr" target="#b15">[16]</ref> and CAL <ref type="bibr" target="#b11">[12]</ref> to learn the location of objects, and extract the characteristics of a specific location in a feature map through the attention-map. In addition to being used to learn location information, the attention mechanism can also improve the expressiveness of features. For example, MAMC <ref type="bibr" target="#b13">[14]</ref> and API-Net <ref type="bibr" target="#b14">[15]</ref> learn unique features by simultaneously training the differences between a pair of similar images through the attention mechanism, and CAP <ref type="bibr" target="#b41">[41]</ref> fuses different local features through the attention mechanism. The above methods all show that the attention mechanism is a very powerful method.</p><p>Since the release of Vision Transformer(ViT) <ref type="bibr" target="#b6">[7]</ref>, the ViT architecture has performed quite well in the field of image recognition, so many approaches are proposed to apply this architecture to the FGVC task. For example, FFVT <ref type="bibr" target="#b17">[18]</ref>, AF-Trans <ref type="bibr" target="#b42">[42]</ref>, RAMS-Trans <ref type="bibr" target="#b43">[43]</ref> and TransFG <ref type="bibr" target="#b16">[17]</ref> use ViT as the backbone network, and use the attention-map generated by the image in self-attention as the search for strong discriminative regions. Finally, the features of these regions are processed to complete the recognition task. This approach is similar to the aforementioned method based on the convolutional network. As shown in the left part of <ref type="figure" target="#fig_2">Fig.3</ref>, the difference is that the strength of the attention map is used instead of the response of the feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. A NOVEL PLUG-IN MODULE FOR FINE-GRAINED VISUAL CLASSIFICATION</head><p>To find strong discriminative regions for fine-grained classification tasks, we propose a plug-in module that can be applied to mainstream backbone networks such as ResNet <ref type="bibr" target="#b4">[5]</ref>, EfficientNet <ref type="bibr" target="#b5">[6]</ref>, and ViT <ref type="bibr" target="#b6">[7]</ref>. The overall design concept is to treat each pixel (or patch) on the feature map as an independent feature, which can represent its region. Then we classify these features and use the classification ability as a basis for distinguishing. Finally, the entire network can be completed through end-to-end training. In this chapter, the design of the module structure, the use of the loss function, <ref type="figure">Fig. 4</ref>. Schematic flow of the proposed plug-in module. Backbone Blockk represents the kth block in the backbone network. When the image is input to the network, the feature map output by each block will be input into the Weakly Supervised Selector to screen out areas with strong discrimination or areas that are less related to classification. Finally, a Combiner is used to fuse the features of the selected results to obtain the prediction results. The Lfinal represents the loss function. and the combination with each framework will be introduced in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Module design</head><p>The common point of the FGVC and WSOD methods discussed above is to find a region with strong discrimination. FGVC cuts this area or increases attention during secondary training process, while WSOD frameworks often use multiple instance learning (MIL) for more accurate localization. These two tasks are still slightly different in positioning targets, but these architectures can prove that the pixel-level features in the feature map can represent the importance of this area in the classification task. In the following, we call the value in the pixel of the feature map as the feature point. The dimension of this feature point is R C , where C represents the output feature dimension size of this block.</p><p>We adopt a very simple design, which passes each feature point through a fully connected layer to predict the category. When the highest probability of the predicted result after softmax is greater than a certain value, the feature point is considered as a helpful feature and will be reserved for subsequent fusion. Conversely, the feature point is considered less helpful for fine-grained classification.</p><p>Based on the above concepts, we design an architecture that can be trained end-to-end, as shown in <ref type="figure">Fig.4</ref>. The area framed by the dotted line is the backbone model, and we use f i ? R C?H?W to represent the feature map output by the i th block in the backbone network, where H represents the height of the feature map, W represents the width of the feature map, C represents the size of the feature dimension. This feature map is then fed into a weakly supervised selector and each feature point will be classified by a linear classifier. The feature map after this step is denoted as f i ? R C ?H?W , where C is equal to the number of target classes. Then, the class prediction probability of each feature point is obtained through softmax, and the first few feature points with high confidence score will be selected in the weakly supervised selector.</p><p>The selection algorithm is shown in Algorithm 1, and the selected features are fused through the fusion model, whose architecture is shown in <ref type="figure">Fig.4</ref>. In order to complete the feature fusion, we designed two different architectures. The first one is implemented through a fully connected layer. Assume that the total number of selected feature points is N . Before the feature maps are input to the fully connected layer, they are first concatenated together for the feature dimension. Therefore, the feature map with dimension R N ?C produces a prediction result with dimension R C after passing through the fully connected layer. This architecture can recombine the selected local features into global features that can represent the entire image. The second architecture is implemented through graph convolution, which treats all selected feature points as a graph structure, where nodes represent features at different spatial locations and scales. The graph is input into the graph convolutional network, which can learn the relationship between different nodes. And then, the feature points are aggregated into several super nodes through the pooling layer, and finally the features of these super nodes are averaged, and a linear classifier is used to complete the prediction. The advantage of this approach is that the features of each point can be integrated more efficiently without corrupting the results output by the backbone model. Therefore, we finally use graph convolution as the feature fusion mechanism.</p><p>In addition, in order to allow the model to extract the features of small regions more effectively, we add FPN to the backbone network to effectively fuse features of different scales to achieve more accurate recognition results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The process of forward and backward propagation</head><p>The goal of this architecture is to perform fine-grined classification, so it does not use any artificial labels as training targets other than image-level annotations. The first training goal is to make the features of each feature map f i have the ability to classify. In order to calculate the overall loss, we first average the prediction output of all feature points, as shown in the following Eq.(1), where f l,s ? R C represents the feature point at position s of the l th block of feature map, S denotes the output feature map space of this block, and the size is H ? W . Then the class loss of the entire block is calculated through Cross Entropy, as shown in Eq. <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_0">z l = 1 H ? W s?S f l,s (1) L b = ? l?L log(z cls l )<label>(2)</label></formula><p>In addition to training the overall classification, the selected position of the feature map is marked by M ask ? R H?W . This M ask is a binary data type, where 1 represents the selected area and 0 represents the dropped area. Therefore, S (M ask) represents feature points with strong discrimination, while S (? M ask) represents dropped feature points. Eq. (3) represents the sum of the features of the selected region in the l th block. The loss function of the selected category is L s defined as Eq.(4). Eq. (5) represents the sum of the features that were not selected (or can be said to be selected as background). The flattening loss function for the dropped area is L n defined as Eq. <ref type="formula" target="#formula_1">(6)</ref>.</p><formula xml:id="formula_1">h l = s?S (M ask) f l,s (3) L s = ? l?L i?C log(h i l ) (4) n l = s?S (?M ask) f l,s (5) L s = ? l?L i?C log(1 ? n i l )<label>(6)</label></formula><p>The flattened output is designed to express that this area is less helpful for classification. In fact, this approach is like predicting the "score" of the foreground or background in the object detection framework. In this paper, the highest probability value of the softmax is taken as this score.</p><p>Then, the selected feature f s ? R N ?C will be input into the Combiner to generate prediction results of mixed scales, the output feature is f comb ? R N ?C , where N is the number of super nodes after condesing input nodes. Finally, the features on these super nodes are averaged and input into a linear classifier to output predictions. The Combiner category prediction loss is calculated through Cross Entropy, and this loss function is represented by L c . The entire loss function is defined as Eq. <ref type="bibr" target="#b6">(7)</ref>. This loss function is the weighted sum of the above loss functions, where ? b , ? s , ? n , and ? c are the weights of L b , L s , L n , and L c , respectively:</p><formula xml:id="formula_2">L = ? b L b + ? s L s + ? n L n + ? c L c<label>(7)</label></formula><p>In fact, during training phrase, we set L b =1, L s =0, L n =5, L s =1, and do not use the selected category loss function L s </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Accuracy(%) ResNet-50 <ref type="bibr" target="#b16">[17]</ref> ResNet-50 84.5 ResNet-50(Ours) ResNet-50 88.2 API-Net <ref type="bibr" target="#b14">[15]</ref> DenseNet-161 90.0 Mix+ <ref type="bibr" target="#b39">[39]</ref> ResNet-50 90.2 StackedLSTM <ref type="bibr" target="#b10">[11]</ref> GoogleNet 90.4 CAL <ref type="bibr" target="#b11">[12]</ref> ResNet-101 90.6 DeepFVE <ref type="bibr" target="#b44">[44]</ref> InceptionV3 <ref type="bibr" target="#b45">[45]</ref> 91.0 CCFR <ref type="bibr" target="#b40">[40]</ref> ResNet-50 91.1 RAMS-Trans <ref type="bibr" target="#b43">[43]</ref> ViT-B 16 91.3 TPSKG <ref type="bibr" target="#b46">[46]</ref> ViT-B 16 91.3 AFTrans <ref type="bibr" target="#b42">[42]</ref> ViT-B 16 91.5 FFVT <ref type="bibr" target="#b17">[18]</ref> ViT-B 16 91.6 TransFG <ref type="bibr" target="#b16">[17]</ref> ViT-B 16 91.7 CAP <ref type="bibr" target="#b41">[41]</ref> Xception <ref type="bibr" target="#b47">[47]</ref> 91.8 PIM * (ours) Swin-T 92.8 * PIM denotes the proposed plug-in module.</p><p>as the training target because the prediction loss L c through the Combiner category already has the same function.</p><p>The overall training goal is to locate a local area with strong discrimination, and improve the recognition result through the features of this area. This goal is very similar to the functionality of the previous WSOD and FGVC frameworks, except that finding the local locations (or positions of objects) with strong discrimination are mostly done by the responses of feature maps, often called heatmaps. The downside of this approach is that many algorithms are needed to find the heatmap, and many architectures require a two-stage approach to revise the model.</p><p>In this paper, we propose a simple and easy-to-implement method that mainly uses local features to predict classes. This makes it difficult to "distinguish" the predicted values of local background regions or locally similar parts of different classes. In this case, a strong selection criterion -maximum predicted probability. Finally, the selected local features are fused into global features to complete the final prediction. This method can be easily applied to various mainstream backbone network and only one stage of end-to-end training is required.</p><p>Contrastive learning is widely used in FGVC tasks. For example, TransFG <ref type="bibr" target="#b16">[17]</ref> learns better features through contrastive learning of features. API-Net <ref type="bibr" target="#b14">[15]</ref> uses ranking loss to learn the difference between a pair of image features. CCFR <ref type="bibr" target="#b40">[40]</ref> uses triplet loss to learn regions of discrimination. Contrastive learning seems to play an important role for the FGVC model. However, training a model through contrastive learning usually requires additional parameters, such as margin distance or temperature, etc. Because we hope to keep the training as simple as possible, contrastive learning is not used in this design, only Cross Entropy is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we will introduce the datasets and the setting of experimental hyperparameters, then compare with some current state-of-the-art methods, and finally discuss some factors that affect the recognition accuracy and visualize the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Accuracy(%) Cross-X <ref type="bibr" target="#b48">[48]</ref> ResNet-50 86.4 PAIRS <ref type="bibr" target="#b49">[49]</ref> ResNet-50 87.9 API-Net <ref type="bibr" target="#b14">[15]</ref> DenseNet-161 88.1 CS-Part <ref type="bibr" target="#b50">[50]</ref> ResNet-50 88.5 MGE-CNN <ref type="bibr" target="#b51">[51]</ref> ResNet-101 88.6 FixSENet-154 <ref type="bibr" target="#b52">[52]</ref> SENet-154 89.2 TransFG <ref type="bibr" target="#b16">[17]</ref> ViT 90.8 CAP <ref type="bibr" target="#b41">[41]</ref> Xception 91.0 PIM * (ours) Swin-T 92.8 * PIM denotes the proposed plug-in module.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and implementation details</head><p>The datasets we use are CUB200-2011 <ref type="bibr" target="#b0">[1]</ref> and NA-Birds [2] two fine-grained bird identification datasets. The CUB200-2011 dataset has a total of 200 bird categories, including 5,994 training images and 5,794 testing data. Each category contains about 30 training data. NA-Birds has 555 bird species, 23,929 training images and 24,633 test images. Both datasets provide image-level annotations and keypoint locations, but only image-level annotations will be used in this paper.</p><p>When ResNet-50 <ref type="bibr" target="#b4">[5]</ref>, EfficientNet-B7 <ref type="bibr" target="#b5">[6]</ref> and ViT <ref type="bibr" target="#b6">[7]</ref> are adopted as backbone networks, the input image is a 448?448 color image. When Swin-T <ref type="bibr" target="#b30">[31]</ref> is used as the backbone network, the input image is a 384?384 color image.</p><p>The methods of data augmentation is as follows. If the input image size is 384?384, the first step is to scale the image to 510?510, and if the input image size is 448?448, it is scaled to 600?600. In training phrase, data augmentation is performed via Randon Crop, Random HorizontalFlip, and Random GaussianBlur while in testing phrase, Center Crop is used. During training, the learning rate is set to 0.0005, and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selections Number</head><p>Combiner-1(%) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b31">32]</ref> 92.61 <ref type="bibr">[256,</ref><ref type="bibr">128,</ref><ref type="bibr">64,</ref><ref type="bibr" target="#b31">32]</ref> 92.67 <ref type="bibr">[512,</ref><ref type="bibr">256,</ref><ref type="bibr">128,</ref><ref type="bibr">64]</ref> 92. <ref type="bibr">37 [1024,512,128,64]</ref> 92. <ref type="bibr">48 [1024,512,128,128]</ref> 92.54 <ref type="bibr">[2048,</ref><ref type="bibr">512,</ref><ref type="bibr">128,</ref><ref type="bibr" target="#b31">32]</ref> 92.77 <ref type="bibr">[2048,</ref><ref type="bibr">512,</ref><ref type="bibr">128,</ref><ref type="bibr">128]</ref> 92. <ref type="bibr">34 [2304,576,144,144]</ref> 92.34 the cosine decay is used; the weight decay is set to 0.0005; SGD is used as the optimizer, and the batch size is set to 8; a total of 50 epochs are trained. All experiments are completed on a single Nvidia GeForce RTX 3090, and the Pytorch toolbox is used as the main implementation substrate. It takes about 3 hours to complete the training on CUB200-2011, and about 15 hours for NA-Birds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Compare with state-of-the-art approaches</head><p>In <ref type="table" target="#tab_1">Table I</ref>, we compare our plug-in module (PIM) with state-of-the-art methods on CUB200-2011 dataset. We use a Swin-T model pre-trained on ImageNet22K as the backbone. <ref type="table" target="#tab_1">Table I</ref> show that the proposed PIM can reach 92.8% in Top-1 accuracy, which is 1.0% higher than the previous best method. <ref type="table" target="#tab_1">Table II</ref> shows that the proposed PIM can reach 92.8% in Top-1 accuracy on NA-Birds dataset, which is 1.8% higher than the previous best method. This results show that the proposed PIM can effectively fuse features of different scales, and this feature can effectively identify fine-grained categories.</p><p>To better discuss the accuracy improvement brought by this approach, we test the proposed PIM on four mainstream backbones. As shown in <ref type="table" target="#tab_1">Table III</ref>, the four mainstream backbones, including ResNet-50 <ref type="bibr" target="#b4">[5]</ref>, EfficientNet <ref type="bibr" target="#b5">[6]</ref>, Vit <ref type="bibr" target="#b6">[7]</ref>, and Swin-T <ref type="bibr" target="#b30">[31]</ref> can be improved after adding our proposed PIM. The accuracy rate of Swin-T before joining PIM has exceeded state-of-the-art approaches, which shows that when the capacity of the backbone is good enough, the performance in downstream tasks will be quite good. So it can be predicted that these fine-grained recognition tasks will also become easier to complete after pre-training on large datasets through </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation study</head><p>This section explores the components in the proposed PIM that affect accuracy. We first introduce the evaluation method used, and the experimental results for different evaluation methods are presented in <ref type="table" target="#tab_1">Table IV</ref>. We calculate an accuracy rate for each block and combiner's prediction results, respectively. The first five rows in <ref type="table" target="#tab_1">Table IV</ref> represent these accuracy rates, and the average of the largest one to five of the prediction scores is selected as the prediction result. In <ref type="table" target="#tab_1">Table IV</ref>, the first five rows represent these accuracy rates, and the average of the largest one to five of the above prediction scores is selected as the prediction result, and the results are displayed in top-1 score prediction to top-5 score prediction respectively. We can see that the average of all prediction scores achieves the best result.</p><p>1) Number of Selections: Swin-T has four blocks, each block outputs the number of regions selected by the Weakly Supervised Selector (num selects in Algorithm1), which is represented by a list in this paper. The first number in the list represents the number of selected areas for the first block, and so on.</p><p>We first discuss the number of selected areas. In this experiment, Swin-T is used as the backbone, which has four blocks. In <ref type="table" target="#tab_5">Table V</ref>, the list in the selections number represents the number of selected areas of the four blocks. The fourth column of <ref type="table" target="#tab_5">Table V</ref> shows the top-1 accuracy. It can be observed that the number of selected areas has little effect on the accuracy rate. However, the number of selected areas has a great influence on the amount of operation of the Combiner, because the number of input nodes will directly affect the amount of parameters of the model required for subsequent mixing. Considering the trade-off between the amount of operation and the accuracy rate, we use <ref type="bibr">[256,</ref><ref type="bibr">128,</ref><ref type="bibr">64,</ref><ref type="bibr" target="#b31">32]</ref> as the final number selected areas.</p><p>2) Selected-area and Dropped-area: Then we discuss the impact of the selected area and the dropped area on the accuracy. We experiment with two different selection sizes, <ref type="bibr">[256,</ref><ref type="bibr">128,</ref><ref type="bibr">64,</ref><ref type="bibr" target="#b31">32]</ref> and <ref type="bibr">[2048,</ref><ref type="bibr">512,</ref><ref type="bibr">256,</ref><ref type="bibr">128]</ref>. <ref type="table" target="#tab_1">Table VI</ref> shows that no matter how many are selected, there is a certain difference in the accuracy of the selected area and the dropped area, although the difference is not as large as imagined when the number of selected is small. If the confidence score is not enough, it can be further excluded. If threshold=0.9 is used as the boundary, it can be observed that the dropped area is still generally low.</p><p>3) Combiner: There are different kinds of combiners mentioned in the previous chapter. We experiment with two structures, multi layer perceptron (MLP) and graph convolutional network (GCN). The experimental results are shown in <ref type="table" target="#tab_1">Table  VII</ref>. In the MLP architecture, the reason we only use one layer is the limitation of the amount of parameters and the amount of calculation. If we want to use multiple layers, we must reduce the output feature size or reduce the number of selections. Because the top-1 accuracy of single-layer MLP is already lower than 0.61% of single-layer GCN structure, we choose to use GCN as the main architecture of Combiner. Based on the experimental results shown in <ref type="table">Table 7</ref>, we choose a single-layer GCN as the Combiner, which has the highest accuracy and less computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Pooling Number:</head><p>We found that the number of nodes in the Combiner will affect the accuracy. We discuss the singlelayer, double-layer and three-layer GCN structure. In <ref type="table" target="#tab_1">Table  VIII</ref>, Numbers represents the ratio of the output nodes of each layer to the number of Combiner input nodes. 1/128 means that the output number of nodes in this layer is 1/128 of the number of Combiner input nodes. <ref type="table" target="#tab_1">Table VIII</ref> shows that the best result occurs with a one-layer structure, and the ratio of the number of output nodes to input nodes is 1/32.</p><p>The number of selected areas can affect the accuracy by about ?0.4%, while the GCN architecture (including the number of layers and the amount of fusion) can affect the accuracy by about ?0.6%, which greatly affects the results. The experimental results show that the number of selected regions is best with the setting of <ref type="bibr">[2048,</ref><ref type="bibr">512,</ref><ref type="bibr">128,</ref><ref type="bibr" target="#b31">32]</ref> or <ref type="bibr">[256,</ref><ref type="bibr">128,</ref><ref type="bibr">64,</ref><ref type="bibr" target="#b31">32]</ref>, and the best setting of Combiner is a single-layer architecture of 1/64 or 1/32. 5) Pretrained-Effect: In <ref type="table" target="#tab_1">Table I</ref>, the first and second row shows the recogniztion results of ResNet-50. The first row is the result in TransFG <ref type="bibr" target="#b16">[17]</ref> and the second row is the result of our retrained model. The difference is that we use ImageNet-21K as the pre-training model, and it can be observed that the retrained model can effectively improve the accuracy by 3.7%. 6) Visualization: Finally, we show the effect of adding each component to the backbone in <ref type="table" target="#tab_1">Table IX</ref>. When FPN is added, the overall accuracy can be increased by 0.1%, then by adding Weakly Supervised Selector, it can be increased by 0.1%, and by adding Combiner, it can be increased by 0.7%. This results show that the accuracy of the backbone model can be effectively improved after two feature fusions. In particular, fusion of various scale features through a GCN-Combiner can significantly improve the accuracy.</p><p>We use the above experiments to verify the capabilities of the architecture. The most important point is that it is easy to implement end-to-end training. Finally, the visualization results are used to explore whether this architecture can focus on the discriminative area, and the results are shown in <ref type="figure" target="#fig_4">Fig.6</ref>.</p><p>The heat map in <ref type="figure" target="#fig_4">Fig.6</ref> is calculated using Grad-CAM for the 4 block outputs of Swin-T. The top part of the figure shows the original image, the middle shows the visualization results of the baseline model, and the bottom shows the visualized heat maps after adding the architecture proposed in this paper.</p><p>It can be observed that compared to the baseline model, our method can focus more on the key points, and the background noise will be filtered out relatively cleanly. From the cases of Acdian Flycatcher and Philadelphia Vireo, in addition to focusing more on the object itself, this method can also increase the number of key positions. In the case of the white-crowned sparrow, this method also focuses on the parts around the eyes compared to the focused position of the baseline model. If the background is clean such as the case of Common Tern, the effect of the proposed method will not be significantly different from the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have proposed a novel plug-in module that can be easily applied to popular backbone networks to learn local region features through differentiation. Experimental results show that the proposed method significantly improves the accuracy of fine-grained visual classification and outperforms state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was financially supported by the National Taiwan Normal University (NTNU) within the framework of the Higher Education Sprout Project by the Ministry of Education(MOE) in Taiwan, sponsored by Ministry of Science and Technology, Taiwan, R.O.C. under Grant no. MOST 110-2221-E-003-026, 110-2634-F-003 -007, and 110-2634-F-003 -006. In addition, we thank to National Center for Highperformance Computing (NCHC) for providing computational and storage resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Three similar looking birds, Brewer Blackbird, Shiny Cowbird, and Boat-tailed Grakle. Each row represents three different appearances of the same bird. There are very few differences between different birds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The probability distribution of the predicted value in different patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Schematic flow of the past fine-grained visual classification networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>The schematic diagram of the Combiner. The input is the selected feature points of different scales, which are represented by different colors. The features are fused through the Combiner, and finally the recognition is completed through these fused features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Visualization results on CUB-200-2011, the first row show the original image, second shows the results of Swin-T, bottom shows the results of Swin-T with PIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 torch import torch.nn as nn class WSS(nn.Module): def</head><label>1</label><figDesc>Weakly Supervised Selector, PyTorch-like Code</figDesc><table><row><cell>import __init__(self,</cell></row><row><cell>in_channel: int,</cell></row><row><cell>num_classes: int,</cell></row><row><cell>num_selects: int):</cell></row><row><cell>super().__init__()</cell></row><row><cell>self.fc = nn.Linear(in_channel, num_classes)</cell></row><row><cell>self.num_selects = num_selects</cell></row><row><cell>def forward(self, x):</cell></row><row><cell># [B, H?W, C] = x.shape</cell></row><row><cell># return class_prediction, selected_features</cell></row><row><cell>logits = torch.softmax(self.fc(x))</cell></row><row><cell>_, ids = \</cell></row><row><cell>torch.sort(logits, -1, descending=True)</cell></row><row><cell>selection = ids[:, :self.num_selects]</cell></row><row><cell>return logits, torch.gather(x, 1, selection)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF DIFFERENT METHODS ON CUB-200-2011.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF DIFFERENT METHODS ON NB-BIRDS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III IMPROVEMENT</head><label>III</label><figDesc></figDesc><table><row><cell cols="3">OF DIFFERENT BACKBONES WITH OUR ADDITIONAL</cell></row><row><cell></cell><cell>STRUCTURE.</cell><cell></cell></row><row><cell>Backbone</cell><cell>Original</cell><cell>+PIM</cell></row><row><cell>ResNet-50</cell><cell>88.2%</cell><cell>89.5%(+1.3%)</cell></row><row><cell>EfficientNet-7B  *</cell><cell>88.2%</cell><cell>89.9%(+1.7%)</cell></row><row><cell>Vit</cell><cell>90.1%</cell><cell>91.0%(+0.9%)</cell></row><row><cell>Swin-T</cell><cell>91.9%</cell><cell>92.8%(+0.9%)</cell></row><row><cell cols="3">*  The pre-training dataset for EfficientNet-7B is ImageNet-1K,</cell></row><row><cell cols="3">while the others are ImageNet-21K or -22K.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISION</head><label>IV</label><figDesc>OF DIFFERENT EVALUATION METHODS.</figDesc><table><row><cell>Evaluation methods</cell><cell>Accuracy(%)</cell></row><row><cell>block 1 average score</cell><cell>91.70</cell></row><row><cell>block 2 average score</cell><cell>91.75</cell></row><row><cell>block 3 average score</cell><cell>92.18</cell></row><row><cell>block 4 average score</cell><cell>92.16</cell></row><row><cell>combiner score</cell><cell>91.80</cell></row><row><cell>top-1 average score</cell><cell>91.62</cell></row><row><cell>top-2 average score</cell><cell>91.99</cell></row><row><cell>top-3 average score</cell><cell>92.15</cell></row><row><cell>top-4 average score</cell><cell>92.75</cell></row><row><cell>top-5 average score</cell><cell>92.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V COMPARISION</head><label>V</label><figDesc>OF DIFFERENT EVALUATION METHODS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI DIFFERENT</head><label>VI</label><figDesc>SELECTION NUMBER OF EACH LAYER AND ITS CORRESPONDING ACCURACY..</figDesc><table><row><cell>Selections</cell><cell>Block</cell><cell>Selected</cell><cell>Dropped</cell></row><row><cell></cell><cell>id</cell><cell>Accuracy(%)</cell><cell>Accuracy(%)</cell></row><row><cell></cell><cell>1</cell><cell>91.68</cell><cell>81.43</cell></row><row><cell>[256, 128,</cell><cell>2</cell><cell>91.62</cell><cell>71.03</cell></row><row><cell>64, 32]</cell><cell>3</cell><cell>92.17</cell><cell>50.37</cell></row><row><cell></cell><cell>4</cell><cell>92.29</cell><cell>70.98</cell></row><row><cell></cell><cell>1</cell><cell>91.82</cell><cell>10.01</cell></row><row><cell>[2048, 512,</cell><cell>2</cell><cell>91.44</cell><cell>10.08</cell></row><row><cell>128, 32]</cell><cell>3</cell><cell>92.68</cell><cell>9.12</cell></row><row><cell></cell><cell>4</cell><cell>92.72</cell><cell>8.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII COMPARISION</head><label>VII</label><figDesc>OF DIFFERENT COMBINER STRUCTURE.</figDesc><table><row><cell></cell><cell>Type</cell><cell>Number of Layers</cell><cell>Accuracy(%)</cell></row><row><cell></cell><cell>ADD</cell><cell>0</cell><cell>92.11</cell></row><row><cell></cell><cell>MLP</cell><cell>1</cell><cell>92.06</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>92.77</cell></row><row><cell></cell><cell>GCN</cell><cell>2</cell><cell>92.58</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell>92.67</cell></row><row><cell></cell><cell></cell><cell>TABLE VIII</cell></row><row><cell></cell><cell cols="3">DIFFERENT NUMBER OF GCN LAYERS AND NODES.</cell></row><row><cell>Layers</cell><cell cols="3">Numbers(# of output nodes / # of input nodes) Accuracy(%)</cell></row><row><cell></cell><cell></cell><cell>1/128</cell><cell>92.37</cell></row><row><cell></cell><cell></cell><cell>1/64</cell><cell>92.51</cell></row><row><cell></cell><cell></cell><cell>1/32</cell><cell>92.77</cell></row><row><cell>1</cell><cell></cell><cell>1/16 1/8</cell><cell>92.48 92.56</cell></row><row><cell></cell><cell></cell><cell>1/4</cell><cell>92.49</cell></row><row><cell></cell><cell></cell><cell>1/2</cell><cell>92.48</cell></row><row><cell></cell><cell></cell><cell>1/1</cell><cell>92.30</cell></row><row><cell></cell><cell></cell><cell>1/2, 1/4</cell><cell>92.15</cell></row><row><cell>2</cell><cell></cell><cell>1/4, 1/8 1/8, 1/16</cell><cell>92.32 92.56</cell></row><row><cell></cell><cell></cell><cell>1/16, 1/32</cell><cell>92.44</cell></row><row><cell></cell><cell></cell><cell>1/32, 1/64</cell><cell>92.61</cell></row><row><cell></cell><cell></cell><cell>1/64, 1/128</cell><cell>92.68</cell></row><row><cell></cell><cell></cell><cell>1/128, 1/256</cell><cell>92.63</cell></row><row><cell></cell><cell></cell><cell>/2, /4, /8</cell><cell>91.63</cell></row><row><cell>3</cell><cell></cell><cell>/4, /8, /16</cell><cell>92.11</cell></row><row><cell></cell><cell></cell><cell>/8, /16, /32</cell><cell>92.25</cell></row><row><cell></cell><cell></cell><cell>/16, /32, /64</cell><cell>92.48</cell></row><row><cell></cell><cell></cell><cell>/32, /64, /128</cell><cell>92.53</cell></row><row><cell></cell><cell></cell><cell>TABLE IX</cell></row><row><cell cols="4">IMPROVEMENT OF DIFFERENT BACKBONES WITH OUR ADDITIONAL</cell></row><row><cell></cell><cell></cell><cell>STRUCTURE.</cell></row><row><cell></cell><cell></cell><cell>Components</cell><cell>Accuracy(%)</cell></row><row><cell></cell><cell></cell><cell>Backbone</cell><cell>91.9</cell></row><row><cell></cell><cell cols="2">Backbone + FPN</cell><cell>92.0</cell></row><row><cell></cell><cell cols="2">Backbone + FPN + Selector</cell><cell>92.1</cell></row><row><cell></cell><cell cols="3">Backbone + FPN + Selector + Combiner</cell><cell>92.8</cell></row><row><cell cols="4">self-supervised methods. In this experiments, the pre-training</cell></row><row><cell cols="4">dataset for EfficientNet-7B is ImageNet-1K, while the others</cell></row><row><cell cols="3">are ImageNet-21K or -22K.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="595" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Fine-Grained Visual Categorization, CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>K. Chaudhuri and R. Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Filtration and distillation: Enhancing region attention for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11555" to="11562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3029" to="3038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Counterfactual attention learning for fine-grained visual categorization and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="1025" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5219" to="5227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13130" to="13137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">See better before looking closer: Weakly supervised data augmentation network for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transfg: A transformer architecture for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/2103.07976</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature fusion vision transformer for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<idno>abs/2107.02341</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3059" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5131" to="5139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-produced guidance for weakly-supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Danet: Divergent activation for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6588" to="6597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Comprehensive attention self-distillation for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Instance-aware, context-focused, and memory-efficient weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10595" to="10604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attribute mix: Semantic data augmentation for fine grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Visual Communications and Image Processing (VCIP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="243" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Re-rank coarse classification with local region enhanced features for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2102.09875</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Context-aware attentional pooling (cap) for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wharton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hewage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A free lunch from vit: Adaptive attention multi-scale fusion transformer for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2110.01240</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">RAMS-Trans: Recurrent Attention Multi-Scale Transformer for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="4239" to="4248" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">End-to-end learning of fisher vector encodings for part features in fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Korsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bodesheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
		<editor>Pattern Recognition (C. Bauckhage, J. Gall, and A. Schwing</editor>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="142" to="158" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transformer with peak suppression and knowledge guidance for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/2107.06538</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cross-x learning for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8241" to="8250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aligned to the object, not to the image: A unified pose-aligned representation for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1876" to="1885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Classification-specific parts for improving fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Korsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bodesheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition -41st DAGM German Conference, DAGM GCPR 2019</title>
		<editor>G. A. Fink, S. Frintrop, and X. Jiang</editor>
		<meeting><address><addrLine>Dortmund, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">11824</biblScope>
			<biblScope unit="page" from="62" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning a mixture of granularity-specific experts for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8330" to="8339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
