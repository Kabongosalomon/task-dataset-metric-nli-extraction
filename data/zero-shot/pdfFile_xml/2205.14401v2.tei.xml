<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongyao</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Centre for Perceptual and Interactive Intelligence Limited</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
							<email>gaopeng@pjlab.org.cnhsli@ee.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Masked Autoencoders (MAE) have shown great potentials in self-supervised pretraining for language and 2D image transformers. However, it still remains an open question on how to exploit masked autoencoding for learning 3D representations of irregular point clouds. In this paper, we propose Point-M2AE, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds. Unlike the standard transformer in MAE, we modify the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of 3D shapes. For the encoder that downsamples point tokens by stages, we design a multi-scale masking strategy to generate consistent visible regions across scales, and adopt a local spatial self-attention mechanism during fine-tuning to focus on neighboring patterns. By multi-scale token propagation, the lightweight decoder gradually upsamples point tokens with complementary skip connections from the encoder, which further promotes the reconstruction from a global-to-local perspective. Extensive experiments demonstrate the state-of-the-art performance of Point-M2AE for 3D representation learning. With a frozen encoder after pretraining, Point-M2AE achieves 92.9% accuracy for linear SVM on ModelNet40, even surpassing some fully trained methods. By fine-tuning on downstream tasks, Point-M2AE achieves 86.43% accuracy on ScanObjectNN, +3.36% to the secondbest, and largely benefits the few-shot classification, part segmentation and 3D object detection with the hierarchical pre-training scheme. Code is available at https://github.com/ZrrSkywalker/Point-M2AE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>top. Despite its superiority on grid-based 2D images, we ask the question: can MAE-style masked autoencoding be adapted to irregular point clouds as a powerful 3D representation learner?</p><p>To tackle this challenge, we propose Multi-scale Masked autoencoders for learning the hierarchical representations of point clouds via self-supervised pre-training, termed as Point-M2AE. We represent a point cloud as a set of point tokens depicting different spatial local regions, and inherit MAE's pipeline to first encode visible point tokens and then reconstruct the masked 3D coordinates. Different from 2D images, masked autoencoding for 3D point clouds has three characteristics to be specially considered. Firstly, it is critical to understand the relations between local parts and the overall 3D shapes, which have strong geometric and semantic dependence. As examples, the network can recognize an airplane starting from its wing, or segment the wing's part from the airplane's global feature. Therefore, we regard the standard transformer with the plain encoder and decoder is sub-optimal for capturing such local-global spatial relations in 3D, which directly downsamples the input into a low-resolution representation as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (Top). We modify both the encoder and decoder into multi-stage hierarchies for progressively encoding multi-scale features of point clouds, constructing an asymmetric U-Net <ref type="bibr" target="#b40">[41]</ref> like architecture in <ref type="figure" target="#fig_0">Figure 1</ref> (Bottom). Secondly, as our Point-M2AE encodes multi-scale point clouds unlike the single-scale 2D images, the unmasked visible regions are required to be both block-wise within one scale and consistent across scales, which are respectively for reserving complete local geometries and ensuring coherent feature learning for the network. For this, we introduce a multi-scale masking strategy, which generates random masks at the final scale with a high ratio (e.g., 80%), and back-projects the unmasked positions to all preceding scales. Thirdly, to better reconstruct 3D geometries from a local-to-global perspective, we utilize skip connections to complement the decoder with fine-grained information from the corresponding stages of the encoder. During fine-tuning on downstream tasks, we also adopt a local spatial self-attention mechanism with increasing attention scopes for point tokens at different stages of the encoder, which refocus each token within neighboring detailed structures.</p><p>By the multi-scale pre-training, Point-M2AE can encode point clouds from local-to-global hierarchies and then reconstructs the masked coordinates from global-to-local perspectives, which learns powerful 3D representations and performs superior transfer ability. After self-supervised pre-training on ShapeNet <ref type="bibr" target="#b6">[7]</ref>, Point-M2AE achieves 92.9% classification accuracy for linear SVM on ModelNet40 <ref type="bibr" target="#b52">[53]</ref> with the frozen encoder, which surpasses the runner-up CrossPoint <ref type="bibr" target="#b1">[2]</ref> by +1.2% and even outperforms some fully supervised methods. By fine-tuning on various downstream tasks, Point-M2AE achieves 86.43% (+3.36%) accuracy on ScanObjectNN <ref type="bibr" target="#b44">[45]</ref> and 94.0% (+0.8%) accuracy on ModelNet40 <ref type="bibr" target="#b52">[53]</ref> for shape classification, 86.51% (+0.91%) instance mIoU on ShapeNet-Part <ref type="bibr" target="#b57">[58]</ref> for part segmentation, and 95.0% (+2.7%) accuracy on 10-way 20-shot ModelNet40 for few-shot classification. Our multi-scale masked autoencoding also benefits the 3D object detection on ScanNetV2 <ref type="bibr" target="#b9">[10]</ref> by +1.3% AP 25 and +1.3% AP 50 , which provides the detection backbone with a hierarchical understanding of the point clouds.</p><p>We summarize the contributions of our paper as follows:</p><p>1. We propose Point-M2AE, a strong masked autoencoding framework, which conducts hierarchical point cloud encoding and reconstruction for better learning multi-scale spatial geometries of 3D shapes.</p><p>2. We introduce a U-Net like transformer architecture for MAE-style pre-training on point clouds, and adopt a multi-scale masking strategy to generate consistent visible regions across scales.</p><p>3. Point-M2AE achieves state-of-the-art performance for transfer learning on various downstream tasks, which indicates our approach to be a powerful representation learner for 3D point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Pre-training by Masked Modeling. Compared to contrastive learning methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> that learn from inter-sample relations, self-supervised pre-training by masked autoencoding builds the pretext tasks to predict the masked parts of the input signals. The series of GPT <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b4">5]</ref> and BERT <ref type="bibr" target="#b11">[12]</ref> apply masked modeling to natural language processing and achieve extraordinary performance boost on downstream tasks with fine-tuning. Inspired by this, BEiT <ref type="bibr" target="#b3">[4]</ref> proposes to match image patches with discrete tokens via dVAE <ref type="bibr" target="#b39">[40]</ref> and pre-train a standard vision transformer <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b59">60]</ref> by masked image modeling. On top of that, MAE <ref type="bibr" target="#b19">[20]</ref> directly reconstructs the raw pixel values of masked tokens and performs great efficiency with a high mask ratio. The follow-up works further improve the performance of MAE by momentum encoder <ref type="bibr" target="#b65">[66]</ref>, contrastive learning <ref type="bibr" target="#b2">[3]</ref>, and modified reconstruction targets <ref type="bibr" target="#b50">[51]</ref>. For self-supervised pre-training on 3D point clouds, the masked autoencoding has not been widely adopted. Similar to BEiT, Point-BERT <ref type="bibr" target="#b59">[60]</ref> utilizes dVAE to map 3D patches to tokens for masked point modeling, but heavily relies on constrastive learning <ref type="bibr" target="#b20">[21]</ref>, complicated data augmentation, and the costly two-stage pre-training. In contrast, our Point-M2AE is a pure masked autoencoding method of one-stage pre-training, and follows MAE to reconstruct the input signals without dVAE mapping. Different from previous MAE methods adopting standard plain transformer, we propose a hierarchical transformer architecture along with the multi-scale masking strategy to better learn a strong and generic representation for 3D point clouds.</p><p>Self-supervised Learning for Point Clouds. 3D representation learning without annotations has been widely studied in recent years. Mainstream methods mainly build the pretext tasks to reconstruct the transformed input point cloud based on the encoded latent vectors, such as rotation <ref type="bibr" target="#b33">[34]</ref>, deformation <ref type="bibr" target="#b0">[1]</ref>, rearranged parts <ref type="bibr" target="#b41">[42]</ref> and occlusion <ref type="bibr" target="#b48">[49]</ref>. From another perspective, PointContrast <ref type="bibr" target="#b54">[55]</ref> utilizes contrastive learning between features of the same points from different views to learn discriminative 3D representations. DepthContrast <ref type="bibr" target="#b62">[63]</ref> further extends the contrast for depth maps of different augmentations. CrossPoint <ref type="bibr" target="#b1">[2]</ref> conducts cross-modality contrastive learning between point clouds and their corresponding rendering images to acquire rich self-supervised signals. Point-BERT <ref type="bibr" target="#b59">[60]</ref> and Point-MAE <ref type="bibr" target="#b32">[33]</ref> respectively introduce BERT-style <ref type="bibr" target="#b10">[11]</ref> and MAE-style <ref type="bibr" target="#b19">[20]</ref> pre-training schemes for 3D point clouds with standard transformer networks and performs competitively on various downstream tasks, but both of them can only encode point clouds with a single resolution and ignores the local-global relations between 3D shapes. In this paper, we propose Point-M2AE, an MAE-style framework with a hierarchical transformer for multi-scale point cloud pre-training. We achieve state-of-the-art downstream performance by learning the multi-scale representation of point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The overall pipeline of Point-M2AE is shown in <ref type="figure" target="#fig_1">Figure 2</ref>, where we encode and reconstruct the point cloud by a hierarchical network architecture. In Section 3.1, We first introduce the masking strategy of Point-M2AE with multi-scale representations of point clouds. Then in Section 3.2 and Section 3.3, we present the details of our encoder and decoder with multi-stage hierarchies.  </p><formula xml:id="formula_0">{ % &amp; , % $ } { !"# &amp; , !"# $ } { # &amp; , # $ } Reconstructed</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-scale Masking</head><p>To build a U-Net <ref type="bibr" target="#b40">[41]</ref> like masked autoencoder for hierarchical learning, we encode the point cloud by S scales with different number of points at each scale, and correspondingly modify the standard plain encoder into the S-stage architecture. Following MAE, we embed the point cloud into discrete point tokens and randomly mask them for reconstruction. Importantly, for irregular-distributed points in the multi-scale architecture, the unmasked visible spatial regions are required to be consistent not only within one scale, but also across different scales. This is because the block-wise parts of 3D shapes tend to preserve more complete fine-grained geometries, and the unmasked positions are better to be shared across all scales for coherent feature learning of the encoder. Therefore, as shown in <ref type="figure">Figure 3</ref>, we first construct the S-scale coordinate representations of the input point cloud and back-project the random masks from the final S-th scale to the earlier scales to avoid fragmented visible parts.</p><p>S-scale Representations. We denote the input point cloud as P ? R N ?3 and regard it as the 0-th scale. For the i-th scale, 1 ? i ? S, we utilize Furthest Point Sampling (FPS) to downsample the points from the (i ? 1)-th scale, which produces seed points P i ? R Ni?3 for scale i of N i points. Then, we adopt k Nearest-Neighbour (k-NN) to aggregate the neighboring k points for each seed point and obtain the neighbor indices I i ? R Ni?k . By successively downsampling and grouping, we acquire the S-scale representations {P i , I i } S i=1 of the input point cloud, where the number of points N i gradually decreases and the inclusion relations between scales are recorded in I i .</p><p>Back-projecting Visible Positions. For seed points P S at the final S-th scale, we randomly mask them with a large proportion (e.g., 80%) and denote the remaining visible points as</p><formula xml:id="formula_1">P v S ? R N v S ?3</formula><p>of N S points. We then back-project the unmasked positions P v S to ensure the consistent visible regions across scales. For the i-th scale, 1 ? i &lt; S, we retrieve all the k nearest neighbors of P v i+1 from the indices I i+1 to serve as the visible positions P v i , and mask the others. By recursively back-projecting, we obtain the visible and masked positions of all S scales, denoted as</p><formula xml:id="formula_2">{P v i , P m i } S i=1 , where P v i ? R N v i ?3 , P m i ? R N m i ?3 and N i = N v i + N m i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Encoder</head><p>Based on the multi-scale masking, we embed the initial tokens of visible points P v 1 for the 1-st scale and them into the hierarchical encoder with S stages. Every stage is equipped with K stacked encoder blocks, and each block contains a self-attention layer and a Feed Forward Network (FFN) of MLP layers. Between every two consecutive stages, we introduce spatial token merging modules to aggregate adjacent visible tokens and enlarge receptive fields for downsampling the point clouds.</p><formula xml:id="formula_3">-th Scale 1-st Scale Input Point Cloud 2-nd Scale FPS, -NN Back-project FPS, -NN Back-project ? Mask at the S-th Scale ! " # " FPS, -NN { $ , $ } { ! , ! } { # , # } ! ? "</formula><p>$ " <ref type="figure">Figure 3</ref>: Multi-scale masking strategy. To obtain a consistent visible regions across scales, we first represent the input point cloud by multi-scale coordinates and generate the random mask at the highest one. Then, we back-project the unmasked visible positions to all earlier scales.</p><p>Token Embedding and Merging. Indexed by I 1 , we utilize a mini-PointNet <ref type="bibr" target="#b34">[35]</ref> to extract and fuse the features of every seed point from P v 1 ? R N v 1 ?3 with its k nearest neighbors. After that, we obtain the initial point tokens</p><formula xml:id="formula_4">T v 1 ? R N v 1 ?C1</formula><p>for the 1-st stage of the encoder, which embeds N e 1 local patterns of the 3D shape. Between the (i ? 1)-th and i-th stages, 1 &lt; i ? S, we merge T v i?1 ? R Ni?1?Ci?1 to acquire the downsampled point tokens for the i-th stage. We utilize MLP layers and a max pooling to integrate every k tokens nearest to P v i indexed by I i , which outputs T v i ? R Ni?Ci . Due to our multi-scale masking, the merged T v i corresponds to the same visible parts of T v i?1 , which enables the consistent feature encoding across different scales. For larger i of deeper stages, we set higher feature dimension C i to encode spatial geometries with richer semantics.</p><p>Local Spatial Self-Attention. During pre-training, we expect point tokens in the multi-stage encoder to capture global cues for 3D shapes, which benefits the reconstruction of masked parts. However, when fine-tuning on downstream tasks without masked autoencoding, point tokens in the shallower stages are better to mainly focus on local information and not to be disturbed by long-range signals, referring to the inductive bias of 3D locality <ref type="bibr" target="#b35">[36]</ref>. Thus, during fine-tuning, we modify the original self-attention layer in the encoder with a local spatial constraint that only neighboring tokens within a ball query would be available for attention calculation. As the point tokens are downsampled by stages, we set increasing radii {r i } S i=1 of multi-scale ball queries for gradually expanding the attention scopes, which fulfills the local-to-global feature aggregation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Decoder</head><p>Via the hierarchical encoder, we obtain the encoded visible tokens {T v i } S i=1 of all scales. Starting from the highest S-th scale, we assign a shared learnable mask token to all the masked positions P m S , and concatenate them with the visible tokens T v S . We denote them as</p><formula xml:id="formula_5">{H v 1 , H m 1 } with coordinates {P v S , P m S },</formula><p>which serve as the input of the hierarchical decoder. We design the decoder to be lightweight with S ? 1 stages and only one decoder block for each stage, which enforces the encoder to embed more semantics of the point clouds. Each decoder block consists of a vanilla self-attention layer and an FFN. We do not apply the local constraint to the attention in the decoder, since a global understanding between visible and mask tokens is crucial to the reconstruction.</p><p>Point Token Upsampling. We upsample the point tokens between stages to progressively recover the fine-grained geometries of 3D shapes before reconstruction. We regulate that the j-th stage of the decoder corresponds to the (S + 1 ? j)-th stage of the encoder, both of which contain point tokens of the same (S + 1 ? j)-th scale with the feature dimension C S+1?j . Between the (j ? 1)th and j-th stage, 1 &lt; j ? S ? 1, we upsample the tokens   <ref type="bibr" target="#b45">[46]</ref> 85.7 SO-Net <ref type="bibr" target="#b24">[25]</ref> 87.3 FoldingNet <ref type="bibr" target="#b56">[57]</ref> 88.4 MAP-VAE <ref type="bibr" target="#b18">[19]</ref> 88.4 VIP-GAN <ref type="bibr" target="#b17">[18]</ref> 90.2 DGCNN + Jiasaw <ref type="bibr" target="#b42">[43]</ref> 90.6 DGCNN + OcCo <ref type="bibr" target="#b48">[49]</ref> 90.7 DGCNN + CrossPoint <ref type="bibr" target="#b1">[2]</ref> 91.2</p><formula xml:id="formula_6">{H v j?1 , H m j?1 } from the coordinates {P v S+2?j , P m S+2?j } into {P v S+1?j , P m S+1?j } via</formula><p>Transformer + OcCo <ref type="bibr" target="#b59">[60]</ref> 89.6 Point-BERT <ref type="bibr" target="#b59">[60]</ref> 87.4 Point-M2AE 92.9 Improvement +1.7  </p><formula xml:id="formula_7">v j ? R N S+1?j ?C S+1?j of the decoder with T v S+1?j ? R N S+1?j ?C S+1?j from the corresponding (S + 1 ? j)-th</formula><formula xml:id="formula_8">P m 1 ? R N m 1 ?3 from the 2-nd scale P m 2 ? R N m 2 ?3</formula><p>. This is because {P v 1 , P m 1 } of the 1-st scale could well represent the overall 3D shape and simultaneously preserve enough local patterns, which already constructs a comparatively challenging pretext task for pre-training. If we further upsample</p><formula xml:id="formula_9">{H v S?1 , H m S?1 } into {H v S</formula><p>, H m S } and reconstruct the masked raw points from P m 1 , the extra spatial noises and computational overhead would adversely influence our performance and efficiency. Therefore, for every token in H m S?1 ? R N m 2 ?C2 , we reconstruct its k nearest neighbors recorded in I 2 by a reconstruction head of one linear projection layer and compute the loss by l 2 Chamfer Distance <ref type="bibr" target="#b14">[15]</ref>, formulated as,</p><formula xml:id="formula_10">P m 2?1 = Linear(H m S?1 ), where P m 2?1 ? R N m 2 ?k?3 ,<label>(1)</label></formula><formula xml:id="formula_11">L CD = ChamferDistance(P m 2?1 , P m 2?1 ),<label>(2)</label></formula><p>where P m 2?1 and P m 2?1 denote the predicted and ground-truth reconstruction coordinates from the 2-nd scale to the 1-st scale. We only utilize L CD for supervision without contrastive loss to conduct a pure masked autoencoding for self-supervised pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In Section 4.1 and Section 4.2, we introduce the pre-training experiments of Point-M2AE and report the fine-tuning performance on various downstream tasks. We also conduct ablation studies in Section 4.3 to validate the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Self-supervised Pre-training</head><p>Settings. We pre-train our Point-M2AE on ShapeNet <ref type="bibr" target="#b6">[7]</ref> dataset, which contains 57,448 synthetic 3D shapes of 55 categories. We set the stage number S as 3, and construct a 3-stage encoder and a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Downstream Tasks</head><p>For fine-tuning on downstream tasks, we discard the hierarchical decoder in pre-training and append different heads onto the hierarchical encoder for different tasks.</p><p>Shape Classification. We fine-tune Point-M2AE on two shape classification datasets: the widely adopted ModelNet40 <ref type="bibr" target="#b52">[53]</ref> and the challenging ScanObjectNN <ref type="bibr" target="#b44">[45]</ref>. For local spatial attention layers, we set the ball queries' radii of 3-scale point clouds as {0.32, 0.64, 1.28}. We follow Point-BERT to use the voting strategy <ref type="bibr" target="#b28">[29]</ref> for fair comparison on ModelNet40. To handle the noisy spatial structures, we increase k of k-NN into {32, 16, 16} for ScanObjectNN to encode local patterns with larger receptive fields. As reported in  <ref type="table" target="#tab_4">Table 3</ref>, our Point-M2AE outperforms the secondbest Point-BERT by a significant margin, +3.79%, +0.69% and +3.36%, respectively for the three splits, indicating our great advantages under complex circumstances by multi-scale encoding. As ScanObjectNN of real-world scenes has a large semantic gap with the pre-trained synthetic ShapeNet, Point-M2AE also exerts strong transfer ability to understand point clouds of another domain.</p><p>Part Segmentation. We evaluate Point-M2AE for part segmentation on ShapeNetPart <ref type="bibr" target="#b57">[58]</ref>, which predicts per-point part labels and requires detailed understanding for local patterns. We adopt an extremely simple segmentation head to validate the effectiveness of our pre-training for well capturing both high-level semantics and fine-grained details. By the hierarchical encoder, we obtain   Few-shot Classification. We conduct experiments for few-shot classification on ModelNet40 <ref type="bibr" target="#b52">[53]</ref> to evaluate the performance of Point-M2AE with limited fine-tuning data. As reported in <ref type="table" target="#tab_6">Table 4</ref>.2, Point-M2AE achieves the best performance for all four settings, and surpasses Point-BERT by +2.2%, +2.0%, +1.3%, and +2.7%, respectively. Our approach also shows smaller deviations than other transformer-based methods, which indicates Point-M2AE has learned to produce more universal 3D representations for well adapting to downstream tasks under low-data regimes.</p><p>3D Object Detection To further evaluate our hierarchical pre-training on 3D object detection, we apply Point-M2AE to serving as the feature backbone on the indoor ScanNetV2 <ref type="bibr" target="#b9">[10]</ref> dataset. We select 3DETR-m <ref type="bibr" target="#b31">[32]</ref> as our baseline, which consists of a 3-block encoder and a transformer decoder.</p><p>Considering the quite different dataset statistics, e.g., 2k input points for ShapeNet <ref type="bibr" target="#b6">[7]</ref> and 50k input points for ScanNetV2, we adopt the same encoder architecture with that of 3DETR-m, and keep our hierarchical decoder with skip connections unchanged for self-supervised pre-training on ScanNetV2. More details of models and training are in Appendix. As reported in <ref type="table" target="#tab_6">Table 4</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We conduct ablation study by modifying one of the components at a time during pre-training and explore the best masking strategy. We report the classification accuracy on ModelNet40 <ref type="bibr" target="#b52">[53]</ref> by linear SVM to evaluate the pre-trained representations. For downstream tasks, we train the network from scratch to validate the significance of our hierarchical pre-training.</p><p>Hierarchical Modules. As reported in <ref type="table" target="#tab_10">Table 7</ref>, on top of our final solution, Point-M2AE, in the first row, we respectively experiment with removing the hierarchical encoder, hierarchical decoder, and skip connections from our framework. Specifically, we replace our encoder and decoder with 1-stage plain architectures similar to MAE, which contains 15 and 2 vanilla transformer blocks, respectively. We observe the absence of multi-stage structures either in encoder or decoder hurts the performance, and the hierarchical encoder plays a better role than the decoder. Also, the skip connections well benefits the accuracy by providing complementary information for the decoder.</p><p>Masking Strategy. In <ref type="table" target="#tab_11">Table 8</ref>, we report Point-M2AE with different mask settings. Without the multi-scale masking, we randomly generate masks at each scale, which leads to fragmented visible regions for all scales. With this strategy, the network would 'peek' different parts of the point cloud at different stages, which disturbs the representation learning and harms the performance by -4.5% accuracy. For different mask ratios, we find the 80% ratio performs the best to build a properly challenging pretext task for self-supervised pre-training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Visualization</head><p>Multi-scale Masking. To ease the understanding of our multi-scale masking strategy, we visualize the input point cloud, the 3-scale representations, the reconstructed point cloud, and 3-scale masked point clouds, respectively in each row of <ref type="figure" target="#fig_4">Figure 5</ref>. As shown, different scales can represent different levels of geometric details and semantics for point clouds. By the multi-scale masking strategy, we observe the visible positions of masked point clouds are block-wise within one scale and consistent across scales, which is significant for our hierarchical pre-training.</p><p>Fine-grained Information. The fine-grained 3D structures, e.g., thin branches of a plant, fingers of a human, engines of a plane, are significant to distinguish similar shapes and can be well encoded by our hierarchical representations. In <ref type="figure" target="#fig_3">Figure 4</ref>, we compare our Point-M2AE with multi-stage, [H], and single-scale, [NH], architectures by visualizing their extracted point features and reconstructed point clouds during pre-training. In contarst to the single-scale network, the multi-scale one indicates higher feature responses in the fine-grained structures and reconstructs more accurate spatial details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose Point-M2AE, a multi-scale masked autoencoder for self-supervised pre-training on 3D point clouds. With a hierarchical architecture, Point-M2AE learns to produce powerful 3D representations by encoding multi-scale point clouds and reconstructing the masked coordinates from a global-to-local upsampling scheme. Extensive experiments have demonstrated the superiority of Point-M2AE to be a strong 3D representation learner. For limitations and future work, we will focus on applying Point-M2AE for wider 3D applications, e.g., outdoor and open-world scene understanding. We do not foresee negative social impact from the proposed work.   <ref type="figure">Figure 6</ref>: Pipeline of the multi-scale masking. We first obtain the multi-scale representation of input point clouds by FPS and k-NN. Then, we random mask the points at the highest level and back-project the visible positions into precedent scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Additional Related Work</head><p>Transformers. Transformers <ref type="bibr" target="#b47">[48]</ref> are first proposed in natural language processing to capture the inter-word relations in a long sentence, and have dominated most language tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b4">5]</ref>. Motivated by this, Vision Transformers <ref type="bibr" target="#b13">[14]</ref> and DETR <ref type="bibr" target="#b5">[6]</ref> introduce the transformer architecture into computer vision, and stimulate follow-up works to effectively apply transformers to a wide range of vision tasks, such as image classification <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, object detection <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b15">16]</ref>, semantic segmentation <ref type="bibr" target="#b53">[54]</ref> and so on <ref type="bibr" target="#b25">[26]</ref>. For 3D understanding, transformer-based networks are also adopted for shape classification, part segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b63">64]</ref>, 3D object detection from point clouds <ref type="bibr" target="#b31">[32]</ref> and monocular images <ref type="bibr" target="#b61">[62]</ref>. As a pioneer work, PCT <ref type="bibr" target="#b16">[17]</ref> utilizes neighbor embedding layers to aggregate local features and processes the downsampled point clouds by transformer blocks. PoinTr <ref type="bibr" target="#b58">[59]</ref> and Point-BERT <ref type="bibr" target="#b59">[60]</ref> divide point clouds into multiple spatial local patches and utilize standard transformers of plain architectures to encode the patches. On top of that, we propose Point-M2AE with a hierarchical encoder-decoder transformer, which is designed for MAE-style self-supervised point cloud pre-training and can well capture the multi-scale features of point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Implementation Details</head><p>Positional Encodings. To complement the 3D spatial information, we apply positional encodings to all attention layers in Point-M2AE. For point tokens T v i or {H m i , H v i } at stage i, we utilize a two-layer MLP to encode its corresponding 3D coordinates P v i or {P m i , P v i } into C i -channel vectors, and element-wisely add them with the token features before feeding into the attention layer.</p><p>Self-supervised Pre-training. Following previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b48">49]</ref>, we sample 2,048 points from each 3D shape in ShapeNet <ref type="bibr" target="#b6">[7]</ref> for pre-training Point-M2AE. We pre-train the network for 300 epochs with a batch size 128 and adopt AdamW <ref type="bibr" target="#b23">[24]</ref> as the optimizer. We set the initial learning rate and the weight decay as 10 ?4 and 5?10 ?2 , respectively, and adopt the cosine scheduler along with a 10-epoch warm-up. We utilize the common random scaling and random translation for data augmentation during pre-training. For linear SVM on ModelNet40 <ref type="bibr" target="#b52">[53]</ref>, after the hierarchical encoder, we use both max and average pooling to aggregate the features between point tokens, and sum the two pooled features as the encoded global feature of the point cloud.</p><p>Shape Classification. We fine-tune Point-M2AE on two datasets for shape classification. The widely adopted ModelNet40 <ref type="bibr" target="#b52">[53]</ref> consists synthetic 3D shapes of 40 categories, in which 9,843 samples are for training, and the other 2,468 are for validation. The challenging ScanObjectNN <ref type="bibr" target="#b44">[45]</ref> contains 11,416 training and 2,882 validation point clouds of 15 categories, which are captured from the noisy real-world scenes and thus have domain gaps with the pre-trained ShapeNet <ref type="bibr" target="#b6">[7]</ref> dataset. ScanObjectNN is divided into three splits for evaluation, OBJ-BG, OBJ-ONLY and PB-T50-RS, where PB-T50-RS is the most difficult for recognition. We respectively sample 1,024 and 2,048 points from each 3D shape of ModelNet40 and ScanObjectNN, and utilize only 3-channel coordinates  <ref type="figure">Figure 7</ref>: Learning curves of Point-M2AE with and without pre-training. We visualize the accuracy (Left) and loss curves (Right) on ModelNet40 <ref type="bibr" target="#b52">[53]</ref> and ScanObjectNN <ref type="bibr" target="#b44">[45]</ref>. We zoom in on the converged accuracy and loss for comparison.</p><p>Before Fine-tuning After Fine-tuning <ref type="figure">Figure 8</ref>: t-SNE <ref type="bibr" target="#b46">[47]</ref> visualization on ModelNet40 <ref type="bibr" target="#b52">[53]</ref>. We show the features distribution extracted by Point-M2AE before (Left) and after (After) the fine-tuning.</p><p>as inputs. The same training settings are adopted for the two datasets. We fine-tune the network for   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Additional Visualization</head><p>Multi-scale Masking Pipeline. In <ref type="figure">Figure 6</ref>, we show the simplified masking pipeline, which clearly illustrates how the mask is generated at the highest scale and back-projects to precedent layers.</p><p>Learning Curves. To compare the training with and without pre-training, we present their loss and accuracy curves on ModelNet40 <ref type="bibr" target="#b52">[53]</ref> and ScanObjectNN <ref type="bibr" target="#b44">[45]</ref>. As shown in <ref type="figure">Figure 7</ref>.2, the curves with pre-training converge faster and achieve higher classification accuracy than the curves without pre-training. This fully demonstrates the effectiveness of Point-M2AE's hierarchical pre-training. <ref type="bibr" target="#b46">[47]</ref>. In <ref type="figure">Figure 8</ref>, we visualize the features distribution extracted by Point-M2AE before and after fine-tuning on ModelNet40 <ref type="bibr" target="#b52">[53]</ref>. As shown, Point-M2AE right after pre-training can already produce discriminative features for different categories without fine-tuning. Then, the fine-tuning further clusters the features of the same category and separates those of different categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t-SNE</head><p>Local Spatial Attention. We visualize the attention weights with and without the local attention on ModelNet40 <ref type="bibr" target="#b52">[53]</ref> in <ref type="figure" target="#fig_7">Figure 9</ref>. As shown, with the local attention, the query point (marked by star) only has large attention values within a local spatial range (marked by yellow dotted circles), other than scattering over the entire 3D shape (marked by yellow arrows). This enables each point to concentrate more on neighboring local features in early stages for capturing detailed structures.</p><p>Part Segmentation Results. The fine-grained 3D patterns learned by our hierarchical architecture largely benefits 3D downstream tasks with dense prediction, e.g., part segmentation. In <ref type="figure" target="#fig_0">Figure 10</ref>, we compare our Point-M2AE with multi-stage, [H], and single-scale, [NH], architectures by visualizing the extracted point features and the segmentation results on ShapeNetPart <ref type="bibr" target="#b57">[58]</ref>. As shown, the multi-scale architecture predicts more fine-grained part labels for the objects.   <ref type="table" target="#tab_2">Table 11</ref>, we explore the best stage number of both encoder and decoder for learning multi-scale point cloud features during pre-training. As reported, the 3-stage encoder with 2-stage decoder performs the best. If the decoder also has three stages as the encoder, and reconstructs the point cloud at the 1-th scale, the performance would be adversely influenced.</p><p>Transformer Blocks. In each stage, we apply several transformer blocks to encode features of the point tokens. We experiment with different block numbers in each stage of the encoder and decoder in <ref type="table" target="#tab_2">Table 12</ref>. We observe that stacking five blocks per stage for encoder and only one block for decoder achieve the highest accuracy. This asymmetric architecture enforces the encoder to contain more semantic information of the point cloud, which benefits the transfer capacity of Point-M2AE. Fine-tuning Settings. For fine-tuning on downstream classification tasks, we obtain the global feature from point tokens by pooling, and apply a MLP-based head for classification. In <ref type="table" target="#tab_2">Table 10</ref>, we investigate different pooling operations along with the class token method to integrate features of all point tokens. Referring to <ref type="bibr" target="#b13">[14]</ref>, we concatenate a learnable class token with the point tokens at the 1st scale, and feed them into the hierarchical encoder. After encoding, we directly utilize this class token as the global feature for classification. As reported, 'max + ave. pooling' performs the best for fine-tuning, which is our default in all shape classification experiments. We also show the classification results without local spatial attention layers, which illustrates the significance of encoding local features with increasing receptive fields. Pre-training Loss Functions. Except for the Chamfer Distance loss <ref type="bibr" target="#b14">[15]</ref> with L2 normalization (L2-norm CD), we further evaluate the L1-normalized Chamfer Distance loss (L1-norm CD), Earth Mover's Distance loss (EMD), and their combinations. As shown in the table 13, the original L2-norm CD loss performs better than all other compared losses. We denote the reconstructed and ground-truth point sets as S 1 and S 2 . Compared to EMD loss that requires an optimal mapping for every point between S 1 and S 2 , L2-norm CD loss only optimizes the separate pair-wise distances and is thus more robust to the variation of 3D structures. Compared to L1-norm CD loss, L2 norm of Euclidean Distances can better depict spatial distribution and pay more attention to the far away points.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of MAE (Top) and our Point-M2AE (Bottom). MAE for 2D image pretraining adopts standard transformer of the plain encoder and decoder, while Point-M2AE introduces a hierarchical transformer with skip connections for multi-scale point cloud pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overall pipeline of Point-M2AE. After the multi-scale masking, we embed point tokens at the 1-st scale and feed the visible ones into a hierarchical encoder-decoder transformer, which captures both high-level semantics and fine-grained patterns of the point cloud during pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>stage of the encoder via skip connections, and adopt a linear projection layer to fuse their features. For the mask tokens H m j , we keep them unchanged, since the encoder only contains visible tokens without the masked ones. Point Reconstruction. After S ? 1 stages of the decoder, we acquire {H v S?1 , H m S?1 } with coordinates {P v 2 , P m 2 } and reconstruct the masked values from the mask tokens H m S?1 . Other than predicting values at the 0-th scale of the input point cloud P , we reconstruct the coordinates of P m 1 , namely, recovering the masked positions of the 1-st scale</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of fine-grained information. We denote the outputs from hierarchical and non-hierarchical architectures as [NH] and [H], respectively. For an input point cloud (Middle), we visualize its extracted features (Left) and reconstruction results (Right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of multi-scale point clouds. In each row, we visualize the input point clouds, their multi-scale representations, the reconstructed coordinates, and multi-scale masked point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of local spatial attention. We visualize the attention weights without (Top) and with (Bottom) local spatial attention. The query points are marked by stars. The attention scopes are marked by arrows and dotted circles in yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Visualization of part segmentation results. We denote the outputs from hierarchical and non-hierarchical architectures as [NH] and [H], respectively. For an input point cloud (Middle), we visualize its extracted features (Left) and part segmentation results (Right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>the token propagation module. Specifically, we obtain the k nearest neighbors of each point token in {H v j?1 , H m j?1 } indexed by I S+2?j , and recover their neighbors' features by weighted interpolation referring to PointNet++<ref type="bibr" target="#b35">[36]</ref>, which generates the tokens {H v</figDesc><table /><note>j , H m j } of the j-th stage.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Linear evaluation on Model-Net40<ref type="bibr" target="#b52">[53]</ref> by SVM. We report different self-supervised learning methods and underline the second-best one.</figDesc><table><row><cell>Method</cell><cell>Acc. (%)</cell></row><row><cell>3D-GAN [52]</cell><cell>83.3</cell></row><row><cell>Latent-GAN</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Shape classification on ModelNet40<ref type="bibr" target="#b52">[53]</ref>. '#points' and 'Acc.' denote the number of points for training and the overall accuracy. [S] represents finetuning after self-supervised pre-training.</figDesc><table><row><cell>Method</cell><cell cols="2">#points Acc. (%)</cell></row><row><cell>PointNet [35]</cell><cell>1k</cell><cell>89.2</cell></row><row><cell>PointNet++ [36]</cell><cell>1k</cell><cell>90.5</cell></row><row><cell>PointCNN [27]</cell><cell>1k</cell><cell>92.2</cell></row><row><cell>[S] SO-Net [25]</cell><cell>5k</cell><cell>92.5</cell></row><row><cell>DGCNN [50]</cell><cell>1k</cell><cell>92.9</cell></row><row><cell>PCT [17]</cell><cell>1k</cell><cell>93.2</cell></row><row><cell>Point Transformer [64]</cell><cell>-</cell><cell>93.7</cell></row><row><cell>Transformer [60]</cell><cell>1k</cell><cell>91.4</cell></row><row><cell>[S] Transformer + OcCo [60]</cell><cell>1k</cell><cell>92.1</cell></row><row><cell>[S] Point-BERT [60]</cell><cell>1k</cell><cell>93.2</cell></row><row><cell>[S] Point-BERT</cell><cell>4k</cell><cell>93.4</cell></row><row><cell>[S] Point-BERT</cell><cell>8k</cell><cell>93.8</cell></row><row><cell>[S] Point-M2AE</cell><cell>1k</cell><cell>94.0</cell></row></table><note>Skip Connections. To further complement the fine-grained geometries, we channel-wisely con- catenate the visible tokens H</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Shape classification on ScanObjectNN<ref type="bibr" target="#b44">[45]</ref>. We report the accuracy (%) on the three splits of ScanObjectNN. [S] represents fine-tuning after self-supervised pre-training.</figDesc><table><row><cell>Method</cell><cell cols="3">OBJ-BG OBJ-ONLY PB-T50-RS</cell></row><row><cell>PointNet [35]</cell><cell>73.3</cell><cell>79.2</cell><cell>68.0</cell></row><row><cell>PointNet++ [36]</cell><cell>82.3</cell><cell>84.3</cell><cell>77.9</cell></row><row><cell>DGCNN [50]</cell><cell>82.8</cell><cell>86.2</cell><cell>78.1</cell></row><row><cell>PointCNN [27]</cell><cell>86.1</cell><cell>85.5</cell><cell>78.5</cell></row><row><cell>Transformer [60]</cell><cell>79.86</cell><cell>80.55</cell><cell>77.24</cell></row><row><cell>[S] Transformer + OcCo [60]</cell><cell>84.85</cell><cell>85.54</cell><cell>78.79</cell></row><row><cell>[S] Point-BERT [60]</cell><cell>87.43</cell><cell>88.12</cell><cell>83.07</cell></row><row><cell>[S] Point-M2AE</cell><cell>91.22</cell><cell>88.81</cell><cell>86.43</cell></row><row><cell>Improvement</cell><cell>+3.79</cell><cell>+0.69</cell><cell>+3.36</cell></row></table><note>2-stage decoder for hierarchical learning. We adopt 5 blocks in each encoder stage, but only 1 block per stage for the lightweight decoder. For the 3-scale point clouds, we set the point numbers and token dimensions respectively as {512, 256, 64} and {96, 192, 384}. We also set different k for the k-NN at different scales, which are {16, 8, 8}. We mask the highest scale of point clouds with a high ratio of 80% and set 6 heads for all the attention modules. The detailed training settings are in Appendix. Linear SVM. After pre-training on ShapeNet, we test the 3D representation capability of Point- M2AE via linear evaluation on ModelNet40 [53]. We sample 1,024 points from each 3D shape of ModelNet40 and utilize our frozen encoder to extract their features. On top of that, we train a linear SVM and report the classification accuracy in Table 1. As shown, Point-M2AE achieves the best performance among all existing self-supervised methods for point clouds, and surpasses the second-best CrossPoint [2] by +1.7%. Point-M2AE also exceeds Point-BERT [60] by +5.5%, which is a masked point modeling method with a MoCo loss [21] but adopts a standard transformer and conducts single-scale learning. It is worth noting that even if we freeze all our parameters, Point-M2AE with 92.9% accuracy still outperforms many fully trained methods on ModelNet40, e.g., 90.5% by PointNet++ [36], 92.8% by DensePoint [28], etc. The experiments fully demonstrate the superior 3D representation capacity of our Point-M2AE.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 ,</head><label>2</label><figDesc>Point-M2AE achieves 94.0% accuracy on ModelNet40 with 1024 points per sample, which surpasses Point-BERT fine-tuned with 1024 points by +0.8% and 8192 points by +0.2%. For ScanObjectNN in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Few-shot classification on ModelNet40<ref type="bibr" target="#b52">[53]</ref>. We report the average accuracy (%) and standard deviation (%) of 10 independent experiments.</figDesc><table><row><cell>Method</cell><cell>5-way</cell><cell></cell><cell cols="2">10-way</cell></row><row><cell></cell><cell>10-shot</cell><cell>20-shot</cell><cell>10-shot</cell><cell>20-shot</cell></row><row><cell>DGCNN [50]</cell><cell cols="4">91.8 ? 3.7 93.4 ? 3.2 86.3 ? 6.2 90.9 ? 5.1</cell></row><row><cell>[S] DGCNN + OcCo [49]</cell><cell cols="4">91.9 ? 3.3 93.9 ? 3.1 86.4 ? 5.4 91.3 ? 4.6</cell></row><row><cell>Transformer [60]</cell><cell cols="4">87.8 ? 5.2 93.3 ? 4.3 84.6 ? 5.5 89.4 ? 6.3</cell></row><row><cell cols="5">[S] Transformer + OcCo [60] 94.0 ? 3.6 95.9 ? 2.3 89.4 ? 5.1 92.4 ? 4.6</cell></row><row><cell>[S] Point-BERT [60]</cell><cell cols="4">94.6 ? 3.1 96.3 ? 2.7 91.0 ? 5.4 92.7 ? 5.1</cell></row><row><cell>[S] Point-M2AE</cell><cell cols="4">96.8 ? 1.8 98.3 ? 1.4 92.3 ? 4.5 95.0 ? 3.0</cell></row><row><cell>Improvement</cell><cell>+2.2</cell><cell>+2.0</cell><cell>+1.3</cell><cell>+2.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Part segmentation on ShapeNetPart<ref type="bibr" target="#b57">[58]</ref>.</figDesc><table><row><cell cols="3">'mIoU C ' (%) and 'mIoU I ' (%) denote the mean IoU</cell></row><row><cell cols="3">across all part categories and all instances in the</cell></row><row><cell>dataset, respectively.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">mIoU C mIoU I</cell></row><row><cell>PointNet [35]</cell><cell>80.39</cell><cell>83.70</cell></row><row><cell>PointNet++ [36]</cell><cell>81.85</cell><cell>85.10</cell></row><row><cell>DGCNN [50]</cell><cell>82.33</cell><cell>85.20</cell></row><row><cell>Transformer [60]</cell><cell>83.42</cell><cell>85.10</cell></row><row><cell>[S] Transformer + OcCo [60]</cell><cell>83.42</cell><cell>85.10</cell></row><row><cell>[S] Point-BERT [60]</cell><cell>84.11</cell><cell>85.60</cell></row><row><cell>[S] Point-M2AE</cell><cell>84.86</cell><cell>86.51</cell></row><row><cell>Improvement</cell><cell>+0.75</cell><cell>+0.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>3D</figDesc><table><row><cell cols="3">object detection on Scan-</cell></row><row><cell cols="3">NetV2 [10]. We report the performance (%)</cell></row><row><cell cols="3">of self-supervised learning methods based</cell></row><row><cell cols="3">on VoteNet [13] and 3DETR-m [32].</cell></row><row><cell>Method</cell><cell cols="2">AP 25 AP 50</cell></row><row><cell>VoteNet [13]</cell><cell>58.6</cell><cell>33.5</cell></row><row><cell>[S] STRL [22]</cell><cell>59.5</cell><cell>38.4</cell></row><row><cell>[S] PointContrast [55]</cell><cell>59.2</cell><cell>38.0</cell></row><row><cell>[S] DepthContrast [63]</cell><cell>61.3</cell><cell>-</cell></row><row><cell>3DETR [32]</cell><cell>62.1</cell><cell>37.9</cell></row><row><cell>3DETR-m [32]</cell><cell>65.0</cell><cell>47.0</cell></row><row><cell>[S] Point-M2AE</cell><cell>66.3</cell><cell>48.3</cell></row><row><cell>Improvement</cell><cell>+1.3</cell><cell>+1.3</cell></row></table><note>3-scale point tokens of {512, 256, 64} points, and perform feature propagation in PointNet++ [36] to independently upsample the tokens into 2048 points of the input point cloud. Then, we concatenate the upsampled 3-scale features for each point and predict the part label by stacked linear projection layers. As reported in Table 4.2, Point-M2AE achieves the best 86.51% instance mIoU with the simple segmentation head, surpassing the second-best Point-BERT by +0.91%. Note that Point-BERT [60] and other methods [35, 36, 50] adopt hierarchical segmentation heads to progressively upsample the point features from intermediate layers, while our head contains no hierarchical structure and only relies on the pre-trained encoder to capture the multi-scale information of point clouds. The results fully demonstrate the significance of Point-M2AE's multi-scale pre-training to segmentation tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Hierarchical Modules. 'H' represents the encoder and decoder with multi-stage hierarchies. 'Skip C.' denotes the skip connections.</figDesc><table><row><cell cols="4">Encoder Decoder Skip C. Acc. (%)</cell></row><row><cell>H</cell><cell>H</cell><cell></cell><cell>92.9</cell></row><row><cell>-</cell><cell>-</cell><cell></cell><cell>90.7</cell></row><row><cell>-</cell><cell>H</cell><cell></cell><cell>91.5</cell></row><row><cell>H</cell><cell>-</cell><cell></cell><cell>92.2</cell></row><row><cell>H</cell><cell>H</cell><cell>-</cell><cell>92.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Different Masking Strategy.</figDesc><table><row><cell>'MS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>With and without the pre-training.</figDesc><table><row><cell>With and Without Pre-training. We report the performance of Point-M2AE on downstream tasks with and without the pre-training in Ta-</cell><cell cols="3">'ModelNet40-FS' denotes the few-shot classifica-tion on 10-way 20-shot ModelNet40 [53].</cell></row><row><cell>ble 9. For 'w/o', we randomly initialize the parameters and train the network from scratch.</cell><cell>Dataset</cell><cell cols="2">w/o (%) w (%)</cell></row><row><cell>As shown, the pre-training can largely boost</cell><cell>ModelNet40 [53]</cell><cell>92.5</cell><cell>94.0</cell></row><row><cell>the performance on four datasets respectively by +1.5%, +2.5%, +3.8%, and +1.1%, which indicates the superiority and significance of our hierarchical pre-training.</cell><cell>ScanObjectNN [45] ModelNet40-FS [53] ShapeNetPart [58]</cell><cell>83.9 91.2 85.4</cell><cell>86.4 95.0 86.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Transformer stages. We experiment different stage number of the hierarchical encoder and decoder in Point-M2AE.</figDesc><table><row><cell>Encoder</cell><cell>Decoder</cell><cell>Acc. (%)</cell></row><row><cell>3</cell><cell>2</cell><cell>92.9</cell></row><row><cell>2</cell><cell>1</cell><cell>91.8</cell></row><row><cell>4</cell><cell>3</cell><cell>90.4</cell></row><row><cell>3</cell><cell>3</cell><cell>90.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Transformer blocks. Based on the 3-stage encoder and 2-stage decoder, we experiment different block numbers per stage. Each stage in Point-M2AE encodes the corresponding scale of the point cloud. In</figDesc><table><row><cell>Encoder</cell><cell>Decoder</cell><cell>Acc. (%)</cell></row><row><cell>5</cell><cell>1</cell><cell>92.9</cell></row><row><cell>4</cell><cell>1</cell><cell>92.7</cell></row><row><cell>3</cell><cell>1</cell><cell>92.6</cell></row><row><cell>5</cell><cell>2</cell><cell>91.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Fine-tuning settings. For 'max + ave. pooling', we adopt max and average pooling to obtain two global features and sum them as the input of classification head. 'w/o Local Atten.' denotes vanilla global self-attention.</figDesc><table><row><cell>Settings</cell><cell cols="2">ModelNet40 ScanObjectNN</cell></row><row><cell>max pooling</cell><cell>93.3</cell><cell>85.98</cell></row><row><cell>average pooling</cell><cell>92.8</cell><cell>85.66</cell></row><row><cell>max + ave. pooling</cell><cell>94.0</cell><cell>86.43</cell></row><row><cell>class token</cell><cell>93.4</cell><cell>86.02</cell></row><row><cell>w/o Local Atten.</cell><cell>93.5</cell><cell>85.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>Pre-training losses. 'CD' and 'EMD' denote Chamfer Distance and Earth Mover's Distance losses.</figDesc><table><row><cell cols="4">L2-norm CD L1-norm CD EMD Acc. (%)</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>92.9</cell></row><row><cell>-</cell><cell></cell><cell>-</cell><cell>91.1</cell></row><row><cell>-</cell><cell>-</cell><cell></cell><cell>91.9</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>92.4</cell></row><row><cell>-</cell><cell></cell><cell></cell><cell>91.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work is supported by the National Natural Science Foundation of China (Grant No. 62206272), Shanghai Committee of Science and Technology (Grant No. 21DZ1100100), Centre for Perceptual and Interactive Intelligence Limited, and the General Research Fund through the Research Grants Council of Hong Kong (Grant No. 14204021, 14207319). 300 epochs with a batch size 32, and set the learning rate as 5?10 ?4 with a weight decay 5?10 ?2 . For other training hyper-parameters, we keep them the same as the pre-training experiment.</p><p>Part Segmentation. ShapeNetPart <ref type="bibr" target="#b57">[58]</ref> contains 16,881 synthetic 3D shapes of 16 object categories and 50 part categories, where 14,007 and 2,874 samples are respectively for training and validation. We sample 2,048 points from each shape as inputs, and predict the part categories for all points. We fine-tune Point-M2AE for 300 epochs with a batch size 16 and set the learning rate as 2?10 ?4 with a weight decay 0.1. Other training settings are the same as the shape classification experiments.</p><p>Few-shot Classification. We follow previous works <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b48">49]</ref>, to adopt the "K-way N-shot" settings on ModelNet40 <ref type="bibr" target="#b52">[53]</ref> for few-shot classification. We randomly select K out of 40 classes and sample N +20 3D shapes per class, N for training and 20 for testing. We evaluate Point-M2AE on four few-shot settings: 5-way 10-shot, 5-way 20-shot, 10-way 10-shot, and 10-way 20-shot. To alleviate the variance of random sampling, we conduct 10 independent runs for each few-shot setting and report the average accuracy and standard deviation. We adopt the same training settings as shape classification experiments but only fine-tune Point-M2AE for 150 epochs.</p><p>3D Object Detection. We pre-train and fine-tune Point-MAE for 3D object detection both on ScanNetV2 <ref type="bibr" target="#b9">[10]</ref>. The dataset contains 1,513 scanned indoor scenes with axis-aligned 3D bounding boxes for 18 categories, 1,201 for training and 312 for validation. As we adopt the same encoder architecture in 3DETR-m <ref type="bibr" target="#b31">[32]</ref> with 2 stages, we set the stage number of decoder as 1, which accords with the regulation of S-stage encoder and (S ? 1)-stage decoder. We pre-train Point-M2AE for 1,080 epochs with the learning rate 5?10 ?4 , and follow other hyper-parameters in the experiment of pre-training on ShapeNet <ref type="bibr" target="#b6">[7]</ref>. For fine-tuning, we adopt the same settings as training 3DETR-m from scratch in the original paper <ref type="bibr" target="#b31">[32]</ref> for fair comparison.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised learning for domain adaptation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Idan Achituve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chechik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Crosspoint: Self-supervised cross-modal contrastive learning for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Afham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isuru</forename><surname>Dissanayake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinithi</forename><surname>Dissanayake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaya</forename><surname>Dharmasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanchana</forename><surname>Thilakarathna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranga</forename><surname>Rodrigo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.00680</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Data2vec: A general framework for self-supervised learning in speech, vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03555</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1877" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Votenet: A deep learning label fusion method for multi-atlas segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Niethammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="202" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast convergence of detr with spatially modulated co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<title level="m">Pct: Point cloud transformer. Computational Visual Media</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">View inter-prediction gan: Unsupervised representation learning for 3d shapes by learning global shape memories to support local view predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8376" to="8384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-angle point cloudvae: Unsupervised feature learning for 3d point clouds from multiple angles by joint selfreconstruction and half-to-half prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10441" to="10450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatio-temporal self-supervised representation learning for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6535" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno>PMLR, 2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Uniformer: Unifying convolution and self-attention for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09450,2022.15</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5239" to="5248" />
		</imprint>
	</monogr>
	<note>Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
	<note>Bin Fan, Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dual-stream network for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An end-to-end transformer model for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Masked autoencoders for point cloud self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatian</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06604</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-supervised learning of point clouds via orientation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nayun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1018" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR, 2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolfe</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Discrete variational autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-supervised deep learning on point clouds by reconstructing space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Sauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarne</forename><surname>Sievers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-supervised deep learning on point clouds by reconstructing space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Sauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarne</forename><surname>Sievers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR, 2021. 15</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikaela</forename><forename type="middle">Angelina</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning localized representations of point clouds with graph-convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="402" to="414" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised point cloud pre-training via occlusion completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Masked feature prediction for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09133</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="1912" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pointcontrast: Unsupervised pre-training for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="574" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09886</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pointr: Diverse point cloud completion with geometry-aware transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xumin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Pointbert: Pre-training 3d point cloud transformers with masked point modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xumin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lulu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14819</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xupeng</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02413</idno>
		<title level="m">Pointclip: Point cloud understanding by clip</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanzhuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.13310,2022.15</idno>
		<title level="m">Monodetr: Depth-aware transformer for monocular 3d object detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Self-supervised pretraining of 3d features on any point-cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="10252" to="10263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">End-to-end object detection with adaptive clustering transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09315,2020.15</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07832</idno>
		<title level="m">ibot: Image bert pre-training with online tokenizer</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159,2020.15</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
