<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AUDIO CAPTIONING USING GATED RECURRENT UNITS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ay?eg?l?zkaya</forename><surname>Eren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Ba?kent University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Sert</surname></persName>
							<email>msert@baskent.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Ba?kent University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AUDIO CAPTIONING USING GATED RECURRENT UNITS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-audio captioning</term>
					<term>GRU</term>
					<term>BiGRU</term>
					<term>VG- Gish</term>
					<term>Word2Vec</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Audio captioning is a recently proposed task for automatically generating a textual description of a given audio clip. In this study, a novel deep network architecture with audio embeddings is presented to predict audio captions. Within the aim of extracting audio features in addition to log Mel energies, VGGish audio embedding model is used to explore the usability of audio embeddings in the audio captioning task. The proposed architecture encodes audio and text input modalities separately and combines them before the decoding stage. Audio encoding is conducted through Bi-directional Gated Recurrent Unit (BiGRU) while GRU is used for the text encoding phase. Following this, we evaluate our model by means of the newly published audio captioning performance dataset, namely Clotho, to compare the experimental results with the literature. Our experimental results show that the proposed BiGRU-based deep model outperforms the state of the art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Audio captioning is a newly proposed task to describe the content of an audio clip using natural language sentences <ref type="bibr" target="#b0">[1]</ref>. The purpose of creating captions is not only finding the objects, events, or scenes in the given audio clip but also finding relations between them and generating meaningful sentences. It has great potential for real-life applications such as assisting hearing impaired people and understanding environmental sounds. Additionally, since smart audio-based and video surveillance systems use audio data, audio signal analysis is a critical research area for surveillance systems. These systems can be used for recognizing activities, detecting events, anomalies, and finding semantic relations between video and audio for child-care centers, nursing homes, smart cities, elevators, etc. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>In the field of audio signal processing, a number of tasks, such as audio event classification/detection <ref type="bibr" target="#b5">[6]</ref>, acoustic scene recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, and audio tagging <ref type="bibr" target="#b8">[9]</ref> have received much attention over the past few years. In the audio event detection task, the main aim is to identify (overlapping) sound events <ref type="figure">Fig. 1</ref>: A sample scene from audio-enabled video surveillance <ref type="bibr" target="#b4">[5]</ref>. Without audio, there are only bicycles on the scene. With audio, bus and traffic noise are also captured.</p><p>occurring in the audio clip along with their starting and ending times. The audio tagging task assigns predefined labels to a given audio segment, whereas the acoustic scene recognition task concerns the understanding of the acoustics of the environment and assign labels to it. However, audio captioning is quite a higher level of abstraction of these tasks in the sense of generating descriptive sentences in a natural language. In audio-enabled video surveillance systems, these sentences can be used for the understanding of video scenes and possible abnormality detection within them, as well as indexing and retrieval of video ( <ref type="figure">Figure 1</ref>).</p><p>Captioning is firstly used for describing images and numerous studies have been conducted <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. This is followed by the video captioning task, which aims to generate captions for video clips <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Audio captioning task is first described in <ref type="bibr" target="#b0">[1]</ref>. Drossos et al. propose an encoderdecoder model with three BiGRU (Bi-directional Gated Recurrent Unit) layers in the encoder and two GRU (Gated Recurrent Unit) layers in the decoder to generate audio captions by means of an attention mechanism. They use log Mel energies as audio features and a commercial dataset ProSound Effects <ref type="bibr" target="#b13">[14]</ref> in their experiments. Wu et al. <ref type="bibr" target="#b14">[15]</ref> present another attempt in the field of audio captioning. Their model is an encoder-decoder model with one GRU layer in the encoder and one GRU layer in the decoder. Also, they introduce a new audio captioning dataset for the Chinese language. An encoder-decoder model with semantic attention for generating captions for audios in the wild is presented by Kim et al. and they contribute a large scale dataset AudioCaps of 46K audio clips <ref type="bibr" target="#b15">[16]</ref>. Drossos et al. newly introduce a pub-licly available audio captioning dataset called Clotho <ref type="bibr" target="#b16">[17]</ref> and present the results with the method in <ref type="bibr" target="#b0">[1]</ref>.</p><p>Our motivation is proposing a new deep network using semantic information to improve audio captioning performance. To address this problem, we propose a novel model using VG-Gish <ref type="bibr" target="#b17">[18]</ref> for audio embedding and Word2Vec <ref type="bibr" target="#b18">[19]</ref> for word embedding since their performance is shown in audio classification <ref type="bibr" target="#b19">[20]</ref>. The core contributions of our study are as follows:</p><p>? We propose a novel model specifically designed for the audio captioning task, which encodes audio and text separately. We combine these features and decode them in the GRU layer.</p><p>? The VGGish model demonstrates superior performances in the audio classification tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>. We conduct extensive experiments to demonstrate the performance of these models in the audio captioning task.</p><p>? The usability of the word embedding in the audio captioning task, a well-known word embedding model Word2Vec is explored.</p><p>? Our model performs state-of-the-art results on the newly published audio captioning dataset Clotho.</p><p>The organization of the paper is as follows. Section 2 introduces our proposed method. We present our experimental results and evaluations in Section 3. Finally, we give concluding remarks and possible future directions in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED METHOD</head><p>Our main aim is to generate meaningful captions for a given audio clip. Mathematically:</p><formula xml:id="formula_0">? = argmax ? A,c logp(c|A; ?)<label>(1)</label></formula><p>We aim to maximize the probability of the caption c for a given audio clip A according to model parameters ?. Since captions are vectors of words, c refers to the caption of the given audio record.</p><formula xml:id="formula_1">logp(c|A) = N t=0 logp(c t |A, c 0 , ..., c t?1 )<label>(2)</label></formula><p>where, N is the length of the caption and c 0 to c t?1 is the words in the given caption.</p><p>The overall structure of our proposed model is given in <ref type="figure" target="#fig_0">Figure 2</ref>. The overall architecture consists of three modules: The audio embedding extractor, the Word2Vec word embedding extractor, and sequence modeling, which is based on RNN-GRU encoder-decoder (RNN-GRU-EncDec). The details of these modules are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Audio Feature Embedding</head><p>We use the VGGish model to extract audio features. VG-Gish model is pre-trained on the AudioSet <ref type="bibr" target="#b4">[5]</ref>. The AudioSet is a large-scale audio event dataset and contains 2,084,320 human-labeled 10-second sound clips representing 632 audio event classes.</p><p>Previous studies show that VGGish embeddings achieve good results compared with hand-crafted audio features in audio classification tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. In order to extract audio embedding, we first extract log Mel spectrograms from audio clips. The length of the clips varies between 15 to 30 seconds. Since the length of the longest audio record is 30 seconds, we apply zero-padding to the audio records which are shorter than 30 seconds. We resample them to 16 Khz. We choose window-size of 96 milliseconds (ms) with 50% overlap. We set the number of Mel filters to 64 similar to <ref type="bibr" target="#b16">[17]</ref> and frequency band to 125-7500 Hz. VGGish model extracts 128-dimensional feature vector for each second. After applying VGGish model, we obtain audio features denoted as X = [x 1 , ..., x T ], x t ? R 128 , where x t is a vector that contains 128 features of the audio clip and T is the number of audio frames according to 96 ms window-sizes and 50% overlaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Word Embedding</head><p>We extract word embedding using the Word2Vec model due to its superiority compared with the one-hot-encoding <ref type="bibr" target="#b18">[19]</ref>. We train the Word2Vec model using the captions in Clotho development split. As a result, we generate E = [e 1 , ..., e i ] to represent each word vector in the dataset vocabulary, where e i ? R 256 , 256 is the feature dimension of word embeddings for each word. We use this pre-trained embedding to initialize weights in the embedding layer of our model. It is not used in the testing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Encoder</head><p>The encoding stage consists of two parts: encoding audio and encoding text. We use GRU to learn dependencies between audio frames in a given audio clip and sequences of words in captions since it reduces the number of parameters in the model <ref type="bibr" target="#b21">[22]</ref>. The GRU reads whole sequence and produces one output. A simple GRU model is given as:  where z t is the update gate at time step t, x t is the input for time step t. W represents the weights, ? is the sigmoid function, and h t is the hidden state in time step t.</p><formula xml:id="formula_2">z t = ?(W z .([h t?1 , x t ]))<label>(3)</label></formula><formula xml:id="formula_3">r t = ?(W r .([h t?1 , x t ]))<label>(4)</label></formula><formula xml:id="formula_4">h t = tanh(W.([r t * h t?1 , x t ]))<label>(5)</label></formula><formula xml:id="formula_5">h t = (1 ? z t ) * h t?1 + z t * ? t<label>(6)</label></formula><p>Unlike feed-forward GRU, BiGRU is able to capture information not only from the past and the current state but the sequence is also reversed in time. Since an audio clip is composed as temporal sequences of frames, we use BiGRU to learn the relationship between audio time steps. We use two BiGRU layers in our design. In the encoding stage of our model, the first BiGRU layer has 32 cells and second has 64 cells, which are selected empirically. For text encoding, Word2Vec model weights are used to initialize our model's word embedding layer. This embedding is given to the first GRU layer which has 128 cells. This GRU is used to learn word sequences. In order to combine encoded audio and text, we use the addition method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Decoder</head><p>We design the decoder with a single GRU layer consisting of 128 cells. Its inputs combined feature vector from the encoder and outputs the next predicted word. We use the Sof tmax after the fully connected layer. The decoder performs the prediction word by word and a sequence of the predicted words gives the caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We conduct our experiments on the newly published dataset Clotho <ref type="bibr" target="#b16">[17]</ref>. The development and the evaluation sets of the dataset contain 2893 and 1043 audio clips, respectively. Both of the sets have 5 captions for each audio clip. The lengths of the audio clips is 15 to 30 seconds in duration and captions are 8 to 20 words. We split the development dataset into two parts which are training and validation. 2000 audio records are selected randomly for training and the remaining part is used for validation. We use each audio clip five times with one assigned caption from the caption-list based on the best practice in <ref type="bibr" target="#b16">[17]</ref>. For instance, let a i is an individual audio clip with captions S = [s 1 , s 2 , .., s 5 ], then we use this audio clip instance as 5 separate instances: &lt; a i , s 1 &gt;, &lt; a i , s 2 &gt;, .., &lt; a i , s 5 &gt; in the training. To find start and end of the sequences of captions, we add special &lt; sos &gt; and &lt; eos &gt; in the beginning and end of the captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Details</head><p>The proposed model has approximately 2,000,000 parameters. Adam optimizer and LeakyRelu activation function are used in the training. Batch-size is set to 64. We use a dropout  <ref type="bibr" target="#b22">[23]</ref> is used after each BiGRU and GRU layer in the encoding and decoding phases. Loss function is categorical-cross entropy since it is widely used in the literature <ref type="bibr" target="#b23">[24]</ref>. It is given by</p><formula xml:id="formula_6">L(?) = ? T t=1 log(p ? (w t |w 1 , ..., w t?1 ) (7)</formula><p>where w t is the predicted word based on previous words. To prevent gradient vanishing problem, LeakyReLU activation function is chosen empirically, where the LeakyReLU is given by,</p><formula xml:id="formula_7">lrelu(x) = x x&gt;0 ? x?0 (8)</formula><p>where ? is chosen 0.3 in this study which is the default value of LeakyReLU in Keras <ref type="bibr" target="#b24">[25]</ref>. It uses small gradient when the cell is not active.</p><p>The final hyperparameters such as the batch-size, dropout rate, and activation functions used in the study are chosen based on minimum validation loss in our several experiments. We implemented the system using Keras framework and run on a computer with GPU GTX1660Ti in a system Linux Ubuntu 18.04 and Python 3.6. The model is run for 50 epochs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation</head><p>We perform our evaluations on the public performance dataset Clotho and compare our results with the method introduced with the Clotho <ref type="bibr" target="#b14">[15]</ref>. We evaluate our experiments with widely used metrics in machine translation tasks and also used in the Clotho. To this aim, we use the BLEU <ref type="bibr" target="#b25">[26]</ref>, ME-TEOR <ref type="bibr" target="#b26">[27]</ref>, CIDEr <ref type="bibr" target="#b27">[28]</ref>, and ROUGEL <ref type="bibr" target="#b28">[29]</ref> metrics for the evaluations.</p><p>The metric BLEU n calculates the precision for n-grams. To calculate precision, the matching words in the actual sentence and the predicted sentence is calculated. BLEU does not consider the context of the word in the sentence. The metric range is between [0,1]. If the actual sentence and the predicted sentence is totally the same, then the score is 1. BLEU-1 (B-1) represents 1-gram, whereas BLEU-4 represents 4grams. METEOR calculates recall and precision together and takes a harmonic mean score. It creates an alignment between actual and predicted sentences and makes mapping between them. CIDEr also uses n-gram model and it calculates cosinesimilarity between the actual and predicted sentences. It also considers the Term Frequency Inverse-Document Frequency. ROUGE L calculates Longest Common Subsequences which considers the sequence of the words in the actual and predicted sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results</head><p>We compare our results with the method in the Clotho dataset. To show Word2Vec contribution in our model, we present our results with and without Word2Vec embedding. Our experimental results are presented in <ref type="table" target="#tab_1">Table 1</ref>. The results show that our model outperforms the state-of-the-art.</p><p>The results show that our proposed model with log-Mel features has better results than the literature. The proposed model with VGGish embeddings provides better results than log-Mel features. Additionally, VGGish provide better training performance in terms of time and memory usage.</p><p>The predicted sentences show that our model can generally predict the main content of the audio clip. For instance, our model predicts "People are talking and laughing" whereas the ground truth is "People are talking and laughing with loud person near the end". It predicts sentence in correct order but shorter than the ground truth. In our proposed model, similar concepts are also predicted. To illustrate, our model predicts "Rain is falling heavily and thunder is booming" while the ground truth is "Passing windstorm outside and something is striking against another harder object". Actually they are similar concepts but according to BLEU, it is not assessed as a successful instance because the metric is based on calculating precision on exactly the same words. As another example, our model predicts the caption as "Bicycle is coasting down road slowly" whereas the ground truth is "The engine of vehicle is driving down the road". In this example, our model does not differentiate the bicycle and engine sounds. Some other predicted captions are given below to show our model's performance. Actual-1:Busy restaurant with people eating during rush hour Actual-2:Crowd of people are walking and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper, we present a novel model that combines text and audio features to predict audio captions. The proposed model uses VGGish audio embeddings that provide us semantically embedding and smaller feature dimension than raw audio features and log-Mel band energies. It brings us better training performance. Word2Vec is used to extract word embedding and results show that semantic information can improve audio captioning performance. A novel encoder-decoder model is designed with multiple BiGRU and GRU layers. We evaluate our study in a newly published public dataset called Clotho. Experiments show that our proposed method yields better performance than the state-of-the-art studies.</p><p>The results show that our model is able to predict audio captions. The predicted captions are more general and shorter than the actual truths. The proposed model does not differentiate similar sounds. It can explicitly be stated that we can obtain better results if we have a larger dataset and train it for more epochs. Furthermore, the model generally predicts similar-structured sentences. Improving the language model and adding semantic information can increase the performance.</p><p>According to these results, our future research direction is to strive for improving language modeling and to use data augmentation techniques in an attempt of enhancing the performance of our model. Getting better results on audio captioning can yield improvement in audio analysis. Additionally, multimodal models can be researched to improve the performance of video applications such as video captioning, video retrieval, and surveillance systems which are mainly composed of audio and video analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>(a) Extracting encoded audio feature using VGGish Embeddings and BiGRUs. (b) Extraction of Word2Vec embedding to initialize weights in the embedding layer of text encoder. (c) The model merges encoded audio and encoded text, the decoder decodes given features to predict next word in the given caption. All the time frames share same weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The proposed method loss-validation loss plot rate of 0.5 for input connections. Batch normalization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>In the experiments, 1 epoch with Mel-energy features takes approximately 4 hours whereas 1 epoch with the VGGish model takes approximately 15 minutes according to the given configurations. The minimum validation error is obtained in the 30th epoch for the VGGish model given inFigure 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of the proposed method. RNN-GRU-EncDec is the proposed encoder-decoder based architecture for sequence modeling. (BLEU-1: B-1, BLEU-2: B-2, BLEU-3: B-3, BLEU-4: B-4)</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated audio captioning with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Drossos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharath</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Virtanen</surname></persName>
		</author>
		<ptr target="abs/1706.10006" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Audio surveillance: a systematic review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Crocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Trucco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<idno>abs/1409.7787</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Environmental audio scene and sound event recognition for autonomous surveillance: A survey and comparative studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandrakala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Jayalakshmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive edge and fog computing paradigm for wide area video and audio surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Ledakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanassis</forename><surname>Bouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Kioumourtzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Skitsas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 9th International Conference on Information, Intelligence, Systems and Applications (IISA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Audio event detection from acoustic unit occurrence patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranay</forename><surname>Dighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="489" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tut database for acoustic scene classification and sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annamaria</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 24th European Signal Processing Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1128" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining acoustic and semantic similarity for acoustic scene retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Sert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><forename type="middle">Melih</forename><surname>Ba?bug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="156" to="1563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A joint detection-classification model for audio tagging of weakly labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno>abs/1610.01797</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1502.03044</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Describing multimedia content using attention-based encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1507.01053</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multistream hierarchical boundary network for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shagan</forename><surname>Sah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ptucha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Western New York Image and Signal Processing Workshop</title>
		<imprint>
			<publisher>WNYISPW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno>abs/1804.00819</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Next level libraries licensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prosoundeffects</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Audio caption: Listen and tell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Dinkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">AudioCaps: Generating captions for audios in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris Dongjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="119" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clotho: an audio captioning dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Drossos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Lipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jort</forename><forename type="middle">F</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Ron J. Weiss, and Kevin Wilson</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1310.4546</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Zero-shot audio classification based on class label embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Acoustic Scene Classification Using Spatial Pyramid Pooling with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><forename type="middle">M</forename><surname>Basbug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Sert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -13th IEEE ICSC 2019</title>
		<meeting>-13th IEEE ICSC 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="128" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A critical review of recurrent neural networks for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Chase Lipton</surname></persName>
		</author>
		<idno>abs/1506.00019</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What is the role of recurrent neural networks (RNNs) in an image caption generator?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Camilleri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
		<meeting>the 10th International Conference on Natural Language Generation<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
		<idno>abs/1411.5726</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
	<note>Text Summarization Branches Out</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
