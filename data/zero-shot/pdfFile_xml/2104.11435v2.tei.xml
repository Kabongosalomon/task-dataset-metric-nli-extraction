<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TricubeNet: 2D Kernel-Based Object Representation for Weakly-Occluded Oriented Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomyoung</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghyeon</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihaeng</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">Doyeon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naver</forename><surname>Clova</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Lg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaist</surname></persName>
						</author>
						<title level="a" type="main">TricubeNet: 2D Kernel-Based Object Representation for Weakly-Occluded Oriented Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel approach for oriented object detection, named TricubeNet, which localizes oriented objects using visual cues (i.e., heatmap) instead of oriented box offsets regression. We represent each object as a 2D Tricube kernel and extract bounding boxes using simple image-processing algorithms. Our approach is able to (1) obtain well-arranged boxes from visual cues, (2) solve the angle discontinuity problem, and (3) can save computational complexity due to our anchor-free modeling. To further boost the performance, we propose some effective techniques for size-invariant loss, reducing false detections, extracting rotation-invariant features, and heatmap refinement. To demonstrate the effectiveness of our TricubeNet, we experiment on various tasks for weakly-occluded oriented object detection: detection in an aerial image, densely packed object image, and text image. The extensive experimental results show that our TricubeNet is quite effective for oriented object detection. Code is available at https: //github.com/qjadud1994/TricubeNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is one of the fundamental computer vision tasks, and deep learning-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref> have shown remarkable performance. However, existing detectors often focus on detecting a horizontal bounding box that is not sufficient in some cases. First, for densely arranged oriented objects, an intersection-over-union (IOU) between adjacent horizontal bounding boxes tends to be large, and some of these boxes will be filtered out by non-maximum suppression (NMS). Second, since the horizontal bounding box can contain many redundant areas, it is not suitable for real-world applications that require tighter and more accurate boxes, such as aerial images and scene text images. To detect the object in a more accurate form, oriented object detection has attracted much attention recently. ? This work was done when the author worked at KAIST.  <ref type="figure">Figure 1</ref>. Existing anchor-based methods that regress five or eight offsets require a huge number of anchor boxes (about 4M) and infer a bit scatty boxes. We represent each object as a 2D Tricube kernel and extract bounding boxes using simple image-processing algorithms. Our approach, named TricubeNet, does not require the anchor boxes and can obtain well-arranged boxes.</p><p>Most oriented object detectors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref> adopt Faster R-CNN <ref type="bibr" target="#b31">[32]</ref> or RetinaNet <ref type="bibr" target="#b20">[21]</ref> as their baseline model and additionally infer an angle of the object. They adopt anchor augmentation strategy and regress oriented box offsets in form of 5-offsets (x, y, w, h, ?) or 8-offsets (x1, y1, x2, y2, x3, y3, x4, y4). They have reigned on the throne with state-of-the-art performance, however, some limitations remain. (1) Regressing the box offsets might have trouble in obtaining well-arranged boxes of densely arranged oriented objects as in <ref type="figure">Figure 1;</ref> (2) Regressing the angle offset causes angle discontinuity problem; the angle discontinuity on the boundary leads to the loss fluctuation during training;</p><p>(3) They require huge computational complexity due to the anchor augmentation and a heavy IoU calculation for the oriented box. For example, when they define anchor boxes with three scales, five aspect ratios, and six angles and adopt FPN <ref type="bibr" target="#b19">[20]</ref> architecture with 800 ? 800 input resolution, they require about 4M total anchor boxes (3 ? 5 ? 6 = 90 anchor boxes per a pixel location). Some might argue that anchorfree approaches such as <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b47">48]</ref> can reduce the computational complexity, however, they also suffer from the angle discontinuity problem due to their angle regression.</p><p>In this paper, we introduce a novel approach for the oriented object detection, named TricubeNet. We localize ori-ented objects using visual cues (i.e., heatmap) instead of the box offset regression. As shown in <ref type="figure">Figure 1</ref>, we represent each object as a 2D Tricube kernel whose shape visually describes the width, height, and angle of the object, and then extract bounding boxes using simple imageprocessing algorithms. Our approach is able to (1) obtain well-arranged oriented boxes from visual cues of arranged objects as shown in <ref type="figure">Figure 1</ref>, (2) solve the angle discontinuity problem by taking away the angle regression, (3) save computational complexity due to our anchor-free modeling, and (4) is a simple one-stage anchor-free detector.</p><p>Furthermore, for the competitive result, we should handle some challenging factors of the oriented object detection: various shapes and sizes of objects, densely arranged objects, a huge number of objects, false detections, and complexity of the background. To deal with these challenging factors, we propose some techniques. The first is a Size-Weight Mask (SWM). The pixel-wise mean squared error (MSE) loss causes a size-imbalance problem, that is, a small object tends to be given small loss, weakening the detection of small objects. To make a size-invariant loss function, we propose the SWM. Second, to give a balanced loss between foreground and background pixels and reduce false-positive detections at once, we introduce a False-Positive Example Mining (FPEM) technique. Third, we propose a Multi-Angle Convolution (MAC) module to extract a rotationinvariant feature for the oriented object. Last, we design a repetitive refinement stage to refine the output heatmap and call this technique heatmap cascade refinement.</p><p>We verify the effectiveness of the TricubeNet in various tasks: oriented object detection in the aerial image (DOTA <ref type="bibr" target="#b38">[39]</ref>), the densely packed object image (SKU110K-R <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>), and the scene text image (MSRA-TD500 <ref type="bibr" target="#b44">[45]</ref>, ICDAR 2015 <ref type="bibr" target="#b15">[16]</ref>). We target detection of weakly-occluded oriented objects and choose the above highly practical tasks in real-world applications. The experimental results show that TricubeNet is quite effective to detect the oriented object with a simple anchor-free one-stage process.</p><p>In summary, our contributions are as follows:</p><p>? We propose a novel oriented object detector, TricubeNet, which localizes oriented objects using visual cues (i.e., heatmap) instead of the box offset regression.</p><p>? Our approach can obtain well-arranged oriented boxes, solve the angle discontinuity problem, and save computational complexity by eliminating anchor boxes.</p><p>? We propose some techniques (i.e., SWM, FPEM, MAC, and heatmap cascade refinement) to properly detect the oriented object and boost the performance.</p><p>? We verify the effectiveness of our TricubeNet from extensive experimental results on various tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Two-stage object detectors consist of two processes: extract object region candidates and crop the region of interest (ROI) of each object and predict the class and bounding box offsets of the object. R-CNN <ref type="bibr" target="#b8">[9]</ref> uses a selective search <ref type="bibr" target="#b36">[37]</ref> method to generate the bounding box candidates and feeds them to the classifier. SPP <ref type="bibr" target="#b11">[12]</ref> and Fast R-CNN <ref type="bibr" target="#b7">[8]</ref> crop the ROIs from feature maps and feed them to the classifier. Faster R-CNN <ref type="bibr" target="#b31">[32]</ref> generates bounding box candidates from a region proposal network (RPN), which allows training in an end-to-end manner. Recently, Cascade R-CNN <ref type="bibr" target="#b3">[4]</ref> designs iterative refinement steps for the highquality bounding box prediction. Although the two-stage object detectors achieve state-of-the-art performance, they require a high computational complexity. One-stage object detectors classify and regress the bounding boxes at once using anchor boxes. SSD <ref type="bibr" target="#b22">[23]</ref> densely produces the bounding boxes from multi-level feature maps using various sizes of anchor boxes and removes overlapping bounding boxes using NMS post-processing. Reti-naNet <ref type="bibr" target="#b20">[21]</ref> uses a focal loss to alleviate the class-imbalance problem between positive and negative anchor boxes. Anchor-free object detectors have recently been proposed and eliminate the anchor box in the network design. Cor-nerNet <ref type="bibr" target="#b17">[18]</ref> is a keypoint-based anchor-free approach that represents objects as pairs of corner keypoints and groups them. CenterNet <ref type="bibr" target="#b47">[48]</ref> represents objects as center points with width and height regression. The anchor-free detectors are simple and efficient but report a lower performance than the two-stage detectors.</p><p>Oriented object detectors often adopt the object detectors and additionally regress the angle for oriented objects. Adopting the Faster-RCNN <ref type="bibr" target="#b31">[32]</ref> as a baseline detector, RRPN <ref type="bibr" target="#b25">[26]</ref> exploits rotated anchor boxes and changes the IoU calculation for rotated boxes, RoI Transformer <ref type="bibr" target="#b6">[7]</ref> extracts geometry-robust pooled features, and Gliding vertex <ref type="bibr" target="#b39">[40]</ref> employs a simple object representation method which glides the vertex of the horizontal bounding box on each corresponding side. Adopting the RetinaNet <ref type="bibr" target="#b20">[21]</ref> as a baseline detector, RSDet <ref type="bibr" target="#b28">[29]</ref> proposes an eight-parameter regression model using a rotation sensitivity error (RSE) for handling the angle discontinuity problem and R 3 Det [41] is a fast one-stage detector with a feature refinement module to handle the feature misalignment problem. SCRDet <ref type="bibr" target="#b43">[44]</ref> proposes an IoU loss to alleviate the angle discontinuity problem. These approaches adopt an anchor augmentation strategy with a huge amount of box candidates to obtain oriented boxes. To reduce computational complexity by eliminating anchor box, DRN <ref type="bibr" target="#b27">[28]</ref> adopts CenterNet <ref type="bibr" target="#b47">[48]</ref> as their baseline detector taking anchor-free modeling and additionally regresses the angle offset with a proposed feature selection module and dynamic refinement head to dynamically refine the network. Note that DRN represents objects as center points and obtains other box offsets using regression, suffering from the angle discontinuity problem. Meanwhile, we represent the whole object region as a 2D Tricube kernel and obtain a bounding box using image-processing algorithms, detouring the angle discontinuity problem. The text detectors widely have adopted a segmentationbased anchor-free approach by utilizing the fact that there is no occlusion in the text. PixelLink <ref type="bibr" target="#b5">[6]</ref> performs the instance segmentation using a pixel-wise eight-neighbors link prediction. CRAFT <ref type="bibr" target="#b1">[2]</ref> is a character-level detection approach instead of word-level detection by representing each character as a 2D Gaussian kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">2D Tricube kernel</head><p>Recent key-point detection approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48]</ref> represent each key-point as a 2D Gaussian kernel and show remarkable performances. Motivated by these approaches, we assume that a 2D kernel function is a good candidate for representing the key features and is an easy-to-learn form for deep neural networks. For the oriented object detection, however, the 2D Gaussian kernel is not a suitable choice because its circular form often fails to represent the angle of an object. For example, in <ref type="figure">Figure 3</ref>(d), the 2D Gaussian kernel for the rotated square-form object cannot represent the angle of the object. For suitable modeling for the oriented object detection, we choose a 2D Tricube kernel as our object representation method. Unlike the 2D Gaussian kernel that has a circular-form distribution, the 2D Tricube kernel  has a rectangular-form distribution as in <ref type="figure">Figure 3</ref>(a) and is defined as </p><formula xml:id="formula_0">(1 ? |x| 3 ) ? ? (1 ? |y| 3 ) ? where ? is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture</head><p>As illustrated in <ref type="figure">Figure 2</ref>, TricubeNet consists of a fully convolutional encoder-decoder architecture. Let I ? R H?W ?3 be an input image, where H is height and W is width. The network predicts the output heatmap H ? R H R ? W R ?C , where R is the downsampling rate and C is the number of categories. We adopt Hourglass-104 <ref type="bibr" target="#b26">[27]</ref> network as our backbone network. We apply a bilinear upsampling layer on the output heatmap for higher pixel precision; the downsampling rate R is set to 2 in all experiments. Multi-Angle Convolution (MAC) Module. In our heatmap-based detection framework, extracting rotationinvariant features is important to detect orient objects. However, a convolutional neural network (CNN) has a limitation in extracting a rotation-invariant feature. To this end, we design a multi-angle convolution (MAC) module that is light and effective to extract the rotation-invariant features. The MAC module is illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>First, we define n kinds of angles, ? i , i ? (1, . . . , n), where the range is [0, ? 2 ). Next, when the input feature map X consists of K channels, we divide the channel of the X by n by applying a convolutional layer with a kernel size of 1 and channel of K n . We denote the divided feature map</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Rotation Convolution</head><p>Input: Feature maps X ? R H?W ?K , rotation angle ? Output: Rotation-invariant feature maps :</p><formula xml:id="formula_1">X ? R H?W ?K X ? Rescale(X, 1/(sin ? + cos ?)) X ? Rotate(X, ?) X ? Conv3?3(X) X ? Rotate(X, 2? ? ?) X ? Rescale(X, sin ? + cos ?) returnX</formula><p>as F i . Then, we apply a rotation convolutional layer with an angle of ? i and kernel size 3 on F i . Last, we concatenate the outputs of the rotation convolution with n different angles applied. Formally, the output of the MAC moduleX, which is the rotation invariant feature map, is defined as:</p><formula xml:id="formula_2">X = concat([RConv(F i , ? i ), . . . , RConv(F n , ? n )]),</formula><p>(1) where concat is the concatenate operation and RConv is the rotation convolutional layer with an angle of ?. In all experiments, we set n = 4 and (? 1 , ? 2 , ? 3 , ? 4 ) = (0, ? 6 , ? 4 , ? 3 ). A recent study, DRN <ref type="bibr" target="#b27">[28]</ref>, proposed a rotation convolutional layer, which rotates the kernel of CNN to extract the rotation-invariant feature. However, rotating the kernel of CNN requires heavy computations and additional parameters. For the efficient implementation of the rotation convolutional layer, we approximate the rotating of the kernel by rotating feature maps and then applying a normal CNN.</p><p>The detailed process of our rotation convolutional layer is described in Algorithm 1. First, we re-scale the feature map with a scale factor of 1/(sin ? +cos ?) to keep the original shape after rotating and then rotate the feature map by ?. Then, we apply a normal convolutional layer with kernel size 3 on the rotated feature map and return the output feature map back to the original angle and scale.</p><p>Heatmap Cascade Refinement. The two-stage detectors, such as Cascade R-CNN <ref type="bibr" target="#b3">[4]</ref>, take advantage of a progressive cascade refinement of the predicted bounding box; it repeatably crops the region of the predicted bounding box and predicts refined bounding box. In contrast, the cascade refinement step is less studied on the heatmap-based anchorfree approaches. Here, in our study, we propose a heatmap cascade refinement, which progressively refines the output heatmap in a pixel-wise manner, as illustrated in <ref type="figure">Figure 2</ref>. Specifically, when the last feature map our backbone network is denoted as X, the r-th refined feature map Y r and r-th refined heatmap H r are defined as: </p><formula xml:id="formula_3">Y 1 = M AC(X), H 1 = f c(Y 1 ), Y 2 = M AC(X + Y 1 ), H 2 = f c(Y 2 ), . . . Y r = M AC(X + Y r?1 ), H r = f c(Y r ),<label>(2)</label></formula><p>where M AC is the multi-angle convolution module and f c is a convolutional layer with a kernel size of 1 and a channel of C. Through the heatmap cascade refinement step, we progressively refine the output heatmap by taking the rotationinvariant feature map of the previous refinement step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">From bounding boxes to heatmap</head><p>Here, we describe how to generate a ground truth heatmap. The whole generation process is illustrated in <ref type="figure" target="#fig_4">Figure 5</ref>. First, we create a square 2D Tricube kernel and normalize it to a value between zero to one. Then, we reshape the kernel to the same size as the ground truth oriented bounding box. Last, we insert this reshaped kernel into the heatmap. If two Tricube kernels overlap, we take an element-wise maximum operation. The generated ground truth heatmap is denoted as? ? R H R ? W R ?C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective</head><p>We optimize the TricubeNet using pixel-wise meansquared error (MSE) objective function between H and? However, giving the pixel-wise MSE loss function has two problems. First is a size-imbalance problem. Since we assign the 2D Tricube kernel according to the size of each object, large objects tend to have large losses and small objects tend to have little losses; this weakens the detection of small objects. Second is a class-imbalance problem. Most of the pixels in the heatmap are background pixels, therefore, it interferes with focusing on the foreground pixels. To alleviate the above problems, we propose a size-weight mask (SWM) and false-positive example mining (FPEM). Size-Weight Mask (SWM). For the size-invariant loss function, we weight the MSE loss according to the size of the object using the size-weight mask (SWM); a large object is given a low weight and a small object is given a high weight. The SWM is denoted as</p><formula xml:id="formula_4">M ? R H R ? W R ?C .</formula><p>For each object, foreground pixels in the SWM contain a weight that is inversely proportional to the size of the object. Also, background pixels contain a weight of 1. This mask is generated in the same manner as the ground truth heatmap (Section 3.3) and illustrated in <ref type="figure" target="#fig_4">Figure 5</ref>. When we denote the size of the i-th object as S i , the set of pixels of i-th object as P obj i , and the number of objects in an image as N , the SWM for i-th object, M i , is defined as:</p><formula xml:id="formula_5">S = N i=1 S i , M i (p) = S Si?N p ? P obj i 1 otherwise.<label>(3)</label></formula><p>False-Positive Example Mining (FPEM). To alleviate the class-imbalance problem, we adopt Online Hard Example Mining (OHEM) <ref type="bibr" target="#b33">[34]</ref> that is an effective sampling method to make more focus on the foreground objects. However, focusing too much on the foreground objects yields a lot of false-positive detections. For balanced sampling while reducing false-positives, we propose false-positive exampling mining (FPEM). Specifically, we extract the positive and false-positive pixels during training in an online manner:</p><formula xml:id="formula_6">P pos = {p |?(p) &gt; 0}, P f p = {p |?(p) = 0 and H(p) &gt; 0},<label>(4)</label></formula><p>where P pos and P f p is the set of positive and false-positive pixels, respectively. Then we sample the ratio of |P pos | and |P f p | to be 1:3. Here, the order of P f p is sorted according to the distance between? and H and then sampled in the top-k manner. The final objective function L is defined as,</p><formula xml:id="formula_7">L = 1 S p?P M (p) ? (H(p) ??(p)) 2 ,<label>(5)</label></formula><p>where P is the set of sampled pixels from FPEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">From heatmap to oriented bounding boxes</head><p>We apply simple post-processing algorithms to extract the oriented bounding boxes from the heatmap, as illustrated in <ref type="figure" target="#fig_5">Figure 6</ref>. First, we obtain a binary heatmap S ? [0, 1] H R ? W R ?C from the output heatmap H with a threshold ? . By increasing the ? , we can separate the weaklyoccluded objects. Second, through the connected component labeling (CCL) <ref type="bibr" target="#b12">[13]</ref> algorithm, we allocate an ID of each Tricube kernel. Third, we obtain the rotated rectangular box with a minimum area containing each Tricube kernel using OpenCV <ref type="bibr" target="#b2">[3]</ref> functions. Last, since we drop some part of the Tricube kernel through the threshold ? , we scale up the kernel back to its original size by multiplying a scale factor s. For the choice of ? and s, we adaptively set the s according to the ? utilizing the distribution of the Tricube kernel, i.e., y = (1 ? |x| 3 ) ? . Note that the distribution of the Tricube kernel is maintained even if the input resolution or the object size is changed, therefore, we can adaptively set s = 1 1?(1?|? | 3 ) ? , where ? is set to 7 as described in 3.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>To demonstrate our effectiveness, we experiment on various practical datasets of weakly-occluded objects: aerial image, densely packed object image, and scene text image. DOTA <ref type="bibr" target="#b38">[39]</ref> is the largest dataset for oriented object detection in aerial images; it consists of 2806 images with sizes ranging from 800?800 to 4000?4000. The number of training, validation, and testing images are 1411, 458, and 937, respectively. It contains 15 categories of objects and 188,282 instances with a wide variety of scales, orientations, and shapes. Following other conventions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref>, we resize the image and crop a series of 1024?1024 patches from the original images with a stride of 824. For training, we resize the images at three scales (0.75, 1.0, and 1.5). For single-scale testing and multi-scale testing, we resize the images at one scale (1.0) and three scales (0.75, 1.0, and 1.5), respectively. The performance on the test set is measured on the official DOTA evaluation server 1 . SKU110K-R <ref type="bibr" target="#b27">[28]</ref> is an extended dataset of SKU110K <ref type="bibr" target="#b9">[10]</ref> for the densely packed oriented object detection. It contains thousands of supermarket store images with various scales from 1840?1840 to 4320?4320, viewing angles, lighting conditions, and noise levels. Each image contains an average of 154 tightly packed objects, up to 718 objects. The rotation data augmentation with six angles <ref type="figure">(</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training details</head><p>For SKU110K-R, MSRA-TD500, and ICDAR2015 datasets, we set the input resolution to 800?800 and apply random cropping, random rotating, color jittering for data augmentation. For DOTA dataset, we set the input resolution to 1024?1024 and apply random rotating and color jittering data augmentation. Adam <ref type="bibr" target="#b16">[17]</ref> is used as the optimizer and the learning rate is set to 2.5e-4 for all datasets. For DOTA dataset, we train the model with 140 epochs and drop the learning rate by a factor 10 at 90 and 120 epochs. For SKU110K-R dataset, we train the model with 20 epochs without the learning rate decay. Following existing text detection approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, we pre-train the TricubeNet on SynthText <ref type="bibr" target="#b10">[11]</ref> dataset with 3 epochs and finetuned on MSRA-TD500 and ICDAR2015 datasets. For MSRA-TD500 and ICDAR2015 datasets, we train the model with 180 epochs and drop the learning rate by a factor 10 at 120 and 160 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Testing details</head><p>We extract oriented bounding boxes using the postprocessing algorithm described in Section 3.5. For DOTA dataset, we apply two kinds of test time augmentation: multi-scale testing and flip augmentation. For multi-scale testing, three scales (0.75, 1.0, and 1.5) and Soft-NMS for oriented bounding boxes are used. For the flip augmentation, we average output heatmaps. For the evaluation metric of DOTA, MSRA-TD500, and ICDAR2015, we adopt the mean average precision (mAP) of the 0.5 polygon IoU threshold. For the SKU110K-R dataset, we adopt COCOstyle <ref type="bibr" target="#b21">[22]</ref> evaluation method for oriented boxes: mAP at IoU=0.5:0.05:0.95, average precision AP 75 at IoU of 0.75, and average recall AR 300 at IoU=0.5:0.05:0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">State-of-the-art comparisons</head><p>We compare the performance of TricubeNet with the state-of-the-art methods on the test set of each dataset: <ref type="table" target="#tab_1">Table  1</ref> for DOTA, <ref type="table" target="#tab_2">Table 2</ref> for SKU110K-R, <ref type="table" target="#tab_3">Table 3</ref> for MSRA-TD500 and ICDAR2015. For the comparison, we apply two-steps heatmap cascade refinement. The anchor-based two-stage and one-stage detectors require a huge number of anchor boxes and multiple loss functions for the box offsets regression; it demands huge computational complexity and careful hyperparameter tuning. Meanwhile, anchorfree detectors can alleviate the above problems concerned with the anchor box by eliminating the anchor box in the network design, but their performance is slightly inferior to that of anchor-based detectors. However, our TricubeNet achieves highly competitive performance in all datasets despite its anchor-free setting. Especially, our outstanding per- formance in small vehicles (SV) shows that TricubeNet can accurately detect small objects without anchor box tuning, and we can obtain well-arranged oriented boxes as shown in <ref type="figure" target="#fig_6">Figure 7</ref>(a). In addition, the sufficiently high performance of 94.7% AP 50 on SKU110K-R shows that TricubeNet is effective in detecting densely packed objects. When applying the same test augmentation, Gliding vertex <ref type="bibr" target="#b39">[40]</ref>, one of the two-stage detectors, shows the state-of-the-art performance (75.0% v.s. 74.2%) as in <ref type="table" target="#tab_1">Table 1</ref>, we argue that our efficiency is higher than them because we exploit the advantages of the one-stage anchor-free setting and simply solve the loss discontinuity problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation study</head><p>To analyze the effectiveness of each component of TricubeNet, we conduct an ablation study. For the evaluation, we experiment on DOTA validation set following all parameter settings as given in Section 4.2.</p><p>First, we investigate which 2D kernel is more effective in  <ref type="bibr" target="#b50">[51]</ref> 32.9 67.0 28.1 +0.55M +40G RCL <ref type="bibr" target="#b27">[28]</ref> 33.4 67. <ref type="bibr" target="#b5">6</ref> 28.5 +0.49M +34G MAC (ours) 33.6 67. <ref type="bibr" target="#b6">7</ref> 28.7 +0.43M +28G</p><p>representing the object. For the comparison, we choose one of four different types of 2D kernels: 2D Tricube kernel, 2D Gaussian kernel, binary rectangle, effective rectangle. The effective rectangle is a shrunk binary rectangle, which is employed in FSAF <ref type="bibr" target="#b49">[50]</ref>, and we use a binary rectangle shrunk by 40% as the effective rectangle. <ref type="table">Table 4</ref> shows the DOTA validation score of each kernel. The effective rectangle achieves higher performance than the binary rectangle because the binary rectangle often fails to separate weaklyoccluded objects. Although the Gaussian kernel has a limitation in representing the angle of the object as discussed in Section 3.1, the Gaussian kernel shows a 2.81% higher performance than the effective rectangle; this result demonstrates the advantage of the kernel-based object representation. Since the Tricube kernel can solve the limitation of the Gaussian kernel while taking advantage of the kernel-based object representation, the Tricube kernel is the most suitable choice for the oriented object and achieves the highest performance among the four types of kernels. Second, we evaluate the effect of proposed techniques, i.e., size-weight mask (SWM), false-positive example mining (FPEM), multi-angle convolution (MAC) module, and heatmap cascade refinement. For a more precise analysis, we measure the performance using COCO-style evaluation metric as in <ref type="table">Table 5</ref>. AP S , AP M , and AP L denote the AP when the object size is smaller than 32 ? 32, between 32 ? 32 and 96 ? 96, and larger than 96 ? 96, respectively. In addition, we report the number of parameters and GFlops. We set the baseline model as the hourglass-104 network trained using MSE loss without any proposed techniques and evaluate the effect of each technique. When applying both SWM and FPEM, AP and AP S are considerably improved by 1.9% and 4.2%, respectively; it clearly shows that SWM and FPEM help in solving size-imbalance and class-imbalance problems. The MAC module, which is designed for extracting the rotation-invariant feature, yields 1.3% performance improvement. One-step heatmap refinement further improves the performance by 2.3%. However, two-step heatmap refinement only yields 0.3% improvement; we conclude that two-step heatmap refinement is enough to produce a high-quality heatmap.</p><p>Last, in <ref type="table" target="#tab_4">Table 6</ref>, we evaluate our MAC module by comparing with a conventional convolution (CNN), deformable convolution <ref type="bibr" target="#b50">[51]</ref> (DCN), and rotation convolution layer <ref type="bibr" target="#b27">[28]</ref> (RCL). Compared to CNN and DCN, our MAC module can properly extract rotation-invariant features, so the performance is improved by 1.3% and 0.7%, respectively. Since both MAC and RCL are designed to extract rotationinvariant features, our MAC module shows a similar performance improvement to the RCL. However, the RCL requires heavier computations and about 60% more GPU memory usage than our MAC during the training. From the above result, we prove that the proposed MAC module is more effective and efficient in our TricubeNet, which is a fully-heatmap-based approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Results</head><p>We collect some qualitative results on each dataset: <ref type="figure" target="#fig_6">Figure 7(a)</ref> for DOTA, <ref type="figure" target="#fig_6">Figure 7</ref>(b) for MSRA-TD500, <ref type="figure" target="#fig_6">Figure  7</ref>(c) for ICDAR 2015, and <ref type="figure" target="#fig_6">Figure 7(d)</ref> for SKU110K-R. From the results, we can notice that our TricubeNet handles the challenging factors of oriented object detection. Specifically, as in <ref type="figure" target="#fig_6">Figure 7</ref>(a), TricubeNet can properly detect objects of various sizes within the multi-category classification problem; as in <ref type="figure" target="#fig_6">Figure 7</ref>(c), TricubeNet can accurately detect the rotated text despite the complex background; as in <ref type="figure" target="#fig_6">Figure 7(d)</ref>, TricubeNet is outstanding in the detection of a huge number of densely arranged oriented objects. Furthermore, we can identify that the consistency of the rotated bounding boxes detecting the arranged objects is very high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>We present a novel approach for oriented object detection, named TricubeNet. Our main concept is that we localize objects using visual cues (i.e., heatmap) instead of box offsets regression. We represent each object as a 2D Tricube kernel and extract bounding boxes using simple image-processing algorithms. Unlike anchor-based approaches, TricubeNet is able to obtain well-arranged oriented boxes from visual cues, solve the angle discontinuity problem by taking away the angle regression, and save the computational complexity due to our anchor-free modeling. Additionally, for the better fit in oriented object detection, we propose some effective techniques: SWM for the sizeinvariant loss function; FPEM for balancing between foreground and background pixels and reducing false-positive detections; MAC module to extract the rotation-invariant features; heatmap cascade refinement for progressively refine the output heatmap. The extensive experimental results on various datasets show that our TricubeNet is considerably effective and efficient for oriented object detection.</p><p>For future work, our failure cases are should be addressed. TricubeNet is quite effective to detect weaklyoccluded oriented objects, however, it has trouble detecting heavily occluded objects. Also, the extracted box offsets from our post-processing algorithm are integer type, which weakens the precise detection of tiny objects. Addressing these limitations would give a great improvement. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Overview of TricubeNet. It produces one channel heatmap per category where each oriented object is represented as a 2D Tricube kernel. The backbone network consists of a fully convolutional encoder-decoder architecture. H and W are the height and width of the image, respectively; C is the number of categories; R is the downsampling rate. From the heatmap cascade refinement, we progressively refine the output heatmap (H) and extract bounding boxes from the lastly refined heatmap (H3) using simple image-processing algorithms. (a): 2D Tricube kernel, (b): 2D Gaussian kernel, (c): object as 2D Tricube kernel, and (d): object as 2D Gaussian kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 4 3 4 4</head><label>44</label><figDesc>Rotation Conv 3?3, K/Rotation Conv 3?3, K/Rotation Conv 3?3, K/4 H ? W ? K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of multi-angle convolution (MAC) module which extracts rotation-invariant features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>set to 7 in all experiments. The 2D Tricube kernel can properly represent the angle of the object as shown in Figure 3(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of the ground truth (GT) heatmap and sizeweight mask generation procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>From heatmap to oriented bounding boxes. First, (a) from the heatmap, (b) we obtain binary heatmap using threshold ? . (c) We label each kernel using the connected component labeling (CCL) algorithm and (d) extract the contour points and find the minimum-area rectangle for each kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results of TricubeNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.635, 0.419, 0.616, 0.498, 0.485, 0.530, 0.504) (0.450, 0.634, 0.422, 0.620, 0.502, 0.485, 0.526, 0.499) (0.225, 0.930, 0.194, 0.910, 0.282, 0.778, 0.313, 0.798) (0.228, 0.928, 0.193, 0.908, 0.283, 0.780, 0.312, 0.797) (0.230, 0.935, 0.200, 0.917, 0.276, 0.771, 0.315, 0.801) .474, 0.560, 0.038, 0.152, 0.522) (0.475, 0.559, 0.031, 0.156, 0.463) (0.253, 0.853, 0.036, 0.158, 0.572) (0.255, 0.853, 0.040, 0.156, 0.519) (0.252, 0.853, 0.034, 0.164, 0.5400</figDesc><table><row><cell></cell><cell></cell><cell cols="2">5-offsets</cell><cell></cell><cell>8-offsets</cell><cell>ours</cell></row><row><cell>Output Input</cell><cell>( x , (0</cell><cell>y , w ,</cell><cell>h ,</cell><cell>)</cell><cell>( x1 , y1 , x2 , y2 , x3 , y3 , x4 , y4 ) (0.452, 0?</cell><cell>4M</cell></row><row><cell>Post-processing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>4M ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>-45 ? , -30 ? , -15 ? , 15 ? , 30 ? , and 45 ? ) is performed to original images. After the augmentation, the number of training, validation, and testing images are 57,533, 4,116, and 20,587, respectively. MSRA-TD500 [45] is a dataset for multi-lingual, long, and oriented text detection in both indoors and outdoors natural images. The images contain English and Chinese scripts and each text is labeled by a rotated rectangle. It consists of 300 training images and 200 testing images. ICDAR2015 [16] is proposed in Robust Reading Competition for incidental scene text detection. There are 1000 training images and 500 testing images. Each text is annotated as word level with a quadrangle of four vertexes. Experiment results on DOTA dataset. MS and Flip denote the multi-scale and flip test time augmentation, respectively. PL-Plane, BD-Baseball Diamond, BR-Bridge, GTF-Ground Field Track, SV-Small Vehicle, LV-Large Vehicle, SH-Ship, TC-Tennis Court, BC-Basketball Court, ST-Storage Tank, SBF-Soccer Ball Field, RA-Roundabout, HA-Harbor, SP-Swimming Pool, and HC-Helicopter. 65.82 33.77 58.94 55.77 50.94 54.78 90.33 66.34 68.66 48.73 51.76 55.10 51.32 35.88 57.94 RRPN [26] ResNet-101 80.94 65.75 35.34 67.44 59.92 50.91 55.81 90.67 66.92 72.39 55.06 52.23 55.14 53.35 48.22 61.01 R 2 CNN [15] ResNet-101 88.52 71.20 31.66 59.30 51.85 56.19 57.25 90.81 72.84 67.38 56.69 52.84 53.08 51.94 53.58 60.67 ICN [1] ResNet-101 81.36 74.30 47.70 70.32 64.89 67.82 69.98 90.76 79.06 78.20 53.64 62.90 67.02 64.17 50.23 68.16 RoI Trans [7] ResNet-101 88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 69.56 SCRDet [44] ResNet-101 89.41 78.83 50.02 65.59 69.96 57.63 72.26 90.73 81.41 84.39 52.76 63.62 62.01 67.62 61.16 69.83 SCRDet [44] ResNet-101 89.98 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61 Gliding vertex [40] ResNet-101 89.64 85.00 52.26 77.34 73.01 73.14 86.82 90.74 79.02 86.81 59.55 70.91 72.94 70.86 57.32 75.02 77.13 17.70 64.05 53.30 38.02 37.16 89.41 69.64 59.28 50.30 52.91 47.89 47.40 46.30 54.13 82.00 53.80 68.50 70.20 78.70 73.60 91.20 87.10 84.70 64.30 68.20 66.10 69.30 63.70 74.10 .12 49.24 72.98 77.64 74.53 84.65 90.81 86.02 85.38 58.69 63.59 73.82 69.67 71.08 75.26</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>MS Flip</cell><cell>PL</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP</cell></row><row><cell>two-stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">R-DFPN [42] 80.92 one-stage ResNet-101</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="19">FR-O [39] 79.42 RetinaNet [21] ResNet-101 ResNet-50 88.87 74.46 40.11 58.03 63.10 50.61 63.63 90.89 77.91 76.38 48.26 55.85 50.67 60.23 34.23 62.22</cell></row><row><cell>R 3 Det [41]</cell><cell>ResNet-101</cell><cell></cell><cell cols="16">89.54 81.99 48.46 62.52 70.48 74.29 77.54 90.80 81.39 83.54 61.97 59.82 65.44 67.46 60.05 71.69</cell></row><row><cell>R 3 Det [41]</cell><cell>ResNet-152</cell><cell></cell><cell cols="16">89.49 81.17 50.53 66.10 70.92 78.66 78.21 90.81 85.26 84.23 61.81 63.77 68.16 69.83 67.17 73.74</cell></row><row><cell>RSDet [29]</cell><cell>ResNet-101</cell><cell></cell><cell cols="16">89.80 82.90 48.60 65.20 69.50 70.10 70.20 90.50 85.60 83.40 62.50 63.90 65.60 67.20 68.00 72.20</cell></row><row><cell cols="4">RSDet [29] 90.10 anchor-free ResNet-152</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CenterNet [48]</cell><cell>Hourglass-104</cell><cell></cell><cell cols="16">89.02 69.71 37.62 63.42 65.23 63.74 77.28 90.51 79.24 77.93 44.83 54.64 55.93 61.11 45.71 65.04</cell></row><row><cell>CenterNet [48]</cell><cell>Hourglass-104</cell><cell></cell><cell cols="16">89.56 79.83 43.80 66.54 65.58 66.09 83.11 90.72 83.72 84.30 55.62 58.71 62.48 68.33 50.77 69.95</cell></row><row><cell>DRN [28]</cell><cell>Hourglass-104</cell><cell></cell><cell cols="16">88.91 80.22 43.52 63.35 73.48 70.69 84.94 90.14 83.85 84.11 50.12 58.41 67.62 68.60 52.50 70.70</cell></row><row><cell>DRN [28]</cell><cell>Hourglass-104</cell><cell></cell><cell cols="16">89.45 83.16 48.98 62.24 70.63 74.25 83.99 90.73 84.60 85.35 55.76 60.79 71.56 68.82 63.92 72.95</cell></row><row><cell>DRN [28]</cell><cell>Hourglass-104</cell><cell></cell><cell cols="16">89.71 82.34 47.22 64.10 76.22 74.43 85.84 90.57 86.18 84.89 57.65 61.93 69.30 69.63 58.48 73.23</cell></row><row><cell>TricubeNet (ours)</cell><cell>Hourglass-104</cell><cell></cell><cell cols="16">87.51 73.62 43.21 63.67 76.97 72.97 84.36 89.21 83.59 84.60 47.29 61.77 73.36 68.74 69.40 72.17</cell></row><row><cell>TricubeNet (ours)</cell><cell>Hourglass-104</cell><cell></cell><cell cols="16">88.28 80.46 47.32 70.09 76.97 72.97 84.52 90.73 83.87 84.60 56.92 62.91 73.36 68.74 71.63 74.22</cell></row><row><cell>TricubeNet (ours)</cell><cell>Hourglass-104</cell><cell></cell><cell cols="2">88.75 82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results on SKU110K-R using the COCO-style metric.</figDesc><table><row><cell>Method</cell><cell cols="4">mAP AP 50 AP 75 AR 300</cell></row><row><cell>YOLOv3-Rotate [31]</cell><cell>49.1</cell><cell>-</cell><cell>51.1</cell><cell>58.2</cell></row><row><cell cols="2">CenterNet-4point [48] 34.3</cell><cell>-</cell><cell>19.6</cell><cell>42.2</cell></row><row><cell>CenterNet [48]</cell><cell>54.7</cell><cell>-</cell><cell>61.1</cell><cell>62.2</cell></row><row><cell>DRN [28]</cell><cell>55.9</cell><cell>-</cell><cell>63.1</cell><cell>63.3</cell></row><row><cell>TricubeNet (ours)</cell><cell>57.7</cell><cell>94.7</cell><cell>65.2</cell><cell>62.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Evaluation on MSRA-TD500 and ICDAR2015 testset. * denotes for multi-scale test. R, P, and H denote recall, precision, and H-mean respectively. AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell></cell><cell></cell><cell cols="3">MSRA-TD500</cell><cell cols="3">ICDAR 2015</cell></row><row><cell cols="2">Method</cell><cell>R</cell><cell>P</cell><cell>H</cell><cell>R</cell><cell>P</cell><cell>H</cell></row><row><cell cols="2">two-stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Wang et al. [38]</cell><cell cols="6">82.1 85.2 83.6 86.0 89.2 87.6</cell></row><row><cell cols="5">Gliding vertex [40] 84.3 88.8 86.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">one-stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SegLink [33]</cell><cell cols="6">70.0 86.0 77.0 76.8 73.1 75.0</cell></row><row><cell cols="2">RRD* [19]</cell><cell cols="6">73.0 87.0 79.0 80.0 88.0 83.8</cell></row><row><cell cols="2">Lyu et al.* [25]</cell><cell cols="6">76.2 87.6 81.5 79.7 89.5 84.3</cell></row><row><cell cols="2">Direct* [14]</cell><cell cols="6">81.0 91.0 86.0 80.0 85.0 82.0</cell></row><row><cell cols="2">anchor-free</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Zhang et al. [46]</cell><cell cols="6">67.0 83.0 74.0 43.0 71.0 54.0</cell></row><row><cell cols="2">EAST* [49]</cell><cell cols="6">67.4 87.3 76.1 78.3 83.3 80.7</cell></row><row><cell cols="2">TextSnake [24]</cell><cell cols="6">73.9 83.2 78.3 80.4 84.9 82.6</cell></row><row><cell cols="2">PixelLink* [6]</cell><cell cols="6">73.2 83.0 77.8 82.0 85.5 83.7</cell></row><row><cell cols="2">CRAFT [2]</cell><cell cols="6">78.2 88.2 82.9 84.3 89.8 86.9</cell></row><row><cell cols="2">TricubeNet (ours)</cell><cell cols="6">80.8 90.4 85.3 75.2 90.1 82.0</cell></row><row><cell cols="8">Table 4. The effect of each kernel representing an object.</cell></row><row><cell cols="8">kerenl Tricube Gaussian Effective Rect Binary Rect</cell></row><row><cell>mAP</cell><cell>75.26</cell><cell>72.12</cell><cell></cell><cell>69.31</cell><cell></cell><cell cols="2">58.52</cell></row><row><cell cols="8">Table 5. Ablation study for the proposed techniques: size-weight</cell></row><row><cell cols="8">mask (SWM), false-positive example mining (FPEM), multi-angle</cell></row><row><cell cols="8">convolution (MAC) module, one-step heatmap refinement (Cas-</cell></row><row><cell cols="7">cade 1), and two-step heatmap refinement (Cascade 2).</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Params GFlops</cell></row><row><cell>Baseline</cell><cell>30.4 61.6</cell><cell>25.2</cell><cell>16.3</cell><cell>32.4</cell><cell>37.6</cell><cell>188M</cell><cell>1015G</cell></row><row><cell>+ SWM</cell><cell>31.1 64.0</cell><cell>25.6</cell><cell>19.2</cell><cell>33.0</cell><cell>36.0</cell><cell>+0M</cell><cell>+0G</cell></row><row><cell>+ FPEM</cell><cell>32.3 66.8</cell><cell>25.2</cell><cell>20.5</cell><cell>35.4</cell><cell>37.6</cell><cell>+0M</cell><cell>+0G</cell></row><row><cell>+ MAC</cell><cell>33.6 67.7</cell><cell>28.7</cell><cell>20.2</cell><cell>35.9</cell><cell cols="2">39.8 +0.4M</cell><cell>+28G</cell></row><row><cell cols="2">+ Cascade 1 35.3 69.6</cell><cell>30.3</cell><cell>24.1</cell><cell>37.2</cell><cell cols="2">40.1 +1.2M</cell><cell>+85G</cell></row><row><cell cols="2">+ Cascade 2 35.6 70.1</cell><cell>30.9</cell><cell>24.6</cell><cell>38.5</cell><cell cols="3">40.3 +2.1M +143G</cell></row></table><note>AP</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>The analysis of each method for extracting rotationinvariant features. AP AP 50 AP 75 Params GFlops CNN 32.3 66.8 25.2 +0.00M +00G DCN</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://captain-whu.github.io/DOTA</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research was supported in part by Autonomous Driving Center, R&amp;D Division, Hyundai Motor Company when the authors worked at KAIST.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Seyed Majid Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Character region awareness for text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmin</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bado</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwalsuk</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The OpenCV Library. Dr. Dobb&apos;s Journal of Software Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoong</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayoung</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyuk-Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01315</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qikai</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2849" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Precise detection in densely packed scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast connected-component labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyan</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kesheng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1977" to="1987" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-oriented and multi-lingual scene text detection with direct regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu-Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5406" to="5419" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangbang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-oriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7553" to="7563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic refinement network for oriented and densely packed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjia</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kekai</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haolei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning modulated loss for rotated object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chijun</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08299</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2550" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Arbitrary shape scene text detection with adaptive text region representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6449" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05612</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks. Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">132</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Position detection and direction prediction for arbitrary-oriented ships via multitask rotation region convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="50839" to="50849" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4159" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection in remote sensing images based on polar coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
