<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Graph Data Learning via Latent Graph Convolutional Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
							<email>jiangbo@ahu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Ahui University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Zhang</surname></persName>
							<email>zhangziyanahu@163.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Ahui University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Ahui University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Graph Data Learning via Latent Graph Convolutional Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Representation (GCR) has achieved impressive performance for graph data representation. However, existing GCR is generally defined on the input fixed graph which may restrict the representation capacity and also be vulnerable to the structural attacks and noises. To address this issue, we propose a novel Latent Graph Convolutional Representation (LatGCR) for robust graph data representation and learning. Our LatGCR is derived based on reformulating graph convolutional representation from the aspect of graph neighborhood reconstruction. Given an input graph A, LatGCR aims to generate a flexible latent graph A for graph convolutional representation which obviously enhances the representation capacity and also performs robustly w.r.t graph structural attacks and noises. Moreover, LatGCR is implemented in a self-supervised manner and thus provides a basic block for both supervised and unsupervised graph learning tasks. Experiments on several datasets demonstrate the effectiveness and robustness of LatGCR.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Convolutional Networks (GCNs) have been widely studied for graph data representation and learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Graph convolutional representation (GCR) is the core operation powering GCNs. The aim of GCR is to generate context-aware embeddings for graph nodes by aggregating the messages from their neighbors via some differentiable aggregation functions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. For example, Kipf et al. <ref type="bibr" target="#b1">[2]</ref> propose a graph convolution operation by exploring the first-order approximation of graph Laplacian spectral filter. Hamilton et al. <ref type="bibr" target="#b6">[7]</ref> present Graph Sample and Aggregate (GraphSAGE) for inductive graph representation and learning by using graph sampling and aggregating techniques. Petar et al. <ref type="bibr" target="#b3">[4]</ref> propose Deep Graph Infomax (DGI) to learn node's representation in an unsupervised manner. Klicpera et al. <ref type="bibr" target="#b8">[9]</ref> propose Personalized Propagation of Neural Predictions (PPNP) which combines GCN and PageRank together for graph semi-supervised learning. Zhu et al. <ref type="bibr" target="#b9">[10]</ref> propose Robust GCN (RGCN) by adopting robust learnable Gaussian distributions for message propagation. However, the above GCRs are generally defined on the input fixed graph which may restrict the representation capacity and also be very vulnerable to the structural attacks and noises <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref>. To address this issue, one kind of popular way is to incorporate graph learning modules into GNNs' training via optimizing a general joint loss function, i.e., 'graph learning loss + GNN's loss'. For example, Wu et al. <ref type="bibr" target="#b12">[13]</ref> propose GCN-Jaccard to denoise input graph by deleting the edges with low similarities. Li et al. <ref type="bibr" target="#b13">[14]</ref> propose AdaGCN to learn an optimal graph for GNN learning. Jiang et al. <ref type="bibr" target="#b14">[15]</ref> propose Graph Learning Convolutional Network (GLCN) by integrating graph learning module into GCN architecture for semi-supervised learning problem. Yang et al. <ref type="bibr" target="#b15">[16]</ref> propose Topology Optimization based Graph Convolutional Networks (TO-GCN) for semi-supervised learning by jointly refining the graph structure and learning the parameters of GCN. Jin et al. <ref type="bibr" target="#b11">[12]</ref> develop Pro-GNN to adaptively train a more optimal graph for GNN's learning via designing a joint loss function.</p><p>In this paper, we propose a novel Latent Graph Convolutional Representation (LatGCR) for robust graph data representation and learning. Our LatGCR is derived based on reformulating GCR from the aspect of graph neighborhood reconstruction. The main difference between LatGCR and previous related works is that LatGCR gives a basic block which can be used within many GNN architectures by replacing traditional graph convolution layer with LatGCR block. We will discuss the detailed differences between LatGCR and previous related works including recent Pro-GNN <ref type="bibr" target="#b11">[12]</ref> and GeCN <ref type="bibr" target="#b5">[6]</ref> in ?4. Specifically, given an input observed graph A, LatGCR aims to generate a flexible latent graph A for GCR in a self-supervised manner which enhances the representation capacity and also obviously performs robustly w.r.t graph structural attacks and noises.</p><p>Overall, we summarize the main contributions of this paper as follows:</p><p>? We propose a novel self-supervised Latent Graph Convolutional Representation (LatGCR) based on the reformulation of GCR from graph neighborhood reconstruction.</p><p>? LatGCR can be efficiently implemented via a simple recurrent architecture, i.e., LatGCR block, which provides a general basic block for GNNs.</p><p>? Based on the proposed LatGCR block, we propose an end-to-end LatGC neural network (LatGCN) for robust graph data representation and learning.</p><p>Experimental results on both semi-supervised classification and unsupervised clustering tasks demonstrate the effectiveness and robustness of the proposed LatGCR and LatGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Revisiting Graph Convolutional Representation</head><p>As the main aspect of Graph Convolutional Networks (GCNs) for graph data representation and learning, Graph Convolutional Representations (GCRs) have been widely studied in recent years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8]</ref>. The aim of GCRs is to generate context-aware representations for graph nodes by aggregating the representations from their neighbors via some specific aggregation functions.</p><p>One popular formulation of GCR is to employ the weighted mean aggregation function for neighbor's information aggregation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>. Let G(A, Z) denotes the input graph where A ? R n?n denotes the adjacency matrix with A ii = 1 and Z = (z 1 , z 2 ? ? ? z n ) ? R n?d denotes the collection of node features. Then, the weighted mean-type GCR <ref type="bibr" target="#b6">[7]</ref> can be formulated as</p><formula xml:id="formula_0">h i = 1 d i j?Ni?i A ij z j W<label>(1)</label></formula><p>where d i = j?Ni?i A ij and N i represents the neighbor set of node i. Matrix W ? R d?d denotes the graph convolutional parameter which is learned adaptively based on the specific downstream task. The output H = (h 1 , h 2 ? ? ? h n ) ? R n?d provides the graph convolutional representations of graph nodes. Comparing with input Z, H involves more context information encoded in A and also provides task-relevant representations for nodes via learned W.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Latent Graph Convolution Representation</head><p>The above GCR is defined on the fixed input graph A which may restrict the representation capacity and also has been demonstrated to be very vulnerable to the structural attacks and noises in A <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18]</ref>. To address this issue, we present a novel Latent Graph Convolution Representation (LatGCR) for robust graph data representation and learning. Our LatGCR is motivated based on the reformulation of the above GCR (Eq.(1)) from the aspect of neighborhood reconstruction <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>. Specifically, Eq.(1) provides the optimal solution to the following node reconstruction problem,</p><formula xml:id="formula_1">h i = 1 d i j?Ni?i A ij z j W = arg min hi j?Ni?i A ij h i ? z j W 2 (2)</formula><p>where ? denotes Frobenius norm function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LatGCR model formulation</head><p>Let G(A, Z) be the input observed graph with adjacency matrix A ? R n?n (A ii = 1) and node features Z ? R n?d . The aim of LatGCR is to estimate a latent and flexible graph A to better support GCR. Based on the reformulation of GCR (Eq.(2)), we present our LatGCR model by integrating latent graph estimation and graph convolutional representation jointly as</p><formula xml:id="formula_2">{ A , H } = arg min A,H A ? A 2 + ? i j?Ni?i A ij h i ? z j W 2 (3) s.t. A ij ? 0</formula><p>where A denotes the estimated latent graph and H = (h 1 , h 2 ? ? ? h n ) ? R n?d denotes the output Latent GCRs for graph nodes. Matrix W ? R d?d denotes the graph convolutional parameter and parameter ? &gt; 0 is the trade-off hyper-parameter balancing two terms. The first term in Eq. <ref type="formula">(3)</ref> represents the latent graph estimation/reconstruction while the second term denotes the GCR. Note that, when ? ? 0, the first term is penalized very largely and we can have A = A. In this case, LatGCR degenerates to standard GCR (Eq. <ref type="formula" target="#formula_0">(1)</ref>). Overall, there are four main aspects of the above LatGCR model.</p><p>? Self-supervised joint learning: In LatGCR, the estimation of latent graph is conducted in a self-supervised way. LatGCR conducts both latent graph generation and GCR jointly to boost their respective representation ability. Therefore, LatGCR can be potentially used in both supervised and unsupervised learning tasks.</p><p>? Robustness: When A contains some noises/errors, i.e., A = A + E where E denotes the noises/errors. Then Eq.(3) can be re-formulated as</p><formula xml:id="formula_3">{ A , H , E } = arg min A,H,E E 2 + ? i j?Ni?i A ij h i ? z j W 2 s.t. A = A + E, A ij ? 0<label>(4)</label></formula><p>That is, LatGCR acts as recovering/generating a latent 'clear' graph A from the input noisy graph A for GCR and thus performs robustly w.r.t. graph noises and attacks. This is one important property of LatGCR and will be validated in Experiments in detail.</p><p>? Sparsity: The estimated graph A inherits the same sparse pattern from input graph A, i.e., we can easily prove that if A ij = 0, then we have A ij = 0, as also seen from Eq.(5) below.</p><p>? Efficient implementation: In LatGCR, both graph estimation and graph convolutional representation are implemented via simple one-step update rules which thus can be computed very efficiently, as discussed in ?3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LatGCR implementation.</head><p>The optimum A and H can be obtained via a simple update algorithm which alternatively conducts the following Latent Graph Estimation (LGE) and Graph Convolutional Representation (GCR) steps.</p><p>LGE-step: Solving A while fixing H, the problem becomes</p><formula xml:id="formula_4">A = arg min A A ? A 2 + ? i j?Ni?i A ij h i ? z j W 2 s.t. A ij ? 0 which is equivalent to A ij = arg min Aij A ij ? ? 2 h i ? z j W 2 ? A ij 2 s.t. A ij ? 0</formula><p>It has a simple closed-form solution which is given as</p><formula xml:id="formula_5">A ij = max A ij ? ? 2 h i ? z j W 2 , 0<label>(5)</label></formula><p>GCR-step: Solving H while fixing A, the problem becomes to the standard GCR Eq.(2). The exact optimal solution is</p><formula xml:id="formula_6">H = D ?1 AZW (6) where D is the diagonal matrix with D ii = j?Ni?i A ij .</formula><p>Remark.</p><p>(1) As discussed before, since the optimal A inherits the sparse pattern of A, in implementation of Eq. <ref type="formula" target="#formula_5">(5)</ref>, we only need to compute the element A ij with j ? N i ? i which is efficient, as further analyzed in section Complexity analysis.</p><p>(2) In real implementation, instead of using Eq.(5), we use the following update rule to avoid the possible numerical issue when ? is large, i.e.,</p><formula xml:id="formula_7">A ij = max A ij ? ? 2 h i ? z j W 2 , , j ? N i ? i (7)</formula><p>where is a very small positive number. That is, when ? is large enough, we have A ij = , j ? N i ? i. In this case, the above GCR (Eq. <ref type="formula">(6)</ref>) becomes to the unweighed mean aggregation which also gives a reasonable solution.</p><p>LatGCR block. From network architecture aspect, the above LatGCR implementation can be designed via a recurrent architecture, i.e., LatGCR block, which consists of Latent Graph Estimation (LGE) and Graph Convolutional Representation (GCR) submodules, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 1(B)</ref>, LGE and GCR are alternatively conducted in LatGCR. On the contrary, the graph A is fixed in traditional GCR (shown in <ref type="figure" target="#fig_0">Fig. 1(A)</ref>). Therefore, LatGCR is more flexible than GCR. More importantly, LatGCR block performs more robustly than GCR, as demonstrated in Experiments.</p><formula xml:id="formula_8">(A) GCR ' ?1 (B) LatGCR ' = ? ? 2 ? ?1 ? GCR max ? , 0</formula><p>LGE 2 { } </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LatGC Neural Networks</head><p>LatGCR gives a basic block which can be used within many GNN architectures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2]</ref> by replacing the traditional graph convolution layer with LatGCR module. Here, we adopt the GNN architecture utilized in traditional GCN <ref type="bibr" target="#b1">[2]</ref> and design an end-to-end multi-layer neural network architecture, named Latent Graph Convolutional Network (LatGCN) for graph data learning. Concretely, the proposed LatGCN contains one input layer, several hidden propagation layers and one final perceptron layer, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. For each hidden propagation layer, it takes features H (l) and initial graph A as input and outputs features H (l+1) by using LatGCR module with parameter W (l) . LatGCN can be used in many graph learning tasks. For example, when applying LatGCN for semi-supervised node classification tasks, the last perceptron layer outputs the final label predictions P for all nodes. The convolutional parameter W (l) of each hidden layer can be learned via minimizing the cross-entropy function on the labelled nodes, as discussed in work <ref type="bibr" target="#b1">[2]</ref>.</p><p>Softmax Comparison with related works. Exploiting graph learning for GNNs has been studied in recent years. The main differences between LatGCN and previous graph learning guided GCNs including Pro-GNN <ref type="bibr" target="#b11">[12]</ref>, TO-GCN <ref type="bibr" target="#b15">[16]</ref> and GLCN <ref type="bibr" target="#b14">[15]</ref> are follows. First, LatGCN is designed based on our proposed new LatGCR block <ref type="figure" target="#fig_0">(Figure 1 (B)</ref>). In contrast, previous works generally incorporate graph learning into GNN's training via designing a joint loss function. Second, LatGCR is derived based on the joint reconstruction framework (Eq. <ref type="formula">(3)</ref>) which is implemented in a self-supervised manner. This makes LatGCR be a general block which can be used within many GNN's architectures to derive various kinds of LatGCNs.</p><formula xml:id="formula_9">H (1) P LatGCR ReLU A LatGCR H (0) H (L)</formula><p>LatGCR is also different from recent GeC and GeCN <ref type="bibr" target="#b5">[6]</ref>. (1) LatGCR aims to generate a flexible latent graph for graph convolution while GeC incorporates neighborhood selection into graph convolution.</p><p>(2) LatGCR is derived based on neighborhood reconstruction while GeC is designed based on graph Laplacian regularization. (3) LatGCR implements both graph estimation and GCR via simple one-step update rules while GeC <ref type="bibr" target="#b5">[6]</ref> adopts T -step update rules for both neighborhood selection and graph convolution operation. Thus, the implementation of LatGCR is generally more efficient than GeC <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>To verify the effectiveness and robustness of LatGCR block and LatGCN, we test it on both semisupervised node classification and unsupervised clustering tasks on three standard benchmark datasets, i.e., Cora, Citeseer and Pubmed <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Semi-supervised node classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Experimental setting</head><p>Similar to the architecture of GCN <ref type="bibr" target="#b1">[2]</ref>, LatGCN consists of one input layer, two LatGCR layers and one final perceptron layer. The skip-connection strategy is also utilized in LatGCN, as suggested in work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>. We optimize the network weight matrices of all LatGCR modules by minimizing the cross-entropy loss function <ref type="bibr" target="#b1">[2]</ref>. For fair comparison, we use the same attacked data setting used in work <ref type="bibr" target="#b11">[12]</ref> and employ two types of attacks, i.e., Metattack <ref type="bibr" target="#b10">[11]</ref> and Random Attack <ref type="bibr" target="#b17">[18]</ref>. For Metattack <ref type="bibr" target="#b10">[11]</ref>, we utilize the most destructive attack variant 'Meta-Self' and set the perturbation level from 0 to 25% with step 5%. For Pubmed dataset, we use the approximate version 'A-Meta-Self' as used in work <ref type="bibr" target="#b11">[12]</ref> For Random Attack <ref type="bibr" target="#b17">[18]</ref>, we apply the variant 'Add' and set the perturbation level from 0 to 100% with step 20%. Following the experimental setup in previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, we set the number of units in each hidden layer to 16 and train our LatGCN by using Adam algorithm <ref type="bibr" target="#b21">[22]</ref> with learning rate of 0.001. The recurrent time of each LatGCR block is set to 3 and the hyper-parameter ? is determined based on validation set. We provide additional experiments across different settings of parameter ? in ?5.3.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Comparison results</head><p>We first compare LatGCN with some traditional baseline methods including weighted mean-type GCN (GCN-m) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>, Graph Attention Networks (GAT) <ref type="bibr" target="#b2">[3]</ref>. To demonstrate the robustness of LatGCN, we also compare LatGCN with some recent robust GNNs including Robust Graph Convolutional Networks (RGCN) <ref type="bibr" target="#b9">[10]</ref>, SimP-GCN <ref type="bibr" target="#b20">[21]</ref>, Property Graph Neural Networks (Pro-GNN) <ref type="bibr" target="#b11">[12]</ref> and Graph elastic Convolutional Networks (GeCN) <ref type="bibr" target="#b5">[6]</ref>.</p><p>Effectiveness analysis. <ref type="figure" target="#fig_2">Figure 3</ref> and 4 summarize the comparison results across different levels of Metattack <ref type="bibr" target="#b10">[11]</ref> and Random Attack <ref type="bibr" target="#b17">[18]</ref>, respectively. For each attack level, the results are the average performance of 10 runs with different network initializations. The overall average performance of comparison methods for all attack levels are reported in the legend of each <ref type="figure">Figure.</ref> Here, we can observe that (1) Traditional GCN-m <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref> and GAT <ref type="bibr" target="#b2">[3]</ref> are vulnerable to the structural attacks and noises. Comparing with them, LatGCN obtains obviously better performance under various attacks and noises. This clearly demonstrates the effectiveness and robustness of the proposed LatGCR module for robust graph data learning. (2) LatGCN obtains better performance than some recent robust GNN methods including RGCN <ref type="bibr" target="#b9">[10]</ref> and SimP-GCN <ref type="bibr" target="#b20">[21]</ref>, which indicates the more robustness of the proposed LatGCN w.r.t graph structural attacks and noises. (3) Comparing with some recent graph learning guided GNNs including Pro-GNN <ref type="bibr" target="#b11">[12]</ref> and GeCN <ref type="bibr" target="#b5">[6]</ref>, LatGCN generally obtains the best average learning performance on attacked graph data. This further demonstrates the effectiveness of the proposed self-supervised latent graph estimation in LatGCR for noisy graph data representation.</p><p>Efficiency analysis. <ref type="figure" target="#fig_4">Figure 5</ref> shows the average running time of each epoch in training LatGCN on semi-supervised node classification tasks on the attacked datasets used in work <ref type="bibr" target="#b11">[12]</ref>. All methods are implemented by PyTorch on NVIDIA A6000. We can note that (1) The methods using fixed graph GNNs, such as GCN-m, SimP-GCN <ref type="bibr" target="#b20">[21]</ref> and RGCN <ref type="bibr" target="#b9">[10]</ref>, generally run faster than graph learning guided GNN methods. (2) Our LatGCN performs obviously faster than most of graph learning guided methods including GAT <ref type="bibr" target="#b2">[3]</ref>, GeCN <ref type="bibr" target="#b5">[6]</ref> and Pro-GNN <ref type="bibr" target="#b11">[12]</ref>, especially on the larger dataset Pubmed <ref type="bibr" target="#b18">[19]</ref>. It demonstrates the efficiency of the proposed LatGCN on conducting robust graph data learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Unsupervised clustering</head><p>To evaluate the effectiveness of the proposed self-supervised LatGCR, we further test it on unsupervised clustering tasks. Following the experimental setting in previous work <ref type="bibr" target="#b23">[24]</ref>, we first use Singular Value Decomposition (SVD) to replace projection step to obtain low-dimensional embeddings for graph nodes. Then, we utilize LatGCR to obtain context-aware representations for graph nodes and employ K-means clustering algorithm <ref type="bibr" target="#b24">[25]</ref> to obtain the final clustering results <ref type="bibr" target="#b23">[24]</ref>. We set the recurrent time and parameter ? to 1 and 0.03 respectively. Similar to work <ref type="bibr" target="#b23">[24]</ref>, we adopt three widely used performance measurements <ref type="bibr" target="#b24">[25]</ref>, i.e., clustering accuracy (Acc), normalized mutual information (NMI) and macro F1-score (F1) for evaluation.</p><p>We compare our LatGCR with some other popular clustering approaches including Graph Variational Autoencoder (VGAE) <ref type="bibr" target="#b25">[26]</ref>, Marginalized Graph Autoencoder (MGAE) <ref type="bibr" target="#b26">[27]</ref>, Adversarially Regularized Variational Graph Autoencoder (ARVGE) <ref type="bibr" target="#b27">[28]</ref> and Attributed Graph Clustering (AGC) <ref type="bibr" target="#b23">[24]</ref>. <ref type="table" target="#tab_2">Table 1</ref> summarizes the comparison results on all original datasets <ref type="bibr" target="#b18">[19]</ref>. The results of these comparison methods have been reported in work <ref type="bibr" target="#b23">[24]</ref> and we use them directly. From <ref type="table" target="#tab_2">Table 1</ref>, we can note that comparing with some other clustering methods, the proposed LatGCR can obtain the best average performance in most cases, which further indicates the effectiveness of the proposed LatGCR on conducting unsupervised clustering tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Visualization results</head><p>To demonstrate the effectiveness of proposed LatGCR, we utilize 2D t-SNE <ref type="bibr" target="#b22">[23]</ref> visualization to show the feature representation ability of LatGCR comparing with baseline method GCN-m <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b1">2]</ref>. <ref type="figure" target="#fig_5">Figure 6</ref> shows 2D t-SNE <ref type="bibr" target="#b22">[23]</ref> visualization results of the feature maps output by the first hidden layer of GCN-m and LatGCN on Cora and Citeseer <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12]</ref> datasets under 0.25% Metattack <ref type="bibr" target="#b10">[11]</ref>. Different colors denote different classes. One can note that, LatGCR obtains clearer and compacter embeddings than baseline method GCN-m, which intuitively demonstrates that the proposed LatGCR can obtain more robust and discriminative feature representations for graph data with structural attacks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Parameters analysis</head><p>One main hyper-parameter in LatGCR is the balanced parameter ? (Eq.(3)). <ref type="table" target="#tab_3">Table 2</ref> shows the semi-supervised classification results of LatGCN with different ? values on three datasets under 100% Random Attack <ref type="bibr" target="#b17">[18]</ref>. We can see that our LatGCN can achieve relatively stable results in a certain range of parameter ? value which indicates that LatGCR is generally insensitive to the value of hyper-parameter ? in a certain range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper proposes a novel Latent Graph Convolutional Representation (LatGCR) for robust graph data representation and learning. LatGCR is proposed based on a joint reconstruction framework, i.e., graph structure reconstruction + node's feature reconstruction. It can estimate a latent and flexible graph for GCR in a self-supervised way and provides a general basic block for GNNs. The main advantage of LatGCR is that it can perform robustly w.r.t graph structural attacks and noises. Experiments on several benchmark datasets demonstrate the effectiveness and robustness of LatGCR. In our future work, we will extend LatGCR to address the data with multiple graphs and further apply it on some more applications and tasks, such as computer vision, recommendation, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architectures of GCR and our LatGCR block.Complexity analysis. The whole computation complexity of the proposed LatGCR block involves three parts, i.e., linear projection, LGE and GCR. The complexity of projection ZW is O(ndd ). The computation cost of LGE-step mainly focuses on computing h i ? z j W 2 with j ? N i ? i and the complexity is O(|?|d ) where |?| denotes edge number and d denotes dimension of feature vector h i . The complexity of GCR-step is O(|?|d ). In summary, the whole LatGCR complexity is O(ndd ) + O(2r|?|d ) where r is the recurrent time for alternatively conducting LGE and GCR and set to 3 in experiments. Note that, LatGCR does not bring very high complexity when comparing with GCR (Eq.(1)) whose main complexity is O(ndd ) + O(|?|d ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of LatGCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Semi-supervised classification performance under Metattack<ref type="bibr" target="#b10">[11]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Semi-supervised classification performance under Random Attack<ref type="bibr" target="#b17">[18]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The empirical average running time in each epoch of different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>(a) GCN-m result on Cora dataset (b) LatGCN result on Cora dataset (c) GCN-m result on Citeseer dataset (d) LatGCN result on Citeseer dataset 2D t-SNE [23] visualizations of the feature map output from the first layer of GCN-m and LatGCN. Different colors denote different classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison results on clustering task. The best results are marked by bold.</figDesc><table><row><cell>Method</cell><cell cols="2">Cora Acc% NMI%</cell><cell>F1%</cell><cell cols="2">Citeseer Acc% NMI%</cell><cell>F1%</cell><cell cols="2">Pubmed Acc% NMI%</cell><cell>F1%</cell></row><row><cell>VGAE</cell><cell>55.95</cell><cell>38.45</cell><cell cols="2">41.50 44.38</cell><cell>22.71</cell><cell cols="2">31.88 65.48</cell><cell>25.09</cell><cell>50.95</cell></row><row><cell>MGAE</cell><cell>63.43</cell><cell>45.57</cell><cell cols="2">38.01 63.56</cell><cell>39.75</cell><cell cols="2">39.49 43.88</cell><cell>8.16</cell><cell>41.98</cell></row><row><cell cols="2">ARVGE 63.80</cell><cell>45.00</cell><cell cols="2">62.70 54.40</cell><cell>26.10</cell><cell cols="2">52.90 58.22</cell><cell>20.62</cell><cell>23.04</cell></row><row><cell>AGC</cell><cell>68.92</cell><cell>53.68</cell><cell cols="2">65.61 67.00</cell><cell>41.13</cell><cell cols="2">62.48 69.78</cell><cell>31.59</cell><cell>68.72</cell></row><row><cell cols="2">LatGCR 69.19</cell><cell>53.48</cell><cell cols="2">65.50 67.83</cell><cell>42.07</cell><cell cols="2">63.31 70.52</cell><cell>32.77</cell><cell>69.71</cell></row><row><cell cols="2">5.3 Model analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of LatGCN with different parameter ? values. 77.11 76.36 75.96 72.79 70.82 67.25 Citeseer 71.09 71.27 71.68 71.5 72.16 70.44 69.73 Pubmed 84.87 85.36 86.03 86.07 86.42 86.66 86.40</figDesc><table><row><cell>?</cell><cell>0.05</cell><cell>0.1</cell><cell>0.5</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>Cora</cell><cell>76.06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iterative deep graph learning for graph neural networks: Better and robust node embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="19314" to="19326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gecns: Graph elastic convolutional networks for data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reliable graph neural networks via robust aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13272" to="13284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Combining neural networks with personalized pagerank for classification on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Johannes Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust graph convolutional networks against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery amp; Data Mining</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial examples for graph data: Deep insights into attack and defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tyshetskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Docherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4816" to="4823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with graph learningconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11313" to="11320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Topology optimization based graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4054" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Incomplete Graph Representation and Learning via Partial Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adversarial attacks and defenses on graphs: A review and empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2003</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Node similarity preserving graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attributed graph clustering via adaptive graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, (IJCAI)</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4327" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Data</forename><surname>Clustering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms and Applications</title>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mgae: marginalized graph autoencoder for graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;17 -Proceedings of the 2017 ACM Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2609" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
