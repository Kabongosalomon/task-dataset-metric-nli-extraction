<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">JOURNAL OF IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE, Caifeng</roleName><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Shan</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">JOURNAL OF IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Action Recognition</term>
					<term>Skeleton Sequence</term>
					<term>Graph Convolutional Network</term>
					<term>EfficientNet</term>
					<term>Separable Convolution !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One essential problem in skeleton-based action recognition is how to extract discriminative features over all skeleton joints. However, the complexity of the recent State-Of-The-Art (SOTA) models for this task tends to be exceedingly sophisticated and over-parameterized. The low efficiency in model training and inference has increased the validation costs of model architectures in large-scale datasets. To address the above issue, recent advanced separable convolutional layers are embedded into an early fused Multiple Input Branches (MIB) network, constructing an efficient Graph Convolutional Network (GCN) baseline for skeleton-based action recognition. In addition, based on such the baseline, we design a compound scaling strategy to expand the model's width and depth synchronously, and eventually obtain a family of efficient GCN baselines with high accuracies and small amounts of trainable parameters, termed EfficientGCN-Bx, where "x" denotes the scaling coefficient. On two large-scale datasets, i.e., NTU RGB+D 60 and 120, the proposed EfficientGCN-B4 baseline outperforms other SOTA methods, e.g., achieving 92.1% accuracy on the cross-subject benchmark of NTU 60 dataset, while being 5.82? smaller and 5.85? faster than MS-G3D, which is one of the SOTA methods. The source code in PyTorch version and the pretrained models are available at https://github.com/yfsong0709/EfficientGCNv1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>H UMAN action recognition becomes increasingly crucial and achieves promising progress in various applications during the past decade, such as video surveillance, human-computer interaction, video retrieval and so on <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. One essential problem in human action recognition is how to extract discriminative and rich features to fully describe the variations of spatial configurations and temporal dynamics in human actions.</p><p>Currently, skeleton-based representations are very popular for human action recognition, as human skeletons provide a compact data form to depict dynamic changes in human body movements <ref type="bibr" target="#b3">[4]</ref>. Skeleton data is a time series of 3D coordinates of multiple skeleton joints, which can be either estimated from 2D images by pose estimation methods <ref type="bibr" target="#b4">[5]</ref> or directly collected by multimodal sensors such as Kinect <ref type="bibr" target="#b5">[6]</ref>. Moreover, compared to conventional RGB based action recognition methods, skeleton-based representations are more robust to the variations of illumination, camera viewpoints and other background changes. These merits inspire researchers to develop various methods to explore informative features from skeleton motion sequences for action recognition. The development of skeleton-based action recognition can be divided mainly into two phases. In early years, conventional methods adopt Recurrent Neural Network (RNN)-based or Convolutional Neural Network (CNN)based models to analyze skeleton sequences. For example, Du et al. <ref type="bibr" target="#b6">[7]</ref> employ a hierarchical bidirectional RNN to capture rich dependencies between different body parts. Li et al. <ref type="bibr" target="#b7">[8]</ref> design a simple yet effective CNN architecture for action classification from trimmed skeleton sequences. In recent years, due to the greatly expressive power for depicting structural data, graph-based models <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> have been proposed for modeling dynamic skeleton sequences. Yan et al. <ref type="bibr" target="#b10">[11]</ref> firstly propose a Spatial Temporal Graph Convolutional Networks (ST-GCN) for skeleton-based action recognition, after that increasing number of studies <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> are reported based on GCN models.</p><p>Nevertheless, for learning discriminative and rich features from skeleton sequences, current State-Of-The-Art (SOTA) models are often exceedingly sophisticated and over-parameterized, where the network often contains a multi-stream architecture with a large number of model parameters, which leads to a complicated training procedure and high computational cost (and thus low inference speed). For example, the 2s-AGCN in <ref type="bibr" target="#b12">[13]</ref> contains about 6.94 million parameters, and it takes nearly 4 GPU-days for model training on the NTU RGB+D 60 dataset <ref type="bibr" target="#b14">[15]</ref>. And the DGNN <ref type="bibr" target="#b15">[16]</ref> contains more than 26 million parameters, which makes it very hard to do parameter tuning on largescale datasets. The high model complexity has seriously limited the development of skeleton-based action recognition, while there are few literatures on this issue.</p><p>To tackle this problem, some efforts are made in this paper to extremely reduce the redundant trainable parameters while maintaining the model performance. an early fused Multiple Input Branches (MIB) architecture is constructed to capture rich features from both spatial configurations and temporal dynamics of joints in skeleton sequences. The early fusion strategy has been widely used to fuse multi-modal information for various visual tasks, such as Video Question Answering (VQA) <ref type="bibr" target="#b16">[17]</ref>. In this paper, we emphasize that the proposed MIB mainly aims to reduce the model parameters and computational costs of multistream GCN models for more efficient skeleton-based action recognition. In details, three input branches including joint positions (relative and absolute), motion velocities (one or two temporal steps), and bone features (lengths and angles) are fused in the early stage of the whole network, rather than the conventional late fusion at score layer in most multistream GCN models <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. The optimal fusion stage is chosen by exhaustive search (see Sec. <ref type="bibr">5.3.4)</ref>. To the best of our knowledge, it is the first time to investigate the impacts of different fusion locations and assess the optimal fusion layer in GCN-based skeleton action recognition. Secondly, besides the Basic Layer (BasicLayer) proposed in ST-GCN <ref type="bibr" target="#b10">[11]</ref>, we extend four kinds of convolutional layers in CNN, i.e., Bottleneck Layer (BottleLayer) <ref type="bibr" target="#b19">[20]</ref>, Separable Layer (SepLayer) <ref type="bibr" target="#b20">[21]</ref>, Expanded Separable Layer (EpSepLayer) <ref type="bibr" target="#b21">[22]</ref>, and Sandglass Layer (SGLayer) <ref type="bibr" target="#b22">[23]</ref>, to the GCN network for extracting temporal dynamics and compressing the model size. These four layers can obviously reduce the amount of parameter tuning costs in training, and accelerate the model inference in testing.</p><p>Thirdly, in order to determine the structural hyperparameters for each block, we resort to the compound scaling method proposed by Tan and Le <ref type="bibr" target="#b23">[24]</ref>, which uniformly scales the network width, depth and resolution with a set of fixed scaling coefficients. Due to its high efficiency, the original compound scaling strategy has gradually become a popular baseline constructing method for many visual recognition tasks. However, it is quite hard to directly utilize the compound scaling strategy in GCN-based models because the resolution scaling in original EfficientNet <ref type="bibr" target="#b23">[24]</ref> cannot be implemented on graph data explicitly. To address this issue, we modify the original scaling strategy to adapt to graph data, by removing the resolution scaling factor and rebuilding the constraint between width and depth factors. Such a strategy improves the model performance in an efficient way, bringing a competitive model accuracy with significantly fewer model parameters and lower computational cost than other GCN-based SOTA methods.</p><p>Finally, for more accurate recognition, inspired by Hou et al. <ref type="bibr" target="#b24">[25]</ref>, an attention module, named Spatial Temporal PA-ResGCN-B19 <ref type="bibr" target="#b26">[27]</ref> 4s-Shift-GCN <ref type="bibr" target="#b50">[51]</ref> 2s-AGCN <ref type="bibr" target="#b12">[13]</ref> MS-G3D <ref type="bibr" target="#b17">[18]</ref> SGN <ref type="bibr" target="#b35">[36]</ref> DynamicGCN <ref type="bibr" target="#b33">[34]</ref> NAS-GCN <ref type="bibr" target="#b49">[50]</ref> EfficientGCN Other methods Joint Attention (ST-JointAtt), is proposed and inserted into each block of the model. This attention module aims to find the most essential joints from the whole skeleton sequence, and eventually enhances the model ability to extract discriminative features. Compared to other attention modules such as STC-attention (STCAtt) <ref type="bibr" target="#b25">[26]</ref> and Part-wise Attention (PartAtt) <ref type="bibr" target="#b26">[27]</ref>, this new module jointly deals with the spatial and temporal attentions, while STCAtt is asynchronous and PartAtt ignores the temporal differences. In addition, compared with the previous PartAtt module, the proposed ST-JointAtt module can be used without manual division of parts in skeleton graph, thus eliminates the need of designing appropriate pooling rules over joints in each part. Accordingly, we choose the ST-JointAtt module for building an efficient and general baseline with as few manual interventions as possible.</p><p>Combining these efforts mentioned above, a family of efficient GCN baselines with relatively small amounts of trainable parameters while keeping competitive performance to other SOTA methods is obtained and termed EfficientGCN-Bx, where "x" denotes the scaling coefficient. The whole pipeline of EfficientGCN is shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, where the three input sequences (Joint, Velocity and Bone) are initially extracted from the original skeleton sequence. Next, each sequence is sent to a separate input branch consisting of several convolutional blocks. Then, the three branches will be fused and passed through the main stream. Similar to the input branches, the main stream is also composed of a number of convolutional blocks. Finally, the features of all frames and joints are globally averaged and fed into a Fully Connected (FC) layer for action classification. In this paper, three types of EfficientGCN with scaling coefficients of {0,2,4} are provided to verify the effectiveness of our approach. Compared to the most popular baseline, i.e., 2s-AGCN <ref type="bibr" target="#b12">[13]</ref>, the EfficientGCN-B0 achieves over 1% relative performance improvement on both NTU RGB+D 60 <ref type="bibr" target="#b14">[15]</ref> and 120 <ref type="bibr" target="#b27">[28]</ref> datasets, while only needs 23.93? smaller amount of model parameters and achieves 13.67? faster inference speed. Besides, EfficientGCN-B4 obtains the SOTA performance on the two datasets, e.g., 92.1% on the crosssubject benchmark of NTU 60 dataset. Furthermore, when considering the model size and graph computational cost, EfficientGCN-B4 is 5.82? smaller and 5.85? faster than MS-G3D <ref type="bibr" target="#b17">[18]</ref>, which is one of the best SOTA methods in the field. For a clear illumination, <ref type="figure" target="#fig_3">Fig. 2</ref> is drawn to demonstrate the accuracy-parameter performance of EfficientGCN, where the EfficientGCN is remarkably better than other SOTA methods.</p><p>This work is an extension of an earlier and preliminary version presented in <ref type="bibr" target="#b26">[27]</ref>, namely ResGCN. Compared to our previous work, main modifications and contributions of this paper are summarized as follows:</p><p>? For temporal convolutional layers, ResGCN proves that the BottleLayer is efficient for the GCN network. Besides, this work further introduces other three types of layers (SepLayer, EpSepLayer and SGLayer) to skeleton-based action recognition, further improving the model efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>In previous work, each block in the model is constructed with manually selected hyper-parameters, where the number of layers and channels are fixed. In contrast, this study employs a compound scaling strategy to configure the model's width and depth with a scaling coefficient, which brings an effective and more flexible approach to design the model architecture.</p><p>? In ResGCN, an attention module named PartAtt is proposed to assign spatial attentions to body parts, while this paper offers a fine-grained module (ST-JointAtt), which not only considers the spatial attention, but also distinguishes the most essential temporal frames.</p><p>? Compared to the preliminary version, EfficientGCN achieves a better performance with a significantly lower number of parameters and calculations on two large-scale datasets, i.e., NTU RGB+D 60 &amp; 120. For example, EfficientGCN-B4 obtains a 92.1% accuracy on the cross-subject benchmark with only 2.03 million parameters, significantly smaller and faster than ResGCN.</p><p>The remainder of this paper is organized as follows: Sec. 2 describes recent studies related to our work. Sec. 3 briefly introduces several crucial techniques of the proposed EfficientGCN. Sec. 4 presents the details of our EfficientGCN baselines. Extensive experiments on two large-scale datasets are reported in Sec. 5, and the conclusion is given in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Skeleton-based Action Recognition</head><p>Due to its compactness to the RGB-based representations, action recognition based on skeleton data has received increasing attentions. In an earlier work <ref type="bibr" target="#b28">[29]</ref>, a convolutional co-occurrence feature learning framework is proposed, where a hierarchical methodology is employed to gradually aggregate different levels of contextual information. The study in <ref type="bibr" target="#b29">[30]</ref> designs a view adaptive model to automatically regulate observation viewpoints during the occurrence of an action, so as to obtain view invariant representations of human actions. However, due to the ignorance of spatial configurations, these CNN or RNN-based models gradually fade out from the stage of frontier research.</p><p>Inspired by the booming graph-based methods, Yan et al. <ref type="bibr" target="#b10">[11]</ref> firstly introduce GCN into the skeleton-based action recognition task, and propose the ST-GCN to model the spatial configurations and temporal dynamics of skeletons synchronously. Following this work, Song et al. <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b30">[31]</ref> aim to solve the occlusion problem in this task, and propose a multi-stream GCN to extract rich features from more activated skeleton joints. Liu et al. <ref type="bibr" target="#b17">[18]</ref> explore the effects of multi-adjacency GCN and dilated temporal CNN, and design a sophisticated model named MS-G3D to disentangle multi-scale graph convolutions. Furthermore, the study in <ref type="bibr" target="#b31">[32]</ref> provides a decoupling GCN to boost the graph modeling ability with no extra computation. To achieve global joint relationship modeling, Shi et al. <ref type="bibr" target="#b12">[13]</ref> introduce the Nonlocal method <ref type="bibr" target="#b32">[33]</ref> into a two-stream GCN model, named 2s-AGCN, which significantly improves the recognition accuracy. Similar as 2s-AGCN, Dynamic GCN proposed by Ye et al. <ref type="bibr" target="#b33">[34]</ref> offers a novel method to model global dependency, by which the model achieves outstanding accuracy for skeleton-based action recognition. Although these methods achieve considerable performance, the increasing computational cost caused by the multi-stream structure becomes the obstacle to apply them in real scenarios. Therefore, how to reduce the complexity of the GCN models is still a challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Efficient Models</head><p>The model efficiency commonly indicated by the number of trainable parameters and Floating-point Operations Per Second (FLOPs), is always a non-negligible indicator in deep learning tasks. Extensive studies have made efforts to enhance the efficiencies of neural networks, i.e., reducing the amount of model parameters or FLOPs, such as MobileNetv1 <ref type="bibr" target="#b20">[21]</ref>, MobileNetv2 <ref type="bibr" target="#b21">[22]</ref>, MobileNeXt <ref type="bibr" target="#b22">[23]</ref>, and EfficientNet <ref type="bibr" target="#b23">[24]</ref>. The model family of MobileNet mainly cuts the model size by separable convolutions, which factorizes standard convolutions into a depth-wise convolution applied to every channel individually and a 1 ? 1 pointwise convolution to combine the outputs of the depthwise convolution. To further determine the structural hyperparameters in neural networks, compound scaling <ref type="bibr" target="#b23">[24]</ref> is proposed to build a family of EfficientNet models.</p><p>Some existing studies for skeleton-base action recognition have also been considering the model complexity problem. The study of <ref type="bibr" target="#b34">[35]</ref> constructs a lightweight network with CNN-based blocks, which is not as accurate as GCN models. The work in <ref type="bibr" target="#b35">[36]</ref> adopts a complex data preprocessing strategy, whose inputs include positions, velocities, frame indexes and joint types. This data preprocessing module enables the model to recognize actions with a shallow model, thereby achieves a very fast inference speed with 188 sequences/(second*GPU), yet its performance is obviously lower than other SOTA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attention Models</head><p>Attention mechanisms have become an integral part of compelling sequence modeling in various tasks. Traditional attention modules for image processing can be divided into two categories: 1) channel-wise and 2) spatial-wise. Specifically, SENet <ref type="bibr" target="#b36">[37]</ref> uses a bottleneck structure to obtain attention scores at channel dimension, providing a paradigm to build channel-wise attentions. Based on SENet, CBAM <ref type="bibr" target="#b37">[38]</ref> not only focuses on channel-wise attention, but also utilizes convolutional layers to calculate attention maps at spatial dimension for adaptive feature refinement. Along with the popularity of Self Attention (SelfAtt) in Natural Language Processing (NLP), the Non-Local <ref type="bibr" target="#b32">[33]</ref> method employs Self-Att at spatial dimension, which globally explores attentions for the relationship between each pair of pixels. With respect to action recognition, Baradel et al. <ref type="bibr" target="#b38">[39]</ref> introduce the attention mechanism into an RGB-based action recognition model, which uses human poses to calculate spatial and temporal attentions. The study in <ref type="bibr" target="#b39">[40]</ref> firstly introduces attention modules into skeleton-based action recognition, where a spatial-temporal attention Long Short-Term Memory (LSTM) is built to allocate different levels of attention to the discriminative joints within each frame. Si et al. <ref type="bibr" target="#b40">[41]</ref> also incorporate attention modules within LSTM units. Both of the two models apply attention modules for each frame individually, which may attend to some unstable noisy features. Besides, 2s-AGCN <ref type="bibr" target="#b12">[13]</ref> offers a variant of attention model based on the Non-Local structure, and its improved version Dynamic-GCN <ref type="bibr" target="#b33">[34]</ref> proposes another way to obtain the globally spatial attentions. In addition, Cheng et al. <ref type="bibr" target="#b31">[32]</ref> embeds attention into its DropGraph module, leading to a significant accuracy increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY TECHNIQUES</head><p>In this section, we briefly discuss several crucial techniques used in the proposed EfficientGCN. Firstly, the data preprocessing module is introduced and formulated. Then, the GCN layer is reviewed. Finally, we compare the separable convolution to the standard convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Preprocessing</head><p>Data preprocessing is very essential for skeleton-based action recognition, according to previous studies <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b41">[42]</ref>. In this work, the input features after various preprocessing are mainly divided into three classes: 1) joint positions, 2) motion velocities and 3) bone features.</p><p>Suppose that the original 3D coordinate set of an action sequence is X = {x ? R Cin?Tin?Vin }, where C in , T in , V in denote the input coordinates, frames, and joints, respectively. Then the relative position set is obtained as the nor- * = </p><formula xml:id="formula_0">i |i = 1, 2, ? ? ? , V in }, where r i = x[:, :, i] ? x[:, :, c],<label>(1)</label></formula><p>and c represents the index of the center spine joint. Next, the input of joint positions is formed by the concatenation of X and R. Moreover, it is easy to obtain the two sets of motion velocities, F = {f t |t = 1, 2, ? ? ? , T in } for fast motion and S = {s t |t = 1, 2, ? ? ? , T in } for slow motion, with the following definitions</p><formula xml:id="formula_1">f t = x[:, t + 2, :] ? x[:, t, :], s t = x[:, t + 1, :] ? x[:, t, :].<label>(2)</label></formula><p>And the input of motion velocities is acquired by concatenating F and S for each joint to obtain a feature vector at each time. Finally, the input of bone features consists of the bone lengths L = {l i |i = 1, 2, ? ? ? , V in } and the bone angles</p><formula xml:id="formula_2">A = {a i |i = 1, 2, ? ? ? , V in }.</formula><p>To obtain these two sets, the lengths and angles of each bone are calculated by</p><formula xml:id="formula_3">l i = x[:, :, i] ? x[:, :, i adj ], a i,w = arccos( li,w ? l 2 i,x +l 2 i,y +l 2 i,z ),<label>(3)</label></formula><p>where i adj means the adjacent joint of the i-th joint, and w ? {x, y, z} denotes the 3D coordinates. <ref type="figure" target="#fig_4">Fig. 3</ref> displays the calculation diagram for these three inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Convolution</head><p>According to Yan et al. <ref type="bibr" target="#b10">[11]</ref>, graph convolutional operation for each frame t can be written as</p><formula xml:id="formula_4">f out (v ti ) = vtj ?N (vti) 1 Z ti (v tj ) f in (v tj ) ? w(l ti (v tj )),<label>(4)</label></formula><p>where v ti denotes the i-th joint at the t-th frame, f in (?) and f out (?) are the input and output features of corresponding joints, N (v ti ) is the neighbor set of v ti , the normalizing term Z ti is set to balance the contributions of different neighbors, w(?) is a weight function to allocate weights indexed by the label function l ti (?), which is designed to construct several neighbor sets N (v ti ) by assigning different labels to each graph node. There are three label functions in <ref type="bibr" target="#b10">[11]</ref>, but we only choose the distance based partition in our model,</p><formula xml:id="formula_5">which defines l ti (v tj ) = d(v ti , v tj ), where d(v ti , v tj )</formula><p>denotes the graphic distance between v ti and v tj . The joints with the same distance will form a subset and share a learnable weight function w(?). Generally, with the adjacency matrix A, Eq. 4 can be transformed into:</p><formula xml:id="formula_6">f out = D d=0 W d f in (? ? 1 2 d A d ? ? 1 2 d M d ),<label>(5)</label></formula><p>where D is a predefined maximum graphic distance, f in and f out denote the input and output feature maps, means element-wise product, A d represents the d-th order adjacency matrix that marks the pairs of joints with a graphic distance d, and ? d is used to normalize A d . W d and M d are both learnable parameters, which are utilized to implement the convolution operation and tune the importance of each edge, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Separable Convolution</head><p>Separable convolution is initially designed as the core layers based on which MobileNet <ref type="bibr" target="#b20">[21]</ref> is built, aiming at the deployment of deep learning models on computationally limited platforms such as robotics, self-driving car, augmented reality, etc.. As its name implies, separable convolution factorizes a standard convolution into a depthwise convolution and a point-wise convolution. Concretely, for depth-wise convolution, a convolutional filter is only applied to one corresponding channel, while the point-wise convolution uses a 1 ? 1 convolution layer to combine the output of depth-wise convolution and to adjust the number of output channels. The comparison of standard convolution and separable convolution is displayed in <ref type="figure">Fig. 4</ref>.</p><p>Concretely, suppose that the input feature size is D f ? D f , the kernel size is D k ? D k , the number of input and output channels are C in and C out , then the calculational process of standard convolution is illustrated in the top row of <ref type="figure">Fig. 4</ref>. This brings a batch of trainable</p><formula xml:id="formula_7">parameters numbered D k ? D k ? C in ? C out ? D f ? D f .</formula><p>With respect to separable convolution shown in the bottom two rows of <ref type="figure">Fig. 4</ref>, the computational cost is changed to</p><formula xml:id="formula_8">D k ? D k ? C in ? D f ? D f + C in ? C out ? D f ? D f .</formula><p>Note that the most of trainable parameters are contained in the point-wise convolution <ref type="bibr" target="#b20">[21]</ref>, which is implemented by a 1 ? 1 convolution. Thus, if the numbers of input and output channels are big enough, e.g., &gt; 256, then the computational cost of separable computation will be decreased by nearly D k ? D k times compared to that of standard convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EFFICIENTGCN</head><p>This part provides technical details to the proposed Ef-ficientGCN for skeleton-based action recognition. Firstly, the MIB architecture is discussed and an example of   EfficientGCN-B0 is constructed. Then, four kinds of convolutional layers popularly used in CNNs are extended to graph convolution to increase the efficiency of GCN blocks. Next, a compound scaling strategy is utilized to synchronously scale the width and depth of EfficientGCN-B0, generating a family of efficient baselines. Finally, an attention module is proposed to enhance the discrimination of skeleton features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Architecture</head><p>After the data preprocessing module designed in Sec. 3.1, three types of input data are obtained, i.e., Joint, Velocity and Bone. For current high-performance complex models, they usually apply a multi-stream architecture to handle these input data. For example, Shi et al. <ref type="bibr" target="#b12">[13]</ref> take the joint data and bone data as inputs for feeding to two GCN branches with the same model structures separately, and eventually choose the fusion results of two streams as the final decision. This is an effective way to augment the input data and enhance the model performance. However, a multi-stream network often means high computational cost and difficulties of parameter turning on large-scale datasets. Thus, we devise the MIB architecture that fuses the three input branches at the early stage of our model, then apply one main stream to extract discriminative features. This architecture not only retains the rich input features, but also significantly suppresses the model complexity with fewer parameters, thus is easier to be trained. An example of EfficientGCN with the MIB architecture is demonstrated in <ref type="figure" target="#fig_7">Fig. 5</ref>. Concretely, the input branches are formed by orderly stacking a BatchNorm layer for fast convergence, an initial block implemented by ST-GCN layer <ref type="bibr" target="#b10">[11]</ref> for data-to-feature transformation, and two GCN blocks with attentions for informative feature extraction. After the input branches, a concatenation operation is employed to fuse the feature maps of three branches and then send them to the main stream, which is constructed by two GCN blocks. Finally, the output feature map of the main stream is globally averaged to a feature vector, and an FC layer is used to determine the final action class.</p><formula xml:id="formula_9">(b) BottleLayer (a) BasicLayer (c) SepLayer (d) EpSepLayer (e) SGLayer * 1 Conv ( , ), /2 1 * 1 Conv ( , ) * 1 Conv ( , ), /2 1 * 1 Conv ( , ) * 1 D-Conv ( , ), /2 1 * 1 P-Conv ( , ) 1 * 1 Conv ( , ) * 1 D-Conv ( , ), /2 1 * 1 P-Conv ( , ) * 1 D-Conv ( , ) 1 * 1 P-Conv ( , ) 1 * 1 P-Conv ( , ) * 1 D-Conv ( , ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Block Details</head><p>Inspired by MS-G3D <ref type="bibr" target="#b17">[18]</ref> which achieves a considerable performance, as the subplot of <ref type="figure" target="#fig_7">Fig. 5</ref> shows, the basic components of EfficientGCN (i.e., GCN blocks) are implemented by orderly stacking a Spatial Graph Convolutional (SGC) layer, several Temporal Convolutional (TC) layers and an attention module. The depth for each GCN block is defined as the number of TC layers stacked in this block. Besides, for each layer, a residual link is utilized to make the model optimization more easily than the original unreferenced feature projection. It also should be noted that the first TC layer has a stride of 2 for each block in the main stream, which aims to compress the features and reduce the convolutional costs.</p><p>In detail, the SGC layer is implemented by a graph convolution mentioned in Sec. 3.2, and the attention module can be implemented by the proposed ST-JointAtt (see Sec. <ref type="bibr">4.4)</ref> or other traditional attention modules. For the implementation of TC layer, except for the basic L ? 1 convolution (BasicLayer) originally used in ST-GCN model <ref type="bibr" target="#b10">[11]</ref>, we introduce four types of convolutional architectures widely used in CNN literatures to further boost the efficiency of GCN models (see <ref type="figure">Fig. 6</ref>). Specifically, for BottleLayer, a bottleneck structure <ref type="bibr" target="#b19">[20]</ref> is utilized for temporal convolution, which is also used in our preliminary version of this paper, i.e., ResGCN <ref type="bibr" target="#b26">[27]</ref>. The other three layers, i.e., SepLayer, EpSepLayer and SGLayer, are inspired by three versions of separable convolutions <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, all of which are composed of depth-wise convolutions and point-wise convolutions mentioned in Sec. 3.3. Note that the block with a certain layer, e.g., BasicLayer, is denoted as BasicBlock for simplicity, and by analogy to BottleBlock, SepBlock, EpSepBlcok and SGBlock.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scaling Strategy</head><p>Empirically, expanding both the width (e.g., WRN <ref type="bibr" target="#b43">[44]</ref>) and the depth (e.g., ResNet <ref type="bibr" target="#b19">[20]</ref>) of networks will benefit the model capability and hence improve the performance. Commonly, the model's width and depth are defined as the numbers of channels and layers. These two factors are often considered independently, and determined by handcrafted adjustment. However, in a recent work et al. <ref type="bibr" target="#b23">[24]</ref>, Tan and Le show that it is critical to balance all dimensions of network, e.g., width/depth/resolution, based on which a compound scaling strategy is proposed to scale network width, depth, and resolution with a set of fixed scaling coefficients. The obtained EfficientNets significantly outperform other convolutional networks, but with much smaller model sizes. Inspired by that, after removing the resolution factor, we propose a new scaling strategy for skeleton-based action recognition, by which a family of models are constructed in a principled way: where m w and m d are width and depth multipliers, ? is a compound coefficient to state the available resources for model scaling, ? and ? are both hyper-parameters to control the resource assignments to the model's width and depth. In this paper, ? and ? are set to 1.2 and 1.35 by a small grid search (see Sec. 5.4). Here, the resolution factor is omitted due to the pre-defined skeleton structure. The modification of skeleton's resolution will destroy the graph convolutional operation.</p><formula xml:id="formula_10">width: m w = ? ? depth: m d = ? ? s.t. ? 2 ? ? ? 2 ? ? 1, ? ? 1 (6)</formula><p>The constraint conditions in Eq. 6 are utilized to constrain the increasing speed of the model size. When doubling the model depth or width, the FLOPs will approximately increase to 2 or 4 times. Thus, the FLOPs of the scaled model will be (? 2 ? ?) ? ? 2 ? times than the baseline. Note that this increase may differ from theoretical values due to the rounding function (see Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Spatial Temporal Joint Attention</head><p>Previous attention modules for skeleton-based action recognition are mainly implemented by a Multi-layer Perception (MLP) like SENet structure <ref type="bibr" target="#b36">[37]</ref>, such as AGC-LSTM <ref type="bibr" target="#b40">[41]</ref> and MS-AAGCN <ref type="bibr" target="#b25">[26]</ref>. These modules are usually performed on each channel or spatial dimension independently, while other dimensions are globally averaged to a single unit. The preliminary version of this paper <ref type="bibr" target="#b26">[27]</ref> proposes a Par-tAtt module which only works on the spatial dimension. However, intuitively, the spatial and temporal information could be relevant to each other. Thus, separately considering frames and joints is sub-optimal for weighting the importance of skeleton joints in different action phases. To address this issue, inspired by coordinate attention <ref type="bibr" target="#b24">[25]</ref>, we propose a novel attention module, named Spatial Temporal Joint Attention (ST-JointAtt), to jointly distinguish the most informative joints in certain frames from the whole skeleton sequence.</p><p>The overview of the proposed ST-JointAtt module is shown in <ref type="figure" target="#fig_9">Fig. 7</ref>, from which the input features are firstly averaged in frame-and joint-level respectively. Then, these pooled feature vectors are concatenated together and fed through an FC layer to compact information. Next, two independent FC layers are utilized to obtain two sets of attention scores for frame dimension and joint dimension respectively. Finally, the scores of frames and joints are multiplied by channel-wise outer-product, and the result can be seen as the attention scores for the whole action sequence. The proposed ST-JointAtt module can be formulated as</p><formula xml:id="formula_11">f inner = ?((pool t (f in ) ? pool v (f in )) ? W )<label>(7)</label></formula><p>f</p><formula xml:id="formula_12">out = f in (?(f inner ? W t ) ? ?(f inner ? W v ))<label>(8)</label></formula><p>where f in and f out denote input and output feature maps, ? denotes concatenation operation, ? and mean channelwise outer-product and element-wise product, pool t (?) and pool v (?) are average pooling operations on frame-and jointlevel respectively, ?(?) and ?(?) represent Sigmoid and HardSwish <ref type="bibr" target="#b42">[43]</ref> activation functions, and W ? R C? C r ,</p><formula xml:id="formula_13">W t ? R C r ?C , W v ? R C r ?C are trainable parameters.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Loss Function</head><p>Suppose that the final FC layer in the EfficientGCN model outputs a logits vector z ? R Q , where Q is the number of action classes, the classification probabilities for the input sample can be computed by a Softmax function, i.e.,</p><formula xml:id="formula_14">y i = e zi Q j=1 e zj , i = 1, 2, ? ? ? , Q,<label>(9)</label></formula><p>where z i denotes the i-th element of z. Then, a crossentropy loss is calculated as the objective function for model optimization:</p><formula xml:id="formula_15">L = ? Q i=1 y i log? i<label>(10)</label></formula><p>where y ? R Q is the one-hot vector indicating the ground truth of action class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussion</head><p>In this section, we interpret why the EfficientGCN method can achieve superior accuracy but with much fewer model parameters than traditional GCN models. Firstly, the use of separable convolution in TC layers brings high efficiency to the model, by which the EfficientGCN reduces model parameters and FLOPs significantly. It should be noticed that the parameter reduction caused by separable convolution may not always hurt the model performance, since separable convolution has been proved to capture the most effective part of standard convolutions and meanwhile discard other redundant parts <ref type="bibr" target="#b44">[45]</ref>. Moreover, as to the superior accuracy achieved by the Ef-ficientGCN, it is mainly attributed to the compound scaling strategy and the ST-JointAtt module, where the former one is based on an empirical experience in CNN-based visual recognition that carefully balancing network depth, width, and resolution can lead to better performance <ref type="bibr" target="#b23">[24]</ref>, and the latter one further enhances the learning of spatial-temporal joint features through making the GCN attends on those informative joints and frames in action sequences.</p><p>To validate the above analysis, extensive ablation studies have been performed to evaluate the impacts and contributions of the above three components in the next section (see Section Sec. 5.3, 5.4, and 5.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>In this section, we evaluate the proposed EfficientGCN on two large-scale datasets, i.e., NTU RGB+D 60 <ref type="bibr" target="#b14">[15]</ref> and NTU RGB+D 120 <ref type="bibr" target="#b27">[28]</ref>. Ablation studies are also performed to validate the contribution of each component in our model. For simplicity, all experiments in ablation studies choose EfficientGCN-B0 with SGBlock (r rd = 2) as the baseline model (details can be seen in Appendix A). Finally, result analysis and visualization are reported to prove the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">NTU RGB+D 60</head><p>This large-scale indoor dataset is provided in <ref type="bibr" target="#b14">[15]</ref>, which contains 56880 human action videos collected by three Kinect v2 cameras. These actions consist of 60 classes, where the last 10 classes are all interactions between two subjects. For simplicity, the input frame number is set to 300, and the sequences with less than 300 frames are padded by 0 at the end. Each frame contains no more than 2 skeletons, and each skeleton is composed of 25 joints. The authors of this dataset recommend two benchmarks: 1) cross-subject (Xsub) contains 40320 training videos and 16560 evaluation videos divided by splitting the 40 subjects into two groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) cross-view (X-view) recognizes the videos collected by cameras 2 and 3 as training samples (37920 videos)</head><p>, while the videos collected by camera 1 are treated as evaluation samples (18960 videos). Note that there are 302 wrong samples selected by <ref type="bibr" target="#b27">[28]</ref> that need to be ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">NTU RGB+D 120</head><p>This is currently the largest indoor skeleton-based action recognition dataset <ref type="bibr" target="#b27">[28]</ref>, which is an extended version of the NTU RGB+D 60. It totally contains 114480 videos performed by 106 subjects from 155 viewpoints. These videos consist of 120 classes, extended from the 60 classes of the previous dataset. Similarly, two benchmarks are suggested: 1) cross-subject (X-sub120) divides subjects into two groups, to construct training and evaluation sets (63026 and 50922 videos respectively). 2) cross-setup (X-set120) contains 54471 videos for training and 59477 videos for evaluation, which are separated based on the distance and height of their collectors. According to <ref type="bibr" target="#b27">[28]</ref>, 532 bad samples of this dataset should be ignored in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>In our experiments, the maximum number of training epochs is set to 70. The initial learning rate is set to 0.1 and decays with a cosine schedule after the 10th epoch. Moreover, a warmup strategy <ref type="bibr" target="#b19">[20]</ref> is applied over the first 10 epochs, gradually increasing the learning rate from 0 to the initial value for a stable training procedure. The stochastic gradient descent (SGD) with the Nesterov momentum of 0.9 and the weight decay of 0.0001 is employed to tune the parameters. The hyper-parameters D and L defined in Sec. 3.2 and 4.2 are set to 2 and 5 respectively, which are determined by a grid search (see Appendix B). Other structural parameters will be discussed in ablation studies (Sec. 5.3).</p><p>In addition, a dropout layer with 0.25 drop probability is added after the GAP layer and before the final FC layer to avoid overfitting. It also should be noticed that the activated function used in all convolutional blocks is chosen as Swish function <ref type="bibr" target="#b45">[46]</ref>, which is similar with ReLU function but smooth and differentiable everywhere. In our experiments on X-view benchmark, a special data transformation <ref type="bibr" target="#b12">[13]</ref> is performed for view alignment. All our experiments are performed on two TITAN RTX GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>In this part, we mainly discuss the contributions of different components in the proposed EfficientGCN, including the selection of TC layers, the choice of the attention modules, the importance of data preprocessing module, and the necessity of the early fused architecture. This section explains why we use these settings to construct the baseline model EfficientGCN-B0. All of the experiments in this section are performed more than 10 times to compute the mean accuracy and standard error for convincing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Comparisons of TC Layers</head><p>In Sec. 4.2 and <ref type="figure">Fig. 6</ref>, five types of TC layers are provided, namely BasicLayer, BottleLayer, SepLayer, EpSepLayer and SGLayer. To select the best TC layer for skeleton-based action recognition, we test them with the EfficientGCN-B0 model on X-sub benchmark, and the results are presented in  Tab. 1, where r rd and r ep denote the ratios of reducing and expanding channels in the corresponding layer. As shown in Tab. 1, although the EpSepLayer (r ep = 4) obtains the highest mean accuracy (90.1%), the SGLayer with r rd = 2 achieves the optimal trade-off between performance and computational cost, resulting in the competitive mean accuracy of 90.0%, lower standard derivation of 0.10%, faster inference speed with 2.73G FLOPs, and smaller model size with 0.29M parameters. Therefore, we choose the SGLayer with r rd = 2 to build the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Comparisons of Attention Modules</head><p>To enhance the model ability to extract informative features, the ST-JointAtt module is designed and embedded into the convolutional blocks. We compare ST-JointAtt with other attention modules, i.e., ChannelAtt, FrameAtt, Join-tAtt, STCAtt, and PartAtt, and the results are presented in Tab. 2, where the first three attention modules are designed by ourselves based on the SENet <ref type="bibr" target="#b36">[37]</ref> structure, and the last two are proposed in <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, respectively. It is observed that, after incorporating attention modules, the model achieves obvious accuracy improvement. And ST-JointAtt produces the best accuracy, while other five modules are slightly worse. This indicates the importance of inserting attention modules and the effectiveness of the ST-JointAtt module in finding the most informative joints and frames from the whole skeleton sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Necessity of Data Preprocessing</head><p>Data preprocessing is essential to enhance the model performance, proven by several previous studies <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b41">[42]</ref>. We summarize these preprocessing methods into three classes, i.e., joint positions, motion velocities and bone features. To explore the necessity of each input branch, we present Tab. 3, from which the model with three inputs is  clearly better than others. With the increase of branches, the model performance is improved. This phenomenon further confirms the effectiveness of the data preprocessing module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Necessity of Early Fused Architecture</head><p>Originally proposed by 2s-AGCN <ref type="bibr" target="#b12">[13]</ref>, multi-stream models fused at the final score layer gradually dominate the model architectures for skeleton-based action recognition. However, the late fusion strategy at score layer requires training three models independently to deal with three types of input data. So, this strategy cannot be implemented within an end-to-end training pipeline and thereby increases the training complexity. Furthermore, we find that many parameters in multi-stream models are redundant, with no contribution to model performance. Thus, we construct an early fused MIB architecture in this paper. Tab. 4 gives the results of models with different fusion stages. It is seen that fusing after the 2nd stage is the inflection point of the accuracy-parameter curve, which is the best model to balance accuracy and complexity. Although fusing at the score layer brings a comparable model accuracy, the model size and computational cost are greatly increased, leading to an exceedingly sophisticated and over-parameterized model. Accordingly, the MIB fused after the 2nd stage is chosen as the architecture of our baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparisons of Compound Scaling Strategies</head><p>In this section, we test several compound scaling strategies with different width and depth scaling hyper-parameters (? and ?), which are mentioned in Sec. 4.3. To choose the best setting of ? and ? within the constraint conditions in Eq. 6, we set ? to {1.0, 1.1, 1.2, 1.3, 1.4}, respectively, and the hyper-parameter ? can be calculated by Eq. 6 with a precision of 0.05. Then, from the baseline model EfficientGCN-B0, we construct five EfficientGCN-B2 and five EfficientGCN-B4 with different scaling hyper-parameters. These models' accuracies and parameter numbers are shown in <ref type="figure" target="#fig_10">Fig. 8</ref>, from which the EfficientGCN-B4 obtains the best performance when ? = 1.2 and ? = 1.35. It is also worth noting that, merely increasing either width or depth may hurt the model performance, whose accuracies with EfficientGCN-B4 are obviously lower than those of compound scaling strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparisons with SOTA Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">NTU RGB+D 60 Dataset</head><p>We compare our EfficientGCN family against previous SOTA methods on both X-sub and X-view benchmarks of NTU 60 datasets. The results are displayed in Tab. 5. It should be noted that most previous studies in action recognition only report the best accuracies in comparisons. We also report the best accuracy for each EfficientGCN model for fair comparisons. There are several typical comparisons shown as follows: From Tab. 5, the best performance of the baseline model EfficientGCN-B0 are 90.2% and 94.9% for X-sub and Xview benchmark, respectively. When applying the proposed scaling strategy with coefficients of 2 and 4, the EfficientGCN-B2 and EfficientGCN-B4 are built, and achieve better performances on the two benchmarks. Especially for EfficientGCN-B4, its accuracy on X-sub benchmark 92.1%, outperforming other SOTA methods. Here, three typical methods should be noticed. 1) The first one is ST-GCN <ref type="bibr" target="#b10">[11]</ref>, which is currently the most popular backbone model for skeleton-based action recognition. Compared to ST-GCN, our EfficientGCN-B4 leads over 10% on X-sub benchmark. 2) 2s-AGCN <ref type="bibr" target="#b12">[13]</ref> is another popular baseline for skeleton-based action recognition. The proposed baseline EfficientGCN-B0 outperforms 2s-AGCN in both accuracy and efficiency.</p><p>3) The third one is MST-GCN <ref type="bibr" target="#b18">[19]</ref>, which is the current SOTA method with the GCN technique. The EfficientGCN-B4 is slightly better than MST-GCN in accuracy on X-sub benchmark. With respect to the comparisons within the EfficientGCN family, the accuracy shows a gradually improving trend with the increase of scaling coefficient.</p><p>STA-LSTM <ref type="bibr" target="#b39">[40]</ref> and AGC-LSTM <ref type="bibr" target="#b40">[41]</ref> are also enhanced by attention modules. However, there are obvious differences between EfficientGCN and these two models, e.g., our attention module works jointly on frames and joints, while their models use the attention modules for frames and joints individually. The performance of EfficientGCN-B4 greatly exceeds STA-LSTM over 10% on the two benchmarks, and outperforms AGC-LSTM by 2.9% and 1.1% on X-sub and X-view benchmarks, respectively. Moreover, 2s-AGCN and its improved versions Dynamic-GCN <ref type="bibr" target="#b33">[34]</ref> can also be considered as a variant of globally spatial attention (non-local structure), while they achieve comparable performances to our model. In addition, DC-GCN+ADG <ref type="bibr" target="#b31">[32]</ref> utilizes attention mechanism to guide its DropGraph module. This model is better than EfficientGCN-B4 on X-view benchmark, but significantly worse on X-sub benchmark. As an increasingly popular technique, Neural Architecture Search (NAS) has been proposed for automatically searching efficient model structures. The NAS-based methods usually explore all potential topological structures from a large search space, while the hyper-parameters of model blocks (e.g., the width and depth of convolutional layers) are hardly considered. Different from the NAS-based methods, the compound scaling strategy mainly aims to optimize the hyper-parameters of convolutional layers in each model block with fixed topological structures (see <ref type="figure" target="#fig_7">Fig. 5</ref> and <ref type="figure">Fig. 6</ref>), which is easier to implement and cheaper to deploy in real tasks. There have been some studies introducing differentiable NAS into skeleton-based action recognition, e.g., the NAS-GCN <ref type="bibr" target="#b49">[50]</ref> which has achieved promising results in skeleton action recognition, but is still much more complex than our method, as shown in Tab. 5. In the future, it is worthy to study the combination of exploring both topology structures and hyper-parameters for better action recognition models.</p><p>It should also be noticed that there are four SOTA models (MS-G3D <ref type="bibr" target="#b17">[18]</ref>, DC-GCN+ADG <ref type="bibr" target="#b31">[32]</ref>, 4s-Shift-GCN <ref type="bibr" target="#b50">[51]</ref>, and MST-GCN <ref type="bibr" target="#b18">[19]</ref>) producing slightly higher accuracies than ours on the X-view benchmark. The inferior performance of the EfficientGCN in cross-view action recognition can be explained from two aspects: 1) in contrast to the 3-stream fusion in the EfficientGCN, the DC-GCN+ADG, the 4s-Shift-GCN and the MST-GCN all adopt 4-stream (joint, bone, motion, and bone motion) fusion strategy, where the additional bone motion information may enhance the robustness : These results are implemented based on the released codes.</p><p>of cross-view skeleton features; 2) both the MS-G3D and the MST-GCN introduce multi-scale graph convolution to aggregate more context information in skeleton feature extraction. Though superior performance can be achieved on the X-view benchmark by these models, both the 4-stream fusion strategy and the multi-scale graph convolution definitely increase the model complexities and computational costs.</p><p>These results imply that the proposed EfficientGCN is a strong baseline with competitive performance compared to SOTA methods. We consider that this is caused by the superior capability of the compound scaling strategy to balance the model accuracy and complexity, hereby a wider and deeper model can be constructed and easily trained to achieve better performance. Moreover, the proposed ST-JointAtt module contributes to the model accuracy, which makes the model prone to discover the most informative joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">NTU RGB+D 120 Dataset</head><p>As a newly released dataset, there are fewer papers reporting results on the NTU 120 dataset. For comprehensive comparisons, four popular models, i.e., ST-GCN <ref type="bibr" target="#b10">[11]</ref>, SR-TSL <ref type="bibr" target="#b41">[42]</ref>, AS-GCN <ref type="bibr" target="#b47">[48]</ref> and 2s-AGCN <ref type="bibr" target="#b12">[13]</ref>, are implemented by ourselves, based on their released codes. Tab. 6 presents the experimental results, from which we can find the proposed EfficientGCN achieves the highest performance, compared to other models. For example, EfficientGCN-B4 outperforms the current SOTA method, MST-GCN <ref type="bibr" target="#b33">[34]</ref>, by 1.2% on X-sub120 benchmark. Similar to the results on NTU 60 dataset, the performance of EfficientGCN is mainly attributed to the contribution of the compound scaling strategy. Comparisons with SOTA methods on X-sub benchmark in accuracy (%), FLOPs (?10 9 ) and parameter number (?10 6 ). The models in three parts are compared with EfficientGCN-B0, B2, B4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Acc.  <ref type="bibr" target="#b13">[14]</ref> 85.9 32.80 12.01? 6.21 21.41? RA-GCNv2 <ref type="bibr" target="#b30">[31]</ref> 87.3 32.80 12.01? 6.21 21.41? AS-GCN <ref type="bibr" target="#b47">[48]</ref> 86.8 26.76 9.80? 9.50 32.76? 2s-AGCN <ref type="bibr" target="#b12">[13]</ref> 88.5 37.32 13.67? 6.94 23.93? SGN <ref type="bibr" target="#b35">[36]</ref> 89.0 --0.69 2.37? AGC-LSTM <ref type="bibr" target="#b40">[41]</ref> 89. : These results are implemented based on the released codes. ? : These results are provided by their authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Model Complexity</head><p>In order to verify the efficiency of our model, we compare our EfficientGCN family with other methods in terms of accuracy and model complexity (FLOPs and number of parameters) on X-sub benchmark of NTU 60 dataset. The experimental results are presented in Tab. 7. This table is divided into three parts, where the models are grouped into three parts with different accuracies. The ratios following FLOPs and parameter number denote the ratio between the model and its corresponding EfficientGCN. Due to the lack of reported FLOPs and parameter numbers for most models, we obtain the results by their released codes or directly asking their authors for helps. Note that the FLOPs of SGN <ref type="bibr" target="#b35">[36]</ref> and Dynamic-GCN <ref type="bibr" target="#b33">[34]</ref> are reported by their authors but not presented in this table, because these two models contain well-designed data transformation modules, which resize the original skeleton sequence to a very short sequence (e.g., 20 frames), instead of performing on the whole 300 frames. Thus, we ignore the FLOPs of these two models and only give the numbers of their parameters for fair comparisons.</p><p>In the top part of Tab. 7, it is observed that there is a large gap between the efficiencies of EfficientGCN-B0 and previous models. Compared to the first GCN baseline for skeleton-based action recognition, i.e., ST-GCN <ref type="bibr" target="#b10">[11]</ref>, EfficientGCN-B0 outperforms by 8.7% in accuracy, with a 5.98? fewer FLOPs and a 10.68? fewer parameters. DGNN <ref type="bibr" target="#b15">[16]</ref> obtains the same accuracy as EfficientGCN-B0, but its amount of trainable parameters is exceedingly larger, about 90? than EfficientGCN-B0. SGN <ref type="bibr" target="#b35">[36]</ref> is a lightweight and efficient model for skeleton-based action recognition, which achieves 89.0% accuracy with only 0.69?10 6 parameters. However, it is still worse than our EfficientGCN-B0 in both model accuracy and model size.</p><p>As to the middle part, EfficientGCN-B2 achieves 91.4% accuracy with 4.05?10 9 FLOPs and 0.51?10 6 parameters, which are about 4.57? faster and 7.14? smaller than the preliminary version of this paper (PA-ResGCN-B19). Furthermore, the bottom part shows that EfficientGCN-B4 achieves a SOTA accuracy with a small amount of trainable parameters. Though it has been around 4? larger than the EfficientGCN-B0, it is still much fewer than the other models with similar performance. These results clearly show that the proposed method brings a remarkable improvement in both model accuracy and complexity, which will benefit to the real applications of skeleton-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Discussion and Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Confusion Matrices and Failure Cases</head><p>Although EfficientGCN receives promising results on the large-scale datasets, there are still some actions difficult to be well recognized. As <ref type="figure" target="#fig_11">Fig. 9</ref> displays, we draw the confusion matrices of some actions for the proposed EfficientGCN-B0 and EfficientGCN-B4, respectively, where the selected actions are determined according to their insufficient accuracies (less than 80% on X-sub benchmark). From the top row of <ref type="figure" target="#fig_11">Fig. 9</ref>, two groups of similar actions should be noticed. The first one is marked by the red rectangles, including reading, writing, playing with a phone, and typing on a keyboard. All these actions are mainly performed by the slight shaking of two hands, which are extremely similar at spatial configurations and temporal dynamics. The second group, surrounded by green rectangles, consists of two similar actions, i.e., wearing a shoe and taking off a shoe. These two actions have similar spatial configurations, but different temporal dynamics. With respect to EfficientGCN-B4 (bottom row of <ref type="figure" target="#fig_11">Fig. 9</ref>), the recognition accuracies of actions in the second group receive a significant improvement, while the actions in the first group are still hard to be distinguished.</p><p>This issue is mainly caused by the lack of joints to represent two hands, thus the information of two hands is generally insufficient. Furthermore, the fringe joints of NTU 60 dataset often contain much noise (e.g., the 4th, 7th, and 8th frames of throwing in <ref type="figure" target="#fig_1">Fig. 11</ref>), which makes a huge influence on capturing the discriminative features. However, our approach is not particularly designed for dealing with noises. Therefore, it is still challenging to recognize such subtle actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">Attention Maps</head><p>To illustrate the characteristics of the ST-JointAtt module, we depict the attention maps of two randomly selected samples at four stages of the EfficientGCN-B4 model. As shown in <ref type="figure" target="#fig_1">Fig. 10</ref>, for the two different action samples, the left and right subfigures share similar attention maps at all stages. For each subfigure, the top two feature maps at the first two GCN stages show the stronger selectivity on spatial joints, and the bottom two at high-level GCN stages display additional selectivity on time frames. It should be noticed that the frames in the later of the sequence which are out of the duration of actions are padded by zeros, the attention maps at the last stage successfully distinguish these uninformative frames with small weights. From these figures, it shows that the proposed ST-JointAtt module pays more attention to informative joints at the early convolutional stages, while distinguishes informative frames in the later convolutional stages. Meanwhile, the joint sensitivity is weakened at the later stages, which may be caused by the over-smoothing problem in GCN as the adjacent skeleton joints tend to become indistinguishable in deeper layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.3">Class Activation Maps</head><p>To show how our model works, the activation maps of some skeleton sequences are calculated by class activation map <ref type="bibr" target="#b52">[53]</ref>, as presented in <ref type="figure" target="#fig_1">Fig. 11</ref> compared with the preliminary PartAtt module proposed in <ref type="bibr" target="#b26">[27]</ref>, this new attention module results in more reasonable attention weights in temporal dimension, by which only informative moving joints in certain frames are captured (e.g., all joints in the first two frames of kicking are not activated).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Generalization of EfficientGCN</head><p>From the above experimental results, the EfficientGCN shows an outstanding performance for the skeleton-based action recognition task. To further validate the generalization ability of the EfficientGCN, we apply the EfficientGCN-B4 to other skeleton-based tasks such as person reidentification (ReID) <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>.</p><p>Following the procedure in <ref type="bibr" target="#b54">[55]</ref>, four open benchmarks on skeleton-based person ReID are employed, including BIWI <ref type="bibr" target="#b55">[56]</ref>, IAS-A/IAS-B <ref type="bibr" target="#b56">[57]</ref>, and KGBD <ref type="bibr" target="#b57">[58]</ref>. The same training/testing splits for each dataset are adopted as mentioned in <ref type="bibr" target="#b54">[55]</ref>, and the number of input frame is set to 80 for KGBD and 10 for others in this paper. Note that the skeleton sequences with more than 80 (or 10) frames will be split to several samples with the same person ID. The experimental results on the four benchmarks are displayed in Tab. 8. It can be seen that the EfficientGCN-B4 model significantly outperforms the other SOTA models on three benchmarks, indicating that our model has an excellent performance on skeleton-based person ReID task, further showing its great potential to other skeleton-based motion analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have constructed a family of efficient but strong baselines based on a set of techniques for boosting model efficiencies. Different from other multi-stream models, the proposed EfficientGCN fuses three input branches at early stage, obviously eliminating the redundant parameters. In order to further reduce the model complexity, four TC layers are designed according to the bottleneck structure and separable convolution, which significantly saves the computational cost. Moreover, a compound scaling strategy is utilized to uniformly scale the model width and depth, further reducing the model complexity. On two large-scale datasets, NTU RGB+D 60 &amp; 120, the proposed EfficientGCN-B4 achieves the SOTA performance, while its FLOPs and number of parameters are obviously fewer than other models. Thus, the new baselines will have a huge potential for developing more complicated models. In the future, we will extend the proposed baseline with the object appearance, which is likely responsible for the recognition of some extremely similar actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A NETWORK ARCHITECTURE</head><p>Here we present the details to calculate the scaled width and depth. Firstly, the initial channels and TC layers of four blocks are set to {48, 24, 64, 128} and {0.5, 0.5, 1, 1}, respectively. Then, the rounding functions are given as:</p><formula xml:id="formula_16">C ? = (C 0 /16 ? ? ? ) * 16 (S.1) L ? = (L 0 ? ? ? ) (S.2)</formula><p>where C ? and L ? denote the numbers of scaled channels and TC layers with scaling coefficient ?, C 0 and L 0 denote the initial channels and TC layers, ? and ? are defined in Sec. 4.3, and (?) represents the step function formulated as:</p><formula xml:id="formula_17">(x) = x , if x ? x &gt; 0.5 x , if x ? x ? 0.5 (S.3)</formula><p>where ? and ? mean up and down rounding functions, respectively. Finally, the network architectures of models with scaling coefficients {2, 4}, i.e., EfficientGCN-B2 and EfficientGCN-B4, can be calculated by initial channels, initial TC layers, and rounding functions. The architectures of these three baselines are shown in Tab. S.1, Tab. S.2 and Tab. S.3. : This BasicBlock is fixed and without attentions for stable training. ? : Actually, the initial depths of the two blocks in input branches are both 0.5. The depth of 0 is obtained after rounding.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B GRID SEARCH FOR RECEPTIVE FIELD</head><p>There are two hyper-parameters mentioned in Sec. 3.2 and 4.2, i.e., D for the maximum spatial graph distance and L for the temporal window size. They directly determine the receptive field of the base convolutional operation, thus have implication on the model performance. The effects of these two hyper-parameters are discussed in this section, and the experimental results are shown in Tab. S.4 and Tab. S.5. For the maximum graph distance D, we can find that there is an obvious decline when D &lt; 2. However, D &gt; 3 is not better than D = 2 or D = 3 because an oversized receptive field will make the skeleton graph over-smoothing. Similarly, the temporal window size L is also required to choose a suitable value by balancing the model accuracy and complexity. Thus, according to these two tables, we set D = 2 and L = 5 (Bold in tables) by choosing a high model accuracy and a low model complexity simultaneously. Note that the settings of D = 2, L = 11 and D = 3, L = 9 are slightly more accurate than D = 2, L = 5, but their model complexities are considerably higher than the selected setting. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>Yi-Fan Song, Zhang Zhang, and Liang Wang are with the School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS), Beijing 100190, China, and also with the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing 100190, China. (Email: yifan.song@cripac.ia.ac.cn, zzhang@nlpr.ia.ac.cn, wangliang@nlpr.ia.ac.cn) ? Caifeng Shan is with the College of Electrical Engineering and Automation, Shandong University of Science and Technology (SDUST), Qingdao 266590, China, and also with the Artificial Intelligence Research, Chinese Academy of Sciences (CAS-AIR), Beijing 100190, China. (Email: caifeng.shan@gmail.com)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Firstly, arXiv:2106.15125v2 [cs.CV] 3 Mar 2022 The overall pipeline of our approach, where ? represents concatenation operation, GAP and FC denote the Global Average Pooling operation and Fully Connected layer, respectively. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Model size vs. model accuracy on the cross-subject benchmark of NTU 60 dataset. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>The demonstration of input data. (a) is the relative positions, (b) is the motion velocities, and (c) demonstrates the 3D lengths and the 3D angles of a bone. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 1 Fig. 4 .</head><label>14</label><figDesc>b) Depth-wise Convolutional Operation * = (a) Standard Convolutional Operation * = (c) Point-wise Convolutional Operation 1 Standard convolution vs. separable convolution for skeletonbased action recognition, where C in and Cout denote the numbers of input and output channels, D f and D k denote the sizes of feature map and convolutional kernel, and * represents convolutional operation.(Best viewed in color.) malized position features, i.e., R = {r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>SGC</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>The overview of the proposed EfficientGCN model, where the two numbers in each block denote input and output channels, Q is the number of action classes, ? and represent concatenation and element-wise product, and /2 represents a stride of 2. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>/ 2 PFig. 6 .</head><label>26</label><figDesc>-Conv: Point-wise Convolution D-Conv: Depth-wise Convolution = / = * The details of various convolutional layers, where C in and Cout denote the numbers of input and output channels, r rd and rep are employed to reduce or expand the inner channels. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>The overview of the proposed ST-JointAtt module, where C, T, V denote the numbers of input channels, frames and joints respectively, r rd = 4 is utilized to compact the features, ? represents the outer-product, BN denotes the batch normalization, HardSwish<ref type="bibr" target="#b42">[43]</ref> and Sigmoid are both activated functions. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Model size vs. model accuracy of different width and depth scaling hyper-parameters (? and ?). (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Confusion matrices of EfficientGCN-B0 (top) and EfficientGCN-B4 (bottom) with failure actions (less than 80% accuracy on X-sub benchmark), where the numbers in coordinate axes represent the indexes of each action category, the red and green rectangles denote two groups of similar actions. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>The subfigures (a) and (b) describe the attention maps of two randomly selected samples respectively, where the four parts of each subfigure are calculated by the attention modules in four stages of EfficientGCN-B4 model. A small square with darker color denotes a higher attention weight for the corresponding spatial temporal joint. All these attention maps are performed on X-sub benchmark. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Attention Depth GAP: Global Average Pool FC: Fully Connected Layer SGC: Spatial Graph Convolution TC: Temporal Convolution</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Joint</cell><cell>Velocity</cell><cell>Bone</cell></row><row><cell></cell><cell>BatchNorm</cell><cell>BatchNorm</cell><cell>BatchNorm</cell></row><row><cell></cell><cell>Initial Block</cell><cell>Initial Block</cell><cell>Initial Block</cell></row><row><cell></cell><cell>6, 64</cell><cell>6, 64</cell><cell>6, 64</cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Branch</cell><cell>GCN Block</cell><cell>GCN Block</cell><cell>GCN Block</cell></row><row><cell></cell><cell>64, 48</cell><cell>64, 48</cell><cell>64, 48</cell></row><row><cell></cell><cell>Attention</cell><cell>Attention</cell><cell>Attention</cell></row><row><cell></cell><cell>GCN Block</cell><cell>GCN Block</cell><cell>GCN Block</cell></row><row><cell></cell><cell>48, 16</cell><cell>48, 16</cell><cell>48, 16</cell></row><row><cell></cell><cell>Attention</cell><cell>Attention</cell><cell>Attention</cell></row><row><cell></cell><cell>GCN Block</cell><cell cols="2">Block Structure</cell></row><row><cell></cell><cell>48, 64, /2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Attention</cell><cell>64, 128</cell><cell></cell></row><row><cell></cell><cell>GCN Block</cell><cell>TC, /2</cell><cell></cell></row><row><cell>Main</cell><cell>64, 128, /2</cell><cell cols="2">128, 128</cell></row><row><cell>Stream</cell><cell>Attention</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>TC</cell><cell></cell></row><row><cell></cell><cell>GAP</cell><cell cols="2">128, 128</cell></row><row><cell></cell><cell>FC, 128,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Output</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Comparisons of different TC layer types on X-sub benchmark in accuracy (%), FLOPs (?10 9 ) and parameter number (?10 6 ).</figDesc><table><row><cell>Layer</cell><cell cols="3">Mean?Std. FLOPs # Param.</cell></row><row><cell>BasicLayer</cell><cell>90.0?0.12</cell><cell>2.96</cell><cell>0.34</cell></row><row><cell>BottleLayer (r rd = 4)</cell><cell>89.6?0.15</cell><cell>2.62</cell><cell>0.26</cell></row><row><cell>SepLayer</cell><cell>89.6?0.15</cell><cell>2.62</cell><cell>0.26</cell></row><row><cell>EpSepLayer (rep = 1)</cell><cell>89.6?0.21</cell><cell>2.80</cell><cell>0.28</cell></row><row><cell>EpSepLayer (rep = 2)</cell><cell>89.9?0.19</cell><cell>3.08</cell><cell>0.32</cell></row><row><cell>EpSepLayer (rep = 4)</cell><cell>90.1?0.15</cell><cell>3.63</cell><cell>0.41</cell></row><row><cell cols="2">SGLayer (r rd = 2) (Baseline) 90.0?0.10</cell><cell>2.73</cell><cell>0.29</cell></row><row><cell>SGLayer (r rd = 4)</cell><cell>89.8?0.13</cell><cell>2.63</cell><cell>0.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Comparisons of different attention modules on X-sub benchmark in accuracy (%), FLOPs (?10 9 ) and parameter number (?10 6 ).</figDesc><table><row><cell>Attention</cell><cell cols="3">Mean?Std. FLOPs # Param.</cell></row><row><cell>w/o Att</cell><cell>88.9?0.16</cell><cell>2.72</cell><cell>0.24</cell></row><row><cell>w/ ChannelAtt</cell><cell>89.3?0.18</cell><cell>2.72</cell><cell>0.25</cell></row><row><cell>w/ FrameAtt</cell><cell>88.5?0.19</cell><cell>2.72</cell><cell>0.24</cell></row><row><cell>w/ JointAtt</cell><cell>89.1?0.18</cell><cell>2.72</cell><cell>0.24</cell></row><row><cell>w/ STCAtt [26]</cell><cell>89.5?0.14</cell><cell>2.74</cell><cell>0.30</cell></row><row><cell>w/ PartAtt [27]</cell><cell>89.4?0.21</cell><cell>2.72</cell><cell>0.33</cell></row><row><cell cols="2">w/ ST-JointAtt (Baseline) 90.0?0.10</cell><cell>2.73</cell><cell>0.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">Comparisons of different inputs on X-sub benchmark in accuracy (%),</cell></row><row><cell cols="3">FLOPs (?10 9 ) and parameter number (?10 6 ).</cell><cell></cell></row><row><cell>Inputs</cell><cell cols="3">Mean?Std. FLOPs # Param.</cell></row><row><cell>Joint</cell><cell>87.7?0.15</cell><cell>1.28</cell><cell>0.17</cell></row><row><cell>Velocity</cell><cell>86.6?0.21</cell><cell>1.28</cell><cell>0.17</cell></row><row><cell>Bone</cell><cell>88.4?0.14</cell><cell>1.28</cell><cell>0.17</cell></row><row><cell>Joint + Velocity</cell><cell>89.4?0.22</cell><cell>1.94</cell><cell>0.23</cell></row><row><cell>Joint + Bone</cell><cell>88.9?0.22</cell><cell>1.94</cell><cell>0.23</cell></row><row><cell>Velocity + Bone</cell><cell>89.7?0.13</cell><cell>1.94</cell><cell>0.23</cell></row><row><cell cols="2">Joint + Velocity + Bone (Baseline) 90.0?0.10</cell><cell>2.73</cell><cell>0.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Comparisons of different fusion stages on X-sub benchmark in accuracy (%), FLOPs (?10 9 ) and parameter number (?10 6 ).</figDesc><table><row><cell></cell><cell></cell><cell>Fusion</cell><cell></cell><cell cols="4">Mean?Std. FLOPs # Param.</cell></row><row><cell></cell><cell></cell><cell>Before 1st stage</cell><cell></cell><cell>88.8?0.30</cell><cell>2.29</cell><cell>0.24</cell></row><row><cell></cell><cell></cell><cell>After 1st stage</cell><cell></cell><cell>89.7?0.13</cell><cell>2.48</cell><cell>0.27</cell></row><row><cell></cell><cell cols="4">After 2nd stage (Baseline) 90.0?0.10</cell><cell>2.73</cell><cell>0.29</cell></row><row><cell></cell><cell></cell><cell>After 3rd stage</cell><cell></cell><cell>89.8?0.25</cell><cell>3.43</cell><cell>0.38</cell></row><row><cell></cell><cell></cell><cell>After 4th stage</cell><cell></cell><cell>89.5?0.22</cell><cell>3.84</cell><cell>0.52</cell></row><row><cell></cell><cell></cell><cell cols="2">At the score layer</cell><cell>89.9?0.06</cell><cell>3.85</cell><cell>0.52</cell></row><row><cell></cell><cell>92.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model Accuracy (%)</cell><cell>91 91.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">[1.0, 2.00] (only depth)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[1.1, 1.65]</cell><cell></cell></row><row><cell></cell><cell>90.5</cell><cell></cell><cell></cell><cell></cell><cell cols="2">[1.2, 1.35] (selected)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[1.3, 1.15]</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">[1.4, 1.00] (only width)</cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>2.5</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell cols="4">Number of Parameters (M)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Comparisons with SOTA methods on NTU 60 dataset in accuracy (%). The top part consists of several models without GCN techniques, while the other part contains some graph-based models.</figDesc><table><row><cell>Model</cell><cell cols="3">Conference X-sub X-view</cell></row><row><cell>HBRNN [7]</cell><cell>CVPR15</cell><cell>59.1</cell><cell>64.0</cell></row><row><cell>ST-LSTM [47]</cell><cell>ECCV16</cell><cell>69.2</cell><cell>77.7</cell></row><row><cell>STA-LSTM [40]</cell><cell>AAAI17</cell><cell>73.4</cell><cell>81.2</cell></row><row><cell>HCN [29]</cell><cell>IJCAI18</cell><cell>86.5</cell><cell>91.1</cell></row><row><cell>VA-fusion [30]</cell><cell>TPAMI19</cell><cell>89.4</cell><cell>95.0</cell></row><row><cell>ST-GCN [11]</cell><cell>AAAI18</cell><cell>81.5</cell><cell>88.3</cell></row><row><cell>SR-TSL [42]</cell><cell>ECCV18</cell><cell>84.8</cell><cell>92.4</cell></row><row><cell>RA-GCNv1 [14]</cell><cell>ICIP19</cell><cell>85.9</cell><cell>93.5</cell></row><row><cell>RA-GCNv2 [31]</cell><cell>TCSVT20</cell><cell>87.3</cell><cell>93.6</cell></row><row><cell>AS-GCN [48]</cell><cell>CVPR19</cell><cell>86.8</cell><cell>94.2</cell></row><row><cell>2s-AGCN [13]</cell><cell>CVPR19</cell><cell>88.5</cell><cell>95.1</cell></row><row><cell>AGC-LSTM [41]</cell><cell>CVPR19</cell><cell>89.2</cell><cell>95.0</cell></row><row><cell>DGNN [16]</cell><cell>CVPR19</cell><cell>89.9</cell><cell>96.1</cell></row><row><cell>PL-GCN [49]</cell><cell>AAAI20</cell><cell>89.2</cell><cell>95.0</cell></row><row><cell>NAS-GCN [50]</cell><cell>AAAI20</cell><cell>89.4</cell><cell>95.7</cell></row><row><cell>SGN [36]</cell><cell>CVPR20</cell><cell>89.0</cell><cell>94.5</cell></row><row><cell>4s-Shift-GCN [51]</cell><cell>CVPR20</cell><cell>90.7</cell><cell>96.5</cell></row><row><cell>MS-G3D [18]</cell><cell>CVPR20</cell><cell>91.5</cell><cell>96.2</cell></row><row><cell>DC-GCN+ADG [32]</cell><cell>ECCV20</cell><cell>90.8</cell><cell>96.6</cell></row><row><cell cols="3">PA-ResGCN-B19 [27] ACMMM20 90.9</cell><cell>96.0</cell></row><row><cell cols="3">Dynamic-GCN [34] ACMMM20 91.5</cell><cell>96.0</cell></row><row><cell>MST-GCN [19]</cell><cell>AAAI21</cell><cell>91.5</cell><cell>96.6</cell></row><row><cell>EfficientGCN-B0</cell><cell>-</cell><cell>90.2</cell><cell>94.9</cell></row><row><cell>EfficientGCN-B2</cell><cell>-</cell><cell>91.4</cell><cell>95.7</cell></row><row><cell>EfficientGCN-B4</cell><cell>-</cell><cell>92.1</cell><cell>96.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc>Comparisons with SOTA methods on NTU 120 dataset in accuracy (%). The top part consists of several models without GCN techniques, while the other part contains some graph-based models.</figDesc><table><row><cell>Model</cell><cell cols="2">Conference X-sub120</cell><cell>X-set120</cell></row><row><cell>PA-LSTM [15]</cell><cell>CVPR16</cell><cell>25.5</cell><cell>26.3</cell></row><row><cell>ST-LSTM [47]</cell><cell>ECCV16</cell><cell>55.7</cell><cell>57.9</cell></row><row><cell>FSNet [52]</cell><cell>TPAMI19</cell><cell>59.9</cell><cell>62.4</cell></row><row><cell>ST-GCN [11]</cell><cell>AAAI18</cell><cell>70.7</cell><cell>73.2</cell></row><row><cell>SR-TSL [42]</cell><cell>ECCV18</cell><cell>74.1</cell><cell>79.9</cell></row><row><cell>RA-GCNv1 [14]</cell><cell>ICIP19</cell><cell>74.4</cell><cell>79.4</cell></row><row><cell>RA-GCNv2 [31]</cell><cell>TCSVT20</cell><cell>81.1</cell><cell>82.7</cell></row><row><cell>AS-GCN [48]</cell><cell>CVPR19</cell><cell>77.9</cell><cell>78.5</cell></row><row><cell>2s-AGCN [13]</cell><cell>CVPR19</cell><cell>82.5</cell><cell>84.2</cell></row><row><cell>SGN [36]</cell><cell>CVPR20</cell><cell>79.2</cell><cell>81.5</cell></row><row><cell>4s-Shift-GCN [51]</cell><cell>CVPR20</cell><cell>85.9</cell><cell>87.6</cell></row><row><cell>MS-G3D [18]</cell><cell>CVPR20</cell><cell>86.9</cell><cell>88.4</cell></row><row><cell>DC-GCN+ADG [32]</cell><cell>ECCV20</cell><cell>86.5</cell><cell>88.1</cell></row><row><cell cols="2">PA-ResGCN-B19 [27] ACMMM20</cell><cell>87.3</cell><cell>88.3</cell></row><row><cell cols="2">Dynamic-GCN [34] ACMMM20</cell><cell>87.3</cell><cell>88.6</cell></row><row><cell>MST-GCN [19]</cell><cell>AAAI21</cell><cell>87.5</cell><cell>88.8</cell></row><row><cell>EfficientGCN-B0</cell><cell>-</cell><cell>86.6</cell><cell>85.0</cell></row><row><cell>EfficientGCN-B2</cell><cell>-</cell><cell>88.0</cell><cell>87.8</cell></row><row><cell>EfficientGCN-B4</cell><cell>-</cell><cell>88.7</cell><cell>88.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7</head><label>7</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Activated joints in 10 contextual frames of EfficientGCN-B4 for the sample actions, i.e., drinking water, throwing, taking off a jacket, waving hand, and kicking. The red points denote the activated joints, while blue points represent non-activated joints. (Best viewed in color.)</figDesc><table><row><cell>Drinking</cell></row><row><cell>water</cell></row><row><cell>Throwing</cell></row><row><cell>Taking off</cell></row><row><cell>a jacket</cell></row><row><cell>Waving</cell></row><row><cell>hand</cell></row><row><cell>Kicking</cell></row><row><cell>Fig. 11.</cell></row></table><note>, in which the activated joints in several sampled frames are displayed. From this figure, we can find that the EfficientGCN-B4 model successfully concentrates on the most informative joints, i.e., left arm for drinking water, throwing and waving hand, upper body for taking off a jacket, and left leg for kicking. This implies that the proposed ST-JointAtt module works well. Besides,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8</head><label>8</label><figDesc>Comparisons with other person ReID methods in rank-1 accuracy (%).</figDesc><table><row><cell>Method</cell><cell cols="4">BIWI IAS-A IAS-B KGBD</cell></row><row><cell>D 13 +KNN [56]</cell><cell>39.3</cell><cell>33.8</cell><cell>40.5</cell><cell>46.9</cell></row><row><cell>D 16 +Adaboost [59]</cell><cell>41.8</cell><cell>27.4</cell><cell>39.2</cell><cell>69.9</cell></row><row><cell>Single-layer LSTM [60]</cell><cell>15.8</cell><cell>20.0</cell><cell>19.1</cell><cell>39.8</cell></row><row><cell>Multi-layer LSTM [61]</cell><cell>36.1</cell><cell>34.4</cell><cell>30.9</cell><cell>46.2</cell></row><row><cell>PoseGait [62]</cell><cell>33.3</cell><cell>41.4</cell><cell>37.1</cell><cell>90.6</cell></row><row><cell cols="2">Locality-Awareness-SGE [55] 63.3</cell><cell>60.1</cell><cell>62.5</cell><cell>90.6</cell></row><row><cell>EfficientGCN-B4</cell><cell>64.5</cell><cell>67.3</cell><cell>62.4</cell><cell>96.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE S . 1</head><label>S1</label><figDesc>The architecture of EfficientGCN-B0 network. Each row describes a block with the following settings and output shape, where ?3 represents three input branches, /2 denotes a stride of 2, Q is the number of action classes.? 3 (48 ? T in ? V in ) ? 3 2 SGBlock?3 0 ? (48, 16) ? 3 (16 ? T in ? V in ) ? 3</figDesc><table><row><cell>Stage</cell><cell>Block</cell><cell>Depth</cell><cell>Channels</cell><cell>Shape</cell></row><row><cell>-</cell><cell>BN?3</cell><cell>-</cell><cell>(6, 6) ? 3</cell><cell>(6 ? T in ? V in ) ? 3</cell></row><row><cell>-</cell><cell>BasicBlock?3</cell><cell>1</cell><cell cols="2">(6, 64) ? 3 (64 ? T in ? V in ) ? 3</cell></row><row><cell cols="4">1 (64, 48) -SGBlock?3 0  ? Concat -16 ? 3, 48</cell><cell>48 ? T in ? V in</cell></row><row><cell>3</cell><cell>SGBlock</cell><cell>1</cell><cell>48, 64, /2</cell><cell>64 ? T in /2 ? V in</cell></row><row><cell>4</cell><cell>SGBlock</cell><cell>1</cell><cell>64, 128, /2</cell><cell>128 ? T in /4 ? V in</cell></row><row><cell>-</cell><cell>GAP</cell><cell>-</cell><cell>128, 128</cell><cell>128</cell></row><row><cell>-</cell><cell>FC</cell><cell>-</cell><cell>128, Q</cell><cell>Q</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE S . 2</head><label>S2</label><figDesc>The architecture of EfficientGCN-B2 network. Each row describes a block with the following settings and output shape, where ?3 represents three input branches, /2 denotes a stride of 2, Q is the number of action classes.? 3 (64 ? T in ? V in ) ? 3 1 SGBlock?3 1 (64, 64) ? 3 (64 ? T in ? V in ) ? 3 2 SGBlock?3 1 (64, 32) ? 3 (32 ? T in ? V in ) ? 3</figDesc><table><row><cell>Stage</cell><cell>Block</cell><cell>Depth</cell><cell>Channels</cell><cell>Shape</cell></row><row><cell>-</cell><cell>BN?3</cell><cell>-</cell><cell>(6, 6) ? 3</cell><cell>(6 ? T in ? V in ) ? 3</cell></row><row><cell cols="4">-(6, 64) -BasicBlock?3 1 Concat -32 ? 3, 96</cell><cell>96 ? T in ? V in</cell></row><row><cell>3</cell><cell>SGBlock</cell><cell>2</cell><cell>96, 96, /2</cell><cell>96 ? T in /2 ? V in</cell></row><row><cell>4</cell><cell>SGBlock</cell><cell>2</cell><cell>96, 192, /2</cell><cell>192 ? T in /4 ? V in</cell></row><row><cell>-</cell><cell>GAP</cell><cell>-</cell><cell>192, 192</cell><cell>192</cell></row><row><cell>-</cell><cell>FC</cell><cell>-</cell><cell>192, Q</cell><cell></cell></row></table><note>Q : This BasicBlock is fixed and without attentions for stable training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE S . 3</head><label>S3</label><figDesc>The architecture of EfficientGCN-B4 network. Each row describes a block with the following settings and output shape, where ?3 represents three input branches, /2 denotes a stride of 2, Q is the number of action classes. ? T in ? V in ) ? 3 -BasicBlock?3 1 (6, 64) ? 3 (64 ? T in ? V in ) ? 3 1 SGBlock?3 2 (64, 96) ? 3 (96 ? T in ? V in ) ? 3 2 SGBlock?3 2 (96, 48) ? 3 (48 ? T in ? V in ) ? 3</figDesc><table><row><cell>Stage</cell><cell>Block</cell><cell>Depth</cell><cell>Channels</cell><cell>Shape</cell></row><row><cell cols="5">-(6 -BN?3 -(6, 6) ? 3 Concat -48 ? 3, 144 144 ? T in ? V in</cell></row><row><cell>3</cell><cell>SGBlock</cell><cell>3</cell><cell>144, 128, /2</cell><cell>128 ? T in /2 ? V in</cell></row><row><cell>4</cell><cell>SGBlock</cell><cell>3</cell><cell>128, 272, /2</cell><cell>272 ? T in /4 ? V in</cell></row><row><cell>-</cell><cell>GAP</cell><cell>-</cell><cell>272, 272</cell><cell>272</cell></row><row><cell>-</cell><cell>FC</cell><cell>-</cell><cell>272, Q</cell><cell>Q</cell></row><row><cell cols="5">: This BasicBlock is fixed and without attentions for stable training.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE S . 4</head><label>S4</label><figDesc>Comparisons with different receptive fields on X-sub benchmark in accuracy (%). Comparisons with different receptive fields on X-sub benchmark in FLOPs (?10 9 ) and parameter numbers (?10 6 ).L = 3 2.35/0.27 2.72/0.30 3.09/0.33 3.46/0.36 3.83/0.39 L = 5 2.70/0.29 3.08/0.32 3.45/0.35 3.82/0.38 4.19/0.41 L = 7 3.06/0.32 3.43/0.35 3.80/0.38 4.18/0.41 4.55/0.44 L = 9 3.42/0.35 3.79/0.38 4.16/0.41 4.53/0.44 4.90/0.47 L = 11 3.78/0.37 4.15/0.40 4.52/0.43 4.89/0.46 5.26/0.49</figDesc><table><row><cell></cell><cell>Acc.</cell><cell cols="5">D = 1 D = 2 D = 3 D = 4 D = 5</cell></row><row><cell></cell><cell>L = 3</cell><cell>87.7</cell><cell>88.9</cell><cell>89.2</cell><cell>88.8</cell><cell>88.5</cell></row><row><cell></cell><cell>L = 5</cell><cell>88.1</cell><cell>89.9</cell><cell>89.9</cell><cell>89.7</cell><cell>89.5</cell></row><row><cell></cell><cell>L = 7</cell><cell>88.6</cell><cell>89.8</cell><cell>89.9</cell><cell>89.5</cell><cell>89.7</cell></row><row><cell></cell><cell>L = 9</cell><cell>88.8</cell><cell>89.9</cell><cell>90.0</cell><cell>89.8</cell><cell>89.9</cell></row><row><cell></cell><cell>L = 11</cell><cell>88.7</cell><cell>90.0</cell><cell>89.9</cell><cell>89.7</cell><cell>89.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE S.5</cell><cell></cell></row><row><cell>F. / P.</cell><cell cols="2">D = 1</cell><cell>D = 2</cell><cell>D = 3</cell><cell>D = 4</cell><cell>D = 5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of vision-based methods for action representation, segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="241" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Percept. Psychophys</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Multimedia Expo Workshop (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph edge convolutional neural networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Richly activated graph convolutional network for action recognition with incomplete skeletons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Locationaware graph convolutional networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-scale spatial temporal graph convolutional network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rethinking bottleneck structure for efficient mobile network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internat. Conf. Mach. Learn. (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coordinate attention for efficient mobile network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with multi-stream adaptive graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="9532" to="9545" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia (ACMMM)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1625" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">NTU RGB+D 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI Int. Joint Conf</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">View adaptive neural networks for high performance skeletonbased human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Richly activated graph convolutional network for robust skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1915" to="1925" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decoupling gcn with dropgraph module for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic gcn: Context-enriched topology learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimedia (ACMMM)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Make skeleton-based action recognition model smaller, faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia in Asia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantics-guided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Human action recognition: Posebased attention draws focus to hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Br. Mach. Vis. Conf. (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Network decoupling: From regular to depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Br. Mach. Vis. Conf. (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Actionalstructural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Part-level graph convolutional network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning graph convolutional network for skeleton-based human action recognition by neural searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Skeleton-based online action prediction using scale selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1453" to="1467" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A self-supervised gait encoding approach with localityawareness for 3d skeleton based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">One-shot person re-identification with a consumer depth camera,&quot; in Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Munaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fossati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Menegatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="161" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A feature-based approach to people re-identification using skeleton keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Munaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghidoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Dizmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Menegatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5644" to="5651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Person identification using anthropometric and gait data from kinect sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Araujo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. Artif. Intell</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Enhanced skeleton and face 3d data for person re-identification from depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="69" to="80" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Recurrent attention models for depth-based person identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1229" to="1238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Relational network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Multimedia Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="826" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A model-based gait recognition method with body pose and human prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">107069</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Currently, He is a Ph.D. candidate of the School of Artificial Intelligence, University of Chinese Academy and Sciences (UCAS). His research interests include computer vision, action recognition, action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yi-Fan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Zhengzhou, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Song received the M.Eng. degree from Zhengzhou University</orgName>
		</respStmt>
	</monogr>
	<note>and neural architecture search</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
