<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Iterative Alternating Neural Attention for Machine Reading</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
							<email>alessandro.sordoni@maluuba.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>Québec</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
							<email>phil.bachman@maluuba.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>Québec</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
							<email>adam.trischler@maluuba.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>Québec</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<email>bengioy@iro.umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>Québec</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Montréal, Québec</roleName><forename type="first">Maluuba</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>Québec</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Iterative Alternating Neural Attention for Machine Reading</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test (CBT) dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, the idea of training machine comprehension models that can read, understand, and answer questions about a text has come closer to reality principally through two factors. The first is the advent of deep learning techniques <ref type="bibr" target="#b5">(Goodfellow et al., 2016)</ref>, which allow manipulation of natural language beyond its surface forms and generalize beyond relatively small amounts of labeled data. The second factor is the formulation of standard machine comprehension benchmarks based on Cloze-style queries <ref type="bibr" target="#b8">(Hill et al., 2015;</ref><ref type="bibr" target="#b7">Hermann et al., 2015)</ref>, which permit fast integration loops between model conception and experimental evaluation.</p><p>Cloze-style queries <ref type="bibr" target="#b19">(Taylor, 1953)</ref> are created by deleting a particular word in a natural-language statement. The task is to guess which word was deleted. In a pragmatic approach, recent work <ref type="bibr" target="#b8">(Hill et al., 2015)</ref> formed such questions by extracting a sentence from a larger document. In contrast to considering a stand-alone statement, the system is now required to handle a larger amount of information that may possibly influence the prediction of the missing word. Such contextual dependencies may also be injected by removing a word from a short human-crafted summary of a larger body of text. The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text <ref type="bibr" target="#b7">(Hermann et al., 2015)</ref>. In both cases, the machine comprehension system is presented with an ablated query and the document to which the original query refers. The missing word is assumed to appear in the document. Encouraged by the recent success of deep learning attention architectures <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b18">Sukhbaatar et al., 2015)</ref>, we propose a novel neural attention-based inference model designed to perform machine reading comprehension tasks. The model first reads the document and the query using a recurrent neural network <ref type="bibr" target="#b5">(Goodfellow et al., 2016)</ref>. Then, it deploys an iterative inference process to uncover the inferential links that exist between the missing query word, the query, and the document. This phase involves a novel alternating attention mechanism; it first attends to some parts of the query, then finds their corresponding matches by attending to the document. The result of this alternating search is fed back into the iterative inference process to seed the next search step. This permits our model to reason about different parts of the query in a sequential way, based on the information that has been gathered previously from the document. After a fixed number of iterations, the model uses a summary of its inference process to predict the answer. This paper makes the following contributions. We present a novel iterative, alternating attention mechanism that, unlike existing models <ref type="bibr" target="#b8">(Hill et al., 2015;</ref><ref type="bibr" target="#b9">Kadlec et al., 2016)</ref>, does not compress the query to a single representation, but instead alternates its attention between the query and the document to obtain a fine-grained query representation within a fixed computation time. Our architecture tightly integrates previous ideas related to bidirectional readers <ref type="bibr" target="#b9">(Kadlec et al., 2016)</ref> and iterative attention processes <ref type="bibr" target="#b8">(Hill et al., 2015;</ref><ref type="bibr" target="#b18">Sukhbaatar et al., 2015)</ref>. It obtains state-of-theart results on two machine comprehension datasets and shows promise for application to a broad range of natural language processing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>One of the advantages of using Cloze-style questions to evaluate machine comprehension systems is that a sufficient amount of training and test data can be obtained without human intervention. The CBT <ref type="bibr" target="#b8">(Hill et al., 2015)</ref> and <ref type="bibr">CNN (Hermann et al., 2015)</ref> corpora are two such datasets.</p><p>The CBT 1 corpus was generated from well-known children's books available through Project Gutenberg. Documents consist of 20-sentence excerpts from these books. The related query is formed from an excerpt's 21st sentence by replacing a single word with an anonymous placeholder token. The dataset is divided into four subsets depending on the type of the word replaced. The subsets are named entity, common noun, verb, and preposition. We will focus our evaluation solely on the first two subsets, i.e. CBT-NE (named entity) and CBT-CN (common nouns), since the latter two are relatively simple as demonstrated by <ref type="bibr" target="#b8">(Hill et al., 2015)</ref>.</p><p>The CNN 2 corpus was generated from news articles available through the CNN website. The documents are given by the full articles themselves, which are accompanied by short, bullet-point summary statements. Instead of extracting a query from the articles themselves, the authors replace a named entity within each article summary with an anonymous placeholder token.</p><p>For both datasets, the training and evaluation data consist of tuples (Q, D, A, a), where Q is the query (represented as a sequence of words), D is the document, A is the set of possible answers, and a ∈ A is  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Alternating Iterative Attention</head><p>Our model is represented in <ref type="figure">Fig. 1</ref>. Its workflow has three steps. First is the encoding phase, in which we compute a set of vector representations, acting as a memory of the content of the input document and query. Next, the inference phase aims to untangle the complex semantic relationships linking the document and the query in order to provide sufficiently strong evidence for the answer prediction to be successful. To accomplish this, we use an iterative process that, at each iteration, alternates attentive memory accesses to the query and the document. Finally, the prediction phase uses the information gathered from the repeated attentions through the query and the document to maximize the probability of the correct answer. We describe each of the phases in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bidirectional Encoding</head><p>The input to the encoding phase is a sequence of words X = (x 1 , . . . , x |X | ), such as a document or a query, drawn from a vocabulary V . Each word is represented by a continuous word embedding x ∈ R d stored in a word embedding matrix X ∈ R |V |×d . The sequence X is processed using a recurrent neural network encoder <ref type="bibr" target="#b5">(Goodfellow et al., 2016)</ref> with gated recurrent units (GRU) <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>. For each position i in the input sequence, the GRU takes as input the word embedding x i and updates a hidden Figure 1: Our model first encodes the query and the document by means of bidirectional GRU networks. Then, it deploys an iterative inference mechanism that alternates between attending query encodings (1) and document encodings (2) given the query attended state. The results of the alternating attention is gated and fed back into the inference GRU. Even if the encodings are computed only once, the query representation is dynamic and changes throughout the inference process. After a fixed number of steps T , the weights of the document attention are used to estimate the probability of the answer P (a|Q, D).</p><formula xml:id="formula_0">state h i−1 to h i = f (x i , h i−1 ), where f is defined Q D s t</formula><p>by:</p><formula xml:id="formula_1">r i = σ(I r x i + H r h i−1 ), u i = σ(I u x i + H u h i−1 ), h i = tanh(I h x i + H h (r i · h i−1 )), h i = (1 − u i ) · h i−1 + u i ·h i ,<label>(1)</label></formula><p>where h i , r i and u i ∈ R h are the recurrent state, the reset gate and update gate respectively, I {r,u,h} ∈ R h×d , H {r,u,h} ∈ R h×h are the parameters of the GRU, σ is the sigmoid function and · is the elementwise multiplication. The hidden state h i acts as a representation of the word x i in the context of the preceding sequence inputs x &lt;i . In order to incorporate information from the future tokens x &gt;i , we choose to process the sequence in reverse with an additional GRU <ref type="bibr" target="#b9">(Kadlec et al., 2016)</ref>. Therefore, the encoding phase maps each token x i to a contextual representation given by the concatenation of the forward and backward GRU hidden</p><formula xml:id="formula_2">statesx i = [ − → h i , ← − h i ].</formula><p>We denote byq i ∈ R 2h andd i ∈ R 2h the contextual encodings for word i in the query Q and the document D respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Iterative Alternating Attention</head><p>This phase can be considered a means to uncover a possible inference chain that starts at the query and the document and leads to the answer. The inference is modelled by an additional recurrent GRU network. The recurrent network iteratively performs an alternating search step to gather information that may be useful to predict the answer. In particular, at each time step: (1) it performs an attentive read on the query encodings, resulting in a query glimpse, q t , and (2) given the current query glimpse, it extracts a conditional document glimpse, d t , representing the parts of the document that are relevant to the current query glimpse. In turn, both attentive reads are conditioned on the previous hidden state of the inference GRU s t−1 , summarizing the information that has been gathered from the query and the document up to time t. The inference GRU uses both glimpses to update its recurrent state and thus decides which information needs to be gathered to complete the inference process.</p><p>Query Attentive Read Given the query encodings {q i }, we formulate a query glimpse q t at timestep t by:</p><formula xml:id="formula_3">q i, t = softmax i=1,...,|Q|q i (A q s t−1 + a q ), q t = i q i, tqi</formula><p>where q i, t are the query attention weights and A q ∈ R 2h×s , where s is the dimensionality of the inference GRU state, and a q ∈ R 2h . The attention we use here is similar to the formulation used in <ref type="bibr" target="#b8">(Hill et al., 2015;</ref><ref type="bibr" target="#b18">Sukhbaatar et al., 2015)</ref>, but with two differences. First, we use a bilinear term instead of a simple dot product in order to compute the importance of each query term in the current time step. This simple bilinear attention has been successfully used in <ref type="bibr" target="#b12">(Luong et al., 2015)</ref>. Second, we add a term a q that allows to bias the attention mechanism towards words which tend to be important across the questions independently of the search key s t−1 . This is similar to what is achieved by the original attention mechanism proposed in <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> without the burden of the additional tanh layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Attentive Read</head><p>The alternating attention continues by probing the document given the current query glimpse q t . In particular, the document attention weights are computed based on both the previous search state and the currently selected query glimpse q t :</p><formula xml:id="formula_4">d i, t = softmax i=1,...,|D|d i (A d [s t−1 , q t ] + a d ), d t = i d i, tdi ,</formula><p>where d i, t are the attention weights for each word in the document and A d ∈ R 2h×(s+2h) and a d ∈ R 2h . Note that the document attention is also conditioned on s t−1 . This allows the model to perform transitive reasoning on the document side, i.e. to use previously obtained document information to bias future attended locations, which is particularly important for natural language inference tasks <ref type="bibr" target="#b18">(Sukhbaatar et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gating Search Results</head><p>In order to update its recurrent state, the inference GRU may evolve on the basis of the information gathered from the current</p><formula xml:id="formula_5">inference step, i.e. s t = f ([q t , d t ], s t−1 ), where f is defined in Eq. 1.</formula><p>However, the current query glimpse may be too general or the document may not contain the information specified in the query glimpse, i.e. the query or the document attention weights may be nearly uniform. We include a gating mechanism that is designed to reset the current query and document glimpses in the case that the current search is not fruitful. Formally, we implement a gating mech-</p><formula xml:id="formula_6">anism r = g([s t−1 , q t , d t , q t · d t ])</formula><p>, where · is the element-wise multiplication and g : R s+6h → R 2h . The gate g takes the form of a 2-layer feed-forward network with sigmoid output unit activation. The fourth argument of the gate takes into account multiplicative interactions between query and document glimpses, making it easier to determine the degree of matching between them. Given a query gate g q , producing r q , and a document gate g d , producing r d , the inputs of the inference GRU are given by the reset version of the query and document glimpses, i.e.,</p><formula xml:id="formula_7">s t = f ([r q · q t , r d · d t ], s t−1 )</formula><p>. Intuitively, the model reviews the query glimpse with respect to the contents of the document glimpse and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Answer Prediction</head><p>After a fixed number of time-steps T , the document attention weights obtained in the last search step d i,T are used to predict the probability of the answer given the document and the query P (a|Q, D). Formally, we follow <ref type="bibr" target="#b9">(Kadlec et al., 2016)</ref> and apply the "pointer-sum" loss:</p><formula xml:id="formula_8">P (a|Q, D) = i∈I(a,D) d i,T ,<label>(2)</label></formula><p>where I(a, D) is a set of positions where a occurs in the document. The model is trained to maximize log P (a|Q, D) over the training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training Details</head><p>To train our model, we used stochastic gradient descent with the ADAM optimizer (Kingma and Ba, 2014), with an initial learning rate of 0.001. We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half-epoch, i.e. 2000 batches (for CBT) and 5000 batches for (CNN). We initialize all weights of our model by sampling from the normal distribution N (0, 0.05). Following <ref type="bibr" target="#b16">(Saxe et al., 2013)</ref>, the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero. In order to stabilize the learning, we clip the gradients if their norm is greater than 5 <ref type="bibr" target="#b14">(Pascanu et al., 2013</ref>   <ref type="bibr" target="#b8">(Hill et al., 2015)</ref> and those marked with 2 are from <ref type="bibr" target="#b9">(Kadlec et al., 2016)</ref>.</p><p>the inputs to both the query and the document attention mechanisms. We found that setting embedding regularization to 0.0001, T = 8, d = 384, h = 128, s = 512 worked robustly across the datasets. Our model is implemented in Theano <ref type="bibr" target="#b1">(Bastien et al., 2012)</ref>, using the Keras <ref type="bibr" target="#b4">(Chollet, 2015)</ref> library.</p><p>Computational Complexity Similar to previous state-of-the-art models <ref type="bibr" target="#b9">(Kadlec et al., 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2016)</ref> which use a bidirectional encoder, the major bottleneck of our method is computing the document and query encodings. The alternating attention mechanism runs only for a fixed number of steps (T = 8 in our tests), which is orders of magnitude smaller than a typical document or query in our datasets (see <ref type="table" target="#tab_1">Table 1</ref>). The repeated attentions each require a softmax over ∼1000 locations which is typically fast on recent GPU architectures. Thus, our computation cost is comparable to <ref type="bibr" target="#b9">(Kadlec et al., 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2016)</ref>, but we outperform the latter models on the datasets tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We report the results of our model on the CBT-CN, CBT-NE and CNN datasets, previously described in Section 2. <ref type="table" target="#tab_3">Table 2</ref> reports our results on the CBT-CN and CBT-NE dataset. The Humans, LSTMs and Memory Networks (MemNNs) results are taken from <ref type="bibr" target="#b8">(Hill et al., 2015)</ref> and the Attention-Sum Reader (AS Reader) is a state-of-the-art result recently obtained by <ref type="bibr" target="#b9">(Kadlec et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">CBT</head><p>Main result Our model (line 7) sets a new stateof-the-art on the common noun category by gaining 3.6 and 5.6 points in validation and test over the best baseline AS Reader (line 5). This performance gap is only partially reflected on the CBT-NE dataset. We observe that the 1.4 accuracy points on the validation set do not reflect better performance on the test set, which sits on par with the best baseline. In CBT-NE, the missing word is a named entity appearing in the story which is likely to be less frequent than a common noun. We found that approximatively 27.5% of validation examples and 29.6% of test examples contain an answer that has never been predicted in the training set. These numbers are considerably lower for the CBT-CN, for which only 2.5% and 4.6% of validation and test examples respectively contain an answer that has not been previously seen.</p><p>Ensembles Fusing multiple models generally achieves better generalization. In order to investigate whether this could help achieving better held-out performance on CBT-NE, we adopt a simple strategy and average the predictions of 5 models trained with different random seeds (line 9   <ref type="bibr" target="#b8">(Hill et al., 2015)</ref>, 3 from <ref type="bibr" target="#b9">(Kadlec et al., 2016)</ref> and 4 from <ref type="bibr" target="#b2">(Chen et al., 2016)</ref>.</p><p>improvements over the single model and sits at 74.1 on validation and 71.0 on test.</p><p>Fixed query attention In order to measure the impact of the query attention step in our model, we constrain the query attention weights q i,t to be uniform, i.e. q i,t = 1/|Q|, for all t = 1, . . . , T (line 6). This corresponds to fixing the query representation to the average pooling over the bidirectional query encodings and is similar in spirit to previous work <ref type="bibr" target="#b9">(Kadlec et al., 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2016)</ref>. By comparing line 6 and line 7, we see that the query attention mechanism allows improvements up to 2.3 points in validation and 4.9 points in test with respect to fixing the query representation throughout the search process. A similar scenario was observed on the CNN dataset.  <ref type="bibr" target="#b9">(Kadlec et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main result</head><p>The results show that our model (line 8) improves state-of-the-art accuracy by 4 percent absolute on validation and 3.4 on test with respect to the most recent published result (AS Reader) (line 7). We also report the very recent results of the Stanford AR system that came to our attention during the writeup of this article <ref type="bibr" target="#b2">(Chen et al., 2016</ref>) (line 9). Our model slightly improves over this strong baseline by 0.2 percent on validation and 0.9 percent on test. We note that the latter comparison may be influenced by different training and initialization strategies. First, Stanford AS uses GloVe embeddings <ref type="bibr" target="#b15">(Pennington et al., 2014)</ref>, pre-trained from a large external corpus. Second, the system normalizes the output probabilities only over the candidate answers in the document.</p><p>Ensembles We also report the results using ensembled models.  <ref type="table">Table 4</ref>: Per-category performance of the Stanford AR and our system. The first three categories require local context matching, the next two global context matching and coreference errors are unanswerable questions <ref type="bibr" target="#b2">(Chen et al., 2016)</ref>.</p><p>tackled by the neural models, which perform similarly. It seems that the iterative alternating attention inference is better able to solve more difficult examples such as Ambiguous/Hard. One hypothesis is that, in contrast to Stanford AR, which uses only one fixedquery attention step, our iterative attention may better explore the documents and queries. Finally, Coreference Errors (∼25% of the corpus) includes examples with critical coreference resolution errors which may make the questions "unanswerable". This is a barrier to achieving accuracies considerably above 75% <ref type="bibr" target="#b2">(Chen et al., 2016)</ref>. If this estimate is accurate, our ensemble model (76.1%) may be approaching near-optimal performance on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>We inspect the query and document attention weights for an example article from the CNN dataset. The title of the article is "Dante turns in his grave as Italian language declines", and it discusses the decline of Italian language in schools. The plot is shown in <ref type="figure">Figure 5</ref>.2, where locations attended to in the query and document are in the left and right column respectively. Each row corresponds to an inference timestep 1 ≤ t ≤ 8. At the first step, the query attention focuses on the placeholder token, as its local context is generally important to discriminate the answer. The model first focuses on @entity148, which corresponds to "Greek" in this article. At this point, the model is still uncertain about other possible locations in the document (we can observe small weights The approach to teaching @entity6 in @placeholder schools needs a makeover , she says <ref type="figure">Figure 2</ref>: Visualization of the alternated attention mechanism for an article in CNN, treating about the decline of the Italian language in schools. The title of the plot is the query. Each row correspond to a timestep. The target is @entity3 which corresponds to the word "Italian". across document locations). At t = 2, the query attention moves towards "schools" and the model hesitates between "Italian" and "European Union" (@entity28, see step 3), both of which may satisfy the query. At step 3, the most likely candidates are "European Union" and "Rome" (@entity159). As the timesteps unfold, the model learns that "needs" may be important to infer the correct entity, i.e. "Italian". The query sits on the same attended location, while the document attention evolves to become more confident about the answer.</p><p>We find that, across CBT and CNN examples, the query attention wanders near or focuses on the placeholder location, attempting to discriminate its identity using only local context. For these particular datasets, the majority of questions can be answered after attending only to the words directly neighbouring the placeholder. This aligns with the findings of <ref type="bibr" target="#b2">(Chen et al., 2016)</ref> concerning CNN, which state that the required reasoning and inference levels for this dataset are quite simple. It would be worthwhile to formulate a dataset in which the placeholder is harder to infer using only local neighboring words, and thereby necessitates deeper query exploration.</p><p>Finally, across this work we fixed the number of inference steps T . We found that using 8 timesteps works well consistently across the tested datasets. However, we hypothesize that more (fewer) timesteps would benefit harder (easier) examples. A straight-forward extension of the model would be to dynamically select the number of inference steps conditioned on each example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Works</head><p>Neural attention models have been applied recently to a smörgåsbord of machine learning and natural language processing problems. These include, but are not limited to, handwriting recognition <ref type="bibr" target="#b6">(Graves, 2013)</ref>, digit classification <ref type="bibr" target="#b13">(Mnih et al., 2014)</ref>, machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>, question answering <ref type="bibr" target="#b18">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b7">Hermann et al., 2015)</ref> and caption generation <ref type="bibr" target="#b21">(Xu et al., 2015)</ref>. In general, attention models keep a memory of states that can be accessed at will by learned attention policies. In our case, the memory is represented by the set of document and query contextual encodings.</p><p>Our model is closely related to <ref type="bibr" target="#b18">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b11">Kumar et al., 2015;</ref><ref type="bibr" target="#b7">Hermann et al., 2015;</ref><ref type="bibr" target="#b9">Kadlec et al., 2016;</ref><ref type="bibr" target="#b8">Hill et al., 2015)</ref>, which were also applied to question answering. The pointer-style attention mechanism that we use to perform the final answer prediction has been proposed by <ref type="bibr" target="#b9">(Kadlec et al., 2016)</ref>, which in turn was based on the earlier Pointer Networks of <ref type="bibr" target="#b20">(Vinyals et al., 2015)</ref>. However, differently from our work, <ref type="bibr" target="#b9">(Kadlec et al., 2016)</ref> perform only one attention step and embed the query into a single vector representation, corresponding to the concatenation of the last state of the forward and backward GRU networks. To our knowledge, embedding the query into a single vector representation is a choice that is shared by most machine reading comprehension models. In our model, the repeated, tight integration between query attention and document attention allows the model to explore dynamically which parts of the query are most important to predict the answer, and then to focus on the parts of the document that are most salient to the currently-attended query components.</p><p>A similar attempt in attending different components of the query may be found in <ref type="bibr" target="#b7">(Hermann et al., 2015)</ref>. In that model, the document is processed once for each query word. This can be computationally intractable for large documents, since it involves unrolling a bidirectional recurrent neural network over the entire document multiple times. In contrast, our model only estimates query and document encodings once and can learn how to attend different parts of those encodings in a fixed number of steps. The inference network is responsible for making sense of the current attention step with respect to what has been gathered before. In addition to achieving state-ofthe-art performance, this technique may also prove to be more scalable than alternative query attention models.</p><p>Finally, our iterative inference process shares similarities to the iterative hops in Memory Networks <ref type="bibr" target="#b18">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b8">Hill et al., 2015)</ref>. In that model, the query representation is updated iteratively from hop to hop, although its different components are not attended to separately. Moreover, we substitute the simple linear update with a GRU network. The gating mechanism of the GRU network made it possible to use multiple steps of attention and to propagate the learning signal effectively back through to the first timestep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented an iterative neural attention model and applied it to machine comprehension tasks. Our architecture deploys a novel alternating attention mechanism, and tightly integrates successful ideas from past works in machine reading comprehension to obtain state-of-the-art results on three datasets. The iterative alternating attention mechanism continually refines its view of the query and document while aggregating the information required to answer a query.</p><p>Multiple future research directions may be envisioned. We plan to dynamically select the optimal number of inference steps required for each example. Moreover, we suspect that shifting towards stochastic attention should permit us to learn more interesting search policies. Finally, we believe that our model is fully general and may be applied in a straightforward way to other tasks such as information retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of CBT-NE, CBT-CN and CNN.</figDesc><table><row><cell>the correct answer. All words come from a vocabu-</cell></row><row><cell>lary V , and, by construction, A ⊂ D. For each query, a placeholder token is substituted for the real answer</cell></row><row><cell>a. Statistics on the datasets are reported in Table 1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). We performed a hyperparameter search with embedding regularization in {0.001, 0.0001}, inference steps T ∈ {3, 5, 8}, embedding size d ∈ {256, 384}, en-</figDesc><table><row><cell># Model</cell><cell cols="2">CBT-NE</cell><cell cols="2">CBT-CN</cell></row><row><cell></cell><cell cols="2">Valid Test</cell><cell cols="2">Valid Test</cell></row><row><cell>1 Humans (query) 1</cell><cell>-</cell><cell>52.0</cell><cell>-</cell><cell>64.4</cell></row><row><cell>2 Humans (context+query) 1</cell><cell>-</cell><cell>81.6</cell><cell>-</cell><cell>81.6</cell></row><row><cell>3 LSTMs (context+query) 1</cell><cell cols="2">51.2 41.8</cell><cell cols="2">62.6 56.0</cell></row><row><cell cols="3">4 MemNNs (window memory + self-sup.) 1 70.4 66.6</cell><cell cols="2">64.2 63.0</cell></row><row><cell>5 AS Reader 2</cell><cell cols="2">73.8 68.6</cell><cell cols="2">68.8 63.4</cell></row><row><cell>6 Ours (fixed query attention)</cell><cell cols="2">73.3 66.0</cell><cell cols="2">69.9 64.3</cell></row><row><cell>7 Ours</cell><cell cols="2">75.2 68.6</cell><cell cols="2">72.1 69.2</cell></row><row><cell>8 AS Reader (Ensemble) 2</cell><cell cols="2">74.5 70.6</cell><cell cols="2">71.1 68.9</cell></row><row><cell>9 Ours (Ensemble)</cell><cell cols="2">76.9 72.0</cell><cell cols="2">74.1 71.0</cell></row></table><note>coder size h ∈ {128, 256} and the inference GRU size s ∈ {256, 512}. We regularize our model by ap- plying a dropout (Srivastava et al., 2014) rate of 0.2 on the input embeddings, on the search gates and on</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on the CBT-NE (named entity) and CBT-CN (common noun) datasets. Results marked with 1 are from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Results on the CNN datasets. Results marked with 1 are from (Hermann et al., 2015), 2 from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>reports our results on the CNN dataset. We</cell></row><row><cell>compare our model with a simple word distance</cell></row><row><cell>model, the three neural approaches from (Hermann et</cell></row><row><cell>al., 2015) (Deep LSTM Reader, Attentive Reader and</cell></row><row><cell>Impatient Reader), and with the AS reader</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn / daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep learning. Book in preparation for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01547</idno>
		<title level="m">Text understanding with the attention sum reader network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
		<title level="m">Ask me anything: Dynamic memory networks for natural language processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. EMNLP</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<title level="m">Cloze procedure: a new tool for measuring readability. Journalism and Mass Communication Quarterly</title>
		<imprint>
			<date type="published" when="1953" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2674" to="2682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
