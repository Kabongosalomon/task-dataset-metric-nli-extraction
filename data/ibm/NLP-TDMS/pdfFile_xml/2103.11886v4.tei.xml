<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepViT: Towards Deeper Vision Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
							<email>zhoudaquan21@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
							<email>kang@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">ByteDance US AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">ByteDance US AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
							<email>lianxiaochen@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">ByteDance US AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
							<email>jzihang@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepViT: Towards Deeper Vision Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs) that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the selfattention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The proposed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet. Code is publicly available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref> have demonstrated that transformers <ref type="bibr" target="#b37">[38]</ref> can be successfully applied to vision tasks <ref type="bibr" target="#b17">[18]</ref> with competitive performance compared with convolutional neural networks (CNNs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35]</ref>. Different from CNNs that aggregate global information by stacking multiple convolutions (e.g., 3 Ã— 3) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, vision transformers (ViTs) <ref type="bibr" target="#b6">[7]</ref> take advantages of the self-attention (SA) mechanism <ref type="bibr" target="#b37">[38]</ref> to capture spatial patterns and non-local dependencies. This allows ViTs to aggregate rich global information <ref type="bibr" target="#b6">[7]</ref> Figure 1: Top-1 classification performance of vision transformers (ViTs) <ref type="bibr" target="#b6">[7]</ref> on ImageNet with different network depth {12, 16, 24, 32}. Directly scaling the depth of ViT by stacking more transformer blocks cannot monotonically increase the performance. Instead, the model performance saturates when going deeper. In contrast, with the proposed Re-attention, our DeepViT model successfully achieves better performance when it goes deeper. without handcrafting layer-wise local feature extractions as CNNs and thus achieves better performance. For example, as shown in <ref type="bibr" target="#b36">[37]</ref>, a 12-block ViT model with 22M learnable parameters achieves better results than the ResNet-101 model which has more than 30 bottleneck convolutional blocks in ImageNet classification.</p><p>The recent progress of deep CNN models is largely driven by training very deep models with a large number of layers which is enabled by novel model architecture designs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47]</ref>. This is because a deeper CNN can learn richer and more complex representations for the input images and provide better performance on vision tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b28">29]</ref>. Thus, how to effectively scale CNNs to be deeper is an important theme in recent deep learning fields, which stimulates the techniques like residual learning <ref type="bibr" target="#b8">[9]</ref>.</p><p>Considering the remarkable performance of shallow ViTs, a natural question arises: can we further improve performance of ViTs by making it deeper, just like CNNs?</p><p>Though it seems to be straightforward at the first glance, the answer may not be trivial since ViT is essentially different from CNNs in its heavy reliance on the self-attention mechanism. To settle the question, we investigate in detail the scalability of ViTs along depth in this work.</p><p>We start with a pilot study on ImageNet to investigate how the performance of ViT changes with increased model depth. In <ref type="figure">Fig. 1</ref>, we show the performance of ViTs <ref type="bibr" target="#b6">[7]</ref> with different block numbers (green line), ranging from 12 to 32. As shown, as the number of transformer blocks increases, the model performance does not improve accordingly. To our surprise, the ViT model with 32 transformer blocks performs even worse than the one with 24 blocks. This means that directly stacking more transformer blocks as performed in CNNs <ref type="bibr" target="#b8">[9]</ref> is inefficient at enhancing ViT models. We then dig into the cause of this phenomenon. We empirically observed that as the depth of ViTs increases, the attention maps, used for aggregating the features for each transformer block, tend to be overly similar after certain layers, which makes the representations stop evolving after certain layers. We name this specific issue as attention collapse. This indicates that as the ViT goes deeper, the self-attention mechanism becomes less effective for generating diverse attentions to capture rich representations.</p><p>To resolve the attention collapse issue and effectively scale the vision transformer to be deeper, We present a simple yet effective self-attention mechanism, named as Reattention. Our Re-attention takes advantage of the multihead self-attention(MHSA) structure and regenerates attention maps by exchanging the information from different attention heads in a learnable manner. Experiments show that, Without any extra augmentation and regularization policies, simply replacing the MHSA module in ViTs with Reattention allows us to train very deep vision transformers with even 32 transformer blocks with consistent improvements as shown in <ref type="figure">Fig. 1</ref>. In addition, we also provide ablation analysis to help better understand of the role of Reattention in scaling vision transformers.</p><p>To sum up, our contributions are as follows:</p><p>â€¢ We deeply study the behaviour of vision transformers and observe that they cannot continuously benefit from stacking more layers as CNNs. We further identify the underlying reasons behind such a counter-intuitive phenomenon and conclude it as attention collapse for the first time.</p><p>â€¢ We present Re-attention, a simple yet effective attention mechanism that considers information exchange among different attention heads.</p><p>â€¢ To the best of our knowledge, we are the first to successfully train a 32-block ViT on ImageNet-1k from scratch with consistent performance improvement. We show that by replacing the self-attention module with our Re-attention, new state-of-the-art results can be achieved on the ImageNet-1k dataset without any pretraining on larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Transformers for Vision Tasks</head><p>Transformers <ref type="bibr" target="#b37">[38]</ref> are initially used for machine translation which replace the recurrence and convolutions entirely with self-attention mechanisms <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b16">17]</ref> and achieve outstanding performance. Later, transformers become the dominant models for various natural language processing (NLP) tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref>. Motivated by their success on the NLP tasks, recent researchers attempted to combine the self-attention mechanism into CNNs for computer vision tasks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b48">49]</ref>.. Those achievements also stimulate interests of the community in building purely transformer-based models (without convolutions and inductive bias) for vision tasks. The vision transformer (ViT) <ref type="bibr" target="#b6">[7]</ref> is among the first attempt that uses the pure transformer architecture to achieve competitive performance with CNNs on the image classification task. However, due to the large model complexity, ViT needs to be pre-trained on largerscale datasets (e.g., JFT300M) for performing well on the ImageNet-1k dataset. To solve the data efficiency issue, DeiT <ref type="bibr" target="#b36">[37]</ref> deploys knowledge distillation to train the model with a larger pre-trained teacher model. In this manner, vision transformer can perform well on ImageNet-1k without the need of pre-training on larger dataset. Differently, in this work, we target at a different problem with ViT, i.e., how to effectively scale ViT to be deeper. We propose a new design for the self-attention mechanism so that it can perform well on vision tasks without the need of extra data, teacher networks, and the domain specific inductive bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Depth Scaling of CNNs</head><p>Increasing the network depth of a CNN model is deemed to be an effective way to improve the model performance <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>. However, very deep CNNs are generally harder to train to perform significantly better than the shallow ones in the past <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b43">44]</ref>. How to effectively scale up the CNNs in depth was a long-standing and challenging problem <ref type="bibr" target="#b15">[16]</ref>. The recent progress of CNNs largely benefits from novel architecture design strategies that make training deep CNNs more effective <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b51">52]</ref>. Transformer-alike models have modularized architectures and thus can be easily made deeper by repeating the basic transformer blocks or using larger embedding dimensions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>. However, those straightforward scaling strategies only work well with larger datasets and stronger augmentation policies <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b42">43]</ref> to alleviate the brought training difficulties. In this paper, we observed that with the same dataset, the performance of vision transformers do saturate as the network depth rises. We rethink the self-attention mechanism and present a simple but effective approach to address the difficulties in scaling vision transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Revisiting Vision Transformer</head><p>A vision transformer (ViT) model <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b6">7]</ref>, as depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>(a), is composed of three main components: a linear layer for patch embedding (i.e., mapping the highresolution input image to a low-resolution feature map), a stack of transformer blocks with multi-head self-attention and feed-forward layers for feature encoding, and a linear layer for classification score prediction. In this section, we first review its unique transformer blocks, in particular the self-attention mechanism, and then we provide studies on the collapse problem of self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Head Self-Attention</head><p>Transformers <ref type="bibr" target="#b37">[38]</ref> were extensively used in natural language for encoding a sequence of input word tokens into a sequence of embeddings. To comply with such sequenceto-sequence learning structure when processing images, ViTs first divide an input image into multiple patches uniformly and encode each patch into a token embedding. Then, all these tokens, together with a class token, are fed into a stack of transformer blocks.</p><p>Each transformer block consists of a multi-head self-attention (MHSA) layer and a feed-forward multi-layer perceptron (MLP). The MHSA generates a trainable associate memory with a query (Q) and a pair of key (K)-value (V ) pairs to an output via linearly transforming the input. Mathematically, the output of a MHSA is calculated by:</p><formula xml:id="formula_0">Attention(Q, K, V ) = Softmax(QK / âˆš d)V,<label>(1)</label></formula><p>where âˆš d is a scaling factor based on the depth of the network. The output of the MHSA is then normalized and fed into the MLP to generate the input to the next block. In the above self-attention, Q and K are multiplied to generate the attention map, which represents the correlation between all the tokens within each layer. It is used to retrieve and combine the embeddings in the value V . In the following, we particularly analyze the role of the attention map in scaling the ViT. For convenience, we use A âˆˆ R HÃ—T Ã—T to denote the attention map, with H being the number of SA heads and T the number of tokens. For the h-th SA head, the attention map is computed as</p><formula xml:id="formula_1">A h,:,: = Softmax(Q h K h / âˆš d)</formula><p>with Q h and K h from the corresponding head. When the context is clear, we omit the subscript h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention Collapse</head><p>Motivated by the success of deep CNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, we conduct systematic study in the changes of the performance of ViTs as depth increases. Without loss of generality, we first fix the hidden dimension and the number of heads to 384 and 12 respectively 1 , following the common practice in <ref type="bibr" target="#b36">[37]</ref>. Then we stack different number of transformer blocks (varying from 12 to 32) to build multiple ViT models corresponding to different depths. The overall performances for image classification are evaluated on Im-ageNet <ref type="bibr" target="#b17">[18]</ref> and summarized in <ref type="figure">Fig. 1</ref>. As evidenced by the performance curve, we surprisingly find that the classification accuracy improves slowly and saturates fast as the model goes deeper. More specifically, we can observe that the improvement stops after employing 24 transformer blocks. This phenomenon demonstrates that existing ViTs have difficulty in gaining benefits from deeper architectures.</p><p>Such a problem is quite counter-intuitive and worth exploration, as similar issues (i.e., how to effectively train a deeper model) have also been observed for CNNs at its early development stage <ref type="bibr" target="#b8">[9]</ref>, but properly solved later <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. By taking a deeper look into the transfromer architecture, we would like to highlight that the self-attention mechanism plays a key role in ViTs, which makes it significantly different from CNNs. Therefore, we start with investigating how the self-attention, or more concretely, the generated attention map A varies as the model goes deeper. <ref type="figure">Figure 3</ref>: (a) The similarity ratio of the generated self-attention maps across different layers. The visualization is based on ViT models with 32 blocks pre-trained on ImageNet. For visualization purpose, we plot the ratio of token-wise attention vectors with similarity in Eqn. (2) larger than the average similarity within nearest k transformer blocks. As can be seen, the similarity ratio is larger than 90% for blocks after the 17th one. (b) The ratio of similar blocks to the total number of blocks increases when the depth of the ViT model increases. (c) Similarity of attention maps from different heads within the same block. The similarity between different heads within the blocks is all lower than 30% and they present sufficient diversity.</p><p>To measure the evolution of the attention maps over layers, we compute the following cross-layer similarity between the attention maps from different layers:</p><formula xml:id="formula_2">M p,q h,t = A p h,:,t A q h,:,t A p h,:,t A q h,:,t ,<label>(2)</label></formula><p>where M p,q is the cosine similarity matrix between the attention map of layers p and q. Each element M p,q h,t measures the similarity of attention for head h and token t. Consider one specific self-attention layer and its h-th head, A * h,:,t is a T -dimensional vector representing how much the input token t contributes to each of the T output tokens. M p,q h,t , thus, provides an appropriate measurement on how the contribution of one token varies from layer p to q. When M p,q h,t equals one, it means that token t plays exactly the same role for self-attention in layers p and q.</p><p>Given Eqn.</p><p>(2), we then train a ViT model with 32 transformer blocks on ImageNet-1k and investigate the above similarity among all the attention maps. As shown in <ref type="figure">Fig. 3(a)</ref>, the ratio of similar attention maps in M after the 17th block is larger than 90% . This indicates that the learned attention maps afterwards are similar and the transformer block may degenerate to an MLP. As a result, further stacking such degenerated MHSA may introduce the model rank degeneration issue (i.e., the rank of the model parameter tensor from multiplying the layer-wise parameters together will decrease) and limits the model learning capacity. This is also validated by our analysis on the degeneration of learned features as shown below. Such observed attention collapse could be one of the reasons for the observed performance saturation of ViTs. To further validate the existence of this phenomenon for ViTs with different depths, we conduct the same experiments on ViTs with 12, 16, 24 and 32 transformer blocks respectively and calculate the number of blocks with similar attention maps. The re- sults shown in <ref type="figure">Fig. 3</ref>(b) clearly demonstrate the ratio of the number of similar attention map blocks to the total number of blocks increases when adding more transformer blocks.</p><p>To understand how the attention collapse may hurt the ViT model performance, we further study how it affects feature learning of the deeper layers. For a specific 32-block ViT model, we compare the final output features with the outputs of each intermediate transformer block by investigating their cosine similarity. The results in <ref type="figure" target="#fig_1">Fig. 4</ref> demonstrate that the similarity is quite high and the learned features stop evolving after the 20th block. There is a close correlation between the increase of attention similarity and feature similarity. This observation indicates that attention collapse is responsible for the non-scalable issue of ViTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Re-attention for Deep ViT</head><p>As revealed above, one major obstacle in scaling up ViT to a deeper one is the attention collapse problem. In this sec- <ref type="figure">Figure 5</ref>: Impacts of embedding dimension on the similarity of generated self-attention map across layers. As can be seen, the number of similar attention maps decreases with increasing embedding dimension. However, the model size also increases rapidly. <ref type="table">Table 1</ref>: Top-1 accuracy on ImageNet-1k dataset of vision transformer with different embedding dimensions. The number of model parameters increase quadratically with the embedding dimension. The number of similar attention map blocks with different embedding dimensions are shown in <ref type="figure">Figure 5</ref> . tion, we present two solution approaches, one is to increase the hidden dimension for computing self-attention and the other one is a novel re-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Self-Attention in Higher Dimension Space</head><p>One intuitive solution to conquer attention collapse is to increase the embedding dimension of each token. This will augment the representation capability of each token embedding to encode more information. As such, the resultant attention maps can be more diverse and the similarity between each block's attention map could be reduced. Without loss of generality, we verify this approach empirically by conducting a set of experiments based on ViT models with 12 blocks for quick experiments. Following previous transformer based works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b6">7]</ref>, four embedding dimensions are selected, ranging from 256 to 768. The detailed configurations and the results are shown in Tab. 1.</p><p>From <ref type="figure">Fig. 5</ref> and Tab. 1, one can see that the number of blocks with similar attention maps is reduced and the attention collapse is alleviated by increasing the embedding dimension. Consequently, the model performance is also increased accordingly. This validates our core hypothesis-the attention collapse is the main bottleneck for scaling ViT. Despite its effectiveness, increasing the embedding di-mension also increases the computation cost significantly and the brought performance improvement tends to diminish. Besides, a larger model (with higher embedding dimension) typically needs more data for training, suffering the over-fitting risk and decreased efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Re-attention</head><p>It has been demonstrated in Sec. 3 that the similarity between attention maps across different transformer blocks is high, especially for deep layers. However, we find the similarity of attention maps from different heads of the same transformer block is quite small, as shown in <ref type="figure">Fig. 3(c)</ref>. Clearly, different heads from the same self-attention layer focus on different aspects of the input tokens. Based on this observation, we propose to establish cross-head communication to re-generate the attention maps and train deep ViTs to perform better.</p><p>Concretely, we use the attention maps from the heads as basis and generate a new set of attention maps by dynamically aggregating them. To achieve this, we define a learnable transformation matrix Î˜ âˆˆ R HÃ—H and then use it to mix the multi-head attention maps into re-generated new ones, before being multiplied with V . Specifically, the Reattention is implemented by:</p><formula xml:id="formula_3">Re-Attention(Q, K, V ) = Norm(Î˜ (Softmax( QK âˆš d )))V,<label>(3)</label></formula><p>where transformation matrix Î˜ is multiplied to the selfattention map A along the head dimension. Here Norm is a normalization function used to reduced the layer-wise variance. Î˜ is end-to-end learnable.</p><p>Advantages: The advantages of the proposed Re-attention are two-fold. First of all, compared with other possible attention augmentation methods, such as randomly dropping some elements of the attention map or tuning SoftMax temperature, our Re-attention exploits the interactions among different attention heads to collect their complementary information and better improves the attention map diversity. This is also verified by our following experiments. Furthermore, our Re-attention is effective and easy to implement. It needs only a few lines of code and negligible computational overhead compared to the original self-attention. Thus it is much more efficient than the approach of increasing embedding dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first conduct experiments to further demonstrate the attention collapse problem. Then, we give extensive ablation analysis to show the advantages of the proposed Re-attention. By incorporating Re-attention into the transformers, we design two modified version of vision transformers and name them as deep vision transform-ers (DeepViT). Finally, we compare the proposed DeepViT models against the latest state-of-the-arts (SOTA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment Details</head><p>To make a fair comparison, we first tuned a set of parameters for training the ViT base model and then use the same set of hyper-parameters for all the ablation experiments. Specifically, we use AdamW optimizer <ref type="bibr" target="#b22">[23]</ref> and cosine learning rate decay policy with an initial learning rate of 0.0005. We use 8 Telsa-V100 GPUs and train the model for 300 epochs using Pytorch <ref type="bibr" target="#b24">[25]</ref> library. The batch size is set to 256. We use 3 epochs for learning rate warm-up <ref type="bibr" target="#b21">[22]</ref>. We also use some augmentation techniques such as mixup <ref type="bibr" target="#b45">[46]</ref> and random augmentation <ref type="bibr" target="#b4">[5]</ref> to boost the performance of baseline models following <ref type="bibr" target="#b46">[47]</ref>. When comparing with other methods, we adopt the same set of hyper-parameters as used by the target models. We report results on the Im-ageNet dataset <ref type="bibr" target="#b17">[18]</ref>. For all experiments, the image size is set to be 224Ã—224. To study the scaling capability of current transformer blocks, we set the embedding dimension to 384 and the expansion ratio 3 for the MLP layers. We use 12 heads for all the models. More detailed configurations are shown in Tab. 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">More Analysis on Attention Collapse</head><p>In this section, we show more analysis on the attention map similarity and study how the collapsed attention maps affect the model performance.</p><p>Attention reuse: As discussed above, when the model goes deeper, the attention maps of the deeper blocks become highly similar. This implies that adding more blocks on a deep ViT model may not improve the model performance. To further verify this claim, we design an experiment to reuse the attention maps computed at an early block of ViT to replace the ones after it. Specifically, we run experiments on the ViT models with 24 blocks and 32 blocks but share the Q and K values (and the resulted attention maps) of the last "unique" block to all the blocks afterwards. The "unique" block is defined as the block whose attention map's similarity ratio with adjacent layers is smaller than 90%. More implementation details can be found in the supplementary material. The results are shown in Tab. 3. Surprisingly, for a ViT model with 32 transformer blocks, when we use the same Q and K values for the last 15 blocks, the performance degradation is negligible. This implies the attention collapse problem indeed exists and reveals the inefficacy in adding more blocks when the model is deep. Visualization: To more intuitively understand the attention map collapse across layers, we visualize the learned attention maps from different blocks of the original ViT <ref type="bibr" target="#b6">[7]</ref>. We take a 32-block ViT model as an example and pre-train it on ImageNet. The visualization of the attention maps with original MHSA and Re-attention are shown in <ref type="figure">Fig. 6</ref>. It can be observed that the original MHSA learns the local relationship among the adjacent patches in the shallow blocks and the attention maps tend to expand to cover more patches gradually. In the deep blocks, the MHSA learns nearly uniform global attention maps with high similarity. Differently, after implementing Re-attention, the attention maps at deep blocks keep the diversity and have small similarities from adjacent blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis on Re-attention</head><p>In this subsection, we present two straightforward modifications to the current self-attention mechanism as baselines. We then conduct a series of comparison experiments to show the advantages of our proposed Re-attention.</p><p>Re-attention v.s. Self-attention: We first evaluate the effectiveness of Re-attention by comparing to the pure ViT models using the same set of training hyper-parameters. We directly replace the self-attention module in ViT with Reattention and show the results in Tab. 4 with different number of transformer blocks. As can be seen, the vanilla ViT architecture suffers performance saturation when adding more transformer blocks. This phenomenon coincides with our observations that the number of blocks with similar attention maps increases with the depth as shown in <ref type="figure">Fig.  3(b)</ref>. Interestingly, when replacing the self-attention with our proposed Re-attention, the number of similar blocks are all reduced to be zero and the performance rises consistently as the model depth increases. The performance gain is especially significant for deep ViT with 32 blocks. This might be explained by the fact that the 32 block ViT model has the largest number of blocks with similar attention maps and the improvements should be proportional to the number sim-  Our proposed re-attention mechanism. As shown, the original attention map is mixed via a learnable matrix Î˜ before multiplied with values.</p><p>ilar blocks in the model. These experiments demonstrate that the proposed Re-attention can indeed solve the attention collapse problem and thus enables training a very deep vision transformer without extra datasets or augmentation policies. Comparison to adding temperature in self-attention:</p><p>The most intuitive way to mitigate the over-smoothing phenomenon is to sharpen the distribution of the elements in the attention map of MHSA. We could achieve this by as-signing a temperature Ï„ to the Softmax layer of MHSA:</p><formula xml:id="formula_4">Attention(Q, K, V ) = Softmax QK Ï„ âˆš d V.<label>(4)</label></formula><p>As the attention collapse is observed to be severe on deep layers (as shown in <ref type="figure">Fig. 3</ref>), we design two sets of experiments on a ViT model with 32 transformer blocks: (a) linearly decaying the temperature Ï„ in each block such that the attention map distribution is sharpened and (b) making the temperature learnable and optimized together with the model training. We first check the impact of the Soft-Max temperature on reducing the attention map similarity. As shown in <ref type="figure" target="#fig_3">Fig. 8(a)</ref>, the number of similar blocks are still large. Correspondingly, the feature similarity among blocks are also large as shown in <ref type="figure" target="#fig_3">Fig. 8(b)</ref>. Thus, adding a temperature to the SoftMax only reduces the attention map similarity by a small margin. The classification results on ImageNet are shown in Tab. 5. As shown, using a learnable temperature could improve the performance but the improvement is marginal. Comparison to dropping attentions: Another baseline we have attempted to differentiate the self-attention maps across layers is to use random dropout on the attention maps A. As the dropout will mask out different positions on the attention maps for different blocks, the similarity between attention maps could be reduced. The impacts on the attention maps and the output features of each block are shown in <ref type="figure" target="#fig_3">Fig. 8(a-b)</ref>. It is observed that dropping attention does reduce the cross layer similarity of the attention maps. However, the similarity among features are not reduced by much. This is because the difference between attention maps comes from the zero positions in the generated mask. Those zero values do reduce the similarity between attention maps but not contribute to the features. Thus, the improvement is still not significant as shown in Tab. 5.</p><p>Advantages of Re-attention: Our proposed Re-attention brings more significant improvements over the temperaturetuning and the attention-dropping methods. This is because both adding temperature and dropping attention are regularizing the distribution of the originally over-smoothed selfattention maps, without explicitly encouraging them to be diverse. However, our proposed Re-attention mechanism uses different heads (whose attention maps are dissimilar) as basis and re-generate the attention maps via the transformation matrix Î˜. This process incorporates the interhead information communication and the generated attention maps can encode richer information. It is worth noting that the original MHSA design can be thought as a special case of Re-attention with an identity transformation matrix. By making Î˜ learnable for each block, an optimized pattern could be learned end to end. As shown in <ref type="figure" target="#fig_3">Fig. 8(c)</ref>, the learned transformation matrix assigns a diverse set of weights for each newly generated head. It clearly shows that the combination for each new heads takes different weights from the original heads in the re-attention process and thus reduces the similarity between their attention maps. As shown in <ref type="figure" target="#fig_3">Fig. 8(a)</ref>, our proposed Re-attention achieves the lowest cross layer attention map similarity. Consequently, it also reduces the feature map similarity across layers as shown in <ref type="figure" target="#fig_3">Fig. 8(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with other SOTA models</head><p>With Re-attention, we design two ViT variants, i.e., DeepViT-S and DeepViT-L, based on the ViT with 16 and 32 transformer blocks respectively. For both models, we use Re-attention to replace the self-attention. To have a similar number of parameters with other ViT models, we adjust the embedding dimension accordingly. The hidden dimensions of DeepViT-S and DeepViT-L models are set as 396 and 408 respectively. More details on the model configuration are given in the supplementary material. Besides, motivated by <ref type="bibr" target="#b41">[42]</ref>, we add three CNN layers for extracting the token embeddings, using the same configurations as <ref type="bibr" target="#b41">[42]</ref>. It is worth noting that we do not use the optimized training recipes and the repeated augmentation as <ref type="bibr" target="#b36">[37]</ref> for training our models. The results are shown in Tab. 6. Clearly, our DeepViT model achieves higher accuracy with less parameters than the recent CNN and ViT based models. Notably, without any complicated architecture change as made by T2T-ViT <ref type="bibr" target="#b41">[42]</ref> (adopting a deep-narrow architecture) or DeiT <ref type="bibr" target="#b36">[37]</ref> (introducing token distillation), simply using the Re-attention makes our DeepViT-L outperforms them by 0.4 points with even smaller model size (55M vs. 64M &amp; 86 M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we found the attention collapse problem of vision transformers as they go deeper and propose a novel Re-attention mechanism to solve it with minimum amount of computation and memory overhead. With our proposed Re-attention, we are able to maintain an increasing performance when increasing the depth of ViTs. We hope our observations and methods could facilitate the development of vision transformers in future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Implementation Details</head><p>Attention reuse: As shown in <ref type="figure">Fig. 3(b)</ref> and Tab. 3 in the main paper, the vision transformers with 24 blocks and 32 blocks have 11 and 15 blocks with similar attention maps, respectively. To verify the effectiveness of the attention maps from those blocks, we directly force those blocks to share the same attention map as the last 'unique' block as defined in Sec. 5.2. Specifically, we take the attention map of the last 'unique' block and denote it as A unique . For all the following blocks, the attention output is calculated by:</p><formula xml:id="formula_5">Attention(Q, K, V ) = Norm(Î˜ A unique )V,<label>(5)</label></formula><p>where Î˜ is used to simulate the small variance between attention maps across layers since they are not identical. Norm is batch normalization used to adjust the variance across layers. As shown in Tab. 3, for a ViT with 32 blocks, forcing the top 15 blocks to share the same attention map causes negligible degradation on the classification accuracy on ImageNet. This proves that adding those blocks do not contribute to the accuracy improvement.</p><p>Training loss: We use the cross-entropy (CE) loss as the training loss for all experiments. To minimize the similarity of the attention maps across layers, we add the cosine similarity between layers into the loss function when training the model.</p><formula xml:id="formula_6">Loss train = Loss CE + Î» B l=0 Similarity(A l , A l+1 ) (6)</formula><p>where Similarity(A l , A l+1 ) denotes the cosine similarity between layer l and l + 1 and A l denotes the attention map of layer l. B denotes the number of bottom blocks used for regularization and is a hyper-parameter. We set B to 4, 8 and 12 for training ViT models with 16, 24 and 32 blocks respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DeepViT architecture design</head><p>As observed in <ref type="figure">Fig. 3(a)</ref>, the attention maps of the transformer blocks become similar only at the top blocks. Thus, it is not necessary to apply re-attention to all blocks. To study the optimal number of blocks with re-attention, we conduct a set of experiments on a ViT model with 16 transformer blocks. For each experiment, we only apply reattention on the top K blocks where K ranges from 5 to 15. The rest of the blocks are using the original transformer block structure. We train each model on ImageNet with the same set of training hyper-parameters as those for baseline models as detailed in Sec. 5 in the main paper. The results are shown in <ref type="figure" target="#fig_4">Fig. 9</ref>.</p><p>It is observed that, as the number of re-attention blocks varies, the top-1 classification accuracy changes correspondingly. The highest accuracy appears at the position where the number of re-attention blocks is the same as the number of similar attention map blocks. Based on this observation, we define the architecture of DeepViT-S and DeepViT-L with 5 and 12 re-attention blocks respectively. Detailed configurations are shown in Tab. 7. Note that we adjust the embedding dimension to have a comparable size with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Impacts of hyper-parameters</head><p>In the main paper, all experiments are run with the same set of training hyper-parameters as the one used for reproducing ViT models. However, as shown in <ref type="bibr" target="#b36">[37]</ref>, an improved training recipe could improve the performance of ViT models significantly. In Tab. 8, we present the performance of DeepViT-S and DeepViT-L with the same set of training recipes as DeiT except that we do not use repeated augmentation. In Tab. 8, it is clearly shown that the performance of DeepViT could be further improved with optimized training hyper-parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Similarity calculation</head><p>Cosine similarity between layers To measure the similarity between the attention maps, we define the similarity S p,q between the attention maps of two layers, p and q, as the ratio of the number of similar vector pairs to the total number of pairs between two attention maps:</p><formula xml:id="formula_7">S(p, q) = I |M p,q | , I i,j = 1, if M p,q i,j &gt; Ï„ 0, otherwise<label>(7)</label></formula><p>where Ï„ is a hyper-parameter and used as a threshold for deciding similar vectors 2 .</p><p>Definition of similar blocks A block is counted as a similar block if the similarity between it's attention map and the adjacent block's attention map is larger than 80%. To measure the block similarity for a ViT model with B blocks, we take the ratio of the number of similar blocks to the total number of blocks as a measurement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Comparison between the (a) original ViT with N transformer blocks and (b) our proposed DeepViT model. Different from ViT, DeepViT replaces the self-attention layer within the transformer block with the proposed Re-attention which effectively addresses the attention collapse issue and enables training deeper ViTs. More details are given in Sec. 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>(Left): Cross layer similarity of attention map and features for ViTs. The black dotted line shows the cosine similarity between feature maps of the last block and each of the previous blocks. The red dotted line shows the ratio of similar attention maps of adjacent blocks. The visualization is based on a 32-block ViT model pre-trained on ImageNet-1k. (Right): Feature map cross layer cosine similarity for both the original ViT model and ours. As can be seen, replacing the original self-attention with Re-attention could reduce the feature map similarity significantly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Attention map visualization of the selected blocks of the baseline ViT model with 32 transformer blocks. The first row is based on original Self-attention module and the second is based on Re-attention. As can be seen, the model only learns local patch relationship at its shallow blocks with the rest of attention values near to zero. Though their the scope increases gradually as the block goes deeper, the attention maps tend to become nearly uniform and thus lose diversity. After adding Re-attention, the originally similar attention maps are changed to be diverse as shown in the second row. Only at the last block's attention map, a nearly uniform attention map is learned. (Left): The original self-attention mechanism; (Right):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>(a) Adjacent block attention map similarity with different methods. As can be seen, our proposed Re-attention achieves low cross layer attention map similarity. (b) Cosine similarity between the feature map of the last block and each of the previous block. (c) Visualization of transformation matrix of the last block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>ViT classification accuracy with Re-attention applied on different number of blocks. The black dotted line denotes the cosine similarity ratio between adjacent blocks of the original ViT model with 16 blocks. The red dotted line denotes the top-1 classification accuracy on ImageNet. The accuracy of the model with blocks index k denotes that the Re-attention is applied on top (16 âˆ’ k) blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Model #Blocks Embed Dim. #Param. (M) Top-1 Acc.(%)</figDesc><table><row><cell>ViT</cell><cell>12</cell><cell>256</cell><cell>8.15</cell><cell>74.6</cell></row><row><cell>ViT</cell><cell>12</cell><cell>384</cell><cell>18.51</cell><cell>77.86</cell></row><row><cell>ViT</cell><cell>12</cell><cell>512</cell><cell>33.04</cell><cell>78.8</cell></row><row><cell>ViT</cell><cell>12</cell><cell>768</cell><cell>86</cell><cell>79.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Baseline model specifications. All ablation experiments are based on the ViT models with different number of blocks. The '#B' in 'ViT-#B' denotes the number of transformer blocks in the model.</figDesc><table><row><cell>Model</cell><cell cols="4">#Blocks #Embeddings MLP Size Params. (M)</cell></row><row><cell>ViT-16B</cell><cell>16</cell><cell>384</cell><cell>1152</cell><cell>24.46</cell></row><row><cell>ViT-24B</cell><cell>24</cell><cell>384</cell><cell>1152</cell><cell>36.26</cell></row><row><cell>ViT-32B</cell><cell>32</cell><cell>384</cell><cell>1152</cell><cell>48.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>ImageNet top-1 accuracy of the ViT models with shared self-attention maps. '#Shared blocks' denotes the number of the transformer blocks that share the same attention map.</figDesc><table><row><cell cols="4">#Blocks #Embeddings #Shared blocks Top-1 Acc. (%)</cell></row><row><cell>24</cell><cell>384</cell><cell>0</cell><cell>79.3</cell></row><row><cell>24</cell><cell>384</cell><cell>11</cell><cell>78.7</cell></row><row><cell>32</cell><cell>384</cell><cell>0</cell><cell>79.2</cell></row><row><cell>32</cell><cell>384</cell><cell>15</cell><cell>79.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>ImageNet Top-1 accuracy of deep ViT (DeepViT) models with Re-attention and different number of transformer blocks.</figDesc><table><row><cell>Model</cell><cell cols="3">#Similar Blocks Param. (M) Top-1 Acc. (%)</cell></row><row><cell>ViT-16B [7]</cell><cell>5</cell><cell>24.5</cell><cell>78.9</cell></row><row><cell>DeepViT-16B</cell><cell>0</cell><cell>24.5</cell><cell>79.1 (+0.2)</cell></row><row><cell>ViT-24B [7]</cell><cell>11</cell><cell>36.3</cell><cell>79.4</cell></row><row><cell>DeepViT-24B</cell><cell>0</cell><cell>36.3</cell><cell>80.1 (+0.7)</cell></row><row><cell>ViT-32B [7]</cell><cell>16</cell><cell>48.1</cell><cell>79.3</cell></row><row><cell>DeepViT-32B</cell><cell>0</cell><cell>48.1</cell><cell>80.9 (+1.6)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>ImageNet Top-1 accuracy of the ViT models with Soft-Max temperature Ï„ and the drop attention. The embedding dimension of all the models is set as 384.</figDesc><table><row><cell cols="2"># Blocks # Similar Blocks</cell><cell>Model</cell><cell>Acc. (%)</cell></row><row><cell>32</cell><cell>16</cell><cell>Vanilla</cell><cell>79.3</cell></row><row><cell>32</cell><cell>13</cell><cell>Linearly decayed Ï„</cell><cell>79.0</cell></row><row><cell>32</cell><cell>10</cell><cell>Learnable Ï„</cell><cell>79.5</cell></row><row><cell>32</cell><cell>8</cell><cell>drop attention</cell><cell>79.5</cell></row><row><cell>32</cell><cell>0</cell><cell>Re-attention</cell><cell>80.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Top-1 accuracy comparison with other SOTA models on ImageNet. * denotes our reproduced results. denotes our model trained with training recipes used in DeiT<ref type="bibr" target="#b36">[37]</ref>.</figDesc><table><row><cell>Model</cell><cell cols="3">Params. (M) MAdds (G) Acc. (%)</cell></row><row><cell>ResNet50 [9]</cell><cell>25</cell><cell>4.0</cell><cell>76.2</cell></row><row><cell>ResNet50*</cell><cell>25</cell><cell>4.0</cell><cell>79.0</cell></row><row><cell>RegNetY-8GF [27]</cell><cell>40</cell><cell>8.0</cell><cell>79.3</cell></row><row><cell>Vit-B/16 [7]</cell><cell>86</cell><cell>17.7</cell><cell>77.9</cell></row><row><cell>Vit-B/16*</cell><cell>86</cell><cell>17.7</cell><cell>79.3</cell></row><row><cell>T2T-ViT-16 [42]</cell><cell>21</cell><cell>4.8</cell><cell>80.6</cell></row><row><cell>DeiT-S [37]</cell><cell>22</cell><cell>-</cell><cell>79.8</cell></row><row><cell>DeepVit-S (Ours)</cell><cell>27</cell><cell>6.2</cell><cell>81.4</cell></row><row><cell>DeepVit-S (Ours)</cell><cell>27</cell><cell>6.2</cell><cell>82.3</cell></row><row><cell>ResNet152 [9]</cell><cell>60</cell><cell>11.6</cell><cell>78.3</cell></row><row><cell>ResNet152*</cell><cell>60</cell><cell>11.6</cell><cell>80.6</cell></row><row><cell>RegNetY-16GF [27]</cell><cell>54</cell><cell>15.9</cell><cell>80.0</cell></row><row><cell>Vit-L/16 [7]</cell><cell>307</cell><cell>-</cell><cell>76.5</cell></row><row><cell>T2T-ViT-24 [42]</cell><cell>64</cell><cell>12.6</cell><cell>81.8</cell></row><row><cell>DeiT-B [37]</cell><cell>86</cell><cell>-</cell><cell>81.8</cell></row><row><cell>DeiT-B*</cell><cell>86</cell><cell>17.7</cell><cell>81.5</cell></row><row><cell>DeepVit-L (Ours)</cell><cell>55</cell><cell>12.5</cell><cell>82.2</cell></row><row><cell>DeepVit-L (Ours)</cell><cell>58</cell><cell>12.8</cell><cell>83.1</cell></row><row><cell>DeepVit-L â†‘ 384 (Ours)</cell><cell>58</cell><cell>12.8</cell><cell>84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Structural hyper-parameter of DeepViT-S and DeepViT-L. Note that the embedding dimension is slightly larger than the baseline models. This is to adjust the size of the model to have a comparable size with other methods for a fair comparison.</figDesc><table><row><cell>Model</cell><cell cols="4">#Blocks #Embedding MLP size Split ratio</cell></row><row><cell>DeepViT-S</cell><cell>16</cell><cell>396</cell><cell>1188</cell><cell>11-5</cell></row><row><cell>DeepViT-L</cell><cell>32</cell><cell>420</cell><cell>1260</cell><cell>20-12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>DeepViT model with different training recipes. denotes the model trained with DeiT<ref type="bibr" target="#b36">[37]</ref> training recipes.</figDesc><table><row><cell>Model</cell><cell cols="3">Params. (M) MAdds (G) Acc. (%)</cell></row><row><cell>DeiT-S [37]</cell><cell>22</cell><cell>-</cell><cell>79.8</cell></row><row><cell>DeiT-S (KD) [37]</cell><cell>22</cell><cell>-</cell><cell>81.2</cell></row><row><cell>DeepVit-S (Ours)</cell><cell>27</cell><cell>6.2</cell><cell>81.4</cell></row><row><cell>DeepVit-S (Ours)</cell><cell>27</cell><cell>6.2</cell><cell>82.3</cell></row><row><cell>DeiT-B [37]</cell><cell>86</cell><cell>17.7</cell><cell>81.8</cell></row><row><cell>DeiT-B (KD) [37]</cell><cell>86</cell><cell>17.7</cell><cell>83.4</cell></row><row><cell>DeepViT-L (Ours)</cell><cell>55</cell><cell>12.5</cell><cell>82.2</cell></row><row><cell>DeepViT-L (Ours)</cell><cell>58</cell><cell>12.8</cell><cell>83.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Similar phenomenon can also be found when we vary the hidden dimension size according to our experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">0.5 is selected as a threshold for visualization purpose in this paper</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Coordinate attention for efficient mobile network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02907</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<title level="m">Efficient training of giant neural networks using pipeline parallelism</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02496</idno>
		<title level="m">Convbert: Improving bert with span-based dynamic convolution</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16668</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving convolutional networks with self-calibrated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10096" to="10105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr DollÃ¡r</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Standalone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixconv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09595</idno>
		<title level="m">Mixed depthwise convolutional kernels</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<title level="m">Alexandre Sablayrolles, and HervÃ© JÃ©gou. Training data-efficient image transformers &amp; distillation through attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Standalone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Network representation learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daokun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="28" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10076" to="10085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09164</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Point transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rethinking bottleneck structure for efficient mobile network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2020-08-02" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
