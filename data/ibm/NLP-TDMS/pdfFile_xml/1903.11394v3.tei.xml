<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Region-Adaptive Dense Network for Efficient Motion Deblurring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Purohit</surname></persName>
							<email>kuldeeppurohit3@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Region-Adaptive Dense Network for Efficient Motion Deblurring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address the problem of dynamic scene deblurring in the presence of motion blur. Restoration of images affected by severe blur necessitates a network design with a large receptive field, which existing networks attempt to achieve through simple increment in the number of generic convolution layers, kernel-size, or the scales at which the image is processed. However, these techniques ignore the nonuniform nature of blur, and they come at the expense of an increase in model size and inference time. We present a new architecture composed of region adaptive dense deformable modules that implicitly discover the spatially varying shifts responsible for non-uniform blur in the input image and learn to modulate the filters. This capability is complemented by a self-attentive module which captures non-local spatial relationships among the intermediate features and enhances the spatially varying processing capability. We incorporate these modules into a densely connected encoder-decoder design which utilizes pre-trained Densenet filters to further improve the performance. Our network facilitates interpretable modeling of the spatially-varying deblurring process while dispensing with multi-scale processing and large filters entirely. Extensive comparisons with prior art on benchmark dynamic scene deblurring datasets clearly demonstrate the superiority of the proposed networks via significant improvements in accuracy and speed, enabling almost real-time deblurring.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Though computational imaging has made tremendous progress over the years, handling motion blur in captured content remains a challenge. Motion blur is caused by motion of objects in the scene or the camera during sensor exposure. Apart from significantly degrading the visual quality, the distortions arising from blur lead to considerable performance drop for many vision tasks <ref type="bibr" target="#b21">(Vasiljevic, Chakrabarti, and Shakhnarovich 2016)</ref>. There exist a few commercially available cameras which can capture frames at a high framerate and thus experience less blur but they suffer from noise at high resolution and are quite expensive.</p><p>Motion deblurring is a challenging problem in computer vision due to its ill-posed nature. The past decade has wit-Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PSNR (dB)</head><p>Runtime for an HD image (seconds) <ref type="figure">Figure 1</ref>: Our network outperforms all existing approaches on the dynamic scene deblurring benchmark in terms of accuracy as well as inference time. nessed significant advances in deblurring, wherein major efforts have gone into designing priors that are apt for recovering the underlying undistorted image and the camera trajectory <ref type="bibr" target="#b22">(Vasu and Rajagopalan 2017;</ref><ref type="bibr" target="#b26">Yan et al. 2017</ref>). An exhaustive survey of uniform blind deblurring algorithms can be found in <ref type="bibr" target="#b11">(Lai et al. 2016)</ref>. Few approaches <ref type="bibr" target="#b0">(Chakrabarti 2016;</ref><ref type="bibr" target="#b17">Schuler et al. 2016</ref>) have proposed hybrid algorithms where a Convolutional Neural Network (CNN) estimates the blur kernel, which is then used in an alternative optimization framework for recovering the latent image.</p><p>However, these methods have been developed based on a rather strong constraint that the scene is planar and that the blur is governed by only camera motion. This precludes commonly occurring blur in most practical settings. Realworld blur arises from various sources including moving objects, camera shake and depth variations, causing different pixels to acquire different motion trajectories. A class of algorithms involve segmentation methods to relax the static and fronto-parallel scene assumption by independently restoring different blurred regions in the scene (Hyun <ref type="bibr" target="#b8">Kim, Ahn, and Mu Lee 2013)</ref>. However, these methods depend heavily on an accurate segmentation-map. Few methods <ref type="bibr" target="#b19">(Sun et al. 2015;</ref><ref type="bibr" target="#b6">Gong et al. 2017)</ref> circumvent the segmentation stage by training CNNs to estimate locally linear blur kernels and feeding them to a non-uniform deblurring algorithm based on patch-level prior. However, they are limited in their capability when it comes to general dynamic scenes.</p><p>The afore-mentioned methods are not end-to-end systems and share a severe disadvantage of involving iterative, time-intensive, and cumbersome optimization schemes at the network output for getting the final deblurred result. Use of fully convolutional CNNs to directly estimate the latent sharp image was proposed in  and adopted by recent works to further advance the state-of-the-art. They offer the advantage of enabling generalized dynamic scene deblurring at low latency, by circumventing the iterative optimization stage involving fitting of hand-designed motion models. <ref type="bibr" target="#b20">Tao et al. 2018;</ref><ref type="bibr" target="#b5">Gao et al. 2019)</ref> proposed multi-scale residual networks to aggregate features in a coarse-to-fine manner, while showing benefits of selective parameters sharing and/or recurrent layers. Recently, <ref type="bibr" target="#b29">(Zhang et al. 2019</ref>) proposed a multi-patch hierarchical network and stacked its copies along depth to achieve state-of-the-art performance.</p><p>However, there are two major limitations shared by prior deblurring works. Firstly, the filters of a generic CNN are spatially invariant (with spatially-uniform receptive field), which is a suboptimal modeling of the dynamic scene deblurring process and offers limited accuracy. Secondly, existing methods attempt to increase receptive field by increasing the model's computational footprint, making them unsuitable for real-time applications. As the only other work of this kind, <ref type="bibr" target="#b28">(Zhang et al. 2018b</ref>) recently proposed a design composed of multiple CNNs and Recurrent Neural Networks (RNN) to learn spatially varying weights for deblurring. However, their performance is inferior to the state-ofthe-art <ref type="bibr" target="#b29">(Zhang et al. 2019)</ref> in several aspects. Reaching a trade-off among inference time, accuracy of restoration, and receptive field is a non-trivial task which we address in this paper (see <ref type="figure">Fig. 1</ref>). We investigate position and motion-aware CNN architecture, which can efficiently handle multiple image regions experiencing motion with different magnitude and direction.</p><p>Following recent developments, we adopt an end-toend learning based approach to directly estimate the restored sharp image. For single image deblurring, we build a fully convolutional architecture equipped with filtertransformation and feature modulation capability suited for the task of motion deblurring. Our design leverages the fact that motion blur is essentially an aggregation of various spatially varying transformations of the image, and a network that implicitly adapts to the location and direction of such motion, is a better candidate for the restoration task. Its advantages over prior art are three-fold: 1. It is a dynamic and computationally efficient as it requires only a single forward pass through each layer, obviates the need for repeated processing of the image (at different scales/patch-levels). 2. Its components can be easily introduced into other architectures to improve their performance. 3. The transformations estimated by the network are dynamic and hence can be meaningfully interpreted for any test image.</p><p>The efficacy of our architecture is demonstrated through comprehensive comparisons with the state-of-the-art on image deblurring benchmark. The major contributions of our work are: • We propose an efficient motion deblurring architecture built using dense deformable modules that facilitate position-specific dynamic filter transformation.</p><p>• Our network benefits from estimating image dependent spatial attention-maps to process local features jointly with their globally distributed interdependencies.</p><p>• We embed the above adaptive modules into an densely connected fully convolutional design which benefits from pre-trained filters of DenseNet.</p><p>• Extensive experiments are presented on dynamic scene deblurring benchmark to show state-of-the-art accuracy and near real-time deblurring achieved by our architecture and the interpretability of its dynamic modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Architecture</head><p>An existing technique for accelerating various image processing operations is to down-sample the input image, execute the operation at low resolution, and up-sample the output <ref type="bibr" target="#b1">(Chen et al. 2016</ref>). However, this approach discounts the importance of resolution, rendering it unsuitable for image restoration tasks where high-frequency content of the image is of prime importance (deblurring, super-resolution). Another efficient alternative is a CNN with a fixed but very large receptive field, e.g. Cascaded dilated network <ref type="bibr" target="#b2">(Chen, Xu, and Koltun 2017)</ref>, which was proposed to accelerate various image-to-image tasks. However, simple dilated convolutions are not appropriate for restoration tasks (as shown in <ref type="bibr" target="#b13">(Liu et al. 2018)</ref> for image super-resolution). After several layers of dilated filtering, the output only considers a fixed sparse sampling of input locations, resulting in significant loss of information.</p><p>Till date, the driving force behind performance improvement in deblurring has been use of large number of layers, larger filters, and multi-scale processing which assist in increasing the "static" receptive field of a CNN. Not only do these techniques offer a suboptimal design, they are also difficult to scale since the effective receptive field of deep CNNs is much smaller than the theoretical one (investigated in <ref type="bibr" target="#b14">(Luo et al. 2016)</ref>).</p><p>We claim that a superior alternative to such imageagnostic models is a convolutional framework wherein the filters and the receptive field dynamically adapts to input image instances. Our experiments show that the latter approach is a considerably better choice due to its task-specific efficacy and utility for computationally limited environments, and it delivers consistent performance across diverse magnitudes of blur. Now, we explain the motivation for designing a deblurring network with asymmetric filters. Given a 2D image I and a blur kernel K, the motion blur process can be formulated as:</p><formula xml:id="formula_0">B[x, y] = M/2,M/2 m,n=−M/2 K[m, n]I[x − n, y − n],<label>(1)</label></formula><p>where B is the blurred image, [x, y] represents pixel coordinates, and M ×M is the size of the blur kernel. At any given location [x, y], the sharp intensity can be represented as</p><formula xml:id="formula_1">I[x, y] = B[x, y] K[0, 0] − M/2,M/2 m,n=−M/2 K[m, n]B[x − m, y − n] K[0, 0] 2 + M/2,M/2 m,n=−M/2 M/2,M/2 i,j=−M/2 K[m, n]K[i, j]I[x − n − i, y − n − j] K[0, 0] 2<label>(2)</label></formula><p>which is a 2D infinite impulse response (IIR) model, whose recursive expansion would eventually lead to an expression which contains values from only the blurred image and the kernel.</p><p>The dependence of I[x, y] on a large number of locations in B shows that the deconvolution process requires infinite signal information. If we assume that the boundary of the image contains zeros, eq. 2 is equivalent to applying an inverse filter to B. As visualized in <ref type="bibr" target="#b28">(Zhang et al. 2018b</ref>), the non-zero region of such an inverse deblurring filter is typically much larger than the blur kernel. Thus, if we use a CNN to model the process, a large receptive field should be considered to cover all the pixel positions that are necessary for deblurring. Eq. 2 also shows that only a few coefficients (which are K[m, n] for m, n ∈ [−M/2, M/2]) need to be estimated by the deblurring model, provided we can find an appropriate model with large enough receptive field.</p><p>For this theoretical analysis, we will temporarily assume that the motion blur kernel K is linear (as assumed in prior deblurring works <ref type="bibr" target="#b19">(Sun et al. 2015;</ref><ref type="bibr" target="#b6">Gong et al. 2017)</ref>). Now, consider an image B which is affected by motion blur in the horizontal direction (without loss of generality), implying K[m, n] = 0 for m = 0 (non-zero values present only in the middle row of the kernel). For this case, eq. 2 translates to</p><formula xml:id="formula_2">I[x, y] = B[x, y] K[0, 0] − M n=1 K[0, n]B[x, y − n] K[0, 0] 2 + M n=1 M j=1 K[0, n]K[0, j]I[x, y − n − j] K[0, 0] 2 = ...<label>(3)</label></formula><p>We observe that for this case, I[x, y] can be expressed as a function of only one row of pixels in the blurred image B, which implies that for a horizontal blur kernel, the deblurring filter is also purely horizontal. We use this observation to state a hypothesis that holds for any motion blur kernel: "Deblurring filters are directional/asymmetric in shape". The reason behind this is the well known inherently directional nature of motion blur kernels. Such an operation can be efficiently learnt by a CNN with adaptive and asymmetric filters and this forms the basis for our work. Next, we describe our proposed network in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense encoder decoder backbone</head><p>Inspired by the success of fully convolutional networks that directly estimate the intensities of the deblurred image, we build an encoder-decoder architecture composed of densely connected modules. These modules immensely improve feature extraction capability by reusing features across multiple layers and their connections maximize information flow along the intermediate layers and result in better convergence. Hence, they are more efficient and learn more complex features than a network with residual connections (used extensively in recent deblurring methods).</p><p>A key component of our Region-Adaptive Dense Network is the encoder which progressively extracts a feature pyramid from the input image. The first convolution begins after a space-to-depth module that transforms the image pixels to channel-space using pixel-shuffling by a factor of 2. This allows subsequent computationally intensive operations to be performed at lower spatial resolution, hence reducing computational and memory footprint while increasing the receptive field. This layer is followed by three dense-blocks containing 12, 16, and 24 dense units, respectively, with growth rate (GR) set to 32 and each dense unit consisting of batch norm, ReLU, 1×1 conv (4×GR channels) followed by batch norm, ReLU, 3 × 3 conv (GR channels). The first two dense blocks are followed by 1 × 1 conv and pooling to downsample the features-maps. Our design allows all these filters to be initialized using initial layers of pre-trained DenseNet-121 <ref type="bibr" target="#b7">(Huang et al. 2017)</ref>, and our experiments demonstrate its advantage.</p><p>The decoder is built using our dynamically adaptive components namely, Self-Attention (SA) module and Dense Deformable Module (DDM). SA module accepts the lowresolution output of the encoder and generates a non-locally enhanced feature-map. These features are sequentially processed by 3 DDMs and deconvolution layers to reach the output image resolution. Similar to U-net, intermediate features with in the decoder are concatenated with the corresponding-sized encoder features. Further, result of the final deconvolution layer is enhanced through multi-scale context aggregation through pooling and upsampling at 4 scales, before being fed to the final reconstruction layer. The output is the residual between the ground-truth sharp image and the input blurred image. Note that unlike several prior works, our network does not use large (5 × 5) filters. A schematic of the proposed architecture is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense deformable module (DDM)</head><p>CNNs operate on fixed locations in a regular grid which limits their ability to model unknown geometric transformations. Spatial Transform Networks (STN) <ref type="bibr" target="#b9">(Jaderberg et al. 2015)</ref> introduced spatial transformation learning into CNNs, wherein an image-dependent global parametric transformation is estimated and applied on the feature map. However, such warping is computationally expensive and the transformation is considered to be global across the whole im- age, which is not the case for motion in dynamic and 3D scenes where different regions are affected by different magnitude and direction of motion. To introduce such motionawareness in our network, we adopt deformable convolutions <ref type="bibr">(Dai et al. 2017)</ref>, which enable local transformation learning in an efficient manner. Unlike regular convolutional layers, the deformable convolution also learns to estimate the shapes of convolution filters conditioned on an input feature map. While maintaining filter weights invariant to the input, a deformable convolution layer first learns a pixellevel offset map from the input, and applies it to the regular feature map for re-sampling. Our DDM (shown in <ref type="figure" target="#fig_2">Fig. 3</ref>) contains a densely connected set of deform units capable of learning positioning of filters on the feature sampling grid. Each deform unit in DDM contains a pair of layers: one to estimate the 2D filter offsets for each spatial location and another to apply the learnt filters on values sampled from these locations. The 2D offsets (shown as red-arrows) are encoded in the channel dimension of an estimated tensor (shown in green).</p><p>At each spatial coordinate l 0 , the deform unit estimates 2 offsets for P sampling locations. Our DDM is composed of 3 × 3 kernels, which correspond to P = 9 and the fixed offsets l p ∈ (1, 1), (1, 0), , (1, 1). If w p denotes the learnt filter weight for the p th location, the processed feature valuesF at coordinate l 0 are then obtained from input feature F as:</p><formula xml:id="formula_3">F (l 0 ) = P p=1 w p · F (l 0 + l p + ∆l p )<label>(4)</label></formula><p>where ∆l p are the dynamically estimated sampling offsets for all the channels in the input feature map, which determine the shifting of the P filter locations along horizontal and vertical axes. As a result, the regular convolution filter operates on an irregular grid of pixels. Since the offsets can be fractional, bilinear interpolation is used to sample from the input feature map. All the parts of our network are trainable end-to-end, since bilinear sampling and the grid generation of the warping module are both differentiable <ref type="bibr" target="#b16">(Paszke et al. 2017)</ref>. The 6 deformable layers in our DDM are followed by a 1 × 1 conv layer to reduce the number of channels. Although the focus of our work is an effective network design, our analysis also presents an effective way to boost any existing deblurring network's performance. Replacing ordinary convolution layers with deformable layers is much more efficient than going deeper or wider since the receptive field and the spatial sampling locations become dynamically adaptable according to the scale, shape, and location of blur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-attention module (SA)</head><p>Recent deblurring works have emphasized the advantages of multi-scale/patch processing. It efficiently captures different scales of motion blur, and increases the receptive field of the network. Although it facilitates local growth in receptive field, it does not leverage the relationship between two distant locations in the scene. While this coarse-to-fine approach helps to handle different magnitudes of blur, it cannot leverage the relationship among blurred regions from a global perspective, which is also beneficial for the restoration task at hand. In this work, we employ a better strategy: attention based learnable non-local connections among features at different spatial locations.</p><p>Trainable attention over features for modeling long-range dependencies has shown its benefits in several tasks spanning across language <ref type="bibr" target="#b23">Vaswani et al. 2017)</ref> and vision ), but has not been explored for deblurring. Our work is inspired by the recent work of <ref type="bibr" target="#b27">(Zhang et al. 2018a</ref>) that utilizes non-local attention to connect different scene regions and uses it to improve image generation quality.</p><p>Our SA module selectively aggregates the features at each position by a weighted sum of the features at all positions. This efficient design ensures that similar features are connected to each other regardless of their spatial distances, which helps in directly connecting regions with similar blur. It has two advantages: First, it overcomes the issue of limited receptive field, as any pixel has access to features at every other pixel in the image. Second, it implicitly acts as a gate for propagating only relevant information across the layers. These properties make it suitable for deblurring, since blur affecting various scene-edges is often correlated.</p><p>As illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>, input feature-map A ∈ R C×H×W is transformed into two new feature maps B and C ∈ RĈ ×H×W (Ĉ = C 8 ). Next, we reshape C and B to RĈ ×N (N = H × W ), perform matrix multiplication between their transposed versions, and pass the product through a softmax layer to calculate the attention map S ∈ R N ×N :</p><formula xml:id="formula_4">s ji = exp(B i · C j ) N i=1 exp(B i · C j )<label>(5)</label></formula><p>where s ji measures the i th position's impact on j th position. Note that similarity between feature representations of any two position contributes to greater correlation (higher attention) between them. Finally, A is processed to obtain D which is then reshaped to R C×N . A matrix multiplication between D and the transpose of S yields an enhanced feature-map residual, which is added to A to obtain the final output E as:</p><formula xml:id="formula_5">E j = A j + N i=1 s ji D i (6)</formula><p>The resulting feature E at each position is a weighted sum of the features at all positions and original features. Therefore, it has global context and selectively aggregated contexts according to the spatial attention map, causing similar features to reinforce gains and irrelevant features to get subdued. We place the SA module at the beginning of our region adaptive decoder to minimize memory footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>Since the prime application of our work is efficient deblurring of general 3D and dynamic scenes, we perform training and evaluation of our network on the dynamic scene deblurring benchmark , following recent learning-based works. This dataset is constructed using 240fps videos captured using GoPro camera and contains diverse 3D scenes captured in presence of significant object and camera motion. Following the same train-test split as in , we use 2103 pairs for training and 1111 pairs for evaluation. Training is done for 1 × 10 6 iterations using Adam optimizer with learning rate 0.0001 on patches of 256 × 256 and batch-size of 16. We conduct our experiments on a PC with Intel Xeon E5 CPU, 256 GB RAM and an NVIDIA Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>In this section, we carry out quantitative and qualitative comparisons of our architectures with state-of-the-art end-toend learning based methods for image deblurring, including MSCNN , DeblurGAN <ref type="bibr" target="#b10">(Kupyn et al. 2017)</ref>, SVRNN <ref type="bibr" target="#b28">(Zhang et al. 2018b</ref>), SRN <ref type="bibr" target="#b20">(Tao et al. 2018)</ref>, PSS-SRN <ref type="bibr" target="#b5">(Gao et al. 2019)</ref>, and Stack(4)-DMPHN <ref type="bibr" target="#b29">(Zhang et al. 2019</ref>). Due to the complexity of the blur present in general dynamic scenes, conventional blur model based approaches struggle to perform well. Nevertheless, we also compare with conventional non-uniform deblurring approaches of <ref type="bibr" target="#b25">(Whyte et al. 2012)</ref>, (Hyun Kim, Ahn, and Mu Lee 2013) and MBMF <ref type="bibr" target="#b6">(Gong et al. 2017)</ref>. Public implementations with default parameters were used to obtain qualitative results on selected test images. Quantitative Evaluation: Quantitative comparisons using PSNR and SSIM scores obtained on the GoPro testing set (720 × 1280 images) are presented in <ref type="table">Table 1</ref>. Since traditional methods cannot model combined effects of general camera shake and object motion <ref type="bibr" target="#b25">(Whyte et al. 2012)</ref>  Mu Lee 2013), they fail to faithfully restore most of the images in the test-set. The below par performance of MBMF can be attributed to the fact that it uses synthetic and simplistic blur kernels to train a CNN and employs a traditional deconvolution method to estimate the sharp image, which severely limits its applicability to general dynamic scenes. End-to-end residual networks such MS-CNN, SRN and PSS-SRN use multi-scale strategy or alternative losses to improve large blur handling capability, but fail in challenging situations. The proposed RADN(final) significantly outperforms these works, including the spatially varying model SVRNN. Importantly, our method fares significantly better than the nearest competitor Stack(4)-DMPHN, in terms of inference-time (15× faster, supporting real-time deblurring at 28fps), and accuracy (improvement of ∼ 0.56 dB), while requiring 40% less parameters. RADN(final) + represents our results obtained using geometric self-ensemble <ref type="bibr" target="#b12">(Lim et al. 2017)</ref>, which offers significant boost in test performance without requiring further training or additional parameters. Qualitative Evaluation: Visual comparisons on different dynamic and 3D scenes are given in <ref type="figure">Fig. 5</ref>. It shows that results of prior works suffer from incomplete deblurring or artifacts. In contrast, our network is able to restore scene details (text, edges etc.) more faithfully due to its effectiveness in handling large dynamic blur. An additional advantage over (Hyun <ref type="bibr" target="#b8">Kim, Ahn, and Mu Lee 2013;</ref><ref type="bibr" target="#b25">Whyte et al. 2012</ref>) is that our model waives-off the requirement of parameter tuning during test phase. Additional experiments and qualitative comparisons on other benchmarks are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablations of our proposed network</head><p>We analyze the effect of individual components of our network on its training and testing performance. The test scores of various ablations our network are reported in <ref type="table">Table 1</ref> and a comparison of their training performance is shown in <ref type="figure">Fig.  6(a)</ref>. Decoder is a key component of our deblurring net-  work and To evaluate its importance, we compare the proposed model RADN(Final) with 3 different versions of the decoder design while using the same dense encoder. In version RADN4, we replace the deform units within DDMs with ordinary convolutional layers, forcing the modules to apply rigid filters at all the spatial locations in the features. The drastic decrease in performance demonstrates the importance of spatially adaptive filter-offset learning capability. In version RADN3, we further remove the SA module from RADN4 and observe notable drop in performance. We chose to keep a single SA module in our network, since performance improvement beyond it was marginal and it serves as a good balance between restoration accuracy and processing-time. In RADN2, we replace DDMs with bottleneck blocks <ref type="bibr" target="#b7">(Huang et al. 2017)</ref> which are a simpler alternative to dense blocks. The significant fall in accuracy can be attributed to the efficacy of densely-connected layers and validates our design choice. Finally, to verify the effectiveness of pretraining, we train a baseline RADN1 which is identical to RADN2 except that its encoder layers are not pre-trained (using DenseNet). The PSNR and loss difference clearly shows that such pre-training leads to better initialization in parameter space and eventually, better convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis on a baseline deblurring network</head><p>Here, we evaluate the advantages of our region-adaptive modules by introducing them into a simpler residual encoder decoder baseline adopted from SRN (one of the competing methods). We differentiate this baseline's design from SRN in terms of compactness and computational footprint by employing only 3 × 3 filters for economy and removing the recurrent units. Next, we describe experiments with inclusion of our region-adaptive modules which significantly improve the representational capacity and performance of this network without sacrificing the computational efficiency (more details in supplementary document). One of the key hyper-parameters in this analysis is the number of deformable units that replace normal convolutions (in the trunk of the network). To study its effectiveness, we designed and trained 3 versions of the network wherein the number of such replacements are 0, 3 and 6, respectively. <ref type="figure">Fig. 6(b)</ref> shows comparisons of the convergence plots of these models. It can be observed that the training performance as well as the quantitative results get better with increase in the number of deformable blocks, as it introduces additional filter adaptability into the network. Also note that the performance of the model which contains SA but no deformable layers, is expectedly lower than other models but better than the plain CNN. Finally, the best training performance is delivered by our final model which contains 6 deformable replacements and SAs, which shows that the advantages of the two modules are complementary and their union leads to a superior model. These improvements are also reflected in the quantitative values reported in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>The table also shows that our modules are lightweight since their inclusion has only a marginal effect on the model size. Although the proposed network is already quite efficient, replacing the standard convolutions in our network with grouped convolution and/or separable convolutions can lead to further improvements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Visualization</head><p>This section provides further insights into the effectiveness of our approach by visualizing the filter offsets and featureattention maps dynamically estimated by our network. Dense deformable module: To establish our DDM's interpret-ability and their sensitivity to local motion in the scene, we investigate visible association between the estimated filter transformation-maps and the dominant motion blur in the input image. In first and second row of <ref type="figure">Fig. 7</ref>, we show various test images and a representation of the corresponding offset-maps, respectively. The offset value at each spatial location is calculated using the difference between the horizontal (x) offsets of the 1 st and 3 rd columns of the 3×3 filters from the 2 nd DDM of our network. It can be seen that the offset values are higher for the foreground regions and regions undergoing large motion. This is important for the deblurring process since the pixels corresponding to the foreground objects (e.g., people, vehicles) experience different motion compared to other parts of the scene, due to independent motion or depth differences. This demonstrates that the proposed network can distinguish between differently blurred regions and that the estimated offsets are correlated with the direction and magnitude of the motion blur. Next, we measure the correlation between the amount of scene motion and the average magnitude of the filter offsets estimated by our network. In <ref type="figure">Fig. 8</ref>, we show various test images and the corresponding average value of offsets calculated by all the DDMs. It can be seen that the offset val-ues are small for the first scene ( <ref type="figure">Fig. 8(a)</ref>) where only few regions are blurred. As the blur increases and affects larger number of pixels in the scene <ref type="figure">(Fig. 8(b)</ref> and (c)), the average offset magnitude rises to accommodate the large blur. This shows that our network features a dynamic receptive field and adapts to scenes with varying degrees of blur without altering the filter weights, yielding efficiency to the process. Self-attention module: Here, we visualize the inputs and outputs of the SA module of our network for additional insights. In third row of <ref type="figure">Fig. 7</ref>, we show the residuals estimated within our SA module ( N i=1 s ji D i in Eq.6) for corresponding images in the first row. It can be observed that the SA module learns to amplify the correlation among featuremap magnitudes for all pixels undergoing similar (large) blur, and the spatial distributions agree with corresponding dynamic regions and depth variations. Such enhancement of the intermediate feature-maps is the key reason behind the observed improvement in training and test performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We proposed an efficient motion deblurring architecture composed of convolutional modules that enable spatially adaptive feature learning through filter transformations and feature attention, namely dense deformable module (DDM) and self-attention (SA) module. The DDMs implicitly address the shifts responsible for the local blur in the input image, while the SA module meaningfully connects nonlocally distributed blurred regions. Introducing these modules awards higher capacity to any deblurring network without any notable increase in computational footprint. This effectiveness is shown by incorporating them into a new densely connected encoder decoder backbone, wherein we also show the benefits of pre-training of encoder filters. Compared against existing deep deblurring frameworks, our model achieves the state-of-the-art performance and is able to run at 28fps for 720p images. We believe our spatiallyaware design can be utilized for other image processing and vision tasks as well, and we shall explore them in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Proposed deblurring network and its components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Schematic of our dense deformable module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>A schematic of our self-attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Visual comparisons of deblurring results on images from the GoPro test set. Key blurred patches are shown in (b), while zoomed-in patches from the deblurred results are shown in (c)-(h). Network analysis through comparison of training performance of various ablations of our models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Visualizations of the spatial variations in the estimated filter offsets and attention-based feature modulations on blurred images from the GoPro test set. The second row shows the spatial distribution of the horizontal-offset values for the filter in a DDM of our network. Third row shows the corresponding enhanced feature-map residuals estimated by our SA module. Offset magnitudes: Values mentioned under each image (from GoPro test set) depict the average magnitude of the offsets estimated by all the DDMs in our network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison of different ablations of our baseline residual deblurring network on GoPro testset.</figDesc><table><row><cell>Conv layers</cell><cell>6</cell><cell>6</cell><cell>3</cell><cell>0</cell><cell>0</cell></row><row><cell>Deformable layers</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell>6</cell><cell>6</cell></row><row><cell>SA module</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSNR (dB)</cell><cell cols="5">29.8 30.64 30.69 31.05 31.13</cell></row><row><cell>Size (MB)</cell><cell cols="2">10.6 10.7</cell><cell>10.9</cell><cell>11.2</cell><cell>11.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural approach to blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="221" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bilateral guided upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">203</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast image processing with fully-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2497" to="2506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring with parameter selective sharing and nested skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3848" to="3856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From motion blur to motion flow: a deep learning solution for removing heterogeneous motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3160" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07064</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comparative study for single image blind deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1701" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilevel wavelet-cnn for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="773" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to deblur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1439" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="769" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scalerecurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8174" to="8182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vasiljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05760</idno>
		<title level="m">Examining the impact of blur on recognition by convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From local to global: Edge profiles to camera motion in blurred images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4447" to="4456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonuniform deblurring for shaken images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="168" to="186" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image deblurring via extreme channels prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4003" to="4011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring using spatially variant recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2521" to="2529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5978" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
