<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do Response Selection Models Really Know What&apos;s Next? Utterance Manipulation Strategies For Multi-turn Response Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesun</forename><surname>Whang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Wisenut Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyub</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Kakao Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsuk</forename><surname>Oh</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanhee</forename><surname>Lee</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kijong</forename><surname>Han</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Kakao Enterprise Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hun</forename><surname>Lee</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Kakao Enterprise Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saebyeok</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Wisenut Inc</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Do Response Selection Models Really Know What&apos;s Next? Utterance Manipulation Strategies For Multi-turn Response Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study the task of selecting the optimal response given a user and system utterance history in retrievalbased multi-turn dialog systems. Recently, pre-trained language models (e.g., BERT, RoBERTa, and ELECTRA) showed significant improvements in various natural language processing tasks. This and similar response selection tasks can also be solved using such language models by formulating the tasks as dialog-response binary classification tasks. Although existing works using this approach successfully obtained stateof-the-art results, we observe that language models trained in this manner tend to make predictions based on the relatedness of history and candidates, ignoring the sequential nature of multi-turn dialog systems. This suggests that the response selection task alone is insufficient for learning temporal dependencies between utterances. To this end, we propose utterance manipulation strategies (UMS) to address this problem. Specifically, UMS consist of several strategies (i.e., insertion, deletion, and search), which aid the response selection model towards maintaining dialog coherence. Further, UMS are selfsupervised methods that do not require additional annotation and thus can be easily incorporated into existing approaches. Extensive evaluation across multiple languages and models shows that UMS are highly effective in teaching dialog consistency, which leads to models pushing the state-of-the-art with significant margins on multiple public benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In recent years, building intelligent conversational agents has gained considerable attention in the field of natural language processing (NLP). Among widely used dialog systems, retrieval-based dialog systems <ref type="bibr" target="#b14">(Lowe et al. 2015;</ref><ref type="bibr" target="#b33">Wu et al. 2017;</ref><ref type="bibr" target="#b38">Zhang et al. 2018</ref>) are implemented in a variety of industries because they provide accurate, informative, and promising responses. In this study, we focus on multi-turn response selection in retrieval-based dialog systems. This is a task of predicting the most likely response under a given dialog history from a set of candidates. That works. Are there any suggestions of advanced classes using Python? <ref type="figure">Figure 1</ref>: Example of multi-turn response selection. BERTbased model tends to calculate the matching score of a dialogresponse pair depending on the semantic relatedness of the dialog and the response ((a) &lt; (b)). More details are in Discussion section.</p><p>Existing works <ref type="bibr" target="#b33">(Wu et al. 2017;</ref><ref type="bibr" target="#b41">Zhou et al. 2018;</ref><ref type="bibr" target="#b18">Tao et al. 2019a;</ref><ref type="bibr" target="#b37">Yuan et al. 2019</ref>) have studied utterance-response matching based on attention mechanisms including selfattention <ref type="bibr" target="#b20">(Vaswani et al. 2017</ref>). Most recently, as pre-trained language models (e.g., BERT <ref type="bibr" target="#b2">(Devlin et al. 2019)</ref>, RoBERTa , and ELECTRA <ref type="bibr" target="#b0">(Clark et al. 2020)</ref>) have achieved substantial performance improvements in diverse NLP tasks, multi-turn response selection also has been resolved using such language models <ref type="bibr" target="#b10">(Whang et al. 2020;</ref><ref type="bibr" target="#b15">Lu et al. 2020;</ref><ref type="bibr" target="#b3">Gu et al. 2020;</ref><ref type="bibr" target="#b5">Humeau et al. 2020)</ref>.</p><p>However, we tackle three crucial problems in applying language models to response selection. 1) Domain adaptation based on an additional training on a target corpus is extremely time-consuming and computationally costly. 2) Formulating response selection as a dialog-response binary classification task is insufficient to represent intra-and inter-utterance interactions as the dialog context is formed by concatenating all utterances.</p><p>3) The models tend to select the optimal response depending on how semantically similar it is to a given dialog. As shown in <ref type="figure">Figure 1</ref>, we experiment to verify whether the BERT-based response selection model is trained properly to select the next utterance rather than dialog-related response. The results show that the model tends to give a higher probability score to a response that is more semantically related to the dialog context rather than consistent response. Although it is obvious that the ground truth is suitable for being the next utterance, the model highly depends on its semantic meaning.</p><p>To address these issues, this paper proposes Utterance Manipulation Strategies (UMS) for multi-turn response selection. Specifically, UMS consist of three powerful strategies (i.e., insertion, deletion, and search), which effectively help the response selection model to learn temporal dependencies between utterances as well as semantic matching and maintain dialog coherence. In addition, these strategies are fully selfsupervised methods that do not require additional annotation and can be easily adapted to existing studies. We briefly summarize the main contributions of this paper as follows: 1) We show that existing response selection models are more likely to predict a semantically relevant response with its dialog rather than the next utterance. 2) We propose simple but novel utterance manipulation strategies, which are highly effective in predicting the next utterance. Our model has strengths in effectively performing in-domain classification. 3) Experimental results on three benchmarks (i.e., Ubuntu, Douban, and E-commerce) show that our proposed model outperforms state-of-the-art methods. We also obtain significant improvements in performance compared to the baselines on a new Korean open-domain corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>Language Models for Response Selection Pre-trained Language Models Recently, pre-trained language models, such as BERT <ref type="bibr" target="#b2">(Devlin et al. 2019</ref>) and ELEC-TRA <ref type="bibr" target="#b0">(Clark et al. 2020)</ref>, were successfully adapted to a wide range of NLP tasks, including multi-turn response selection, achieving state-of-the-art results. In this work, we build upon this success and evaluate our method by incorporating it into BERT and ELECTRA. Domain-specific Post-training As contextual language models are pre-trained on general corpora, such as the Toronto Books Corpus and Wikipedia, it is less effective to directly fine-tune these models on downstream tasks if there is a domain shift. Hence, it is a common practice to further train such models with the language modeling objective using texts from the target domain to reduce the negative impact. This has shown to be effective in various tasks including review reading comprehension <ref type="bibr" target="#b34">(Xu et al. 2019)</ref> and SuperGLUE <ref type="bibr" target="#b22">(Wang et al. 2019a)</ref>. Existing works on multiturn response selection <ref type="bibr" target="#b10">(Whang et al. 2020;</ref><ref type="bibr" target="#b3">Gu et al. 2020;</ref><ref type="bibr" target="#b5">Humeau et al. 2020)</ref> also adapted this post-training approach and obtained state-of-the-art results. We also employ this post-training method in this work and show its effectiveness in improving performance. Training Response Selection Models Following several re-searches based on contextual language models for multi-turn response selection <ref type="bibr" target="#b10">(Whang et al. 2020;</ref><ref type="bibr" target="#b15">Lu et al. 2020;</ref><ref type="bibr" target="#b3">Gu et al. 2020)</ref>, a pointwise approach is used to learn a cross-encoder that receives both dialog context and response simultaneously. Suppose that a dialog agent is given a dialog dataset</p><formula xml:id="formula_0">D = {(U i , r i , y i )} N i=1</formula><p>. Each triplet consists of 1) a sequence of utterances U i = [u i 1 , u i 2 , · · · , u i |U | ] representing the historical context, where u i t is a single utterance, 2) a response r i , and 3) a label y i ∈ {0, 1}. Each utterance u i t and response r i are composed of multiple tokens including a special <ref type="bibr">[EOT]</ref> token at the end of each utterance, following the work of <ref type="bibr" target="#b10">Whang et al. (2020)</ref>. In general, the input sequence,</p><formula xml:id="formula_1">X = [[CLS] u 1 u 2 ... u nu [SEP] r [SEP]],</formula><p>is fed into pre-trained language models (i.e., BERT, ELEC-TRA), then output representation of [CLS] token, x [CLS] ∈ R d×1 , is used to classify whether the dialog-response pair is consistent. The relevance score of the dialog utterances and response is formulated as,</p><formula xml:id="formula_2">g(U, r) = σ(w x [CLS] + b),<label>(1)</label></formula><p>where w ∈ R d×1 and b are the trainable parameters. We use binary cross-entropy loss to optimize the models. <ref type="figure">Figure 2</ref> describes the overview of our proposed method, utterance manipulation strategies. We propose a multi-task learning framework, which consists of three highly effective auxiliary tasks for multi-turn response selection, utterance 1) insertion, 2) deletion, and 3) search. These tasks are jointly trained with the response selection model during the finetuning period. To train the auxiliary tasks, we add new special tokens, <ref type="bibr">[INS]</ref>, <ref type="bibr">[DEL]</ref>, and [SRCH] for the utterance insertion, deletion, and search tasks, respectively. We cover how we train the model with these special tokens in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Utterance Manipulation Strategies</head><p>Utterance Insertion Despite the huge success of BERT, it has limitations in understanding discourse-level semantic structure since NSP, one of BERT's objectives, mainly learns semantic topic shift rather than sentence order <ref type="bibr" target="#b9">(Lan et al. 2020)</ref>. In multi-turn response selection, the model needs the ability not only to distinguish the utterances with different semantic meanings but also to discriminate whether the utterances are consecutive even if they are semantically related. We propose utterance insertion to resolve the aforementioned issues.</p><p>We first extract k consecutive utterances from the original dialog context, then randomly select one of the utterances to be inserted. To train the model to find where the selected utterance should be inserted, <ref type="bibr">[INS]</ref> tokens are positioned before each utterance and after the last utterance.</p><p>[INS] tokens are represented as possible position of the target utterance. Input sequence for utterance insertion is denoted as</p><formula xml:id="formula_3">X INS = [[CLS] [INS] 1 u 1 [INS] 2 u 2 ... u t−1 [INS] t u t+1 ... u k [INS] k [SEP] u t [SEP]],</formula><p>where u t is the target utterance and [INS] t is the target insertion token.</p><p>Good morning! What can I do for you?</p><p>How much does a seven-day tour by bus cost?</p><p>Two thousand dollars.</p><p>Does that include hotels and meals?</p><p>Oh, yes, and admission tickets for places of interest as well.</p><p>That sounds reasonable.</p><p>With pleasure. We arrange two kinds of tourist programs for California, a seven-day tour by bus and a five-day flying journey.</p><p>(a) With pleasure. We arrange two kinds of tourist programs for California, a seven-day tour by bus and a five-day flying journey.</p><formula xml:id="formula_4">(b) (d) (e) (c) (a) (b) (c)<label>(d)</label></formula><p>With pleasure. We arrange two kinds of tourist programs for California, a seven-day tour by bus and a five-day flying journey.</p><p>How much does a seven-day tour by bus cost?</p><p>Two thousand dollars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does that include hotels and meals?</head><p>Two thousand dollars.</p><p>With pleasure. We arrange two kinds of tourist programs for California, a seven-day tour by bus and a five-day flying journey.</p><p>I'm thinking of traveling to California in May. Could you recommend some tourist programs for that?</p><p>How much does a seven-day tour by bus cost?</p><p>Does that include hotels and meals?</p><p>Two thousand dollars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speaker 1</head><p>Speaker 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Utterance</head><p>Previous Utterance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Dialog</head><p>Oh, yes, and admission tickets for places of interest as well.</p><p>How much does a seven-day tour by bus cost? <ref type="figure">Figure 2</ref>: An overview of Utterance Manipulation Strategies. Input sequence for each manipulation strategy is dynamically constructed by extracting k consecutive utterances from the original dialog context during the training period. Also, target utterance is randomly chosen from either the dialog context (Insertion, Search) or the random dialog (Deletion).</p><p>Utterance Deletion Recent BERT-based models for multiturn response selection regard the task as a dialog-response binary classification. Even though they are extended in a multi-turn manner using separating tokens (e.g., [SEP], [EOT] ), these models lack utterance-level interaction between dialog context and response. To alleviate this, we propose a highly effective auxiliary task, utterance deletion, to enrich utterance-level interaction in multi-turn conversation.</p><p>As with utterance insertion, k consecutive utterances are extracted from the original dialog context, and then an utterance from a random dialog is inserted among the k extracted utterances. In other words, k + 1 utterances are composed of k utterances from the original conversation and one from different dialogs. To train the model to find an unrelated utterance, [DEL] tokens are positioned before each utterance. The objective of the utterance deletion task is to predict which utterance causes inconsistency. We denote the input sequence for utterance deletion as</p><formula xml:id="formula_5">X DEL = [[CLS] [DEL] 1 u 1 [DEL] 2 u 2 ...[DEL] t u rand [DEL] t+1 u t ... [DEL] k+1 u k [SEP]],</formula><p>where u rand is the utterance from the random dialog and [DEL] t is the target deletion token. Utterance Search Whereas two previous auxiliary tasks are performed in a properly ordered dialog, we design a novel task, utterance search, which aims to find an appropriate utterance from randomly shuffled utterances. The objective of this task is to learn temporal dependencies between semantically similar utterances.</p><p>Given k consecutive utterances same as the previous tasks, we shuffle utterances except for the last utterance and insert [SRCH] tokens before each shuffled utterance. The utterance search aims to find the previous utterance of the last utterance from the jumbled utterances. Input sequence for utterance search is denoted as</p><formula xml:id="formula_6">X SRCH = [[CLS] [SRCH] 1 u 1 [SRCH] 2 u 2 ... [SRCH] t u t ... u k−1 [SEP] u k [SEP]],</formula><p>where {u t } k−1 t=1 is a set of utterances which are randomly shuffled except for the last utterance u k . The previous utterance of u k is denoted as u t (i.e., u k−1 ) and [SRCH] t is the target search token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Learning Setup</head><p>The input sequence of each task is fed into the language models. The output representations of special tokens (i.e., <ref type="bibr">[INS]</ref>, <ref type="bibr">[DEL]</ref>, and [SRCH] ) are used to classify whether each token is in a correct position to be inserted, deleted, and searched. Target tokens for each task (i.e., [INS] t , [DEL] t , and [SRCH] t ) are labeled as 1, otherwise 0. We calculate the probability of the token being a target as follows:</p><formula xml:id="formula_7">p(y TASK = 1|X TASK ) = σ(w x TASK + b),<label>(2)</label></formula><p>where TASK∈{INS, DEL, SRCH} and x TASK is the output representations of each special token. We use binary cross-entropy loss for all auxiliary tasks to optimize each model. The final loss is determined by summing the response selection loss and UMS losses with the same ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup Datasets</head><p>We evaluate our model on three widely used response selection benchmarks: Ubuntu Corpus V1 <ref type="bibr" target="#b14">(Lowe et al. 2015)</ref>,  In a similar manner to the Ubuntu dataset, we take the last utterance of the dialog as a positive response and the rest as a dialog context. Negative responses are randomly sampled from the other conversations. We split the test set into two sets: 1) web is the same as the training set, and 2) clean consists of grammatically correct conversations that are constructed by human annotators and inspected by NLP experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>We evaluated our model using several retrieval metrics, following previous research <ref type="bibr" target="#b14">(Lowe et al. 2015;</ref><ref type="bibr" target="#b33">Wu et al. 2017;</ref><ref type="bibr" target="#b41">Zhou et al. 2018;</ref><ref type="bibr" target="#b37">Yuan et al. 2019</ref>). First, we employ 1 in n recall at k, denoted as R n @k (k = {1, 2, 5}), which gets 1 when a ground truth is positioned in the k selected list and 0 otherwise. In addition, three other metrics [mean average precision (MAP), mean reciprocal rank (MRR), and precision at one (P@1)] are used especially for Douban and Kakao, as these two datasets may contain more than one positive response among candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>We implemented our model 4 by using the PyTorch deep learning framework <ref type="bibr" target="#b17">(Paszke et al. 2019)</ref> based on the opensource code 5 <ref type="bibr" target="#b31">(Wolf et al. 2019</ref>). As we experimented on three different languages (i.e., English, Chinese, and Korean), initial checkpoints for BERT and ELECTRA are adapted from several works <ref type="bibr" target="#b2">(Devlin et al. 2019;</ref><ref type="bibr" target="#b0">Clark et al. 2020;</ref><ref type="bibr" target="#b1">Cui et al. 2020;</ref><ref type="bibr" target="#b10">Lee et al. 2020)</ref>. Specifically, we employ base pre-trained models for all languages except for Chinese (the whole-word masking (WWM) strategy is used for Chinese BERT 6 ). As ELECTRA for Korean is not available, we do not conduct ELECTRA-based experiments on the Kakao Corpus. All experiments, both post-training and fine-tuning, are run on 4 Tesla V100 GPUs. For fine-tuning, we trained the models with a batch size of 32 using the Adam optimizer with an initial learning rate of 3e-5. The maximum sequence length is set to 512 and k for UMS is set to 5.  <ref type="bibr" target="#b3">(Gu et al. 2020)</ref>. In these models, the dialog context is represented as a long document, as in single-turn matching models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Ubuntu Douban E-commerce R 10 @1 R 10 @2 R 10 @5 MAP MRR P@1 R 10 @1 R 10 @2 R 10 @5 R 10 @1 R 10 @2 R 10 @5 CNN <ref type="bibr" target="#b8">(Kadlec, Schmid, and Kleindienst 2015)</ref> 0   <ref type="bibr" target="#b19">(Tao et al. 2019b;</ref><ref type="bibr" target="#b37">Yuan et al. 2019;</ref><ref type="bibr" target="#b3">Gu et al. 2020)</ref>. The underlined numbers mean the best performance for each block and the bold numbers mean state-of-the-art performance for each metric. † denotes statistical significance (p-value &lt; 0.05).</p><p>They mainly utilize speaker information of each utterance in the dialog context to extend BERT into a multi-turn fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>Quantitative Results <ref type="table" target="#tab_4">Table 2</ref> lists the quantitative results on Ubuntu, Douban, and E-Commerce datasets. In our experiments, we set two conditions for pre-trained language models. 1) Two different pre-trained language models (i.e., BERT and ELECTRA) are utilized for fine-tuning. 2) We adapt domain-specific posttraining approach (each post-trained model is denoted as BERT+ and ELECTRA+). Based on these initial settings, we explore how effective UMS are for multi-turn response selection. For all datasets, models with UMS significantly outperform the previous state-of-the-art methods. Specifically, UMS BERT+ achieves absolute improvements of 2.0% and 5.8% in R 10 @1 on Ubuntu and E-Commerce datasets, respectively. For Douban datset, MAP and MRR are considered to be main metrics rather than R 10 @1 because the test set contains more than one ground truth in the candidates. UMS BERT+ achieves absolute improvements of 0.5% in these metrics.</p><p>To evaluate the effectiveness of UMS, we compare the models with UMS and those without them. Since existing BERT-based approaches <ref type="bibr" target="#b15">(Lu et al. 2020;</ref><ref type="bibr" target="#b3">Gu et al. 2020)</ref> reported different performances of BERT, we reimplemented it for a fair comparison with our proposed UMS BERT . The models with UMS consistently show performance improvement regardless of whether language models are post-trained on each corpus or not. For the models without post-training, different results are obtained depending on the dataset. ELECTRA mainly shows better results for the Ubuntu and  Douban datasets, while BERT shows better results for the E-Commerce dataset. By contrast, BERT+ achieves the best performance for all corpora in comparison among the models with post-training. We believe that post-training on domainspecific corpus provides the model with more opportunities to learn whether given two dialogs are relevant through NSP; this has the effect of data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Kakao Corpus</head><p>We report the evaluation results on the Kakao Corpus in <ref type="table" target="#tab_6">Table 3</ref>. As ELECTRA for Korean is unavailable, we only compare BERT and UMS BERT for two test splits. Clean shows better results than Web with respect to all metrics regardless of using UMS. This might be because Clean contains fewer grammatical errors and typos that interfere with an accurate understanding of the context. Also, UMS BERT significantly improves performance compared to the baseline for both split; specifically, it achieves absolute improvements of 5.1% and 6.8% in P@1 on Web and Clean, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial Experiment</head><p>Even though BERT-based models have shown state-of-the-art performance for response selection task, we experiment to know if these models are trained to predict the next utterance properly. Inspired by <ref type="bibr" target="#b6">Jia and Liang (2017)</ref> and <ref type="bibr" target="#b37">Yuan et al. (2019)</ref>, we design an adversarial experiment to investigate whether language models for response selection are trained   <ref type="figure">Figure 3</ref>: R 10 @1 comparison of adversarial example for each model. Lower R 10 @1 means that it is good at predicting the next utterance (ground truth).</p><p>properly. First, we train the models using the original training set, then evaluate them on either original or adversarial test set. To construct the adversarial test set, we randomly extract an utterance from the dialog context and replace it with one of negative responses among candidates (See <ref type="figure">Figure 1)</ref>. In adversarial test set, assuming there are n candidates per conversation, a set of candidates consists of a ground truth, an extracted utterance from the dialog context, and n−2 negative responses. The extracted utterance is not deleted from the original dialog because it can be crucial for selecting the optimal response. <ref type="table" target="#tab_8">Table 4</ref> lists the experimental results of BERT(+) and ELECTRA(+) models. We compare the models without UMS and those with, denoted as baselines and UMS, respectively. Even though the performances drop significantly in the adversarial set regardless of whether UMS are used or not, we observe that the UMS decline less than baselines. Specifically, R 10 @1 score decreases by 58% and 48% on average for baselines and UMS, respectively. It is also encouraging that UMS show an absolute improvement of 12% with respect to R 10 @1 on the adversarial set compared to the 2% improvement on the original set (See <ref type="table" target="#tab_8">Table 4</ref>). In addition, while baselines tend to drop in performance on the adversarial set as training progresses, the performance of UMS shows a tendency to increase significantly. Hence, it is reasonable to assume that our UMS are robust to adversarial examples and are good at in-domain classification.   <ref type="figure">Figure 3</ref> describes the performance of each model, ranking adversarial example (i.e., randomly sampled utterance from the conversation) as the most likely response. While BERTand ELECTRA-based models show similar performance on the original set, ELECTRA-based models outperform BERTbased models with significant margins (a gap of 10%) on the adversarial set regardless of whether they are trained from post-trained checkpoints. For example, different patterns of the evaluation results between BERT+ and ELECTRA are observed according to the test sets (original : BERT+ &gt; ELECTRA, adversarial : BERT+ &lt; ELECTRA). We have two perspectives on these results: 1) Next sentence prediction in BERT overfits the model to predict semantically relevant sentence rather than the next sentence. 2) As ELECTRA is trained through replaced token detection in which the model learns to discriminate between real input tokens and replacements generated from small masked language model, it is more effective in representing contextual information from the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We performed ablation studies on the Ubuntu Corpus to investigate which auxiliary tasks are more crucial for response selection. As shown in <ref type="table" target="#tab_10">Table 5</ref>, we explored the impact of each auxiliary task by constructing all combinations of possible subsets. Based on the observations of using only one auxiliary task (i.e., 3 &gt; 2 ≈ 4) and two tasks (i.e., 5 ≈ 7 &gt; 6), we obtained the results, DEL &gt; INS ≈ SRCH, with respect to the importance of manipulation strategy. As DEL consists of an input sequence that contains an irrelevant utterance to the original dialog context, it may be more advantageous for learning to distinguish dialog consistency and coherence than INS and SRCH. We obtain the best results when all the auxiliary tasks are trained simultaneously with the response selection criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization</head><p>As shown in <ref type="figure">Figure 4</ref>, we visualize the output representations of special tokens learned by our proposed UMS through t-SNE embeddings. Scatter plots colored in orange represent target tokens (i.e., <ref type="bibr">[INS]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Multi-turn Response Selection Early approaches to response selection focused on single-turn response selection <ref type="bibr" target="#b23">(Wang et al. 2013;</ref><ref type="bibr" target="#b4">Hu et al. 2014;</ref><ref type="bibr" target="#b25">Wang et al. 2015)</ref>.</p><p>Recently, multi-turn response selection has received more attention by researchers. <ref type="bibr" target="#b14">Lowe et al. (2015)</ref> proposed dual encoder architecture which uses an RNN-based models to match the dialog and response. <ref type="bibr" target="#b40">Zhou et al. (2016)</ref> proposed the multi-view model that encodes dialog context and response both on the word-level and utterance-level. However, these models have limitations in fully reflecting the relationship between dialog and response. To alleviate this, <ref type="bibr" target="#b33">Wu et al. (2017)</ref> proposed the sequential matching network that utilizes matching matrices to match each utterance with a response. As self-attention <ref type="bibr" target="#b20">(Vaswani et al. 2017</ref>) mechanism has been proved its effectiveness, it is applied in subsequent works <ref type="bibr" target="#b41">(Zhou et al. 2018;</ref><ref type="bibr">Tao et al. 2019a,b)</ref>. <ref type="bibr" target="#b37">Yuan et al. (2019)</ref> recently pointed out that previous approaches construct dialog representation with abundant information but noisy, which deteriorates the performance. They proposed an effective history filtering technique to avoid using excessive history information.</p><p>Most recently, many researches based on pre-trained language models including BERT <ref type="bibr" target="#b2">(Devlin et al. 2019)</ref> and RoBERTa ) are proposed. Generally, most models formulate the response selection task as a dialogresponse binary classification task. <ref type="bibr" target="#b10">Whang et al. (2020)</ref> first applied BERT for multi-turn response selection and obtained state-of-the-art results through further training BERT on domain-specific corpus. Subsequent researches <ref type="bibr" target="#b15">(Lu et al. 2020;</ref><ref type="bibr" target="#b3">Gu et al. 2020)</ref> focused on modeling speaker information and showed its effectiveness in response retrieval. <ref type="bibr" target="#b5">Humeau et al. (2020)</ref> investigated the trade-off relationship between model complexity and computation efficiency in the language models. They proposed poly-encoders that ensure fast inference speed, even though the performance is slightly lower than that of the cross-encoder. Self-supervised Learning Self-supervised learning has been explored in various pre-trained language models <ref type="bibr" target="#b2">(Devlin et al. 2019;</ref><ref type="bibr" target="#b0">Clark et al. 2020;</ref><ref type="bibr" target="#b11">Lewis et al. 2020;</ref><ref type="bibr" target="#b7">Joshi et al. 2020)</ref> and is also applied in several NLP downstream tasks, such as summarization <ref type="bibr" target="#b24">(Wang et al. 2019b</ref>), disfluency detection <ref type="bibr" target="#b1">(Wang et al. 2020)</ref>, and response generation <ref type="bibr" target="#b39">(Zhao, Xu, and Wu 2020)</ref>. Existing works in dialog modeling <ref type="bibr" target="#b32">(Wu, Wang, and Wang 2019;</ref><ref type="bibr" target="#b16">Mehri et al. 2019;</ref><ref type="bibr" target="#b12">Liang, Zou, and Yu 2020)</ref> mainly focused on building enhanced dialog representations through self-supervised learning. Although the methods proposed in <ref type="bibr" target="#b32">Wu, Wang, and Wang (2019)</ref> and <ref type="bibr" target="#b12">Liang, Zou, and Yu (2020)</ref> effectively learn to rank coherent dialog higher than corrupted ones, but they have limitations in identifying the utterance that actually caused the inconsistency. Our strategy is different in that it learns to find which utterance is replaced from the full dialog. By doing so, our model can learn which utterance does not suit the conversation, which makes the model learn not only to discriminate semantic differences but also to build coherent dialog. The method proposed in <ref type="bibr" target="#b16">Mehri et al. (2019)</ref> is somewhat similar to our deletion task, but they directly use the utterance representation to build the loss. We hypothesize that this is the reason behind the inconsistent improvements in <ref type="bibr" target="#b16">Mehri et al. (2019)</ref>, where in some downstream tasks the auxiliary task was actually harmful. On the other hand, our approach introduces special tokens that is different from the [CLS] token used in downstream tasks. Our results show that this approach consistently leads to improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we pointed out the limitations of existing works based on pre-trained language models such as BERT in retrieval-based multi-turn dialog systems. To address these, we proposed highly effective utterance manipulation strategies (UMS) for multi-turn response selection. The UMS are fully applied in self-supervised manner and can be easily incorporated into existing models. We obtained new stateof-the-art results on multiple public benchmark datasets (i.e., Ubuntu, Douban, and E-Commerce) and significantly improved results on Korean open-domain dialog corpus. For the future work, we plan to develop a response selection model which is more robust to adversarial examples by designing various adversarial objectives.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>I'm interested in computer engineering. What level of programming are you capable of? I have some programming experience in C++ and Matlab after taking … Nice try of it. I'd recommend that your take EECS280 and EECS203 as soon as you can. They are important for your computer science major. Hello, is there anything I can help you with? Hi, I want to get some suggestions about next semester's course selections. … That works. Are there any suggestions of advanced classes using Python? (a) Ground Truth (BERT score : 0.813) (b) Adversarial Example (BERT score : 0.993)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Matching Models These baselines, including CNN, LSTM, BiLSTM<ref type="bibr" target="#b8">(Kadlec, Schmid, and Kleindienst 2015)</ref>, MV-LSTM<ref type="bibr" target="#b21">(Wan et al. 2016)</ref>, and Match-LSTM<ref type="bibr" target="#b28">(Wang and Jiang 2016)</ref>, are based on matching between a dialog context and a response. They construct the dialog context by concatenating utterances and regard it as a long document. Multi-turn Matching Models Multi-View<ref type="bibr" target="#b40">(Zhou et al. 2016</ref>) utilize both word-and utterance-level representations. DL2R<ref type="bibr" target="#b36">(Yan, Song, and Wu 2016)</ref> reformulates the last utterance with previous utterances in the dialog context. SMN<ref type="bibr" target="#b33">(Wu et al. 2017)</ref> first constructs attention matrices based on word and sequential representations of each utterance and response, and then obtains matching vectors by using CNN. DUA<ref type="bibr" target="#b38">(Zhang et al. 2018</ref>) utilizes deep utterance aggregation to form a fine-grained context representation. DAM<ref type="bibr" target="#b41">(Zhou et al. 2018</ref>) obtains matching representations of the utterances and response using self-and cross-attention based on Transformer architecture<ref type="bibr" target="#b20">(Vaswani et al. 2017)</ref>. IoI<ref type="bibr" target="#b19">(Tao et al. 2019b</ref>) lets utterance-response interaction go deep in a matching model. MSN<ref type="bibr" target="#b37">(Yuan et al. 2019</ref>) filters only relevant utterances using a multi-hop selector network. BERT-based Models Recently, BERT<ref type="bibr" target="#b2">(Devlin et al. 2019</ref>) is also applied to response selection, such as vanilla BERT<ref type="bibr" target="#b3">(Gu et al. 2020)</ref>, BERT-SS-DA<ref type="bibr" target="#b15">(Lu et al. 2020)</ref>, and SA-BERT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Corpus statistics of multi-turn response selection datasets.Douban Corpus<ref type="bibr" target="#b33">(Wu et al. 2017)</ref>, and E-Commerce Corpus<ref type="bibr" target="#b38">(Zhang et al. 2018</ref>). In addition, a new open-domain dialog corpus, Kakao Corpus, is utilized to evaluate our model. All datasets consist of dyadic multi-turn conversations, and their statistics are summarized inTable 1. Ubuntu Corpus V1 Ubuntu dataset is a large multi-turn conversation corpus that is constructed from Ubuntu internet relay chats. It mainly consists of conversations between two participants who discuss how to troubleshoot the Ubuntu operating system. We utilize the data released byXu et al.   </figDesc><table /><note>(2017), where numbers, URLs, paths are replaced with spe- cial placeholders following previous works (Wu et al. 2017; Zhou et al. 2018). Douban Corpus Douban dataset is a Chinese open-domain dialog corpus, whereas the Ubuntu Corpus is a domain- specific dataset. It is constructed by web-crawling from the Douban group 1 , which is a popular social networking service (SNS) in China. E-commerce Corpus E-Commerce dataset is another Chi- nese multi-turn conversation corpus. It is collected from real- world customer consultation dialogs from Taobao 2 , which is the largest Chinese e-commerce platform. It consists of several types of conversations (e.g., commodity consultation, recommendation, and negotiation) based on various com- modities. Kakao Corpus Kakao dataset is a large Korean open-domain dialog corpus that is constructed by Kakao Corporation 3 . It is mainly web-crawled from Korean SNSs such as Korean Twitter and Reddit.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on Ubuntu, Douban, and E-Commerce datasets. All the evaluation results except ours are cited from published literature</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Evaluation Results on Kakao Corpus.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Adversarial experimental results on Ubuntu Corpus. All models are evaluated using R 10 @1 and MRR metrics.</figDesc><table><row><cell></cell><cell>80 85</cell><cell></cell><cell cols="4">R@1 Comparison of Adversarial Example</cell><cell>Baselines UMS</cell></row><row><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R@1 (%)</cell><cell>60 65 70</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>55</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>45</cell><cell>BERT</cell><cell>BERT+</cell><cell>Model</cell><cell>ELECTRA</cell><cell>ELECTRA+</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Ablation Study on Ubuntu Corpus. We choose ELEC-TRA as the baseline in this analysis. INS, DEL, and SRCH denote that the model trained with utterance insertion, deletion, and search, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>t , [DEL] t , and [SRCH] t ) and those in blue represent the rest of tokens. All representations are extracted from test sets of three datasets (Ubuntu, Douban, and E-Commerce) in this analysis. Overall, the results show UMS BERT+ effectively learns dialog coherence for all datasets. In the case of Ubuntu dataset, insertion and search tasks tend to be less clustered than that of the other two datsets. As many utterances in the Ubuntu dataset mainly consist of many technical terminologies that may cause structural ambiguity, tasks constructed within the same dialog are difficult to be performed. By contrast, the model can easily learn discourse structures on open-domain datasets such as Douban and E-Commerce.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Ubuntu others target</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Douban others target</cell></row><row><cell>(a) Insertion</cell><cell>(b) Deletion</cell><cell>(c) Search</cell><cell>E-Commerce others target</cell></row><row><cell cols="4">Figure 4: t-SNE embeddings of UMS BERT+ output representa-</cell></row><row><cell cols="4">tions for each special token in UMS (i.e., [INS] , [DEL] , and</cell></row><row><cell cols="4">[SRCH] ). All embeddings are sampled from test sets of each</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell></row></table><note>that</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.douban.com 2 https://www.taobao.com 3 https://www.kakaocorp.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/taesunwhang/UMS-ResSel 5 https://github.com/huggingface/transformers 6 https://github.com/ymcui/Chinese-BERT-wwm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13922</idno>
		<title level="m">Revisiting Pre-Trained Models for Chinese Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial Examples for Evaluating Reading Comprehension Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleindienst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03753</idno>
		<title level="m">Improved deep learning baselines for ubuntu corpus dialogs</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reference and Document Aware Semantic Evaluation Methods for Korean Language Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5604" to="5616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1363" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving Contextual Language Models for Response Retrieval in Multi-Turn Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1805" to="1808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pretraining Methods for Dialog Context Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Razumovskaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3836" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-representation fusion network for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="267" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">One Time of Interaction May Not Be Enough: Go Deep with an Interaction-over-Interaction Network for Response Selection in Dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2922" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3266" to="3280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A dataset for research on short-text conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning for Contextualized Extractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2221" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Syntax-Based Deep Matching of Short Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1354" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised learning for disfluency detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="page" from="9193" to="9200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning Natural Language Inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An Effective Domain Adaptive Post-Training Method for BERT in Response Selection</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2020</title>
		<meeting>Interspeech 2020</meeting>
		<imprint>
			<biblScope unit="page" from="1585" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<title level="m">HuggingFace&apos;s Transformers: Stateof-the-art Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-Supervised Dialogue Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3857" to="3867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BERT Post-Training for Review Reading Comprehension and Aspectbased Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2324" to="2335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Incorporating loose-structured knowledge into conversation modeling via recall-gate LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3506" to="3513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to Respond with Deep Neural Networks for Retrieval-Based Human-Computer Conversation System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling Multi-turn Conversation with Deep Utterance Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3740" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning a Simple and Effective Model for Multi-turn Response Generation with Auxiliary Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3472" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-view Response Selection for Human-Computer Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
