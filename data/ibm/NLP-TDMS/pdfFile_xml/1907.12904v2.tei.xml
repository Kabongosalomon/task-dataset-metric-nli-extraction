<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learned Image Downscaling for Upscaling using Content Adaptive Resampler</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjie</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learned Image Downscaling for Upscaling using Content Adaptive Resampler</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional neural network based image super-resolution (SR) models have shown superior performance in recovering the underlying high resolution (HR) images from low resolution (LR) images obtained from the predefined downscaling methods. In this paper we propose a learned image downscaling method based on content adaptive resampler (CAR) with consideration on the upscaling process. The proposed resampler network generates content adaptive image resampling kernels that are applied to the original HR input to generate pixels on the downscaled image. Moreover, a differentiable upscaling (SR) module is employed to upscale the LR result into its underlying HR counterpart. By back-propagating the reconstruction error down to the original HR input across the entire framework to adjust model parameters, the proposed framework achieves a new state-of-the-art SR performance through upscaling guided image resamplers which adaptively preserve detailed information that is essential to the upscaling. Experimental results indicate that the quality of the generated LR image is comparable to that of the traditional interpolation based method and the significant SR performance gain is achieved by deep SR models trained jointly with the CAR model. The code is publicly available on: https://github.com/sunwj/CAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1907.12904v2 [cs.CV] 5 Nov 2019</head><p>3 Model Architecture This section introduces the architecture and formulation of the content adaptive resampler (CAR) model for image downscaling. As shown in <ref type="figure">Fig. 1</ref>, the framework is composed of two major components, i.e., the resampler generation network (Resampler-Net) and the SR network (SRNet). The ResamplerNet is responsible for estimating the content adaptive resampling kernels 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As the smartphone cameras are starting to rival or beat DSLR cameras, a large number of ultra high resolution images are produced everyday. However, it is always reduced from its original resolution to smaller sizes that are fit to the screen of different mobile devices and web applications. Thus, it is desirable to develop efficient image downscaling and upscaling method to make such application more practical and resources saving by only generating, storing and transmitting a single downscaled version for preview and upscaling it to high resolution when details are going to be viewed. Besides, the pre-downscaling and post-upscaling operation also helps to save storage and bandwidth for image or video compression and communication <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Image downscaling is one of the most common image processing operations, aiming to reduce the resolution of the high-resolution (HR) image while keeping its visual appearance. According to the Nyquist-Shannon sampling theorem <ref type="bibr" target="#b4">[5]</ref>, it is inevitable that high-frequency content will get lost during the samplerate conversion. Contrary to the image downscaling task is the image upscaling, also known as resolution enhancement or super-resolution (SR), trying to recover the underlying HR image of the LR input. Image SR is essentially an ill-posed problem because an undersampled image could refer to numerous HR images. The quality of the SR result is very limited due to the ill-posed nature of the problem and the lost frequency components cannot be well-recovered <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Previous work regards image downscaling and SR as independent tasks. Image downscaling techniques pay much attention to enhance details, such as edges, which helps to improve human visual perception <ref type="bibr" target="#b2">[3]</ref>. On the other hand, recent state-of-the-art deep SR models have witnessed their capability to restore HR image from the This work was supported in part by National Natural Science Foundation of China under contract No. 61771348. (Corresponding author: Zhenzhong Chen, E-mail: zzchen@ieee.org) LR version downscaled using traditional filtering-decimation based methods with great performance gain <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. However, the predetermined downscaling operations may be sub-optimal to the SR task and state-of-the-art deep SR models still cannot well recover fine details from distorted textures caused by the fixed downscaling operations.</p><p>In this paper, we proposed a learned content adaptive image downscaling model in which a SR model tries to best recover HR images while adaptively adjusting the downscaling model to produce LR images with potential detailed information that are key to the optimal SR performance. The downscaling model is trained without any LR image supervision. To make sure that the LR image produced by our downscaling model is a valid image, we propose to employ the resampling method where content adaptive non-uniform resampling kernels predicted by a convolutional neural network (CNN) are applied to the original HR image to generate pixels of the LR output. Quantitative and qualitative experimental results illustrate that LR images produced by the proposed model can maintain comparable visual quality as the widely used bicubic interpolation based image downscaling method while advanced SR image quality is obtained using state-of-the-art deep SR models trained with LR images generated from the proposed CAR model.</p><p>Our contributions are concluded as follows:</p><p>• A learned image downscaling model is proposed which is trained under the guidance of the SR model. The proposed image downscaling model produces images that can be well super-resolved while comparable visual quality can be maintained. Experimental results indicate a new state-of-the-art SR performance with the proposed end-to-end image downscaling and upscaling framework. • The resampling method is employed to downscale image by applying content adaptive non-uniform resampling kernels on the original HR input, which can ef-fectively maintain the structure of the HR input in an unsupervised manner. Because directly predicting the LR image by combining low and high-level abstract image features can not guarantee that the generated result is a genuine image without any LR image supervision.</p><p>• The learned content adaptive non-uniform resampling kernels perform non-uniform sampling and also make the size of resampling kernels to be more effective. The generated kernels produced by the proposed CAR model are composed of weights and sampling position offsets in both horizontal and vertical directions, making the learned resampling kernels adaptively change their weights and shape according to its corresponding resampling contents.</p><p>The rest of the paper is organized as follows. Section 2 reviews task independent and task driven image downscaling algorithms. Section 3 introduces the proposed SR guided content adaptive image downscaling framework, and computing process of each component in the framework is explained. Section 4 evaluates and analyzes experimental results for the SR images and downscaled images quantitatively and qualitatively. Finally, Section 5 summarizes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This section presents a review about image downscaling techniques aiming to maintain the visual quality of the LR image. Image downscaling algorithms can be categorized into two groups as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task independent image downscaling</head><p>Earlier work of image downscaling primarily tends to prevent aliasing <ref type="bibr" target="#b4">[5]</ref> which arises during sampling rate reduction. Those methods are based on linear filters <ref type="bibr" target="#b9">[10]</ref>, where the HR image is firstly convolved with a low-pass kernel to push frequency componenets of the image below the Nyquist frequency <ref type="bibr" target="#b10">[11]</ref>, then being sub-sampled into target size. Many frequency-based filters are developed, e.g., the box, bilinear and bicubic filter <ref type="bibr" target="#b11">[12]</ref>. However, the downscaled images tend to be blurred because the high-frequency details are suppressed. Filters that are designed to model the ideal sinc filter, e.g., the Lanczos filter <ref type="bibr" target="#b12">[13]</ref>, tend to produce ringing artifacts near strong image edges. All of these filters are predetermined with some of them having tuning parameters. The same filter is applied globally to the input HR image, ignoring characteristics of image content with varying details.</p><p>Recently, many researchers begin to focus on the aspects of detail preserving and human perception when developing image downscaling algorithm. Kopf et al.firstly proposed a novel content adaptive image downscaling method based on a joint bilateral filter <ref type="bibr" target="#b13">[14]</ref>. The key idea is to optimize the shape and locations of the downsampling kernels to better align with local image features by considering both spatial and color variances of the local region. Öztireli and Gross <ref type="bibr" target="#b14">[15]</ref> proposed a method to downscale HR images without filtering. They consider image downscaling as an optimization problem and use the structural similarity index (SSIM) <ref type="bibr" target="#b15">[16]</ref> as objective to directly optimize the downscaled image against its original image. This approach helps to capture most of the perceptually important details. Weber et al. <ref type="bibr" target="#b16">[17]</ref> proposed an image downscaling algorithm aiming to preserve small details of the input image, which are often crucial for a faithful visual impression. The intuition is that small details transport more information than bigger areas with similar colors. To that end, an inverse bilateral filter is used to emphasize differences rather than punishing them. Gastal and Oliveira <ref type="bibr" target="#b17">[18]</ref> introduced the spectral remapping algorithm to control aliasing during image downscaling. Instead of discarding high-frequency information, it remaps such information into the representable range of the downsampled spectrum. Recently, Liu et al. <ref type="bibr" target="#b18">[19]</ref> proposed a L0-regularized optimization framework for image downscaling, which is composed of a gradient-ratio prior and reconstruction prior. The downscaling problem is solved by iteratively optimize the two priors in an alternative way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task specific image downscaling</head><p>Most image downscaling algorithms only care about the visual quality of the downscaled image, so that the downscaled image may not be optimal to other computer vision tasks. To tackle this problem, task guided image downscaling has emerged. Zhang et al. <ref type="bibr" target="#b2">[3]</ref> took the quality of the interpolated image from the downscaled counterpart into consideration. They proposed an interpolation-dependent image downscaling algorithm by modeling the downscaling operation as the inverse operator of upsampling. Benefiting from the well established deep learning frameworks, Hou et al. <ref type="bibr" target="#b19">[20]</ref> proposed a deep feature consistency network that is applicable to image mapping problems. One of the applications illustrated in the paper is image downscaling. The image downscaling network is trained by keeping the deep features of the input HR image and resulting LR image consistent through another pre-trained deep CNN. Kim et al. <ref type="bibr" target="#b20">[21]</ref> presented a task aware image downscaling model based on the auto-encoder and the bottleneck layer outputs the downscaled image. In their framework, the encoder acts as the image downscaling network and the decoder is the SR network. The task aware downscaled image is obtained by jointly training the encoder and decoder to maximize the SR performance. Similar to the framework presented by <ref type="bibr" target="#b20">[21]</ref>, Li et al. <ref type="bibr" target="#b3">[4]</ref> proposed a convolutional neural network for image compact resolution named CNN-CR, which is composed of a CNN to estimate the LR image and a learned or specified upscaling model to reconstruct the HR image. The generative nature of the encoder like networks implicitly require additional information to constrain the output to be a valid image whose content resembles the HR image. In <ref type="bibr" target="#b19">[20]</ref>, in order to compute feature consistency loss against the HR image, they upsample the downscaled image back to the same size as the HR input using nearest neighbor interpolation. In <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4]</ref>, guidance images, obtained using bicubic downsampling, are employed to constrain the output space of the LR image generating networks. according to its input HR image, later the resampling kernels are applied to the input HR image to produce the downscaled image. The SRNet takes the resulting downscaled image as input and tries to restore the underlying HR image. The entire framework is trained end-to-end in an unsupervised manner where the primary objective we need to optimize is the SR reconstruction error with respect to the input HR image. By back-propagating the gradient of the reconstruction error through the SRNet and ResamplerNet, the gradient descent algorithm adjusts the parameters of the resampler generation network to produce better resampling kernels which make the downscaled image can be super-resolved more easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Content adaptive image downscaling resampler</head><p>We design the proposed content adaptive image downscaling model that is trained using the unsupervised strategy. Methods presented in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4]</ref> synthesize the downscaled image by combining latent representations of the HR image extracted by the CNN and proper constraints are required to make sure that the result is a meaningful image. In this paper, we propose to obtain the downscaled image using the idea of resampling the HR image, which effectively makes the downscaled result look like the original HR image without any constraints.</p><p>The filters for traditional bilinear or bicubic downscaling are basically fixed, with the only variation being the shift of the kernel according to the location of the newly created pixel in the downscaled image. Contrary to this, we propose to use dynamic downscaling filters inspired by the dynamic filter networks <ref type="bibr" target="#b21">[22]</ref>. The downscaling kernels are generated for each pixel in the downscaled image depending on the effective resampling region on the HR image. It can be considered as one type of metalearning <ref type="bibr" target="#b22">[23]</ref> that learns how to resample. However, filter-based image resampling methods generally require a certain minimum kernel size to be effective <ref type="bibr" target="#b18">[19]</ref>. We alleviate this issue by taking the idea from the deformable convolutional networks <ref type="bibr" target="#b23">[24]</ref>. In addition to estimating the content adaptive resampling kernel weights, we also associate spatial offset with each element in the resampling kernel. The content adaptive resampling kernels with position offsets can be considered as learnable dilated (atrous) convolutions <ref type="bibr" target="#b24">[25]</ref> with the learned dilation rate. Besides, the offset for each kernel element can be different in both magnitude and direction, it can perform non-uniform sampling according to the content structure of the input HR image.</p><p>We use a convolutional neural network with residual connections <ref type="bibr" target="#b25">[26]</ref> to estimate the weights and offsets for each resampling kernel. The ResamplerNet consists of downscaling blocks, residual blocks and upscaling blocks. The downscaling and residual blocks are trained to model the context of the input HR image as a set of feature maps. Then, two upscaling blocks are used to learn the content adaptive resampling kernel weights K ∈ R (h/s)×(w/s)×m×n , offsets in the horizontal direction ∆Y ∈ R (h/s)×(w/s)×m×n and offsets in the vertical direction ∆X ∈ R (h/s)×(w/s)×m×n , respectively. h and w are the height and width of the input HR image, s is the downscaling factor, and m, n represent the size of the content adaptive resampling kernel. Each kernel is normalized so that elements of a kernel are summed up to be 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image downscaling</head><p>The estimated content adaptive resampling kernels are applied to the corresponding positions of the input HR image to construct the pixel in the downscaled image. For each output pixel, the same resampling kernel is simultaneously applied to three channels of the RGB image. As illustrated by <ref type="figure" target="#fig_1">Fig. 2</ref>, pixels covered by the resampling kernel are weights summed to obtain the pixel value in the downscaled image.</p><p>Forward pass. Firstly, we need to position each resampling kernel, associated with the pixel of the downscaled image, on the HR image. It can be achieved using the projection operation defined as:</p><formula xml:id="formula_0">(u, v) = (x + 0.5, y + 0.5) × scale − 0.5<label>(1)</label></formula><p>where (x, y) is the indices of a pixel at the x-th row and y-th column in the downscaled image, scale represents the downscaling factor, and the resulting (u, v) is the center of downscaled pixel p LR x,y projected on the input HR image. Equation 1 assumes that pixels have a nonzero size, and the distance between two neighboring samples is one pixel.</p><p>Then, each pixel in the downscaled image is created by local filtering on pixels in the input HR image with the corresponding content adaptive resampling kernel as follows:</p><formula xml:id="formula_1">p LR x,y = m−1 i=0 n−1 j=0 K x,y (i, j) · s HR u + i − m 2 + ∆X x,y (i, j), v + j − n 2 + ∆Y x,y (i, j)<label>(2)</label></formula><p>where K x,y ∈ R m×n is the resampling kernel associated with the downscaled pixel at location (x, y), ∆X x,y ∈ R m×n and ∆Y x,y ∈ R m×n are the spatial offset for each element in the K x,y . The s HR is the sample point value of the input HR image. Due to the location projection and fractional offsets, s HR can refer to nonlattice point on the HR image, therefore, s HR is computed by bilinear interpolating the nearby four pixels around the fractional position:</p><formula xml:id="formula_2">s HR u ,v = (1 − α)(1 − β) · p HR u , v + α(1 − β) · p HR u , v +1 + (1 − α)β · p HR u +1, v + αβ · p HR u +1, v +1<label>(3)</label></formula><p>where u and v are fractional position on the HR image, α = u − u and β = v − v are the bilinear interpolation weights.</p><p>Backward pass. The ResamplerNet is trained using the gradient descent technique and we need to back-propagate gradients from the SRNet through the resampling operation. The partial derivative of the downscaled pixel with respect to the resampling kernel weight can be formulated as:</p><formula xml:id="formula_3">∂ p LR x,y ∂K x,y (i, j) = s HR u + i − m 2 + ∆X x,y (i, j), v + j − n 2 + ∆Y x,y (i, j)<label>(4)</label></formula><p>the partial derivative of downscaled pixel with respect to the element in the resampling kernel is simply the interpolated pixel value. Equation 4 is derived with a single channel image and can be generalized to the color image by summing up values calculated separately on the R, G and B channels.</p><p>The partial derivative of downscaled pixel with respect to the kernel element offset is computed as:</p><formula xml:id="formula_4">∂p LR x,y ∂∆X x,y (i, j) = ∂p LR x,y ∂s HR u ,v · ∂s HR u ,v ∂∆X x,y (i, j) ∂p LR x,y ∂∆Y x,y (i, j) = ∂p LR x,y ∂s HR u ,v · ∂s HR u ,v ∂∆Y x,y (i, j)<label>(5)</label></formula><p>because we employ bilinear interpolation to compute s HR u ,v , therefore, the partial derivative of downscaled pixel with respect to the kernel offset is defined as:</p><formula xml:id="formula_5">∂p LR x,y ∂∆X x,y (i, j) = K x,y (i, j) · (1 − β) · p HR u , v +1 − p HR u , v + β · p HR u +1, v +1 − p HR u +1, v ∂p LR x,y ∂∆Y x,y (i, j) = K x,y (i, j) · (1 − α) · p HR u +1, v − p HR u , v + α · p HR u +1, v +1 − p HR u , v +1<label>(6)</label></formula><p>also because Equation 9 is defined with a single color image, we need to sum up partial derivative of each color component with respect to the offset to obtain the final partial derivative of downscaled pixel with respect to the kernel element offset.</p><p>The pixel value of the downscaled image created using Equation 2 is inherently continuous floating point number. However, common image representation describes the color using integer number ranging from 0 to 255, thus, a quantization step is required. Since simply rounding the floating point number to its nearest integer number is not differentiable, we utilize the soft round method proposed by Nakanishi et al. <ref type="bibr" target="#b26">[27]</ref> to derive the gradient in the back-propagation during the training phase. The soft round function is defined as:</p><formula xml:id="formula_6">round soft (x) = x − α sin 2πx 2π<label>(7)</label></formula><p>where α is a tuning parameter used to adjust the gradient around the integer position. Note that in the forward propagation, the non-differentiable round function is used to get the nearest integer value. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image upscaling</head><p>The proposed CAR model is trained using back-propagation to maximize the SR performance. The image upscaling module can be of any form of SR networks, even the differentiable bilinear or bicubic upscaling operations. After the seminal super-resolution model using deep learning proposed by Dong et al. <ref type="bibr" target="#b27">[28]</ref>, i.e., the SRCNN, many state-of-the-art neural SR models have been proposed <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr">40]</ref>. More reviews and discussion about single image SR using deep learning can be referred to <ref type="bibr" target="#b7">[8]</ref>. This paper employs state-of-the-art SR model EDSR <ref type="bibr" target="#b32">[33]</ref> as the image upscaling module to guide the training of the proposed CAR model.</p><p>The EDSR is one of the state-of-the-art deep SR networks and its superior performance is benefited from the powerful residual learning techniques <ref type="bibr" target="#b25">[26]</ref>. The EDSR is built on the success of the SRResNet <ref type="bibr" target="#b31">[32]</ref> which achieved good performance by simply employing the ResNet [26] architecture on the SR task. The EDSR enhanced SR performance by removing unnecessary parts (Batch Normalization layer <ref type="bibr" target="#b40">[41]</ref>) from the SRResNet and also applying a number of tweaks, such as adding residual scaling operation to stable the training process <ref type="bibr" target="#b32">[33]</ref>. The SRNet part of <ref type="figure" target="#fig_0">Fig. 1</ref> shows the architecture of the EDSR. It is composed of convolution layers converting the RGB image into feature spaces and a group of residual blocks refining feature maps. A global residual connection is employed to improve the efficiency of the gradient back-propagation. Finally, sub-pixel layers are utilized to upsample and transform features into the target SR image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training objectives</head><p>One of the main contributions of our work is that we propose a model to learn image downscaling without any supervision signifing that no constraint is applied to the downscaled image. The only objective guiding the generation of the downscaled image is the SR restoration error. The most common loss function generally defaults to the SR network training is the mean squared error (MSE) or the L2 norm loss, but it tends to lead to poor image quality as perceived by human observers <ref type="bibr" target="#b41">[42]</ref>. Lim et al. <ref type="bibr" target="#b32">[33]</ref> found that using another local metric, e.g., L1 norm, can speed up the training process and produce better visual results. In order to improve the visual fidelity of the super-resolved image, perceptually-motivated metrics, such as SSIM <ref type="bibr" target="#b15">[16]</ref>, MS-SSIM <ref type="bibr" target="#b42">[43]</ref> and perceptual loss <ref type="bibr" target="#b43">[44]</ref> are usually incorporated in the SR network training. To do fair comparisons with the EDSR, we only adopt the L1 norm loss as the restoration metric as suggested by <ref type="bibr" target="#b32">[33]</ref>. The L1 norm loss defined for SR is:</p><formula xml:id="formula_7">L L 1 (Î) = 1 N p∈I | p −p|<label>(8)</label></formula><p>whereÎ is the SR result, p andp represent the ground-truth and reconstructed pixel value, N indicates the number of pixels times the number of color channels.</p><p>We associate spatial offset for each element in the resampling kernel, and the offset is estimated without taking the neighborhoods of the kernel element into account. Independent kernel element offset may break the topology of the resampling kernel.</p><p>To alleviate this problem, we suggest using the total offset distance of all kernel elements as a regularization which encourages kernel elements to stay in their rest position (avoid unnecessary movements). Additionally, since the pixels indexed by the kernel elements that are far from the central position may have less correlation to the resampling result, we assign different weights to their corresponding offset distance in terms of their position relative to the central position. The offset distance regularization term for a single resampling kernel is thus formulated as:</p><formula xml:id="formula_8">L offset x,y = m−1 i=0 n−1 j=0 η + ∆X x,y (i, j) 2 + ∆Y x,y (i, j) 2 · w(i, j) (9) where w(i, j) = (i − m 2 ) 2 + ( j − n 2 ) 2 / m 2 2 + n 2 2</formula><p>is the normalized distance weight, the η is introduced to act as the offset distance weight regulator.</p><p>The inconsistent resampling kernel offset of spatially neighboring resampling kernels may cause pixel phase shift on the resulting LR images, which is manifested as jaggies, especially on the vertical and horizontal sharp edges (e.g., the LR image in <ref type="figure" target="#fig_4">Fig. 5 (b)</ref>). To alleviate this phenomenon, we introduce the total variation (TV) loss <ref type="bibr" target="#b44">[45]</ref> to constrain the movement of spatially neighboring resampling kernels. Instead of constraining the offsets on both vertical and horizontal directions, we only regularize vertical offsets on the horizontal direction and horizontal offsets on the vertical direction, which we name it the partial TV loss. Besides, variations of each resampling kernel offsets are weighted by its corresponding resampling kernel weights, leading to the following formula:</p><formula xml:id="formula_9">L TV = x,y         i, j |∆X ·,y+1 (i, j) − ∆X ·,y (i, j)| · K(i, j) + i, j |∆Y x+1,· (i, j) − ∆Y x,· (i, j)| · K(i, j)        <label>(10)</label></formula><p>Finally, the optimization objective of the entire framework is defined as:</p><formula xml:id="formula_10">L = L L 1 + λL offset + γL TV<label>(11)</label></formula><p>where the L offset is the mean offset distance regularization term of all the resampling kernels, and λ is a scalar introduced to control the strength of offset distance regularization. γ is also a scalar used to tune the contribution of the partial TV loss to the final optimization objective.  <ref type="bibr" target="#b47">[48]</ref>, BSD100 <ref type="bibr" target="#b48">[49]</ref> and Urban100 <ref type="bibr" target="#b49">[50]</ref> were used as suggested by the EDSR paper <ref type="bibr" target="#b32">[33]</ref>. Since we focus on how to downscale images without any supervision, only HR images of the mentioned datasets were utilized. Following the 5 setting in <ref type="bibr" target="#b32">[33]</ref>, we evaluated the peak noise-signal ratio (PSNR) and SSIM <ref type="bibr" target="#b15">[16]</ref> on the Y channel of images represented in the YCbCr (Y, Cb, Cr) color space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Implementation details</head><p>Regarding the implementation of the ResamplerNet, we first subtracted the mean RGB value of the DIV2K training set. During the downsampling process, we gradually increased the channels of the output feature map from 3 to 128 using 3 × 3 convolution operation followed by the LeakyReLU activation. 5 residual blocks with each having features of 128 channels are used to model the context. For the two branches estimating the resampling kernels and offsets, we used the same architecture which is composed of 'Conv-LeakyReLU' pairs with 256 feature channels. A sub-pixel convolution was applied to upscale and transform the input feature maps into resampling kernels and offsets. For the configuration of the EDSR, we adopted the one with 32 residual blocks and 256 features for each convolution in the residual block.</p><p>One of the important hyper-parameters must be determined is the resampling kernel size and the unit offset length. We defined a 3 × 3 kernel size on the downscaled image space. Its actual size on the HR image space is (3 × s) × (3 × s), where s is the downscaling factor. The unit offset length was defined as one pixel on the downscaled image space whose corresponding unit length on the HR image space is s. For the offset distance weight regulator in <ref type="bibr">Equation 9</ref>, we empirically set it to be 1.</p><p>The entire framework was trained on the DIV2K training set using the Adam optimizer <ref type="bibr" target="#b50">[51]</ref> with β 1 = 0.9, β 2 = 0.999 and = 10 −6 . We set the mini-batch size as 16, and randomly crop the input HR image into 192 × 192 (for 4× downscale and SR) and 96 × 96 (for 2× downscale and SR) patches. Training samples were augmented by applying random horizontal and vertical flip. During training, we conducted validation using 10 images from the DIV2K validation set to select the trained model parameters, and the PSNR on validation was performed on full RGB channels <ref type="bibr" target="#b32">[33]</ref>. The initial learning rate was 10 −4 and decreased when the validation performance does not increase within 100 epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of downscaling methods for SR</head><p>This section reports the quantitative and qualitative performance of different image downscaling methods for SR. Then results of ablation studies of the proposed CAR model is presented. We compared the CAR model with four image downscaling methods, i.e., the bicubic downscaling (Bicubic), and other three stateof-the-art image downscaling methods: perceptually optimized image downscaling (Perceptually) <ref type="bibr" target="#b14">[15]</ref>, detail-preserving image downscaling (DPID) <ref type="bibr" target="#b16">[17]</ref>, and L0-regularized image downscaling (L0-regularized) <ref type="bibr" target="#b18">[19]</ref>. We trained SR models using LR images downscaled by those four downscaling algorithms and LR images downscaled by the proposed CAR model. The DPID requires to manually tune a hyper-parameter, which is content variant, to produce better perceptually favorable results. However, it is unpractical for us to generate large amount LR images by manually tuning, also different people may have different perceptual preference. Thus, default value provided by the source code was adopted. <ref type="table" target="#tab_1">Table 1</ref> summarizes the quantitative comparison results of different image downscaling methods for SR. It consists of two parts, one for bicubic upscaling and one for upscaling using the EDSR. We first analyze the SR performance using the EDSR. As shown in <ref type="table" target="#tab_1">Table 1</ref> (Upscaling→EDSR), the proposed CAR model trained under the guidance of EDSR considerably boosts the PSNR metric over all the testing cases, and a noticeable gain on the SSIM metric is also obtained. The significant performance gain is benefited from the joint training of the CAR image downscaling model and the EDSR in the end-to-end manner, where the goal of maximizing SR performance encourages the CAR to estimate better resamplers that produce the most suitable downscaled image for SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Quantitative and qualitative analysis</head><p>When compared to the SR performance of the LR images downscaled by the bicubic interpolation, the three state-of-the-art image downscaling algorithms can hardly achieve satisfying results, although the visual quality superiority of the downscaled images is reported by those original work. This is because those image downscaling methods are designed for better human perception thus the original information is changed considerably, which makes the downscaled image not well adapted to the SR defined by distortion metrics. Additionally, compared to the SR baseline of the bicubic image downscaling, we note a significant performance degradation on the perceptually based image downscaling method. This indicates that downscaled image produced by the SSIM optimization cannot be well super-resolved by the state-of-the-art EDSR. The key reason can be illustrated as the SSIM optimization depends on patch selection which may lead to sub-pixel offset in the downscaled image. Other artifacts may underperform SR includes color splitting and noise exaggeration incurred during SSIM optimization <ref type="bibr" target="#b18">[19]</ref>.</p><p>In addition, we also evaluated SR performance of the CAR model trained under the guidance of the bicubic interpolation based upscaling where the bicubic downscaling was used as the baseline. As reported in <ref type="table" target="#tab_1">Table 1</ref> (Upscaling→Bicubic), the CAR model outperforms the fixed bicubic downscaling methods in terms of upscaling using the fixed bicubic interpolation. The comparison results demonstrate the effectiveness of the proposed CAR model that it is flexible and can be trained under the guidance of differentiable upscaling operations, even if the upscaling operator is not learnable. With this discovery, the proposed CAR model can potentially replace the traditional and commonly used bicubic image downscaling operation under the hood, and end users can obtain extra image zoom in quality gain freely when using the bicubic interpolation for upscaling.</p><p>To further validate the effectiveness of the CAR image downscaling model, we evaluated the CAR model trained with another four state-of-the-art deep SR models, i.e., the SRDenseNet <ref type="bibr" target="#b33">[34]</ref>, D-DBPN <ref type="bibr" target="#b51">[52]</ref>, RDN <ref type="bibr" target="#b34">[35]</ref> and RCAN <ref type="bibr" target="#b36">[37]</ref>, using 4× downscaling and upscaling factor on five testing datasets. We trained all models using the DIV2K training dataset and all other training setup is set to be the same as described in Section 4.1.2. <ref type="table" target="#tab_2">Table 2</ref> presents the PSNR and SSIM of 4× upscaled images corresponding to LR images generated using the bicubic interpolation (MATLAB's imresize function with default settings) and the CAR model. Experimental results (Bicubic and CAR †) demonstrate a consistent performance gain of the SR task on images downscaled using the CAR model against that of the 6  In order to illustrate that the CAR image downscaling model can effectively preserve essential information which can help SR models learn to better recover the original image content, we conducted another experiment. The four state-of-the-art SR models are trained using LR images generated by the proposed CAR model trained jointly with the EDSR. Experimental results shown by the 'CAR ‡' in <ref type="table" target="#tab_2">Table 2</ref> indicate that the performance of SR models trained using LR images generated by the CAR model trained jointly with the EDSR significantly surpasses that trained using images downscaled by the bicubic interpolation. We also observed that the performance gain of the CAR ‡ against the Bicubic is larger than the performance degradation against the CAR †. The two findings lead to the conclusion that the CAR image downscaling model does preserve content adaptive information that are essential to superior SR using deep SR models.</p><p>A qualitative comparison of 4× downscaled images for SR is presented in <ref type="figure" target="#fig_2">Fig. 3</ref>. As can be seen, the CAR model produces downscaled images that are super-resolved with the best visual quality when compared with that of the other four models trained using the EDSR. As shown by the 'Barbara' example, due to obvious aliasing occurred during downscaling by other four methods, the EDSR cannot recover the correct direction of the parallel edge pattern formed by a stack of books. The downscaled image generated by the CAR incurs less aliasing and the EDSR well recovered the direction of the parallel edge pattern. For the 'Comic' example, we can observe that the SR result of the CAR downscaled image preserves more details. Visual results of the 'PPT3' example demonstrates that the SR of downscaled images produced by the CAR better restore continuous edges and produce sharper HR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation studies</head><p>We conducted ablation experiments on the proposed CAR model to verify the effectiveness of our design. We mainly concern 7</p><p>This figure is best viewed in color. Zoom in to see details of the downscaled image.  about the contribution of kernel element offset and the constraint on offset distance to the performance of the SR. <ref type="table" target="#tab_3">Table 3</ref> shows the quantitative ablation results, from which we can observe that the SR performance on all testing cases constantly increases with the addition of kernel element offset and the constraint on offset distance. The baseline model is the CAR without kernel element offset (w/o offset), meaning that the CAR only needs to estimate the resampling kernel weights which will be applied to the position defined by Equation 1 on the HR image (also illustrated as the pixel center in <ref type="figure" target="#fig_1">Fig. 2)</ref>. Then, kernel element offset was incorporated, which brings a noticeable performance improvement. Introducing kernel element offset makes the resampling kernel to be non-uniform and each element in the resampling kernel can seek to proper sampling position to better preserve useful information for the end SR task. Further SR performance is gained by adding kernel element offset distance regularization. The kernel element offset distance regularization encourages the preservation of the resampling kernel topology and avoids unnecessary kernel element movement on the plain region with less structured texture, which potentially makes the training more stable and easier.</p><p>In order to better illustrate how the kernel offset distance regularization works, we visualized an example of resampling kernel elements and its corresponding offsets <ref type="figure">(Fig. 4)</ref> in the configuration of with (w/) and without (w/o) the offset distance regularization. We only visualized the central 9 of many kernel elements and offsets for a better demonstration. <ref type="figure">Fig. 4 (c)</ref> and (d) present kernel elements and offsets by the CAR model trained without offset distance regularization. <ref type="figure">Fig. 4</ref> (e) and (f) show the kernel elements and offsets estimated by the CAR model trained with offset distance regularization. It can be observed that kernel elements estimated by the CAR model trained with offset distance regularization only present obvious movement on the strong edges and textured regions (the wheel ring and handle), and almost hold still at the rather smoothed region (the sky region). The kernel elements estimated by the CAR model trained without offset distance regularization also move towards the strong edges. However, it presents intensive movements on the plain region, which may lead to an unstable training process and sub-optimal testing performance, since the gradient of the resampling kernel depends on the interpolated pixel value (Equation 4). The fixed bicubic kernel is also visualized in <ref type="figure">Fig. 4 (b)</ref>. When compared with the content varying kernels shown in <ref type="figure">Fig.  4 (c)</ref> and (e), the fixed bicubic kernel will be uniformly applied to all the resampling region no matter what image content is going to be resampled.</p><p>The superior SR performance with the CAR model is achieved from the powerful capability of deep neural networks that can approximate arbitrary functions. However, the deep learning model tends to find a tricky way to produce LR images preserving details that are in favor of generating accurate SR images but not for better human perception. <ref type="figure" target="#fig_4">Fig. 5</ref> shows an example of 4× 8  <ref type="figure">Figure 4</ref>: An example of the 4× downscaled resampling kernel elements and its corresponding offsets. In the figure we only visualized one bicubic resampling kernel, since it will be uniformly applied to all resampling regions. We only visualize the centeral 9 kernel elements ((3 × h, 3 × w)) predicted by the CAR model. The kernel element offset is visualized using the color wheel presented in <ref type="bibr" target="#b52">[53]</ref>.</p><p>downscaled image by the CAR and 4× SR image by the jointly trained EDSR. As shown in <ref type="figure" target="#fig_4">Fig. 5 (b)</ref>, the CAR model learned to preserve more information using much fewer pixel spaces by arranging vertical edges in a regular criss-cross way, which makes the vertical edges in the LR image look jaggy. Jaggies are one type of aliasing that normally manifest as regular artifacts near sharp changes in intensity. However, the human visual system finds regular artifacts more objectionable than irregular artifacts <ref type="bibr" target="#b53">[54]</ref>. This problem is possibly caused by the inconsistent movement of the resampling kernels represented by the resampling kernel offsets near the sharp edges. To alleviate it, we introduced the partial TV loss of the horizontal and vertical resampling kernel offsets (Section 3.4) to constrain the rather free movements of the resampling kernel elements. As shown in <ref type="figure" target="#fig_4">Fig. 5 (c)</ref>, we can observe a smoother LR image with much less unsightly artifacts.</p><p>By employing the partial TV loss of the resampling kernel offsets, the CAR model generates better images for perception. However, we also observed SR performance degradation on each testing dataset, which is shown in the 'CAR' and 'CAR (w/o TV loss)' entries of <ref type="table" target="#tab_3">Table 3</ref>. This is because the introduction of the partial TV loss breaks the optimal way of keeping information during downsampling. Other types of aliasing inevitably occurred on the sample-rate conversion, and the SR model cannot recover correct textures from those irregular patterns when compared with that of the regular jaggies. As illustrated by the SR patches shown in <ref type="figure" target="#fig_4">Fig. 5 (b)</ref> and (c), we can recognize that the SR image corresponding the jagged LR image can better represent the original HR patch than that corresponding to the LR image with less jaggies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Comparison with public benchmark SR results</head><p>To compare the SR performance of downscaled images generated by the proposed CAR model with other deep SR models trained neither on images downscaled using predefined operations (such as bicubic interpolation) or images downscaled by end-to-end trained task driven image downscaling models, we listed several commonly compared public benchmark results on <ref type="table" target="#tab_4">Table 4</ref>. The comparison was organized into two groups, SR models trained with LR images produced using the bicubic interpolation method ('Bicubic' group) and LR images generated by models trained under the guidance of SR task ('Learned' group). In each group, it was further split into models trained using L2 loss function and L1 loss function. SR models in the 'Bicubic' group include the SRCNN <ref type="bibr" target="#b27">[28]</ref>, VDSR <ref type="bibr" target="#b28">[29]</ref>, DRRN <ref type="bibr" target="#b54">[55]</ref>, MemNet <ref type="bibr" target="#b55">[56]</ref>, DnCNN <ref type="bibr" target="#b56">[57]</ref>, LapSRN <ref type="bibr" target="#b37">[38]</ref>, ZSSR <ref type="bibr" target="#b57">[58]</ref>, CARN <ref type="bibr" target="#b58">[59]</ref>, SRRAM <ref type="bibr" target="#b59">[60]</ref>. In the 'Learned' group, we compared the CAR model with two recent state-of-the-art image downscaling models trained jointly with deep SR models, i.e., the CNN-CR→CNN-SR <ref type="bibr" target="#b3">[4]</ref> model and the TAD→TAU model <ref type="bibr" target="#b20">[21]</ref>. We also jointly trained the CAR model with the CNN-SR and TAU to do fair comparisons with the performance reported in its corresponding paper. 9  As shown in <ref type="table" target="#tab_4">Table 4</ref>, SR performance of upscaling models trained jointly with learnable image downscaling model outperform that of SR models trained using images downscaled in a predefined manner. This is because that LR images generated by learnable image downscaling models adaptively preserve content dependent information during downscaling, which essentially helps deep SR models learn to better recover original image content. Comparing our model with recent state-of-theart SR driven image downscaling models, the proposed model achieved the best PSNR performance on four out of five testing datasets on the 2× image downscaling and upscaling track. On the 4× track, the proposed model achieved the best performance. Noticeable improvement on the Urban100 dataset <ref type="bibr" target="#b49">[50]</ref> can be observed. This can be possibly explained by the reason that images of the Urban100 are all buildings with rich sharp edges, and TAD is trained under the constraint of producing LR images similar to images downscaled by bicubic interpolation with pre-filtering which inevitably constrain the edges of LR images generated by the TAD to be blurry like that downscaled by the bicubic interpolation. Therefore, the jointly trained up sampler cannot well recover original edges from those blurred edges of the LR images. As to the proposed CAR model, it is trained without any LR constraint, since the resampling method naturally makes the LR result a valid image. Benefiting from the unsupervised training strategy, the CAR model can adaptively preserve essential edge information for better super-resolution.</p><formula xml:id="formula_11">-/ - Learned L2 (CNN-CR)-(CNN-SR) -/ - -/ - -/ - -/ - -/ - (CAR)-(CNN-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation of downscaled images</head><p>This section presents the analysis of the quality of the downscaled image produced by the CAR model from two perspectives. We first analyzed the downscaled image in the frequency domain. We used the 'Barbara' image from the Set14 dataset as an example because it contains a wealth of high-frequency components (as shown in the first example of <ref type="figure" target="#fig_2">Fig. 3</ref>). <ref type="figure" target="#fig_5">Fig. 6</ref> shows the spectrum obtained by applying FFT on the HR image and the downscaled images generated by the CAR model and four downscaling methods been compared. Each point in the spectrum represents a particular frequency contained in the spatial domain image. The point in the center of the spectrum is the DC component, and points closely around the center point represents low-frequency components. The further away from the center a point is, the higher is its corresponding frequency.</p><p>The spectrum of the HR image ( <ref type="figure" target="#fig_5">Fig. 6 (a)</ref>) contains a lot of high-frequency components. During image downscaling, spatialaliasing is inevitably occurred since the sampling rate is below the Nyquist frequency. Aliasing can be spotted in the spectrum as spurious bands that are not presented in the spectrum of the HR image (high frequency component is aliased into low frequency), e.g., the black box marked regions in <ref type="figure" target="#fig_5">Fig. 6</ref> (c-e) compared to thaty of the original spectrum in <ref type="figure" target="#fig_5">Fig. 6 (a)</ref>. One way to remove aliasing is to use a blurry filter upon resampling (so does the default MATLAB imresize function). As shown by the black box marked regions in <ref type="figure" target="#fig_5">Fig. 6 (b)</ref>, aliasing of the downscaled image produced by the MATLAB imresize function is alleviated. Compared with the spectrum of image produced by the three state-of-the-art image downscaling methods ( <ref type="figure" target="#fig_5">Fig. 6</ref> (c-e)), less power exaggeration of low frequency components can be observed in the black box indicated regions. Therefore, the spectrum of the CAR generated image demonstrates that less aliasing is produced during the downscaling by the CAR model. A cropped region from the downscaled 'Barbara' image is shown in <ref type="figure" target="#fig_5">Fig. 6</ref> (g) and less spatial aliasing can be observed from the Bicubic and CAR downscaled image when compared with that of the other three image downscaling methods. Additionally, we analyzed the downscaled image from the perspective of lossless compression. <ref type="table" target="#tab_6">Table 5</ref> presents the average bits-per-pixel (bpp) of the lossless compressed images using the lossless JPEG (JPEG-LS) <ref type="bibr" target="#b60">[61]</ref>. Downscaled images produced by the CAR model can be more easily compressed than that by downscaling algorithms designed for better human perception. The reason why this happened is that downscaling methods designed for better human perception tend to preserve or enhance edges in a pre-defined manner, and much more high frequency components are retained, thus the downscaled images are less compressible on average. When compared with the bpp of compressed bicubic downscaled images, the CAR model achieved similar compression performance. is the image quality preference of the SR task, and the lower part (below the zero axes) is the image quality preference of the image downscaling task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">User study</head><p>To evaluate the visual quality of the generated SR images and LR images corresponding to different downscaling algorithms, we conducted user study which is widely adopted in many image generation tasks. We picked 59 sample images from different testing datasets, i.e., the Set5 (5 images), Set14 (20 images), BSD100 (20 images), and Urban100 (20 images) dataset. Samples are randomly selected with diverse properties, including people, animal, building, natural scenes and computergenerated graphics. We adopted similar evaluation settings used in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>. The user study were conducted as the A/B testing: the original image was presented in the middle place with two variants (SR images or downscaled images) showed in either side, among which one is produced by the CAR method and another is generated by one of the competing methods, i.e., Bicubic, Perceptually <ref type="bibr" target="#b14">[15]</ref>, DPID <ref type="bibr" target="#b16">[17]</ref> and L0-regularized <ref type="bibr" target="#b18">[19]</ref>. Users were required to answer the question 'which one looks better' by exclusively selecting one of the three options from: 1) A is better than B; 2) A equals to B; 3) B is better than A. For all 59 sample images, there are 236 pairwise decisions for each user. All image pairs were shown in random temporal order and the two variants of the original image of a pair are also randomly shown in position A or position B. To test the 11 reliability of the user study result, we add additional 10 image groups by randomly repeating question from the 236 trials. All images were displayed at native resolution of the monitor and zoom functionality of the UI was disabled. Users can only pan the view if the total size of a group of images exceeds the screen resolution. No other restrictions on the viewing way were imposed and users can judge those images at any viewing distance and angle without time limits.</p><p>We invited 29 participants for both the user study on 4× image super-resolution and 4× image downscaling tasks. Answers from each participant are filtered out if it achieves less than 80% consistency <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> on the repeated 10 questions, which generates 29 and 28 valid records for the image super-resolution and image downscaling tasks, respectively. As can be seen from <ref type="figure" target="#fig_6">Fig.  7</ref>, the total preferences of the SR and image downscaling task opted for the CAR model are larger than that of the reference methods. The upper part (above the zero axes) of <ref type="figure" target="#fig_6">Fig. 7</ref> shows the results of the user study on 4× image super-resolution task, each group of bars present the comparison between SR images corresponding to the CAR generated LR images and SR images corresponding to LR images produced by the reference method.</p><p>The results indicate that the CAR model achieves at least 73% preference over all other algorithms. Besides, our algorithm achieves more than 98% preference compared with the Perceptually method, demonstrating that there is a distinct difference between the SR image corresponding to the perceptually based <ref type="bibr" target="#b14">[15]</ref> downscaled image and the original HR image. Although there are about 20% preference on 'A equals to B' plus 'B is better than A' on the DPID and L0-regularized entry, it still cannot compete with the significant superiority of the CAR method. When combined with the objective metrics presented in <ref type="table" target="#tab_1">Table  1</ref>, we can arrive at the conclusion that LR images produced by algorithms solely optimized for better human perception cannot be well recovered.</p><p>The lower part (below the zero axes) of <ref type="figure" target="#fig_6">Fig. 7</ref> shows the users' perceptual preference for the 4× downscaled images generated by the CAR method versus the Bicubic, Perceptually, DPID, and L0-regularized image downscaling algorithms. We observed that the CAR method gets distinct less preference when compared with that of the Perceptually, DPID, and L0-regularized algorithm, which illustrates that the three state-of-the-art algorithms generate more perceptually favored LR images. However, when compared with the Bicubic downscaling algorithm, the user preference for the CAR method is slightly inferior to that for the Bicubic, and at most cases, participants tend to give no preference for both methods. This indicates that the CAR image downscaling method is comparable to the bicubic downscaling algorithm in terms of human perception. Among the three stateof-the-art image downscaling algorithms, the DPID achieves less agreement since its hyper-parameter is content dependent, and during the test, default value is used. The perceptually based image downscaling and the L0-regularized method achieve more than 75% user preference because these methods artificially emphasize the edges of image content, which can be good if the image is going to be displayed at a very small size, such as an icon. Whether it is desirable in other cases is debatable. It does tend to make images look better at first glance, but at the expense of realism in terms of signal fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper introduces the CAR model for image downscaling, which is an end-to-end system trained by maximizing the SR performance. It simultaneously learns a mapping for resolution reduction and SR performance improvement. One major contribution of our work is that the CAR model is trained in an unsupervised manner meaning that there is no assumption on how the original HR image will be downscaled, which helps the image downscale model to learn to keep essential information for SR task in a more optimal way. This is achieved by the content adaptive resampling kernel generation network which estimates spatial non-uniform resampling kernels for each pixel in the downscaled image according to the input HR image. The downscaled pixel value is obtained by decimating HR pixels covered by the resampling kernel. Our experimental results illustrate that the CAR model trained jointly with the SR networks achieves a new state-of-the-art SR performance while produces downscaled images whose quality are comparable to that of the widely adopted image downscaling method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Network architecture. It consists of three parts, the ResamplerNet, the Downscaling module, and the SRNet. The ResamplerNet is designed to estimate content adaptive resampling kernels and its corresponding offsets for each pixel in the downscaled image. The SRNet, can be any form of differentiable upsampling operations, is employed to guide the training of the ResamplerNet by simply minimizing the SR error. The entire framework is trained end-to-end by back-propagating error signals through a the differentiable downscaling module. The composition of each building block is detailed on the blue dashed frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Non-uniform resampling. The black + represents the center of a pixel in the HR image and the red + indicate the position of the resampling kernel element. The blue dashed arrow shows the offset direction and magnitude of the resampling kernel element relative to its corresponding pixel center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results of 4× downscaled image and SR using the EDSR on four example images from the Set14 dataset. (More results are presented in the supplementary file.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Input HR image (b) Bicubic kernel (c) CAR(kernels, w/o offset constraint) (d) CAR (offsets, w/o offset constraint) (e) kernels, w/ offset constraint (f) CAR (offsets, w/ offset constraint)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>An example trade off between perception of the 4× downscaled image and distortion of the 4× SR image. The first row shows the HR patch (a) and patch of SR results (b and c), the second row shows the downscaled version of the HR patch. The introduction of partial TV loss can produce visually better downscaled image at the expense of some SR performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Cropped patch from the downscaled 'Barbara' image Spectrum analysis of the 4× downscaled 'Barbara' image in the Set14 dataset using different downscaling methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>User study results comparing the proposed CAR model against reference image downscaling algorithms for the image super-resolution and image downscaling tasks. Each data point is an average over valid records evaluated on 20 image groups and the error bars indicate 95% confidence interval. The upper part (above the zero axes)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>For training the proposed content adaptive image downscaling resampler generation network under the guidance of EDSR, we employed the widely used DIV2K<ref type="bibr" target="#b45">[46]</ref> image dataset.</figDesc><table><row><cell>4 Experiments</cell></row><row><cell>4.1 Experimental setup</cell></row><row><cell>4.1.1 Datasets and metrics</cell></row></table><note>There are 1000 high-quality images in the DIV2K dataset, where 800 images for training, 100 images for validation and the other 100 images for testing. In the testing, four standard datasets, i.e., the Set5 [47], Set14</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative evaluation results (PSNR / SSIM) of different image downscaling methods for SR on benchmark datasets: Set5, Set14, BSD100, Urban100 and DIV2K (validation set). / 0.9299 34.38 / 0.9426 38.06 / 0.9615 31.45 / 0.9212 37.02 / 0.9573 35.40 / 0.9514 38.94 / 0.9658 4x 28.42 / 0.8104 28.93 / 0.8335 32.35 / 0.8981 25.65 / 0.7805 31.75 / 0.8913 31.05 / 0.8847 33.88 / 0.9174 Set14 2x 30.24 / 0.8688 31.01 / 0.8908 33.88 / 0.9202 29.26 / 0.8632 32.82 / 0.9119 31.56 / 0.9008 35.61 / 0.9404 4x 26.00 / 0.7027 26.39 0.7326 28.64 / 0.7885 24.21 / 0.6684 28.27 / 0.7784 27.67 / 0.7702 30.31 / 0.8382 B100 2x 29.56 / 0.8431 30.18 / 0.8714 32.31 / 0.9021 28.62 / 0.8383 31.47 / 0.8922 30.75 / 0.8816 33.83 / 0.9262 4x 25.96 / 0.6675 26.17 / 0.6963 27.71 / 0.7432 24.61 / 0.6391 27.27 / 0.7341 27.00 / 0.7293 29.15 / 0.8001 Urban100 2x 26.88 / 0.8403 27.38 / 0.8620 32.92 / 0.9359 26.39 / 0.8483 31.64 / 0.9271 30.23 / 0.9172 35.24 / 0.9572 4x 23.14 / 0.6577 23.35 / 0.6844 26.62 / 0.8041 21.58 / 0.6295 26.07 / 0.7967 25.83 / 0.7957 29.28 / 0.8711 DIV2K (validation) 2x 31.01 / 0.9393 33.18 / 0.9317 36.76 / 0.9482 31.23 / 0.8984 35.75 / 0.9419 34.69 / 0.9354 38.26 / 0.9599 4x 26.66 / 0.8521 28.50 / 0.8557 31.04 / 0.8452 26.28 / 0.7381 30.53 / 0.8373 30.18 / 0.8340 32.82 / 0.8837 Note: Red color indicates the best performance and Blue color represents the second.</figDesc><table><row><cell>Upscaling</cell><cell></cell><cell>Bicubic</cell><cell></cell><cell></cell><cell></cell><cell>EDSR</cell><cell></cell></row><row><cell cols="2">Downscaling</cell><cell>Bicubic</cell><cell>CAR</cell><cell>Bicubic</cell><cell>Perceptually</cell><cell>DPID</cell><cell>L0-regularized</cell><cell>CAR</cell></row><row><cell>Set5</cell><cell cols="2">2x 33.66</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results (PSNR / SSIM) of 4× upscaling using different SR networks on benchmark images downscaled by the CAR model. .9002 28.87 / 0.7889 27.77 / 0.7436 26.82 / 0.8087 30.77 / 0.8459 CAR † 33.84 / 0.9187 30.27 / 0.8383 29.16 / 0.8021 29.23 / 0.8719 32.81 / 0.8842 CAR ‡ 33.37 / 0.9138 29.87 / 0.8294 28.95 / 0.7953 28.28 / 0.8541 32.46 / 0.8786</figDesc><table><row><cell>Upscaling</cell><cell>Downscaling</cell><cell>Set5</cell><cell>Set14</cell><cell>B100</cell><cell>Urban100</cell><cell>DIV2K</cell></row><row><cell></cell><cell>Bicubic</cell><cell cols="4">32.02 / 0.8934 28.50 / 0.7782 27.53 / 0.7337 26.05 / 0.7819</cell><cell>-/ -</cell></row><row><cell>SRDenseNet</cell><cell>CAR †</cell><cell cols="5">33.16/ 0.9067 29.85 / 0.8201 28.73 / 0.7794 27.97 / 0.8403 32.24 / 0.8674</cell></row><row><cell></cell><cell>CAR ‡</cell><cell cols="5">32.63 / 0.9047 29.24 / 0.8122 28.44 / 0.7781 27.12 / 0.8248 31.77 / 0.8654</cell></row><row><cell></cell><cell>Bicubic</cell><cell cols="4">32.47 / 0.8980 28.82 / 0.7860 27.72 / 0.7400 26.38 / 0.7946</cell><cell>-/ -</cell></row><row><cell>D-DBPN</cell><cell>CAR †</cell><cell>33.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>07 / 0.9061 29.75 / 0.8189 28.70 / 0.7789 27.98 / 0.8376 32.13 / 0.8664 CAR ‡ 32.71 / 0.9055 29.17 / 0.8076 28.45 / 0.7784 27.00 / 0.8222 31.76 / 0.8650 RDN Bicubic 32.47 / 0.8990 28.81 / 0.7871 27.72 / 0.7419 26.61 / 0.8028 -/ - CAR † 33.34 / 0.9132 29.93 / 0.8308 28.89 / 0.7961 28.53 / 0.8582 32.32 / 0.8756 CAR ‡ 33.15 / 0.9112 29.59 / 0.8227 28.79 / 0.7913 27.69 / 0.8412 32.20 / 0.8747 RCAN Bicubic 32.63 / 0CAR †: the CAR model is trained jointly with its corresponding SR model. CAR ‡: the SR model is trained using the downscaled images generated by the CAR model that is jointly with the EDSR. Note: Red color indicates the best performance and Blue color represents the second. The '-' indicates that results are not provided by the corresponding original publication.bicubic interpolation downscaling. When considering the SR performance of the CAR model trained under the guidance of the bicubic interpolation (Table 1) we can reasonably arrive at the conclusion that the CAR image downscaling model can be learned to adapt to SR models as long as the SR operation is differentiable.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation results (PSNR / SSIM) of the CAR model on the Set5, Set14, BSD100, Urban100 and DIV2K (validation set). / 0.9658 35.61 / 0.9404 33.83 / 0.9262 35.24 / 0.9572 38.26 / 0.9599 CAR (w/o TV loss) 2x 39.00 / 0.9663 35.65 / 0.9411 33.91 / 0.9273 35.45 / 0.9587 38.34 / 0.9601 CAR (w/o offset constrain) 2x 38.73 / 0.9641 35.29 / 0.9372 33.58 / 0.9213 35.19 / 0.9560 38.03 / 0.9581 CAR (w/o offset) 2x 38.09 / 0.9600 34.46 / 0.9331 33.25 / 0.9209 32.91 / 0.9423 37.24 / 0.9497 CAR 4x 33.88 / 0.9174 30.31 / 0.8382 29.15 / 0.8001 29.28 / 0.8711 32.82 / 0.8837 CAR (w/o TV loss) 4x 34.13 / 0.9222 30.46 / 0.8439 29.36 / 0.8096 29.36 / 0.8772 33.05 / 0.8893 CAR (w/o offset constrain) 4x 33.86 / 0.9174 30.18 / 0.8368 29.11 / 0.7998 29.02 / 0.8704 32.74 / 0.8835 CAR (w/o offset) 4x 33.11 / 0.9168 29.68 / 0.8322 28.78 / 0.7921 27.98 / 0.8675 32.18 / 0.8830</figDesc><table><row><cell>Model</cell><cell>Scale</cell><cell>Set5</cell><cell>Set14</cell><cell>B100</cell><cell>Urban100</cell><cell>DIV2K</cell></row><row><cell>CAR</cell><cell>2x</cell><cell>38.94</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Note: Red color indicates the best performance and Blue color represents the second.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Public benchmark results (PSNR / SSIM) of 2× and 4× upscaling using different SR networks on the Set5, Set14, BSD100, Urban100 and DIV2K validation set. / 0.9542 32.42 / 0.9063 31.36 / 0.8897 29.50 / 0.8946 33.05 / 0.9581 VDSR 37.53 / 0.9587 33.03 / 0.9213 31.90 / 0.8960 30.76 / 0.9140 33.66 / 0.9625 DRRN 37.74 / 0.9591 33.23 / 0.9136 32.05 / 0.8973 31.23 / 0.9188 35.63 / 0.9410 MemNet 37.78 / 0.9597 33.28 / 0.9142 32.08 / 0.8978 31.31 / / 0.8937 28.60 / 0.7806 27.58 / 0.7349 26.07 / 0.7837 30.43 / 0.8374 SRRAM 32.13 / 0.8932 28.54 / 0.7800 27.56 / 0.7350 26.05 / 0.7834 -/ -ESRGAN 32.73 / 0.9011 28.99 / 0.7917 27.85 / 0.7455 27.03 / 0.8153</figDesc><table><row><cell>Donwscaling type Loss</cell><cell>Upscaling</cell><cell></cell><cell>Set5</cell><cell>Set14</cell><cell>BSD100</cell><cell>Urban100</cell><cell>DIV2K</cell></row><row><cell>L2</cell><cell>SRCNN</cell><cell></cell><cell cols="4">36.66 0.9195</cell><cell>-/ -</cell></row><row><cell>Bicubic</cell><cell>DnCNN LapSRN</cell><cell></cell><cell cols="5">37.58 / 0.9590 33.03 / 0.9118 31.90 / 0.8961 30.74 / 0.9139 37.52 / 0.9590 33.08 / 0.9130 31.80 / 0.8950 30.41 / 0.9100 35.31 / 0.9400 -/ -</cell></row><row><cell></cell><cell>ZSSR</cell><cell></cell><cell cols="3">37.37 / 0.9570 33.00 / 0.9108 31.65 / 0.8920</cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>L1</cell><cell>CARN SRRAM</cell><cell>2x</cell><cell cols="5">37.76 / 0.9590 33.52 / 0.9166 32.09 / 0.8978 31.92 / 0.9256 36.04 / 0.9451 37.82 / 0.9592 33.48 / 0.9171 32.12 / 0.8983 32.05 / 0.9264 -/ -</cell></row><row><cell></cell><cell>ESRGAN</cell><cell></cell><cell>-/ -</cell><cell>-/ -</cell><cell>-/ -</cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell></cell><cell>(CNN-CR)-(CNN-SR)</cell><cell></cell><cell>38.88 / -</cell><cell>35.40 / -</cell><cell>33.92 / -</cell><cell>33.68 / -</cell><cell>-/ -</cell></row><row><cell>L2</cell><cell>(CAR)-(CNN-SR)</cell><cell></cell><cell cols="5">38.91 / 0.9656 35.55 / 0.9401 33.96 / 0.9281 34.73 / 0.9539 38.15 / 0.9593</cell></row><row><cell>Learned</cell><cell>(CAR)-(EDSR) (TAD)-(TAU)</cell><cell></cell><cell cols="5">39.01 / 0.9662 35.64 / 0.9406 33.84 / 0.9262 35.15 / 0.9568 38.21 / 0.9597 37.69 / -33.90 / -32.62 / -31.96 / -36.13 / -</cell></row><row><cell>L1</cell><cell>(CAR)-(TAU)</cell><cell></cell><cell cols="5">37.93 / 0.9628 34.19 / 0.9312 32.78 / 0.7592 32.54 / 0.9388 36.86 / 0.9524</cell></row><row><cell></cell><cell>(CAR)-(EDSR)</cell><cell></cell><cell cols="5">38.94 / 0.9658 35.61 / 0.9404 33.83 / 0.9262 35.24 / 0.9572 38.26 / 0.9599</cell></row><row><cell></cell><cell>SRCNN</cell><cell></cell><cell cols="5">30.48 / 0.8628 27.49 / 0.7503 26.90 / 0.7101 24.52 / 0.7221 27.78 / 0.8753</cell></row><row><cell></cell><cell>VDSR</cell><cell></cell><cell cols="5">31.35 / 0.8838 28.01 / 0.7674 27.29 / 0.7251 25.18 / 0.7524 28.17 / 0.8841</cell></row><row><cell>L2</cell><cell>SRResNet DRRN</cell><cell></cell><cell cols="5">32.05 / 0.8910 28.53 / 0.7804 27.57 / 0.7354 26.07 / 0.7839 31.68 / 0.8888 28.21 / 0.7720 27.38 / 0.7284 25.44 / 0.7638 29.98 / 0.8270 -/ -</cell></row><row><cell></cell><cell>MemNet</cell><cell></cell><cell cols="4">31.74 / 0.8893 28.26 / 0.7723 27.40 / 0.7281 25.50 / 0.7630</cell><cell>-/ -</cell></row><row><cell>Bicubic</cell><cell>DnCNN</cell><cell></cell><cell cols="4">31.40 / 0.8845 28.04 / 0.7672 27.29 / 0.7253 25.20 / 0.7521</cell><cell>-/ -</cell></row><row><cell></cell><cell>LapSRN</cell><cell></cell><cell cols="5">31.54 / 0.8850 28.19 / 0.7720 27.32 / 0.7280 25.21 / 0.7560 29.88 / 0.8250</cell></row><row><cell></cell><cell>ZSSR</cell><cell></cell><cell cols="3">31.13 / 0.8796 28.01 / 0.7651 27.12 / 0.7211</cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>L1</cell><cell>CARN</cell><cell>4x</cell><cell>32.13</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>SR) 33.73 / 0.9148 30.22 / 0.8329 29.13 / 0.7992 28.56 / 0.8546 32.67 / 0.8800 (CAR)-(EDSR) 33.87 / 0.9176 30.34 / 0.8381 29.18 / 0.8005 29.23 / 0.8710 32.85 / 0.8835 / 0.9174 30.31 / 0.8382 29.15 / 0.8001 29.28 / 0.8711 32.82 / 0.8837 Note: Red color indicates the best performance and Blue color represents the second. The '-' indicates that results are not provided by the corresponding original publication.</figDesc><table><row><cell></cell><cell>(TAD)-(TAU)</cell><cell>31.59 / -</cell><cell>28.36 / -</cell><cell>27.57 / -</cell><cell>25.56 / -</cell><cell>30.25 / -</cell></row><row><cell>L1</cell><cell>(CAR)-(TAU)</cell><cell cols="5">31.85 / 0.8938 28.77 / 0.8028 27.84 / 0.7592 26.15 / 0.7978 31.10 / 0.8512</cell></row><row><cell></cell><cell>(CAR)-(EDSR)</cell><cell>33.88</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Bpp of lossless compressed downscaled images using the JPEG-LS</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Bicubic Perceptually DPID</cell><cell>L 0</cell><cell>CAR</cell></row><row><cell>Set5</cell><cell></cell><cell>11.99</cell><cell>13.27</cell><cell>12.65</cell><cell>16.10</cell><cell>11.80</cell></row><row><cell>Set14</cell><cell></cell><cell>11.23</cell><cell>12.01</cell><cell>12.05</cell><cell>15.07</cell><cell>11.14</cell></row><row><cell>BSD100 Urban100</cell><cell>2x</cell><cell>11.11 11.39</cell><cell>11.90 12.22</cell><cell>11.8 12.19</cell><cell>15.31 15.27</cell><cell>10.69 11.28</cell></row><row><cell>DIV2K</cell><cell></cell><cell>10.42</cell><cell>11.38</cell><cell>11.1</cell><cell>14.04</cell><cell>10.20</cell></row><row><cell>Average</cell><cell></cell><cell>11.228</cell><cell>12.156</cell><cell cols="3">11.958 15.158 11.022</cell></row><row><cell>Set5</cell><cell></cell><cell>14.78</cell><cell>16.52</cell><cell>15.32</cell><cell>17.41</cell><cell>14.39</cell></row><row><cell>Set14</cell><cell></cell><cell>12.61</cell><cell>14.27</cell><cell>13.32</cell><cell>15.19</cell><cell>12.47</cell></row><row><cell>BSD100 Urban100</cell><cell>4x</cell><cell>12.85 12.23</cell><cell>14.42 14.26</cell><cell>13.51 12.94</cell><cell>15.59 14.97</cell><cell>12.61 12.28</cell></row><row><cell>DIV2K</cell><cell></cell><cell>11.54</cell><cell>13.46</cell><cell>12.14</cell><cell>13.91</cell><cell>11.39</cell></row><row><cell>Average</cell><cell></cell><cell>12.802</cell><cell>14.586</cell><cell cols="3">13.446 15.414 12.628</cell></row></table><note>Note: Red color indicates the best performance and Blue color represents the second.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Down-scaling for better transform compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2003.816023</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1132" to="1144" />
			<date type="published" when="2003-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive downsampling to improve image compression at low bit rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2006.877415</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2513" to="2521" />
			<date type="published" when="2006-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interpolation-dependent image downsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2011.2158226</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3291" to="3296" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for image compact-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2018.2872876</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1092" to="1107" />
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Communication in the presence of noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-02" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="447" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2010.2050625</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image superresolution: Historical overview and future challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CRC</title>
		<imprint>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning for single image super-resolution: A brief review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A deep journey into super-resolution: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1904.07523" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Digital image warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Wolberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CS</title>
		<imprint>
			<biblScope unit="volume">10662</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Antialiasing filter design for subpixel downsampling via 12 frequency-domain analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">C</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2011.2165550</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1391" to="1405" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reconstruction filters in computer-graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><forename type="middle">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><forename type="middle">N</forename><surname>Netravali</surname></persName>
		</author>
		<idno>0097-8930</idno>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="221" to="228" />
			<date type="published" when="1988-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lanczos filtering in one and two dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Duchon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Meteor</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1016" to="1022" />
			<date type="published" when="1979-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contentadaptive image downscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Peers</surname></persName>
		</author>
		<idno>173:1-173:8</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptually based downscaling of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<idno>77:1- 77:10</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">Rahim</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rapid, detail-preserving image downscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Waechter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">C</forename><surname>Amend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Guthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Goesele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="205" to="206" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spectral remapping for image downscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Eduardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">M</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
		<idno>145:1-145:16</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">l 0 -regularized image downscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1076" to="1085" />
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning based image transformation using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianxu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bozhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="49779" to="49792" />
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Task-aware image downscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Evolutionary principles in selfreferential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural multi-scale image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><forename type="middle">M</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Okanohara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="718" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4809" to="4817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Residual dense network for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3262" to="3271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast and accurate image super-resolution with deep laplacian pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2865304</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep plugand-play super-resolution for arbitrary blur kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1671" to="1681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Loss functions for image restoration with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iuri</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Imag</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thrity-Seventh Asilomar Conference on Signals, Systems Computers</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Leonid I Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-11" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
	<note type="report_type">PHYS-ICA D.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Line Alberi</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Curves and Surfaces</title>
		<meeting>the 7th International Conference on Curves and Surfaces</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed selfexemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Stochastic sampling in computer graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Cook</surname></persName>
		</author>
		<idno>0730- 0301</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="72" />
			<date type="published" when="1986-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2790" to="2798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Persistent memory residual network for single image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2017.2662206</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Zero-shot superresolution using deep internal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3118" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fast, accurate, and lightweight super-resolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhyuk</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungkon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Ah</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="252" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Ram: Residual attention module for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Hyuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manri</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Seok</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1811.12043" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The locoi lossless image compression algorithm: principles and standardization into jpeg-ls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1324" />
			<date type="published" when="2000-08" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
