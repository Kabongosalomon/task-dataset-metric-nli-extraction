<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reliability Does Matter: An End-to-End Weakly Supervised Semantic Segmentation Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong-liverpool University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong-liverpool University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
							<email>2yunchao.wei@uts.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong-liverpool University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
							<email>kaizhuhuang@xjtlu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong-liverpool University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Alibaba-Zhejiang University Joint Institute of Frontier Technologies</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reliability Does Matter: An End-to-End Weakly Supervised Semantic Segmentation Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised semantic segmentation is a challenging task as it only takes image-level information as supervision for training but produces pixel-level predictions for testing. To address such a challenging task, most recent state-of-the-art approaches propose to adopt two-step solutions, i.e. 1) learn to generate pseudo pixel-level masks, and 2) engage FCNs to train the semantic segmentation networks with the pseudo masks. However, the two-step solutions usually employ many bells and whistles in producing high-quality pseudo masks, making this kind of methods complicated and inelegant. In this work, we harness the image-level labels to produce reliable pixel-level annotations and design a fully end-to-end network to learn to predict segmentation maps. Concretely, we firstly leverage an image classification branch to generate class activation maps for the annotated categories, which are further pruned into confident yet tiny object/background regions. Such reliable regions are then directly served as ground-truth labels for the parallel segmentation branch, where a newly designed dense energy loss function is adopted for optimization. Despite its apparent simplicity, our one-step solution achieves competitive mIoU scores (val: 62.6, test: 62.9) on Pascal VOC compared with those two-step state-of-the-arts. By extending our one-step method to two-step, we get a new state-of-the-art performance on the Pascal VOC (val: 66.3, test: 66.5).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recently, weakly supervised semantic segmentation receives great interest and is being extensively studied. Requiring merely low degree (cheaper or simpler) annotations including scribbles <ref type="bibr" target="#b19">(Lin et al. 2016;</ref><ref type="bibr" target="#b31">Vernaza and Chandraker 2017;</ref><ref type="bibr" target="#b30">Tang et al. 2018b)</ref>, bounding boxes <ref type="bibr" target="#b6">(Dai, He, and Sun 2015;</ref><ref type="bibr" target="#b15">Khoreva et al. 2017)</ref>, points <ref type="bibr" target="#b22">(Maninis et al. 2018;</ref><ref type="bibr" target="#b1">Bearman et al. 2016</ref>) and image-level labels <ref type="bibr" target="#b0">(Ahn and Kwak 2018;</ref><ref type="bibr" target="#b10">Hou et al. 2018;</ref><ref type="bibr" target="#b34">Wei et al. 2018)</ref> for training, weakly supervised semantic segmentation offers a much easy way than its fully supervised counterpart adopting pixel-level masks <ref type="bibr" target="#b4">(Chen et al. 2018a;</ref><ref type="bibr" target="#b21">Long, Shelhamer, and Darrell 2015)</ref>. Among these weakly supervised labels, the image-level annotation is the simplest one to collect yet also the most challenging case since there is no direct mapping between semantic labels and pixels.</p><p>To learn semantic segmentation models using image-level labels as supervision, many existing approaches can be categorized as one-step approaches and two-step approaches. One-step approaches <ref type="bibr" target="#b23">(Papandreou et al. 2015)</ref> often establish an end-to-end framework, which augments multi-instance learning with other constrained strategies for optimization. This family of methods is elegant and easy to implement. However, one significant drawback of these approaches is that the segmentation accuracy is far behind their fully supervised counterparts. To achieve better segmentation performance, many researchers alternatively propose to leverage two-step approaches <ref type="bibr" target="#b33">(Wei et al. 2017;</ref><ref type="bibr" target="#b12">Huang et al. 2018</ref>). This family of approaches usually aim to take bottom-up <ref type="bibr" target="#b9">(Hou et al. 2017)</ref> or top-down <ref type="bibr" target="#b38">(Zhang et al. 2018a;</ref><ref type="bibr" target="#b41">Zhou et al. 2016)</ref> strategies to firstly generate high-quality pseudo pixel-level masks with image-level labels as supervision. These pseudo masks then act as ground-truth and are fed into the off-theshelf fully convolutional networks such as FCN <ref type="bibr" target="#b21">(Long, Shelhamer, and Darrell 2015)</ref> and Deeplab <ref type="bibr" target="#b2">(Chen et al. 2014;</ref> to train the semantic segmentation models. Current state-of-the-arts are mainly two-step approaches, with segmentation performance approaching that of their fully supervised counterparts. However, to produce high-quality pseudo masks, these approaches often employ many bells and whistles, such as introducing additional object/background cues from object proposals <ref type="bibr" target="#b25">(Pinheiro and Collobert 2015)</ref> or saliency maps <ref type="bibr" target="#b13">(Jiang et al. 2013)</ref> in an off-line manner. Therefore, the two-step approaches are usually very complicated and hard to be re-implemented, limiting their application to research areas such as object localization and video object tracking.</p><p>In this paper, we present a simple yet effective one-step approach, which can be easily trained in an end-to-end manner. It achieves competitive segmentation performance compared with two-step approaches. Our approach named Reliable Region Mining (RRM) includes two branches: one to produce pseudo pixel-level masks using image-level annotations, and the other to produce the semantic segmentation results. In contrast to the previous two-step state-of-the-arts <ref type="bibr" target="#b0">(Ahn and Kwak 2018;</ref><ref type="bibr" target="#b18">Lee et al. 2019</ref>) that prefer to mine dense and integral object regions, our RRM only chooses those confident object/background regions that are usually tiny but with high response scores on the class activation maps. We find these regions can be further pruned into more reliable ones by augmenting an additional CRF operation, which are then employed as supervision for the parallel semantic segmentation branch. With limited pixels as supervision, we designed a regularized loss named dense energy loss, which cooperates with the pixel-wise cross-entropy loss to optimize the training process.</p><p>Despite its apparent simplicity, our one-step RRM achieves 62.6 and 62.9 of mIoU scores on the Pascal VOC val and test sets, respectively. These results achieve state-of-the-art performance and it is even competitive compared with those two-step state-of-the-arts, which usually adopt complex bells and whistles to produce pseudo masks. We believe that our proposed RRM offers a new insight to the one-step solution for weakly supervised semantic segmentation. Besides, in order to show the effectiveness of our method, we also extend our method to a two-step framework and get a new state-ofthe-art performance with 66.3 and 66.5 on the Pascal VOC val and test sets. Code will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Semantic segmentation is an important task in computer vision <ref type="bibr" target="#b36">Xiao et al. 2019;</ref><ref type="bibr" target="#b37">Xie et al. 2018)</ref>, which requires to predict pixel-level classification. <ref type="bibr" target="#b21">Long et al. (Long, Shelhamer, and Darrell 2015)</ref> proposed the first fully convolutional network for semantic segmentation. Chen et al. <ref type="bibr" target="#b2">(Chen et al. 2014</ref>) proposed a new deep neural network structure named "Deeplab" to conduct pixel-wise prediction using atrous convolution, and a series of new network structures was developed after that <ref type="bibr" target="#b4">(Chen et al. 2018a;</ref>. However, fully supervised semantic segmentation requires dense pixel-level annotations, which cost expensive human expense. Weakly supervised semantic segmentation has been drawing much attention as less human intervention is needed. There are different categories of weakly supervised semantic segmentation based on the types of supervision: scribble <ref type="bibr" target="#b29">(Tang et al. 2018a;</ref><ref type="bibr" target="#b19">Lin et al. 2016)</ref>, bounding box <ref type="bibr" target="#b28">(Song et al. 2019;</ref><ref type="bibr" target="#b11">Hu et al. 2018;</ref><ref type="bibr" target="#b27">Rajchl et al. 2017)</ref>, point <ref type="bibr" target="#b22">(Maninis et al. 2018;</ref><ref type="bibr" target="#b1">Bearman et al. 2016</ref>) and image-level class label <ref type="bibr" target="#b39">(Zhang et al. 2018b;</ref><ref type="bibr" target="#b31">Vernaza and Chandraker 2017;</ref><ref type="bibr" target="#b40">Zhang et al. 2018c)</ref>. In this paper, we focus on image-level supervised semantic segmentation.</p><p>Image-level weakly supervised semantic segmentation only provides image-level annotation. Most recent approaches are based on class activation map (CAM) <ref type="bibr" target="#b41">(Zhou et al. 2016)</ref>, which is to generate initial object seeds or regions from image-level annotation. Such initial object seeds or regions are converted to generate pseudo labels to train a semantic segmentation model. <ref type="bibr" target="#b33">Wei et al. (Wei et al. 2017)</ref> proposed to erase iteratively the discriminative areas computed by a classification network so that more seed regions can be mined which are then combined with a saliency map to generate the pseudo pixel-level label. <ref type="bibr" target="#b34">Wei et al. (Wei et al. 2018</ref>) also proved that dilated convolution can increase the receptive filed and improve the weakly segmentation network performance. Besides, <ref type="bibr" target="#b32">Wang et al. (Wang et al. 2018</ref>) trained a region network and a pixel network to make prediction from image level to region level and from region level to pixel level gradually. Also, this method takes saliency map as extra supervision. <ref type="bibr">Ahn and Suha (Ahn and Kwak 2018)</ref> designed an affinity network to compute the relationship between different image pixels and exploited this network to get the pseudo object labels for segmentation model training. <ref type="bibr" target="#b12">Huang et al. (Huang et al. 2018</ref>) deployed a traditional algorithm named seed growing to iteratively expand the seed regions.</p><p>However, all the above methods produced high-quality pseudo masks using a wide varieties of techniques, meaning that we need at least one or two extra networks before training FCNs for semantic segmentation prediction. In this work, we try to design one single network for the whole task to simplify the process. We believe this work offers a new perspective for the image-level weakly supervised semantic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method Overview</head><p>Our proposed RRM can be divided into two parallel branches including a classification branch and a semantic segmentation branch. Both branches share the same backbone network, and during training both of them update the whole network at the same time. The overall framework of our method is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The algorithm flow is illustrated in Algorithm 1.</p><p>• The classification branch is used to generate reliable pixellevel annotations. Original CAMs will be processed to generate reliable yet tiny regions. The final remained reliable regions are regarded as labeled regions, while other regions are viewed as unlabeled. These labels are used as supervision information for the semantic segmentation branch for training.</p><p>• The semantic segmentation branch is used to predict pixellevel labels. This branch deploys a new joint loss function combining the cross entropy loss with a newly designed dense energy loss. The cross entropy loss mainly considers labeled pixels, while the dense energy loss takes into account all pixels by making full use of RGB color and pixel positions.</p><p>The overall loss function of our RRM is: L = L class + L joint−seg , where L class represents a conventional classification softmax loss, while L joint−seg is a newly introduced joint loss for the segmentation branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Branch: Generating Labels for Reliable Regions</head><p>High-quality pixel-level annotation has a direct impact on our final semantic segmentation performance as it is the only ground-truth in the training processing. Original CAMs can highlight the most discriminative regions of an object, but they still contain some non-object areas, which are the mislabeled pixels. Therefore, after getting the original CAM regions, post-processing such as dense CRF (Krähenbühl and Koltun 2013) is needed. We followed this basic idea and do further process for generating the reliable labels. We compute the initial CAMs of the training dataset following <ref type="bibr" target="#b41">(Zhou et al. 2016)</ref>. In our network, global average pooling (GAP) is applied on the last convolution layer. The output of GAP is classified with a fully-connected layer. Finally, the fully-connected layer weights are used on the last convolution layer to obtain the heatmap for each class. Besides, inspired by the fact that dilated convolution can increase the respective field , we add dilated convolution into the last three layers. Details of our network settings are reported in our experiment Section.</p><p>Mathematically, given an image I, the CAM of class c is:</p><formula xml:id="formula_0">M c ocam = RS( D ch=0 ω c ch · F ch ), (c ∈ C f g ),<label>(1)</label></formula><p>where C f g = {c 1 , c 2 , ..., c N } includes all foreground classes, M c ocam is the CAM of class c for image I, ω c denotes the weights of the fully-connected layer for class c, and F is the feature maps from the last convolution layer of the backbone. RS(·) is an operation to resize the input to the shape of I.</p><p>Using multi-scale of original images is beneficial for generating a stable CAM. Given I and it is scaled by a factor s i , s i ∈ {s 0 , s 1 , ..., s n }, the multi-scale CAM for I is detonated as:</p><formula xml:id="formula_1">M c cam = n i=0 (M c ocam (s i )/(n + 1)),<label>(2)</label></formula><p>where M c ocam (s i ) is the CAM of class c for the scaled image I with a factor s i . <ref type="figure" target="#fig_1">Figure 2</ref> shows that compared to original CAM (scale=1), the multi-scale CAM provides more accurate object localization.</p><p>The CAM scores are normalized, so that we can get the classification probabilities for each pixel in I,</p><formula xml:id="formula_2">P c f g = M c cam /max(M c cam ), (c ∈ C f g ),<label>(3)</label></formula><p>where max(M c cam ) is the maximum value in the CAM of class c j . The background score is calculated using a similar way as in <ref type="bibr" target="#b0">(Ahn and Kwak 2018)</ref>:</p><formula xml:id="formula_3">P bg (i) = (1 − max c∈C f g (p c f g (i)) γ , γ &gt; 1.<label>(4)</label></formula><p>where i is the pixel position index, γ is the decay rate which helps to suppress background labels. The overall probability map, namely P f g bg , is obtained by concatenating foreground and background probabilities P f g and P bg . After that, we use the dense CRF (Krähenbühl and Koltun 2013) as post-processing to remove some mislabeled pixels, and the CRF pixel label map is:</p><formula xml:id="formula_4">I crf = CRF (I, [P f g , P bg ]).</formula><p>(5) The selected reliable CAM label is:</p><formula xml:id="formula_5">I cam (i) = argmax c∈C (P c f g bg (i)), if max c∈C (P c f g bg (i)) &gt; α 255, else ,<label>(6)</label></formula><p>where C = {c 0 , c 1 , ..., c N } includes all classes and the background (c 0 ). 255 means the class label is not decided yet.</p><p>The final pixel label input to the semantic segmentation branch is:</p><formula xml:id="formula_6">I f inal (i) = I cam (i), if I cam (i) = I crf (i) 255, else<label>(7)</label></formula><p>In <ref type="formula" target="#formula_5">(6)</ref>, max c∈C (P c f g−bg (i)) &gt; α selects the highly confident regions. In <ref type="formula" target="#formula_6">(7)</ref>, I crf (i) = I cam (i) considers the CRF constrains. Taking this strategy, highly reliable regions as well as their labels can be obtained. The regions which are detonated as 255 in <ref type="formula" target="#formula_6">(7)</ref> are regarded as unreliable regions. <ref type="figure" target="#fig_2">Figure 3</ref> shows an example of our approach. It is observed that the original CAM labels <ref type="figure" target="#fig_2">( Figure 3 (c)</ref>) contain most foreground labels but introduce a number of background pixels as foreground. The CRF label <ref type="figure" target="#fig_2">(Figure 3 (d)</ref>) can get accurate boundary but at the same time, many foreground pixels are regarded as background. In other words, the CAM label can provide reliable background pixels and CRF label can provide reliable foreground pixels. Combing the CAM label and CRF label map using our method, some wrong pixel-level labels are removed while the reliable regions are still remained, which is especially obvious at the object boundaries (see the difference between <ref type="figure" target="#fig_2">Figure 3</ref> (e) and (f)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation Branch: Making Predictions</head><p>After getting the reliable pixel-level annotations, they are used as labels for our semantic segmentation branch. Different from the other methods which train their semantic segmentation network with the integral pseudo labels independently, our segmentation branch shares the same backbone network with the classification branch, needing only reliable yet tiny pixel-level labels. Our loss function consists of a cross entropy loss and a energy loss. Cross entropy loss focuses on utilizing the labeled data while the energy loss considers both labeled and unlabeled data. The joint loss is:</p><formula xml:id="formula_7">L joint−seg = L ce + L energy .<label>(8)</label></formula><p>In <ref type="formula" target="#formula_7">(8)</ref>, L ce and L energy represent the cross entropy loss and the dense energy loss, respectively. The cross entropy loss is:  <ref type="formula" target="#formula_5">(6)</ref> and <ref type="formula" target="#formula_6">(7)</ref>, respectively. The white pixels in (e) and (f) are the unreliable regions.</p><formula xml:id="formula_8">L ce = − c∈C,i∈Φ B c (i)log(P c net (i)),<label>(9)</label></formula><p>where B c (i) is a binary indicator, which equals to 1 if the label of pixel i is c and otherwise 0; Φ denotes the labeled regions, Φ = {i|I f inal (i) = 255}; P c net (i) is the output probability of the trained network So far, all labeled pixels has been used for training with cross entropy loss, but there are a large number of unlabeled pixels. In order to make predictions for those unlabeled regions, we design a new shallow loss named dense energy loss considering both RGB colors and spatial positions.</p><p>We firstly define the energy formulation between pixel i and j based on <ref type="bibr" target="#b14">(Joy et al. 2019</ref>):</p><formula xml:id="formula_9">E(i, j) = ca,c b ∈C ca =c b G(i, j)P ca net (i)P c b net (j).<label>(10)</label></formula><p>In <ref type="formula" target="#formula_0">(10)</ref>, both c a and c b are the class labels, P ca net (i) and P c b net (j) are the softmax output of our segmentation branch at pixel i and j, respectively. G(i, j) is a Gaussian kernel bandwidth filter:</p><formula xml:id="formula_10">G(i, j) = 1 W exp(− D i − D j 2 2σ 2 d − I i − I j 2 2σ 2 r ),<label>(11)</label></formula><p>where 1 W is the normalized weights, D is the pixel spatial position while I is the RGB color. σ d and σ r are hyper parameters which control the scale of Gaussian kernels. (10) can be simplified using Potts model <ref type="bibr" target="#b30">(Tang et al. 2018b</ref>):</p><formula xml:id="formula_11">E(i, j) = ca,c b ∈C ca =c b G(i, j)P ca net (i)P c b net (j) = G(i, j) c∈C P c net (i)(1 − P c net (j)).<label>(12)</label></formula><p>Finally, our dense energy loss can be written as:</p><formula xml:id="formula_12">L energy = N i=0 N j=0 j =i S(i)E(i, j).<label>(13)</label></formula><p>In <ref type="formula" target="#formula_0">(13)</ref>, considering the fact that cross entropy loss is designed for supervised learning with label information 100% accurate, but in this task, all pixel labels are not 100% reliable, which means that using cross entropy loss might introduce some errors. Thus, our dense energy loss is applied to mitigate this problem. Based on this idea, we design a soft filter S(i) for pixel i:</p><formula xml:id="formula_13">S(i) = 1 − max c∈C (P c net (i)), i ∈ Φ 1, else<label>(14)</label></formula><p>Algorithm 1 Algorithm flow of our proposed approach. Input: Images I with their image-level class labels C f g ; Output:</p><p>The trained end-to-end network, N et; 1: while iteration is true do 2:</p><p>Use the classification Network branch to get the original CAMs;</p><p>3:</p><p>Get the multi-scale CAMs with (2) for each class;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Use <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_3">(4)</ref> to get foreground probability P f g and background probability P bg ; 5:</p><p>Get the overall CAM probability map P f g bg by combining P bg and P f g ; 6:</p><p>Calculate reliable CAM label I cam and CRF label I crf ;</p><p>7:</p><p>Get the reliable regions and label I f inal from I cam and I crf using (6)(7); 8:</p><p>Produce predictions and update the whole network using loss function L = L class + L ce + L energy ; 9: end while Experiments Dataset and Implementation Details Dataset. Our RRM is trained and validated on PASCAL VOC 2012 <ref type="bibr" target="#b7">(Everingham et al. 2010)</ref> as well as its augmented data, including 10, 582 images for training, 1, 449 images for validating and 1, 456 images for testing. Mean intersection over union (mIoU) is considered as the evaluation criterion. Implementation Details. The backbone network is a ResNet model with 38 convolution layers <ref type="bibr" target="#b35">(Wu, Shen, and Van Den Hengel 2019)</ref>. We remove all the fully connected layers of the original network and engage dilated convolution for the last three resnet blocks (a resnet block is a set of residual units with the same output size), the dilated rate is 2 for the last third layer, and 4 for the last 2 layers. For the semantic segmentation branch, we add two dilation convolution layers of the same configuration after the backbone <ref type="bibr" target="#b35">(Wu, Shen, and Van Den Hengel 2019)</ref>, with kernel size 3, dilated rate 12, and padding size 12. Cross entropy loss is computed for background and foreground individually. σ d and σ r in our dense energy loss are set as 15 and 100, respectively.</p><p>The training learning rate is 0.001 with weight decay being 5e-4. The training images are resized with a ratio randomly sampled from (0.7, 1.3), and they are randomly flipped. Finally, they are normalized and randomly cropped to size 321*321.</p><p>To generate reliable regions, the scale ratio in (2) is set to {0.5, 1, 1.5, 2}, γ in (5) is set to 4 for P f g bg . The CRF parameters in (5) follow the setting in <ref type="bibr" target="#b0">(Ahn and Kwak 2018)</ref>. In <ref type="formula" target="#formula_5">(6)</ref>, an α value is chosen with 40% pixels selected as labeled pixels for each class. During validating and testing, dense CRF is applied as a post-processing method, and the parameters are set as the default values given in . During training, both two branches update the backbone network. During testing, only the segmentation branch is used to produce the predictions. Reproducibility: PyTorch <ref type="bibr" target="#b24">(Paszke et al. 2017</ref>) was used. All the experiments were performed on NVIDIA RTX 2080 Ti. Code now is available at: https://github.com/zbf1991/RRM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Our Approach</head><p>Our RRM has two important aspects: using the reliable yet tiny pseudo masks for supervision and a new joint loss function for end-to-end training. Ablation studies are conducted to illustrate their individual and joint effectiveness, with results reported in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>.  <ref type="table">Table 1</ref>: Performance on PASCAL VOC 2012 val set based on different mined region. Ratio means the proportion of reliable regions which is mined by our method to the whole pixels. "CE loss" means only cross entropy loss was used for our segmentation branch and "Joint loss" means our dense energy loss was combined with cross entropy loss was used for the segmentation branch.</p><p>We firstly validate the influence of different pseudo mask size. We do this by changing α. <ref type="table">Table 1</ref> reports the results. A smaller pseudo mask size means that more reliable regions are selected for the segmentation branch, while a larger size means that fewer reliable pixels are labeled. <ref type="table">Table 1</ref> demonstrates that 20%-60% labeled pixels lead to the best performance. On one hand, too few labeled pixels cannot get satisfied performance since the segmentation network cannot get enough labels for learning. On the other hand, too many labeled pixels means more incorrect labels are used, which are noise for the training processing.      <ref type="table" target="#tab_0">Table 2</ref> shows the effectiveness of our introduced two main parts: reliable region mining and the joint loss. The results obtained using original CAM regions and the mined reliable regions with RRM are compared. It is observed that the pseudo label generated by RRM outperforms CAM labels. If we remove the joint loss from our segmentation branch, it also shows that the reliable pseudo labels generated by RRM improves the segmentation performance.</p><p>In addition, the comparison between Ours-RRM with CE loss and Ours-RRM with Joint loss in <ref type="table" target="#tab_0">Table 2</ref> illustrates the effectiveness of the introduced joint loss. Without the joint loss, the mIoU obtained with RRM with CE loss gets lower. This is because the mined reliable regions with RRM cannot provide sufficient labels for segmentation model training when only considering cross entropy loss. After adopting the joint loss, segmentation performance improves with a big margin from 48.5 to 62.6, which is a 14.1 increase. Similar comparison result is obtained between CAM with CE loss and CAM with Joint loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons with Previous Approaches</head><p>In <ref type="table" target="#tab_1">Table 3 and Table 4</ref>, we make detailed comparisons with other end-to-end network for image-level-only supervised semantic segmentation. Although there are various different networks for this task, only EM-Adapt <ref type="bibr" target="#b23">(Papandreou et al. 2015)</ref> adopts an end-to-end structure, and it can be seen that</p><p>Ours-RRM (one-step) outperforms it with a big margin. First of all, compared with EM-Adapt <ref type="bibr" target="#b23">(Papandreou et al. 2015)</ref>, which uses an expectationmaximization (EM) algorithm to update the network parameters, our method adopts a more direct and explicit learning procedure to update the whole network, using our designed joint loss function. Secondly, EM-Adapt <ref type="bibr" target="#b23">(Papandreou et al. 2015)</ref> can only give a rough segmentation result as only the image-level information is considered, while Ours-RRM (one-step) designs a pilot mechanism to provide reliable pixel-level labels, which leads to more accurate segmentation predictions.</p><p>In order to show the effectiveness and scalability of our idea, we also extend our method to a two-step framework. The difference is that for our one-step method (Ours-RRM (one-step)), we produce the predictions through our segmentation branch directly. Whereas for our two-step method, we firstly used our Ours-RRM (one-step) network to produce the pseudo masks for the training dataset. Following that, we train and evaluate Deeplab <ref type="bibr" target="#b2">(Chen et al. 2014</ref>) with those generated pixel labels, which is named as Our-RRM-VGG (two-step). Using the same setting, we also evaluate the performance when Deeplab-v2 <ref type="bibr" target="#b4">(Chen et al. 2018a</ref>) with ResNet-101 backbone was used, called Our-RRM-ResNet (two-step). The final results can be found in <ref type="table" target="#tab_3">Table 5</ref>. It is observed that among existing methods solely using image-level label without extra data, AffinityNet <ref type="bibr" target="#b0">(Ahn and Kwak 2018)</ref> is the most performing one. However, both Ours-RRM-VGG (two-step) and Ours-RRM-ResNet (two-step) perform much better than it when the same backbone was used. One more thing should be noticed is that AffinityNet <ref type="bibr" target="#b0">(Ahn and Kwak 2018)</ref> used ResNet-38 <ref type="bibr" target="#b35">(Wu, Shen, and Van Den Hengel 2019)</ref> as baseline, which is more powerful than ResNet-101 <ref type="bibr" target="#b18">(Lee et al. 2019)</ref>, and even in this case ours-RRM-ResNet (two-step) still outperforms it with a big margin. Note that AffinityNet <ref type="bibr" target="#b0">(Ahn and Kwak 2018)</ref> applies three different DNNs with many bells and whistles, while we get equivalent results with only one end-to-end network (Ours-RRM (one-step)).</p><p>To the best of our knowledge, the previous state-of-the-art, FickleNet <ref type="bibr" target="#b18">(Lee et al. 2019)</ref>, achieves the mIoU score of 64.9 and 65.3 on PASCAL VOC val and test set, but it uses class agnostic saliency map <ref type="bibr" target="#b20">(Liu et al. 2010)</ref> as extra supplement information and uses two individual networks separately. Ours-RRM-ResNet (two-step) gives a better performance with mIoU scores of 66.3 and 66.5 on PASCAL VOC val and test set, which represents 1.4 and 1.2 improvement. Note that we do not use extra data or information in our case. Therefore, Ours-RRM-ResNet (two-step) is the new stateof-the-art for two-step image-level label weakly supervised semantic segmentation.</p><p>In <ref type="figure" target="#fig_5">Figure 4</ref>, we report some subjective semantic segmentation results of ours methods, which are compared with EM-Adapt <ref type="bibr" target="#b23">(Papandreou et al. 2015)</ref>, the state-of-the-art endto-end network. Ours-RRM (one-step) obtains much better segmentation results on both large and small objects, with much accurate boundaries. We also show some results of our two-step approaches, and it can be seen that among our three methods, ours-RRM-ResNet (two-step) obtains the best performance duo to the powerful network architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed the RRM, an end-to-end network for image-level weakly supervised semantic segmentation. We revisited drawbacks of the state-of-the-arts, which adopt the two-step approach. We proposed a one-step approach through mining reliable yet tiny regions and used them as ground-truth labels directly for segmentation model training. With limited pixels as supervision, we designed a new loss named dense energy loss, which takes shallow features (RGB colors and spatial information) and cooperates with the pixel-wise cross-entropy loss to optimize the training process. Based on our one-step RRM, we extend a two-step method. Both our one-step and two-step approaches achieve state-ofthe-art performance. More importantly, our RRM offers a different perspective from the traditional two-step solutions. We believe that the proposed one-step approach could further boost research in this direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The framework of our proposed RRM network. First of all, original regions are calculated through the classification branch, then the pseudo pixel-level masks are generated. Finally, the pseudo labels are applied as supervision to train the semantic segmentation branch. The whole RRM is jointly optimized end-to-end via a standard back-propagation algorithm during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An example of computing multi-scale CAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>An example of generating reliable pixel labels. (c) is computed only considering the corresponding class label of P f g bg . (d) is the result of (5), (e) and (f) are generating through</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative segmentation results on PASCAL VOC 2012 val set. (a) Original images. (b) Ground-truth. (c) EM-Adapt results. (d) Ours-RRM (one-step) results. (e) Ours-RRM-VGG (two-step) results. (f) Ours-RRM-ResNet (twostep) results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Analysis of our method. CAM means class activate maps directly as pseudo masks. Ours-RRM means that we used our method to produce pseudo masks. Both CAM and ours-RRM use top 40% pixels according toTable 1.</figDesc><table><row><cell>End-to-End Method</cell><cell>bkg plane bike bird boat bottle bus</cell><cell>car</cell><cell cols="4">cat chair cow table dog horse mbk person plant sheep sofa train</cell><cell>tv</cell><cell>mIOU</cell></row><row><cell cols="4">EM-Adapt (Papandreou et al. 2015) 67.2 29.2 17.6 28.6 22.2 29.6 47.0 44.0 44.2 14.6 35.1 24.9 41.0 34.8 41.6</cell><cell>32.1</cell><cell>24.8</cell><cell>37.4 24.0 38.1 31.6</cell><cell>33.8</cell></row><row><cell>Ours-RRM (one-step)</cell><cell cols="3">87.9 75.9 31.7 78.3 54.6 62.2 80.5 73.7 71.2 30.5 67.4 40.9 71.8 66.2 70.3</cell><cell>72.6</cell><cell>49.0</cell><cell>70.7 38.4 62.7 58.4</cell><cell>62.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Performance on the PASCAL VOC 2012 val set, compared with other end-to-end weakly supervised approaches. End-to-End Method bkg plane bike bird boat bottle bus car cat chair cow table dog horse mbk person plant sheep sofa train tv mIOU EM-Adapt (Papandreou et al. 2015) 76.3 37.1 21.9 41.6 26.1 38.5 50.8 44.9 48.9 16.7 40.8 29.4 47.1 45.</figDesc><table><row><cell>8 54.8</cell><cell>28.2</cell><cell>30.0</cell><cell>44.0 29.2 34.3 46.0</cell><cell>39.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Performance on the PASCAL VOC 2012 test set, compared with other end-to-end weakly supervised approaches.</figDesc><table><row><cell>Method</cell><cell>Baseline</cell><cell>Sup.</cell><cell>Extra Data</cell><cell cols="3">End-to-end val (mIoU) test (mIoU)</cell></row><row><cell>Deeplab (ICLR'15) (Chen et al. 2014)</cell><cell>VGG-16</cell><cell>F</cell><cell>-</cell><cell>-</cell><cell>67.6</cell><cell>70.3</cell></row><row><cell>Deeplab-v2 (Chen et al. 2018a)</cell><cell cols="2">ResNet-101 F</cell><cell>-</cell><cell>-</cell><cell>76.8</cell><cell>79.7</cell></row><row><cell>WSSL (ICCV'15) (Papandreou et al. 2015)</cell><cell>VGG-16</cell><cell>B</cell><cell>-</cell><cell>-</cell><cell>60.6</cell><cell>62.2</cell></row><row><cell>BoxSup (ICCV'15) (Dai, He, and Sun 2015)</cell><cell>VGG-16</cell><cell>B</cell><cell>-</cell><cell>-</cell><cell>62.0</cell><cell>64.6</cell></row><row><cell>ScribbleSup (CVPR'16) (Lin et al. 2016)</cell><cell>VGG-16</cell><cell>S</cell><cell>-</cell><cell>-</cell><cell>63.1</cell><cell>-</cell></row><row><cell>Kernel Cut (ECCV'18) (Tang et al. 2018b)</cell><cell cols="2">ResNet-101 S</cell><cell>-</cell><cell>-</cell><cell>75.0</cell><cell>-</cell></row><row><cell>CrawlSeg (CVPR'17) (Hong et al. 2017)</cell><cell>VGG-16</cell><cell>L</cell><cell>YouTube Videos</cell><cell>×</cell><cell>58.1</cell><cell>58.7</cell></row><row><cell>DSRG (CVPR'18) (Huang et al. 2018)</cell><cell>VGG-16</cell><cell>L</cell><cell>MSRA-B</cell><cell>×</cell><cell>59.0</cell><cell>60.4</cell></row><row><cell>DSRG (CVPR'18) (Huang et al. 2018)</cell><cell cols="2">ResNet-101 L</cell><cell>MSRA-B</cell><cell>×</cell><cell>61.4</cell><cell>63.2</cell></row><row><cell>FickleNet (CVPR'19) (Lee et al. 2019)</cell><cell cols="2">ResNet-101 L</cell><cell>MSRA-B</cell><cell>×</cell><cell>64.9</cell><cell>65.3</cell></row><row><cell>EM-Adapt (ICCV'15)(Papandreou et al. 2015)</cell><cell>VGG-16</cell><cell>L</cell><cell>-</cell><cell></cell><cell>38.2</cell><cell>39.6</cell></row><row><cell cols="2">SEC (ECCV'16) (Kolesnikov and Lampert 2016) VGG-16</cell><cell>L</cell><cell>-</cell><cell>×</cell><cell>50.7</cell><cell>51.7</cell></row><row><cell>AugFeed (ECCV'16) (Qi et al. 2016)</cell><cell>VGG-16</cell><cell>L</cell><cell>-</cell><cell>×</cell><cell>54.3</cell><cell>55.5</cell></row><row><cell>AdvErasing (CVPR'17) (Wei et al. 2017)</cell><cell>VGG-16</cell><cell>L</cell><cell>-</cell><cell>×</cell><cell>55.0</cell><cell>55.7</cell></row><row><cell>AffinityNet (CVPR'18) (Ahn and Kwak 2018)</cell><cell>VGG-16</cell><cell>L</cell><cell>-</cell><cell>×</cell><cell>58.4</cell><cell>60.5</cell></row><row><cell>AffinityNet (CVPR'18) (Ahn and Kwak 2018)</cell><cell>ResNet-38</cell><cell>L</cell><cell>-</cell><cell>×</cell><cell>61.7</cell><cell>63.7</cell></row><row><cell>Ours-RRM-VGG (two-step)</cell><cell>VGG16</cell><cell>L</cell><cell>-</cell><cell>×</cell><cell>60.7</cell><cell>61.0</cell></row><row><cell>Ours-RRM-ResNet (two-step)</cell><cell cols="2">ResNet-101 L</cell><cell>-</cell><cell>×</cell><cell>66.3</cell><cell>66.5</cell></row><row><cell>Ours-RRM (one-step)</cell><cell>ResNet-38</cell><cell>L</cell><cell>-</cell><cell></cell><cell>62.6</cell><cell>62.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Comparison with the state-of-the-art approaches on PASCAL VOC 2012 val and test dataset. Sup.-supervision information, GT-ground truth, F-full supervision, L-image-level class label, B-bounding box label, S-scribble label.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The work was supported by National Natural Science Foundation of China under 61972323 and 61876155, and Key Program Special Fund in XJTLU under KSF-T-02, KSF-P-02, KSF-A-01, KSF-E-26.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4981" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Whats the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation using web-crawled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7322" to="7330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3203" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Selferasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="549" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to segment every thing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4233" to="4241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2083" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient relaxations for dense crfs with sparse higher-order potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="287" to="318" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parameter learning and convergent inference for dense random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="513" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10421</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep extreme cut: From extreme points to object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="616" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/15022734" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Augmented feedback in semantic segmentation under image level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="90" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepcut: Object segmentation from bounding box annotations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Passerat-Palmbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Damodaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="674" to="683" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Boxdriven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11693</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Normalized cut loss for weakly-supervised cnn segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1818" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On regularized losses for weaklysupervised cnn segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="507" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning randomwalk label propagation for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7158" to="7166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation by iteratively mining common object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7268" to="7277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ian: the individual aggregation network for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="332" to="340" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Correlation filter selection for visual tracking using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thiyagalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03196</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1084" to="1102" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-produced guidance for weakly-supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
