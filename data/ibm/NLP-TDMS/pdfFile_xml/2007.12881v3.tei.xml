<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MirrorNet: Bio-Inspired Camouflaged Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinnan</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Dayton</orgName>
								<address>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung-Nghia</forename><surname>Le</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khanh-Duy</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Vietnam National University</orgName>
								<address>
									<settlement>Ho Chi Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">University of Information Technology</orgName>
								<orgName type="institution" key="instit2">VNU-HCM</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Triet</forename><surname>Tran</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Vietnam National University</orgName>
								<address>
									<settlement>Ho Chi Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">University of Science</orgName>
								<address>
									<settlement>Ho Chi Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution" key="instit1">John von Neumann Institute</orgName>
								<orgName type="institution" key="instit2">VNU-HCM</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Dayton</orgName>
								<address>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MirrorNet: Bio-Inspired Camouflaged Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>IEEE ACCESS 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-Camouflaged Object Segmentation, Bio-Inspired Network</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Camouflaged objects are generally difficult to be detected in their natural environment even for human beings. In this paper, we propose a novel bio-inspired network, named the MirrorNet, that leverages both instance segmentation and mirror stream for the camouflaged object segmentation. Differently from existing networks for segmentation, our proposed network possesses two segmentation streams: the main stream and the mirror stream corresponding with the original image and its flipped image, respectively. The output from the mirror stream is then fused into the main stream's result for the final camouflage map to boost up the segmentation accuracy. Extensive experiments conducted on the public CAMO dataset demonstrate the effectiveness of our proposed network. Our proposed method achieves 89% in accuracy, outperforming the state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The term "camouflage" was originally used to describe the behavior of an animal or insect trying to hide itself from its surroundings to hunt or avoid being hunted <ref type="bibr" target="#b0">[1]</ref>, namely naturally camouflaged objects <ref type="bibr" target="#b1">[2]</ref>. This ability is useful to reduce risk of being detected and increase their survival probability. For example, chameleons or fishes can change appearance of their bodies to match color and pattern of surrounding environments (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Human adopted this mechanism and began to apply it widely on the battlefield. For example, soldiers and war equipment are applied the camouflage effect by dressing or coloring their appearance to blend them with their surroundings (see <ref type="figure" target="#fig_0">Fig. 1</ref>), namely artificially camouflaged objects <ref type="bibr" target="#b1">[2]</ref>. Artificial camouflage has been also applied into entertainment (e.g., magic show) or art (e.g., body painting). <ref type="figure" target="#fig_0">Figure 1</ref> shows a few examples of both naturally and artificially camouflaged objects in real life, in which these camouflaged objects are not identified obviously.</p><p>Autonomously detecting/segmenting camouflaged objects is thus a difficult task where discriminative features do not play an important role since we have to ignore objects that *Corresponding author. e-mail: tamnguyen@udayton.com  <ref type="bibr" target="#b1">[2]</ref>. From left to right, naturally camouflaged objects (e.g., fish, chameleon, insect) are followed by artificially camouflaged objects (e.g., soldier, body painting). capture our attention. While detecting camouflaged objects is technically challenging on the one hand, it is beneficial in various practical scenarios, on the other, to include surveillance systems and search-and-rescue missions. <ref type="figure" target="#fig_2">Figure 2</ref> shows several examples that camouflaged objects are failed to be detected by state-of-the-art object segmentation networks. Moreover, there are plenty of creatures in nature that have evolved over time to confuse themselves with their surroundings and even harder to be detected. Therefore, the study of detecting these less obvious objects which we call it camouflaged object in this paper, is necessary in the field which targets detecting all objects in all scenes. Note that camouflage is a very subjective concept here. An object, such as a person which is regarded as a common class in MS-COCO <ref type="bibr" target="#b3">[4]</ref>, can be considered as a camouflaged object when he is hiding himself as a sniper. In other words, we treat all objects that are too similar to their surroundings because of their color, texture, or both as camouflaged object and its complement set as non-camouflaged object. It is time-consuming and laborious to collect data from all target objects in particular scenes which presents camouflage to improve models, and it is not well adaptable. Moreover, what camouflage has in common is that its features are very little different from its surroundings. Therefore, it is reasonable to treat all objects of different classes but similar to the background as one class.</p><p>Most state-of-the-art Convolutional Neural Network (CNN)based models [5]- <ref type="bibr" target="#b6">[7]</ref> simulate human brain. Therefore, the CNN-based models may also be fooled just like human <ref type="bibr" target="#b7">[8]</ref>.   <ref type="bibr" target="#b1">[2]</ref>, and ANet-SRM <ref type="bibr" target="#b1">[2]</ref>, and the instance segmentation results of Mask R-CNN <ref type="bibr" target="#b2">[3]</ref>. We remark that all these methods are fine-tuned on the camouflage dataset <ref type="bibr" target="#b1">[2]</ref>. This is also true for camouflaged objects, which attempt to fool our human visual perception. Through time, they evolve to well blend their appearance to the background. On the other hand, color-blind people are often better at detecting camouflaged objects <ref type="bibr" target="#b8">[9]</ref>. This is perhaps because they rely less on colors and more on form and texture to discern the world around them. This motivates us to look at other bio-inspired features.</p><p>An object becomes a successful camouflaged object when it elegantly blends into the surrounding environment to create a familiar natural scene that can hide the object. By changing the viewpoint of the same scene, we expect to have a possibility to escape from such an illusion. This visual-psychological phenomenon motivates our bio-inspired solution to detect camouflaged objects by changing the viewpoint of the scene. We realize that just a simple flipping operation also can generate new views of the same scene. Indeed, the flipped images accidentally break the natural layout which leads to the much difference between the background and the camouflaged objects.</p><p>Inspired by above visual-psychological phenomenon, we are looking at a method to break the capability to prevent human/machine vision from recognizing camouflaged objects through changing the viewpoint. In this paper, we propose a new simple yet efficient bio-inspired framework, which greatly improves the existing camouflage object recognition performance. The framework leverages the advantages of cutting-edge models <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref> , which are well trained on largescale datasets like ImageNet <ref type="bibr" target="#b10">[11]</ref> and MS-COCO <ref type="bibr" target="#b3">[4]</ref> datasets, making the performance of this framework surpass ANet <ref type="bibr" target="#b1">[2]</ref> which are the state-of-the-art camouflaged object segmentation. The proposed framework consists of two streams, the main stream for segmenting original image and the mirror stream utilizing flipped image, whereas the flipped image stream exploits the bio-inspired effort to break the natural settings of camouflaged objects. Indeed, viewpoint change by flipping operation is useful to escape from the easy-to-befooled appearance of camouflaged objects. Leveraging this simple but effective approach can detect images' mapping to discover more details, to further improve the performance. To solve the insufficiency of limited training data, we also utilize object-level flipping for natural data augmentation for network training. Extensive experiments on the benchmark CAMO dataset <ref type="bibr" target="#b1">[2]</ref> for camouflaged object segmentation show the superiority of our proposed method over the state-of-thearts. The code and results will be published on our website 1,2 .</p><p>Our contributions are as follows.</p><p>• Differently from the state-of-the-art ANet <ref type="bibr" target="#b1">[2]</ref>, where classification stream and segmentation stream are applied on the whole image, in this work, we first obtain object proposal, then we apply segmentation on each object proposal. • As an effort to break the natural setting of camouflaged objects, we integrate additional mirror stream which utilizes horizontally flipped images, for mirror detection to support the main stream. We introduce the Data Augmentation in the Wild to solve the data insufficiency.</p><p>• Last but not least, we conduct extensive experiments to demonstrate the performance of the proposed framework. The rest of this paper is organized as follows. Section II first reviews related work. Section III briefly introduces the motivation and discusses details the proposed framework. Section IV reports the experimental results and the ablation study. Finally, Section V draws the conclusion and paves way to the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In literature, the mainstreams of the computer vision community mainly focus on the detection/segmentation of the non-camouflaged objects, i.e., salient object, objects with predefined classes. There exist many research works on both general object detection <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref>, and salient object segmentation <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b25">[26]</ref>. Meanwhile, the camouflage object recognition has not been well explored in the literature. Early works related to camouflage are dedicated to detecting the foreground region even when some of their texture is similar to the background <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>. These works distinguish foreground and background based on simple features such as color, intensity, shape, orientation, and edge. A few methods based on handcrafted low-level features (i.e., texture <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref> and motion <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>) are proposed to tackle the problem of camouflage detection. However, all these methods work for only a few cases of simple and non-uniform background, thus their performances are unsatisfactory in camouflaged object segmentation due to strong similarity between the foreground and the background. Recently, Le et al. <ref type="bibr" target="#b1">[2]</ref> proposed an endto-end network, called ANet, for camouflaged object segmentation through integrating classification information into segmentation. Fan et al. <ref type="bibr" target="#b34">[35]</ref> introduced SINet with two main modules, namely the search module S and the identification module I. Recently, Le et al. <ref type="bibr" target="#b35">[36]</ref> explored the camouflaged instance segmentation by training Mask RCNN [3] on CAMO dataset <ref type="bibr" target="#b1">[2]</ref>.</p><p>Differently from existing work, which only interferes the training process, in this work, we also utilize bio-inspired mirror-stream in the inference process to guide the network towards the right track for camouflage object segmentation. We remark that our mirror-stream and image flipping augmentation of existing work are different for the purpose of usage and philosophy. For data augmentation, it applies image flipping in only the training phase to improve the performance of networks. It means that they consider the original image and its flipped image as two totally different images. They want to learn as much as possible cases that can appear. Meanwhile, our flip-stream is independent from image flipping augmentation. We apply flipped images together with the original images in the inference phase. It means that we consider the normal image and the flipped image as twofaces of the original image. Besides solving the standard case (normal image), we also want to solve the rare case (flipped image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Non-Camo Camo Camo-Flipped Color (RGB) 0.112 0.085 0.085 Color (lαβ) 0.217 0.143 0.143 Texton <ref type="bibr" target="#b41">[42]</ref> 0.323 0.195 0.195 FCN (32s) <ref type="bibr" target="#b39">[40]</ref> 0.520 0.352 0.354 FCN (8s) <ref type="bibr" target="#b39">[40]</ref> 0.641 0.409 0.412 CRF-RNN <ref type="bibr" target="#b42">[43]</ref> 0.786 0.460 0.462 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Bio-inspired Motivation</head><p>In biological vision studies, there exist viewpoint-invariant theories and viewpoint-dependent theories <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>. In viewpoint-invariant theories, once a particular object has been stored, the recognition of that object from novel viewpoints should be unaffected, provided that the necessary features can be recovered from that view. In view-dependent theories, once a particular object has been stored, recognition of that object from novel views may be impaired, relative to recognition of previously stored views. However, these theories were formed from the simple setting with the outstanding stimuli (noncamouflaged objects) and the clear background, i.e., white background. Indeed, it is natural for human vision to easily detect non-camouflaged objects such as salient objects since they are outstanding from the background, while it is harder for human to detect camouflaged objects since they are similar to the background. In other words, the visual difference between the background and the non-camouflaged object(s) should be larger than the one between the background and the camouflaged object(s). However, when we flip the images, humans are not fooled by the natural settings and notice differences in the images. Therefore, the flipped images accidentally break the natural layout which leads to the larger distance between the background and the camouflaged objects.</p><p>To prove this conclusion intuitively, we compute the visual difference between the main objects (the camouflaged and the non-camouflaged objects) with the background. In particular, we adopt the well-known deep learning model FCN <ref type="bibr" target="#b39">[40]</ref> and its variants to compute the difference distance.</p><p>We use the pretrained model on PASCAL-VOC <ref type="bibr" target="#b40">[41]</ref> (with 20 semantic classes and the background class). We extract the confidence score at the second last layer of FCN (h × w × 21). Then, we compute the mean semantic vectors s bg , s f g for the background and the camouflaged/non-camouflaged object regions (obtained from the ground truth maps), respectively. This ends up with 21-dim vector for both regions. 2 normalization is applied for both s bg and s f g . Then the visual difference d between the main objects (the camouflaged/noncamouflaged objects) with the background is computed as:</p><formula xml:id="formula_0">d = s f g − s bg 2 , where . 2 is the Euclidean distance.</formula><p>We also compute the difference between the camo/non-camo object with the background in terms of color (RGB and lαβ), and texture (texton <ref type="bibr" target="#b41">[42]</ref>). <ref type="table" target="#tab_0">Table I</ref> shows the visual difference with different settings. The difference in terms of color is much smaller than the ones of features extracted from deep learning models. lαβ shows a larger distance than RGB in the color space. Meanwhile, the textural features, i.e., Texton <ref type="bibr" target="#b41">[42]</ref>, yield the larger distance which indicates the usefulness of texture in the task of camouflaged object segmentation. The even larger distances in feature space show that features from deep learning models may be helpful to detect and segment camouflaged objects. Note that the features extracted from FCN, a CNN-based model, actually capture the information of textures and edges. In fact, FCN and CNN networks contain convolutional layers which learn spatial hierarchies of patterns by preserving spatial relationships. In these networks, the first convolutional layer can learn basic elements such as texture and edges <ref type="bibr" target="#b43">[44]</ref>. Then, the second convolutional layer can learn patterns composed of basic elements learned in the previous layer. The training process continues until the deep network learns significantly complex patterns and abstract visual concepts. This demonstrates the importance of low-level information such as texture and edges in the task of camouflaged object segmentation.</p><p>In addition, the difference values of camo and camo-flip with the background are identical in terms of color. It shows that the flipped stream is not useful in terms of color. However, when we look at the difference in the feature space, the camo and non-camo values are different (non-camo value is slightly better). It shows that the camo flip in the feature space may be helpful for the task.</p><p>As a closer look, the distance between main object and background is larger in non-camouflaged and smaller in camouflaged images in all FCN and CRF-RNN settings. In camouflaged images, most pixels of camouflaged objects are classified as background class which leads to the small distance. Meanwhile, most pixels of the main non-camouflaged objects are classified with non-background classes which cause the larger distance. We are also interested in the horizontally flipped images. Therefore, we also compute the visual difference between the background and the camouflaged objects in the horizontally flipped images. As shown in <ref type="table" target="#tab_0">Table I</ref>, the camouflaged-flipped images generally produce larger distance. It seems that the horizontally flipped images may contain useful information for the segmentation task. Note that the images of camouflaged objects are taken with the natural settings. Therefore, the flipped images accidentally break the natural layout which lead to the larger distance between the background and the camouflaged objects. This one absolutely paves way to our proposed method.</p><p>The camouflaged object contains intrinsic and extrinsic information. The intrinsic information means the difference between the camouflaged object and the background. The small distance may be observed in the color space. The distance may be larger in another space, i.e., deep learned feature space, semantic space. Meanwhile, the extrinsic information means the external change onto the camouflaged object, for example, rotating, translating, and flipping, so that the camouflaged object can be better recognized. Therefore, our proposed framework captures both intrinsic and extrinsic information of the camouflaged object. <ref type="figure" target="#fig_3">Figure 3</ref> depicts the overview of our proposed MirrorNet, the bio-inspired network for camouflaged object segmentation. The MirrorNet actually consists of two streams, the main stream for original image segmentation and the mirror stream for flipped image segmentation, whereas the horizontally flipped image stream exploits the bio-inspired effort to break the natural setting of camouflaged objects. Each stream traverses through the camouflaged object proposal, camouflaged object segmentation and yields the segmentation masks. The two output masks from the two streams are finally fused to produce a pixel-wisely accurate camouflage map which uniformly covers camouflaged objects. Here, the camouflaged object segmentation targets at the intrinsic information. Meanwhile, the mirror stream aims to capture the extrinsic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MirrorNet Overview</head><p>C. Network Design and Architecture 1) Camouflaged Object Proposal: MirrorNet first attempts to localize the possible positions containing the camouflaged objects via Camouflaged Object Proposal component. Inspired by <ref type="bibr" target="#b13">[14]</ref>, we aim to find the object positions, the object classes, and their camouflage masks in images simultaneously. Follow the standard design in computer vision, the object position is defined by a rectangle with respect to the top-left corner of the image; the object class is defined over the rectangle; the object mask is encoded at every pixel inside the rectangle. Ideally, we want to detect all relevant objects in the image and map each pixel in these objects to its most probable camouflage/noncamouflage label. Here, our Camouflaged Object Proposal component adopts the Region Proposal Network (RPN) <ref type="bibr" target="#b13">[14]</ref>. RPN shares weights with the main convolutional backbone and outputs bounding boxes (RoI / object proposal) at various sizes. For each RoI, a fixed-size feature map is pooled from the image feature map using the RoIPool layer <ref type="bibr" target="#b13">[14]</ref>. The RoIPool layer works by dividing the RoI into a regular grid and then max-pooling the feature map values in each grid cell. This quantization, however, causes misalignments between the RoI and the extracted features due to the harsh rounding operations when mapping the RoI coordinates from the input image space to the image feature map space and when dividing the RoI into grid cells. In order to address this problem, our Camouflaged Object Proposal component integrates Precise RoI Pooling (PrRoI) <ref type="bibr" target="#b44">[45]</ref>. Indeed PrRoI Pooling uses average pooling instead of max pooling for each bin and has a continuous gradient on bounding box coordinates. That is, one can take the derivatives of some loss function with respect to the coordinates of each RoI and optimize the RoI coordinates. PrRoI Pooling is also different from the RoI Align proposed in Mask R-CNN. PrRoI Pooling uses a full integration-based average pooling instead of sampling a constant number of points. This makes the gradient with respect to the coordinates continuous.</p><p>2) Camouflaged Object Segmentation: Following <ref type="bibr" target="#b13">[14]</ref>, we use a multi-task loss L to jointly train the bounding box class, the bounding box position, and the camouflage map (mask) as follows:</p><formula xml:id="formula_1">L = L cls + L loc + L mask ,<label>(1)</label></formula><p>where L cls and L loc are the output of the detection branch. Meanwhile, L mask is defined on the output of the segmentation branch. The object classification loss L cls (p, u) is the multinomial cross entropy loss computed as follows:</p><formula xml:id="formula_2">L cls (p, u) = − log p u ,<label>(2)</label></formula><p>where p u is the softmax output for the true class u. The bounding box regression loss L loc (t u , v) is computed as the Smooth L1 loss <ref type="bibr" target="#b12">[13]</ref> between the regressed box offset t u (corresponding to the ground-truth object class u) and the ground-truth box offset v:</p><formula xml:id="formula_3">L loc (t u , v) = i∈{x,y,w,h} Smooth L1 (t u i − v i )<label>(3)</label></formula><p>The segmentation loss L mask (m, s) is the multinomial cross entropy loss computed as follows:</p><formula xml:id="formula_4">L mask (m, s) = −1 N i∈RoI log m i si ,<label>(4)</label></formula><p>where m i si is the softmax output at pixel i for the true label s i , N is the number of pixels in the RoI.</p><p>3) Mask Fusion: We first flip all detected bounding boxes and their corresponding camouflage map (mask) from the mirror stream. Then, we use threshold θ = 0.5 to eliminate bounding boxes with low prediction scores. After that, the prediction scores of bounding boxes from the two streams are sorted in descending order. Then, we apply the "winner take all" strategy to prune the redundant boxes. In particular, for each bounding box from highest prediction scores to lowest scores, we check the bounding boxes with lower scores, if the lower score bounding box and the higher score bounding box have 50% mutual overlap, the bounding box with lower score will be excluded. Then, we discard the bounding boxes (and their corresponding camouflage maps) classified as noncamouflage.</p><p>Finally we accumulate the camouflage maps (masks) from the retaining bounding boxes and then normalize the output, resulting the final camouflage map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Augmentation in the Wild</head><p>Since non-camouflaged object segmentation attracts most attention compared to camouflaged object segmentation, there are only a few relevant datasets, and most of them have the problem of too few samples. Therefore, we adopt the recently built CAMO dataset <ref type="bibr" target="#b1">[2]</ref> which proposed and benchmarked by the previous state-of-the-art camouflaged object segmentation, for the training of instance segmentation framework.</p><p>The dataset is divided into camouflage and non-camouflage categories, each containing 1,000 training and 250 test set, and a total of 2,500 manually annotated ground truths. Most of the dataset images are mammals, insects, birds, and aquatic animals, each with approximately similar proportions, and a small number of reptiles, human art, soldiers, and amphibians. The diversity of species in this dataset makes our model very adaptable, but it must be pointed out that it also has insufficient samples compared to mainstream datasets like COCO <ref type="bibr" target="#b3">[4]</ref>.</p><p>We first compute the number of connected components on the binary ground truth maps of CAMO dataset (the training part). There are many scenarios where multiple connected components belong to one instance. To avoid cherry pick the training samples, we exclude all training images with more than 2 connected components. Then, we compute the bounding box for the single component in the images. In order to increase the number of training samples, we introduce the Augmentation in Wild. In particular, we first perform translating and flipping instances. Furthermore, it is essentially different from data augmentation for non-camouflaged objects, because to determinate whether an object is camouflaged is not only depends on its own features, but also its surroundings. Therefore, we also clone the object instances and place them onto different image regions with a small color difference in the background. <ref type="figure" target="#fig_4">Figure 4</ref> shows some samples of augmented data. In this way, we increase the number of training samples, thus alleviating the problem of insufficient data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Details</head><p>To demonstrate the generality and flexibility of our proposed MirrorNet, we employ various recent state-of-the-art backbones, i.e., ResNet <ref type="bibr" target="#b9">[10]</ref> and ResNeXt <ref type="bibr" target="#b4">[5]</ref>. Our implementation is based on the published code of Mask R-CNN 3 , which can be adopted to train camouflaged object proposal and camouflaged object segmentation components. Furthermore, we replace the ROI Align layer with a Precise RoI Pooling layer (PrRoI) <ref type="bibr" target="#b44">[45]</ref>.</p><p>The training process is conducted by fine-tuning the available pre-trained model on camouflage images and noncamouflage images of the CAMO dataset <ref type="bibr" target="#b1">[2]</ref> and our Data Augmentation in the Wild. In particular, we set the size of each mini-batch to 256 and used the Stochastic Gradient Descent (SGD) optimization with a moment β = 0.9 and a weight decay of 0.0001. We trained our network for 120k iterations with the base learning rate of 0.00125, which is decreased by 10 times every time we reach the next steps at 80k iterations and 100k iterations. We also remark that we implemented our method in PyTorch, and conducted all the experiments on a computer with a 2.40GHz processor (Intel Xeon CPU E5-2620), 64 GB of RAM, and one GeForce GTX TITAN X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first introduce dataset and evaluation criteria used in experiments. We compare our MirrorNet 3 https://github.com/facebookresearch/maskrcnn-benchmark with state-of-the-art methods on the CAMO dataset <ref type="bibr" target="#b1">[2]</ref>, to demonstrate that the instance segmentation and the mirror stream can boost up the camouflaged object segmentation. We also present the efficiency of our general MirrorNet through the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Benchmark Dataset and Evaluation Criteria</head><p>We used the entire the testing images in the CAMO dataset for the evaluation. We note that the zero-mask ground-truth labels (all pixels have zero values) are for the non-camouflaged object images.</p><p>Similarly to <ref type="bibr" target="#b1">[2]</ref>, we used the F-measure (F β ) <ref type="bibr" target="#b45">[46]</ref>, Intersection Over Union (IOU) <ref type="bibr" target="#b39">[40]</ref>, and Mean Absolute Error (MAE) as the metrics to evaluate obtained results. The first metric, Fmeasure, is a balanced measurement between precision and recall as follows:</p><formula xml:id="formula_5">F β = 1 + β 2 P recision × Recall β 2 × P recision + Recall .<label>(5)</label></formula><p>Note that we set β 2 = 0.3 as used in <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b45">[46]</ref> to put an emphasis on precision. IOU is the area ratio of the overlapping against the union between the predicted camouflage map and the ground-truth map. Meanwhile, MAE is the average of the pixel-wise absolute differences between the predicted camouflage map and the ground-truth. For MAE, we used the raw grayscale camouflage map. For the other metrics, we binarized the results depending on two contexts. In the first context, we assume that camouflaged objects are always present in every image like salient objects; we used an adaptive threshold <ref type="bibr" target="#b46">[47]</ref> θ = µ + η where µ and η are the mean value and the standard deviation of the map, respectively. In the second context which is much closer to a real-world scenario, we assume that the existence of camouflaged objects is not guaranteed in each image; we used the fixed threshold θ = 0.5.</p><p>As proposed in <ref type="bibr" target="#b34">[35]</ref>, we also use E-measure (E φ ) <ref type="bibr" target="#b47">[48]</ref>, S-measure (S α ) <ref type="bibr" target="#b48">[49]</ref>, and weighted F-measure (F ω β ) <ref type="bibr" target="#b49">[50]</ref> as alternative metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with FCN and ANet Variants</head><p>We conduct the experiments on CAMO dataset with two settings, namely, only camouflaged object set, and the full set. To investigate the impact of different backbones on our proposed MirrorNet, we fine-tuned pre-trained models of ResNet (ResNet-50, ResNet-101) <ref type="bibr" target="#b9">[10]</ref>, and ResNeXt (ResNeXt-101, ResNeXt-152) <ref type="bibr" target="#b4">[5]</ref> on CAMO respectively. It is interesting for the computer vision community to see the performance of different frameworks in the camouflaged object segmentation problem. Particularly, we compare our MirrorNet with ANet family networks <ref type="bibr" target="#b1">[2]</ref> (denoted by ANet-DHS, ANet-DSS, ANet-SRM, ANet-WSS), the-state-of-the-art for camouflaged object segmentation, and the original FCNs (DHS <ref type="bibr" target="#b50">[51]</ref>, DSS <ref type="bibr" target="#b51">[52]</ref>, SRM <ref type="bibr" target="#b15">[16]</ref>, and WSS <ref type="bibr" target="#b52">[53]</ref>). All these methods were also fine-tuned on the CAMO dataset with their published parameters.</p><p>As shown in <ref type="table" target="#tab_0">Table II</ref>, ANet variants outperform FCNfinetuned methods and ANet variants with the same backbone. II: Comparison with FCN and ANet variants on two settings: Only Camouflaged Images (the left part), and the full CAMO dataset (the right part). The evaluation is based on F-measure <ref type="bibr" target="#b45">[46]</ref> (the higher the better), IOU <ref type="bibr" target="#b39">[40]</ref> (the higher the better), and MAE (the lower the better). The 1st and 2nd places are shown in blue and red, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset in test</head><p>Only Camouflaged Images Full CAMO dataset Groups Method Adaptive Threshold Fixed Threshold Adaptive Threshold Fixed Threshold   <ref type="bibr" target="#b45">[46]</ref> (the higher the better), IOU <ref type="bibr" target="#b39">[40]</ref> (the higher the better), and MAE (the lower the better). The best results are shown in blue. The detail of each baseline is clearly introduced in Section IV-C. Meanwhile, our proposed MirrorNet significantly outperforms all baselines. MirrorNet achieves the best result with ResNeXt-152 backbone. This is totally consistent with our discussion in Section III-A, i.e., the better backbones tend to work better in MirrorNet. Note that all baselines, i.e., FCN-finetuned or ANet variants are trained and tested separately on different sets of data, for example, training and testing on only camouflaged image sets, or training and testing on the full set. Meanwhile, our MirrorNet and its variants are only trained on the full set. However, their performance is also significantly better in the only camouflaged image set.</p><formula xml:id="formula_6">MAE ⇓ F β ⇑ IOU ⇑ F β ⇑ IOU ⇑ MAE ⇓ F β ⇑ IOU ⇑ F β ⇑ IOU ⇑ FCN-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>In this subsection, we investigate the impact of different components in our proposed MirrorNet such as dual stream, data augmentation, and RoI pooling mechanisms. Experimental results are shown in <ref type="table" target="#tab_0">Table III</ref>. We remark that we use the ResNeXt-152 backbone for all compared methods.</p><p>Dual Stream: We investigate the performance of each stream in the MirrorNet, namely, main stream and mirror stream. We compare our completed MirrorNet, which have both streams, with using single streams (denoted by Baseline 1 for only the main stream and Baseline 2 for only the mirror stream). As shown in <ref type="table" target="#tab_0">Table III</ref>, our completed MirrorNet outperforms baselines using individual stream. In addition, the mirror stream output is better than the main stream output alone.</p><p>Data Augmentation in the Wild: We suspected that one of the major factors affecting the camouflaged objects segmentation was the limited number of training data. To evaluate the impact of Data Augmentation in the Wild, we compare MirrorNet without using Data Augmentation in the Wild in the training process, denoted by Baseline 3. <ref type="table" target="#tab_0">Table III</ref> shows that using Data Augmentation in the Wild in the training process outperforms the one without using the augmented data.</p><p>ROI Pooling: We further investigate the performance of different ROI pooling mechanism, namely, RoI Align (denoted by Baseline 4) and PrROI. As also seen in <ref type="table" target="#tab_0">Table III</ref>, PrRoIbased MirrorNet surpasses RoI Align-based MirrorNet. In addition, RoI Align-based MirrorNet outperforms the main stream and the mirror stream (with PrRoI). This clearly shows the importance of the mask fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with State-of-the-art Methods</head><p>We further compare the performance of MirrorNet with state-of-the-arts reported in <ref type="bibr" target="#b34">[35]</ref>. <ref type="table" target="#tab_0">Table IV</ref> shows the performance of different methods in terms of E-measure (E φ ) <ref type="bibr" target="#b47">[48]</ref>, S-measure (S α ) <ref type="bibr" target="#b48">[49]</ref>, weighted F-measure (F ω β ) <ref type="bibr" target="#b49">[50]</ref>, and MAE <ref type="bibr" target="#b1">[2]</ref>. Note that these metrics were proposed in <ref type="bibr" target="#b34">[35]</ref>. As can be seen, the recently proposed methods tend to achieve better results. Our proposed method, MirrorNet, achieves   <ref type="bibr" target="#b47">[48]</ref> (the higher the better), Smeasure (S α ) <ref type="bibr" target="#b48">[49]</ref> (the higher the better), weighted F-measure (F ω β ) <ref type="bibr" target="#b49">[50]</ref> (the higher the better), and MAE (the smaller the better). The best results are shown in blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Year the best performance in terms of E φ , S α , F ω β , and MAE. MirrorNet with ResNeXt-152 backbone surpasses the stateof-the-art methods by a remarkable margin in all metrics. <ref type="figure" target="#fig_5">Figure 5</ref> shows the visual comparison of different methods. As illustrated in the figure, our MirrorNet variants yield better results than all state-of-the-art methods. Our results are close to the ground truth and focus on the camouflaged objects. From the results, our MirrorNet successfully segments the camouflaged objects images in CAMO dataset <ref type="bibr" target="#b1">[2]</ref>. As discussed in <ref type="bibr" target="#b1">[2]</ref>, CAMO dataset consists of images from challenging scenarios such as object appearance (the camouflaged object has similar color and texture with the background), background clutter, shape complexity, small object, object occlusion, multiple objects, and distraction. This demonstrates that our MirrorNet can well handle a variety of camouflaged objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a simple and flexible end-to-end network, namely MirrorNet, for camouflaged object segmentation. Our bio-inspired MirrorNet leverages both instance segmentation and mirror stream to segment camouflaged objects in images. Particularly, we propose to use the main stream and the mirror stream, which is embedded image flipping, to effectively capture different layouts of the scene, leading to significant performance in identifying camouflaged objects. The extensive experimental results show that our proposed method achieves state-of-the-art performance on the public CAMO dataset.</p><p>In the future, we will investigate different adversarial schemes. In addition, we aim to further explore the problem of camouflaged instance segmentation. Through experiments, our data augmentation slightly improves the performance as shown in the ablation study. Note that data augmentation is not the main focus of this paper. Exploring different data augmentation methods is an interesting topic which is left for future work. We are also interested in applying camouflaged object segmentation into practical systems <ref type="bibr" target="#b58">[59]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>A few examples of camouflaged objects in CAMO dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>arXiv:2007.12881v3 [cs.CV] 11 Mar 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Failure examples of the state-of-the-art method on camouflaged object segmentation. From top to bottom: the original images, the ground-truth maps, the camouflage maps of ANet-DHS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>The overview of our proposed framework. MirrorNet consists of two streams, namely, the main stream for original image segmentation and the mirror stream for horizontally flipped image segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Exemplary samples of Data Augmentation in the Wild. From top to bottom: the original images with the ground-truth map, the flipped images, the cloning instance images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of the results from different methods. From left to right: the original image, the ground truth map, the predicted camouflaged maps of ANet-SRM<ref type="bibr" target="#b1">[2]</ref>, SINet<ref type="bibr" target="#b34">[35]</ref>, and our methods MirrorNet (ResNet-101), MirrorNet (ResNeXt-101), MirrorNet (ResNeXt-152).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>The visual difference between the camouflaged/noncamouflaged objects and the background with different methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Ablation study results on the full CAMO dataset. The evaluation is based on F-measure</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison of state-of-the-art methods on CAMO dataset (Camouflaged object only). The evaluation is based on E-measure (E φ )</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sites.google.com/view/ltnghia/research/camo 2 https://sites.google.com/site/vantam/research</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This research is in part granted by Vingroup Innovation Foundation (VINIF) in project code VINIF.2019.DA19, National Science Foundation (NSF) under Grant No. 2025234, and JSPS KAKENHI Grant No. 20K23355. We gratefully acknowledge NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Survey of object detection methods in camouflaged image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dhawale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of IERI Procedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="351" to="357" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Anabranch network for camouflaged object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page" from="45" to="56" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Alcorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Outsmarting the art of camouflage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prasad</surname></persName>
		</author>
		<ptr target="http://blogs.discovermagazine.com/crux/2016/11/02/outsmarting-the-art-of-camouflage/" />
	</analytic>
	<monogr>
		<title level="j">Discover Magazine</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention r-cnn for accident detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pyramid feature attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3085" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic instance meets salient object: Study on video semantic salient instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1779" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Salient object detection via augmented hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sepulveda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2176" to="2182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contrast based hierarchical spatial-temporal saliency for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Rim Symposium on Image and Video Technology</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="734" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic prior analysis for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3130" to="3141" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeply supervised 3d recurrent fcn for salient object detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video salient object detection using spatiotemporal deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5002" to="5015" />
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Texture segmentation by multiscale aggregation of filter responses and shape elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003-10" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new camouflage texture evaluation method based on wssim and nature image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Technology</title>
		<imprint>
			<date type="published" when="2010-10" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Camouflage performance analysis and evaluation framework based on features fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4065" to="4082" />
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Study on the camouflaged target detection method based on 3d convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modern Applied Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">152</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Foreground object detection using topdown information based on em framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4204" to="4217" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Performance of decamouflaging through exploratory image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W P</forename><surname>Sengottuvelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shanmugam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Emerging Trends in Engineering and Technology</title>
		<imprint>
			<date type="published" when="2008-07" />
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detection of the mobile object with camouflage color under dynamic background based on optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2201" to="2205" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Supplement C</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Foreground object segmentation for moving camera sequences based on foreground-background probabilistic models and prior probability maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bertolino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="3312" to="3316" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Camouflaged object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Camoufinder: Finding camouflaged instances in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">When does the visual system use viewpoint-invariant representations during recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Farah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Brain Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="399" to="415" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Viewpoint-invariant and viewpointdependent object recognition in dissociable neural subsystems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burghund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marsolek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin and Review</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="480" to="489" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A computational model that recovers the 3d shape of an object from a single 2d retinal representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pizlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Steinman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="979" to="991" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="23" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="816" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Category-independent object-level saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1761" to="1768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Enhancedalignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="698" to="704" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unet++: A nested u-net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6409" to="6418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A simple poolingbased design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attentive systems: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="110" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
