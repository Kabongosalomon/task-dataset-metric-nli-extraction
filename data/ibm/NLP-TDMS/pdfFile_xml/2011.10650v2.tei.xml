<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 VERY DEEP VAES GENERALIZE AUTOREGRESSIVE MODELS AND CAN OUTPERFORM THEM ON IMAGES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-16">16 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><forename type="middle">Child</forename><surname>Openai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>CA</roleName><forename type="first">San</forename><surname>Francisco</surname></persName>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 VERY DEEP VAES GENERALIZE AUTOREGRESSIVE MODELS AND CAN OUTPERFORM THEM ON IMAGES</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-16">16 Mar 2021</date>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a hierarchical VAE that, for the first time, generates samples quickly and outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in loglikelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low resolution</head><p>High resolution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Selected samples from our very deep VAE on FFHQ-256, and a demonstration of the learned generative process. VAEs can learn to first generate global features at low resolution, then fill in local details in parallel at higher resolutions. When made sufficiently deep, this learned, parallel, multiscale generative procedure attains a higher log-likelihood than the PixelCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>One potential path to increased data-efficiency, generalization, and robustness of machine learning methods is to train generative models. These models can learn useful representations without human supervision by learning to create examples of the data itself. Many types of generative models have flourished in recent years, including likelihood-based generative models, which include autoregressive models <ref type="bibr">(Uria et al., 2013)</ref>, variational autoencoders (VAEs) <ref type="bibr" target="#b17">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b29">Rezende et al., 2014)</ref>, and invertible flows <ref type="bibr" target="#b8">(Dinh et al., 2014;</ref><ref type="bibr" target="#b9">2016)</ref>. Their objective, the negative log-likelihood, is equivalent to the KL divergence between the data distribution and the model distribution. A wide variety of models can be compared and assessed along this criteria, which corresponds to how well they fit the data in an information-theoretic sense.</p><p>Starting with the PixelCNN (Van den <ref type="bibr" target="#b24">Oord et al., 2016)</ref>, autoregressive models have long achieved the highest log-likelihoods across many modalities, despite counterintuitive modeling assumptions. For example, although natural images are observations of latent scenes, autoregressive models learn dependencies solely between observed variables. That process can require complex function approximators that integrate long-range dependencies <ref type="bibr" target="#b24">(Oord et al., 2016;</ref>. In contrast, VAEs and invertible flows incorporate latent variables and can thus, in principle, learn a simpler model that mirrors how images are actually generated. Despite this theoretical advantage, on the landmark ImageNet density estimation benchmark, the Gated PixelCNN still achieves higher likelihoods than all flows and VAEs, corresponding to a better fit with the data.</p><p>Is the autoregressive modeling assumption actually a better inductive bias for images, or can VAEs, sufficiently improved, outperform autoregressive models? The answer has significant practical stakes, because large, compute-intensive autoregressive models <ref type="bibr">(Strubell et al., 2019)</ref> are increasingly used for a variety of applications <ref type="bibr" target="#b24">(Oord et al., 2016;</ref><ref type="bibr" target="#b0">Brown et al., 2020;</ref><ref type="bibr" target="#b1">Chen et al., 2020)</ref>. Unlike autoregressive models, latent variable models only need to learn dependencies between latent and observed variables; such models can not only support faster synthesis and higher-dimensional data, but may also do so using smaller, less powerful architectures.</p><p>We start this work with a simple but (to the best of our knowledge) unstated observation: hierarchical VAEs should be able to at least match autoregressive models, because autoregressive models are equivalent to VAEs with a powerful prior and restricted approximate posterior (which merely outputs observed variables). In the worst case, VAEs should be able to replicate the functionality of autoregressive models; in the best case, they should be able to learn better latent representations, possibly with much fewer layers, if such representations exist.</p><p>We formalize this observation in Section 3, showing it is only true for VAEs with more stochastic layers than previous work has explored. Then we experimentally test it on competitive natural image benchmarks. Our contributions are the following:</p><p>• We provide theoretical justification for why greater depth (up to the data dimension D, but also as low as some value K D) could improve VAE performance (Section 3)</p><p>• We introduce an architecture capable of scaling past 70 layers, when previous work explored at most 30 (Section 4)</p><p>• We verify that depth, independent of model capacity, improves log-likelihood, and allows VAEs to outperform the PixelCNN on all benchmarks (Section 5.1)</p><p>• Compared to the PixelCNN, we show the model also uses fewer parameters, generates samples thousands of times more quickly, and can be scaled to larger images. We show evidence these qualities may emerge from the model learning an efficient hierarchical representation of images (Section 5.2)</p><p>• We release code and models at https://github.com/openai/vdvae.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>We review prior work and introduce some of the basic terminology used in the field.  <ref type="figure">Figure 2</ref>: Different possible learned generative models in a VAE. Left: A hierarchical VAE can learn an autoregressive model by using the deterministic identity function as an encoder, and learning the autoregression in the prior. Right: Learning the encoder can lead to efficient hierarchies of latent variables (black). If the bottom group of three latent variables is conditionally independent given the first, they can be generated in parallel within a single layer, potentially leading to faster sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">VARIATIONAL AUTOENCODERS</head><p>Variational autoencoders <ref type="bibr" target="#b17">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b29">Rezende et al., 2014)</ref> consist of a generator p θ (x|z), a prior p θ (z), and an approximate posterior q φ (z|x). Neural networks φ and θ are trained end-to-end with backpropagation and the reparameterization trick in order to maximize the evidence lower bound (ELBO):</p><formula xml:id="formula_0">log p θ (x) ≥ E z∼q φ (z|x) log p θ (x|z) − D KL [q φ (z|x)||p θ (z)]<label>(1)</label></formula><p>See <ref type="bibr" target="#b18">Kingma &amp; Welling (2019)</ref> for an in-depth introduction. There are many choices for what networks are used for p θ (x|z), q φ (z|x), and whether p θ (z) is also learned or set to a simple distribution.</p><p>We study VAEs with independent p θ (x|z) -that is, where each observed x i is output without conditioning on any other x j . This ensures generation time does not increase linearly with the dimensionality of the data, and requires that these VAEs learn to incorporate the complexity of the data into a rich distribution over latent variables z. It is possible to have autoregressive p θ (x|z) <ref type="bibr" target="#b10">(Gulrajani et al., 2016)</ref>, but generation is slow for these models. They also sometimes ignore latent variables entirely, becoming equivalent to normal autoregressive models ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">HIERARCHICAL VARIATIONAL AUTOENCODERS</head><p>Much of the early work on VAEs incorporate fully-factorized Gaussian q φ (z|x) and p θ (z). This can lead to poor outcomes if the latent variables required for good generation take on a more complex distribution, as is common with independent p θ (x|z). One of the simplest methods of gaining greater expressivity in both distributions is to use a hierarchical VAE, which has several stochastic layers of latent variables. These variables are emitted in groups z 0 , z 1 , ..., z N , which are conditionally dependent upon each other in some way. For images, latent variables are typically output in feature maps of varying resolutions, with z 0 corresponding to a small number of latent variables at low resolution at the "top" of the network, and z N corresponding to a larger number of latent variables at high resolution at the "bottom".</p><p>One particularly elegant conditioning structure is the top-down VAE, introduced in Sønderby et al. <ref type="bibr">(2016)</ref>. In this model, both the prior and the approximate posterior generate latent variables in the same order:</p><formula xml:id="formula_1">p θ (z) = p θ (z 0 )p θ (z 1 |z 0 )...p θ (z N |z &lt;N ) (2) q φ (z|x) = q φ (z 0 |x)q φ (z 1 |z 0 , x)...q φ (z N |z &lt;N , x)<label>(3)</label></formula><p>A diagram of this process appears in <ref type="figure" target="#fig_0">Figure 3</ref>. A typical implementation of this model has φ first perform a deterministic "bottom-up" pass on the data to generate features, then processes the groups  <ref type="bibr" target="#b11">(He et al., 2016)</ref>. Each convolution is preceded by the GELU nonlinearity <ref type="bibr" target="#b12">(Hendrycks &amp; Gimpel, 2016)</ref>. q φ (.) and p θ (.) are diagonal Gaussian distributions. z is sampled from q φ (.) during training, and p θ (.) when sampling. We use average pooling and nearest-neighbor upsampling for pool and unpool layers.</p><p>of latent variables from top to bottom, using feedforward networks to generate features which are shared between the approximate posterior, prior, and reconstruction network p θ (x|z). We adopt this base architecture as it is simple, empirically effective, and has been postulated to resemble biological processes of perception <ref type="bibr" target="#b6">(Dayan et al., 1995)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">WHY DEPTH MATTERS FOR HIERARCHICAL VAES</head><p>We find that hierarchical VAEs with sufficient depth can not only learn arbitrary orderings over observed variables, but also learn more effective latent variable distributions, if such distributions exist. We present these results below.</p><p>Definition (N -layer VAE). A deep hierarchical VAE with N stochastic layers, independent p(x|z), and the top-down factorization of the prior and approximate posterior in Equations 2-3. Proposition 1. N-layer VAEs generalize autoregressive models when N is the data dimension Proposition 2. N -layer VAEs are universal approximators of N -dimensional latent densities</p><p>Proposition 1 (proof in Appendix, also visualized in <ref type="figure">Figure 2</ref>, left) leads to a possible explanation of why autoregressive models to date have outperformed VAEs: they are deeper, in the sense of statistical dependence. A VAE must be as deep as the data dimension D (3072 layers in the case of 32x32 images) if the images truly require D steps to generate.</p><p>Luckily, however, Proposition 2 (proof and further technical requirements in Appendix) suggests that shorter procedures, if they exist, are also learnable. N = D is an extreme case, where the most effective latent variables z ∈ R D may simply be copies of the observed variables. But if for some K &lt; D there exist latent variables z ∈ R K that the generator can use to more efficiently compress the data, Proposition 2 states a K-layer VAE can learn the posterior and prior distribution over those variables.</p><p>Such shorter generative paths could emerge in two ways. First, as depicted in <ref type="figure">Figure 2</ref> (right), if the model discovers that certain variables are conditionally independent given others, the model can generate them in parallel inside a single layer, where</p><formula xml:id="formula_2">q φ (z N |z &lt;N , x) = d q φ (z (d) N |z &lt;N , x).</formula><p>We hypothesize these efficient hierarchies should emerge in images, as they contain many spatially independent textures, and study this in Section 5.2. Second, the model could learn a low-dimensional representation of the data. <ref type="bibr" target="#b5">Dai &amp; Wipf (2019)</ref> recently showed that when a VAE is trained on data distributed on a K-dimensional manifold embedded in R D , a VAE will only activate K dimensions in its latent space, meaning that the VAE will require fewer layers unless the manifold dimension is D, which is unlikely to be the case for images.</p><p>It is difficult to ascertain the lowest possible value of K for a given dataset, but it may be deeper than most hierarchical VAEs to date. Images have many thousands of observed variables, but early hierarchical VAEs did not exceed 3 layers, until <ref type="bibr" target="#b22">Maaløe et al. (2019)</ref> investigated a Gaussian VAE with 15 layers and found it displayed impressive performance along a variety of measures.  and Vahdat &amp; Kautz (2020) additionally explored networks up to 12 and 30 layers. (These additionally incorporated additional statistical dependencies in the approximate posterior through the usage of inverse autoregressive flow , an alternative approach which we contrast with our approach in Section A.4). Nevertheless, given these results we hypothesize that greater depth may improve the performance of VAEs. In the next section, we introduce an architecture capable of scaling to a greater number of stochastic layers. In Section 5.1 we show depth indeed improves performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">AN ARCHITECTURE FOR VERY DEEP VAES</head><p>We consider a "very deep" VAE to simply be one with greater depth than has previously been explored (and do not define it to be a specific number of layers). As existing implementations of VAEs did not support many more stochastic layers than they were trained on, we reimplemented a minimal VAE with the sole aim of increasing the number of stochastic layers. This VAE consists only of convolutions, nonlinearities, and Gaussian stochastic layers. It does not exhibit posterior collapse even for large numbers of stochastic layers. We describe key architectural choices here and refer readers to our source code for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ARCHITECTURAL COMPONENTS AND INITIALIZATION</head><p>A diagram of our network appears in <ref type="figure" target="#fig_0">Figure 3</ref>. It resembles the ResNet VAE in Kingma et al. <ref type="formula" target="#formula_0">(2016)</ref>, but with bottleneck residual blocks. For each stochastic layer, the prior and posterior are diagonal Gaussian distributions, as used in prior work <ref type="bibr" target="#b22">(Maaløe et al., 2019)</ref>.</p><p>As an alternative to weight normalization and data-dependent initialization , we adopt the default PyTorch weight intialization. The one exception is the final convolutional layer in each residual bottleneck block, which we scale by 1 √ N , where N is the depth (similar to ; ; Zhang et al. <ref type="formula" target="#formula_0">(2019)</ref>). This residual scaling improves stability and performance with many layers, as we show in the Appendix <ref type="table" target="#tab_4">(Table 3)</ref>.</p><p>Additionally, we use nearest-neighbor upsampling for our "unpool" layer, which when paired with our ResNet architecture, allows us to completely remove the "free bits" and KL "warming up" terms that appear in related work. As we detail in the Appendix <ref type="figure">(Figure 5</ref>), when upsampling is done through transposed convolutional layer, the network may ignore layers at low resolution (for instance, 1x1 or 4x4 layers). We found no evidence of posterior collapse in any networks trained with nearest neighbor interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">STABILIZING TRAINING WITH GRADIENT SKIPPING</head><p>VAEs have notorious "optimization difficulties," which are not frequently discussed in the literature but nevertheless well-known by practitioners. These manifest as extremely high reconstruction or KL losses and corresponding large gradient norms (up to 1e15). We address this by skipping updates with a gradient norm above a certain threshold, set by hyperparameter. Though we select high thresholds that affect fewer than 0.01% of updates, this technique almost entirely eliminates divergence, and allows networks to train smoothly. We plot the evolution of grad norms and the <ref type="table">Table 1</ref>: Loss by network with different configurations of stochastic layers on ImageNet-32 (similar trends appear on CIFAR-10). Left: Networks with equal number of layers, but with lower stochastic depth as described in Section 5.1. Increasing depth up to 48 layers still shows gains, which is farther than previous work has explored. Right: Networks with 48 layers, but distributed at different resolutions. We find higher resolutions benefit more from layers. values we select in <ref type="figure">(Figure 6</ref>). An alternative approach to stabilizing networks may be the spectral regularization method introduced in Vahdat &amp; Kautz (2020).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We trained very deep VAEs on challenging natural image datasets. All hyperparameters for experiments are available in the Appendix and in our source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">STATISTICAL DEPTH, INDEPENDENT OF CAPACITY, IMPROVES PERFORMANCE</head><p>We first tested whether greater statistical depth, independent of other factors, can result in improved performance. We trained a network with 48 layers for 600k steps on ImageNet-32, grouping layers to output variables independently instead of conditioning on each other. If the input for the ith topdown block is x i , we can make K consecutive blocks independent by setting x i+1 , ..., x i+K all equal to x i . (Normally, x i+1 = x i + f (block(x i ))). This technique reduces the stochastic depth without affecting parameter count. Stochastic depth shows a clear correlation with performance, even up to 48 layers, which is past what previous work has explored (Table 1, left).</p><p>We then tested our hypothesis at scale. We trained networks on CIFAR-10, ImageNet-32, and ImageNet-64 with greater numbers of stochastic layers, but with fewer parameters than related work (see <ref type="table" target="#tab_3">Table 2</ref>). On CIFAR-10, we trained a model with 45 stochastic layers and only 39M parameters, and found it achieved a test log-likelihood of 2.87 bits per dim (average of 4 seeds). On ImageNet-32 and ImageNet-64, we trained networks with 78 and 75 stochastic layers and only approximately 120M parameters, and achieved likelihoods of 3.80 and 3.52.</p><p>On all tasks, these results outperform all GatedPixelCNN/PixelCNN++ models, and all nonautoregressive models, while using similar or fewer parameters. These results support our hypothesis that stochastic depth, as opposed to other factors, explains the gap between VAEs and autoregressive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">VERY DEEP VAES LEARN AN EFFICIENT HIERARCHICAL ORDERING</head><p>One question that emerges from the analysis in Section 3 is whether VAEs need to be as deep as autoregressive models, or whether they can learn a latent hierarchy of conditionally independent variables which are able to be synthesized in parallel. We qualitatively show this is true in <ref type="figure">Figure  4</ref>. For FFHQ-256 images, the first several layers at low resolution almost wholly determine the global features of the image, even though they only account for less than 1% of the latent variables.</p><p>The rest of the high-resolution variables appear to be spatially independent, meaning they can be emitted in parallel in a number of layers much lower than the dimensionality of the image. This efficient hierarchical representation may underlie the VAE's ability to achieve better log-likelihoods than the PixelCNN while simultaneously sampling thousands of times faster. This can be viewed as a learned parallel multiscale generation method, unlike the handcrafted approaches of Kolesnikov &amp; Lampert <ref type="formula" target="#formula_0">(2017)</ref>  <ref type="figure">Figure 4</ref>: Cumulative percentage of latent variables at a given resolution, and reconstructions of samples on FFHQ-256. We sample latent variables from the approximate posterior until the given resolution, and sample the rest from the prior at low temperature. This shows what images are likely given a subset of latent variables. Low-resolution latents comprise a small fraction of the total latents, but encode significant portions of the global structure. This suggests deep VAEs learn efficient, hierarchical representations of the data.</p><p>Additionally, we found that on all datasets we tested, very deep VAEs used roughly 30% fewer parameters than the PixelCNN <ref type="table" target="#tab_3">(Table 2)</ref>. One possible explanation is that the learned hierarchical generation procedure involves fewer long-range dependencies, or may otherwise be simpler to learn.</p><p>We found that networks in general benefited from more layers at higher resolutions <ref type="table">(Table 1,</ref>  <ref type="bibr">right)</ref>. This suggests that global features may account for a smaller fraction of information than local details and textures, and that it is important to have many latent variables at high resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">VERY DEEP VAES ARE EASILY SCALED TO HIGH DIMENSIONAL DATA</head><p>Scaling autoregressive models to higher resolutions presents several challenges. First, the sampling time and memory requirements of autoregressive models increase linearly with resolution. This scaling makes datasets like FFHQ-256 and FFHQ-1024 intractable for naive approaches. Although clever factorization techniques have been adopted for 256x256 images <ref type="bibr" target="#b23">(Menick &amp; Kalchbrenner, 2018)</ref>, such factorizations may not be as effective for alternate datasets or higher-resolution images.</p><p>Our VAE, in contrast, readily scales to higher resolutions. The same network used for 32x32 images can be applied to 1024x1024 images by introducing a greater number of upsampling layers throughout the network. We found we could train an equal number of steps (1.5M) using a similar number of training resources (32 GPUs for 2.5 weeks) on both 32x32 and 1024x1024 images with few hyperparameter changes (see Appendix for hyperparameters). Samples from both models (displayed in Appendix) require a single forward pass of the model to generate, with only minor differences in runtime. An autoregressive model, on the other hand, would require a thousand times more network evaluations to sample 1024x1024 images and likely require a custom training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK AND DISCUSSION</head><p>Our work is inspired by previous and concurrent work in hierarchical VAEs <ref type="bibr">(Sønderby et al., 2016;</ref><ref type="bibr" target="#b22">Maaløe et al., 2019;</ref><ref type="bibr">Vahdat &amp; Kautz, 2020)</ref>. Relative to these works, we provide some justification for why deeper networks may perform better, introduce a new architecture, and empirically demonstrate gains in log-likelihood. Many aspects of prior work are complementary with ours and could be combined. <ref type="bibr" target="#b22">Maaløe et al. (2019)</ref>, for instance, incorporates a "bottom-up" stochastic path that doubles the depth of the approximate posterior, and Vahdat &amp; Kautz (2020) introduces a number of powerful architecture components and improved training techniques. We seek here not to introduce a significantly better method than these alternatives, but to demonstrate that depth is a key overlooked factor in most prior approaches to VAEs. Diffusion models can be seen as deep VAEs that, like autoregressive models, have a specific analytical posterior. <ref type="bibr" target="#b14">Ho et al. (2020)</ref> showed that such models achieve impressive sample quality with great depth, which is in line with our observations that greater depth is helpful for VAEs. One benefit of the VAEs we outline in this work over diffusion models is that our VAEs generate samples with a single network evaluation, whereas diffusion models currently require a large number of network evaluations per sample.</p><p>Inverse autoregressive flows (IAF) are also closely related, and we discuss the differences with hierarchical models in Section A.4. The work of <ref type="bibr">Zhao et al. (2017)</ref> may also appear to contradict our findings, and we discuss that work in Section A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We argue deeper VAEs should perform better, introduce a deeper architecture, and show it outperforms all PixelCNN-based autoregressive models in likelihood while being more efficient. We hope this encourages work in further improving VAEs and latent variable models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 ABLATIONS OF ARCHITECTURAL COMPONENTS First, we visualize data that suggests upsampling layers and residual connections have an impact on posterior collapse ( <ref type="figure">Figure 5</ref>). Architectural differences may explain why our VAEs do not need "free bits" or KL warmups to avoid posterior collapse.</p><p>In <ref type="table" target="#tab_4">Table 3</ref>, we show residual initialization leads to smoother and better training of very deep VAEs. Without residual initialization, very deep VAEs encounter a high number of unstable updates and have higher losses.</p><p>In <ref type="figure">Figure 6</ref>, we show the max gradient norms experienced throughout training, and show that our skipping criterion avoids a small number of updates that would destabilize the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PROPOSITION 1: N-LAYER VAES GENERALIZE AUTOREGRESSIVE MODELS WHEN N IS</head><p>THE DATA DIMENSION Proposition 1 shows that an autoregressive model with an arbitrary ordering over observed variables in x ∈ R N is equivalent to an N -layer VAE with an approximate posterior that simply outputs the observed variables in the given order, and a generator that performs the identity function (see <ref type="figure">Figure  2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5:</head><p>Relationship between architecture and posterior collapse. We visualize the cumulative KL divergence (or "rate", in bits per dimension) for several different architectures across a 73 layer network on ImageNet-32. When residual connections are removed from the "res block" in the topdown path <ref type="figure" target="#fig_0">(Figure 3)</ref>, the model encodes no information in the first 45 layers of the network and the loss is highest ("FFN"). When a learned convolutional upsampler is used as the "unpool" layer, the first 13 layers of the network encode no information. When nearest-neighbor upsampling is used, the first layers all encode information, and the loss is the lowest.</p><p>Without loss of generality, we simplify notation by assuming each vector-valued latent variable z i only has one element, which we write as z i ∈ R. We assume a prior and approximate posterior distribution following Equation 2 and 3.</p><p>Proof. Let q(z i = x i |z &lt;i , x) = 1, and p(x i = z i |z) = 1. Then p(z|x) = q(z|x), which is well-known to imply equality in the evidence lower bound (ELBO) of Eq. 1. Since log q(z|x) = log p(x|z) = 0, the ELBO becomes log p θ (</p><formula xml:id="formula_3">x) = log p θ (z) = N i=1 log p θ (z i |z &lt;i ) = N i=1 log p θ (x i |x &lt;i )</formula><p>, which is equivalent to an autoregressive model over the observed variables.</p><p>A.3 PROPOSITION 2: N -LAYER VAES ARE UNIVERSAL APPROXIMATORS OF N -DIMENSIONAL LATENT DENSITIES Proposition 2 shows that hierarchical VAEs learn depthwise autoregressive flows, and under certain conditions (described in <ref type="bibr" target="#b15">Huang et al. (2017)</ref>) can express any density over latent variables of N dimensions, given enough capacity.</p><p>Proof. We omit full proof, and refer readers to <ref type="bibr" target="#b15">Huang et al. (2017)</ref>; <ref type="bibr" target="#b25">Papamakarios et al. (2019)</ref>, where universality is established for autoregressive flows. Here we only note that the prior and approximate posterior in an N -layer VAE are autoregressive flows: Let p θ (z) be the prior distribution. p θ (z) can be written using the reparameterization trick as a deterministic function of noise drawn from a known base density p N : p θ (z) = p N ( ) det ∂f ( ,θ) ∂ , where f is a neural network that implements the factorization in Eq. 2. Since f is autoregressive and its Jacobian is lower triangular, p θ (z) can approximate any p(z) that fits the criteria in <ref type="bibr" target="#b15">Huang et al. (2017)</ref>. The same logic applies to q φ (z|x) and p(z|x). It should be noted that this result depends on f being able to implement the inverse CDF of an arbitrary probability density, and so using Gaussian distributions will restrict the densities the VAE can express in practice. This is a limitation of our architecture that we nevertheless adopt since we hypothesize depth, not the elementwise density, is the more important factor. More discussion on this subject, and options for removing this restriction, are described in <ref type="bibr" target="#b15">Huang et al. (2017)</ref> and <ref type="bibr" target="#b16">Huang et al. (2018)</ref>, and we defer studying more expressive elementwise densities to future work. <ref type="figure">Figure 6</ref>: Effect of gradient skipping. We plot the max gradient norm encountered per 500 updates for our best models across datasets. The dashed black line indicates the "skip threshold", or value above which the update is skipped. We choose a high threshold that affects fewer than 0.01 percent of training updates. Without this skip heuristic, networks will diverge when extreme updates are encountered. These updates can have norm as high as 1e15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 A NOTE ON INVERSE AUTOREGRESSIVE FLOW</head><p>Inverse autoregressive flows <ref type="bibr">(IAF, Kingma et al. (2016)</ref>) and are similar to very deep VAEs in that they are universal approximators of posterior distributions in VAEs, even with just a single layer and sufficiently expressive univariate density <ref type="bibr" target="#b16">(Huang et al., 2018)</ref>.</p><p>There are several practical differences between IAFs and deep hierarchical VAEs, however, which can result in qualitatively very different behavior. First, the masked autoregressive components in IAF build statistical dependencies spatially, whereas a very deep hierarchical VAE builds dependencies depthwise, and these inductive biases may better suit different domains. Additionally, IAFs spend an equal amount of computation and parameters on each variable. In contrast, a deep VAE can specify a structure, like a hierarchy of global-to-local variables, which have different computational and modeling capacities for each stage. For images, these differences may result in qualitatively different behavior, and it is not clear whether a single layer IAF can readily learn the sort of rich hierarchical decomposition of images that appear with very deep VAEs. Nevertheless, the two techniques are complementary -IAF was introduced in a deep hierarchical <ref type="bibr">VAE (Kingma et al., 2016)</ref>, in fact, and it is likely that introducing IAF into our architecture (as in Vahdat &amp; Kautz (2020)) would improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 A NOTE ON LEARNING HIERARCHICAL FEATURES</head><p>The work of Zhao et al. (2017) may appear to contradict our work, by suggesting that additional layers in hierarchical VAEs do not lead to additional expressivity, based off their finding that Gibbs sampling from the last stochastic layer is sufficient to recover the data. For high dimensional data like images, however, the last stochastic layer may have many thousands of variables, and Gibbs sampling may take unacceptably long to converge. A hierarchy of latent variables as in our model allows efficient and tractable sampling from this distribution. Additionally, assumptions regarding global maximization of the ELBO may not apply in practice. Nevertheless, we think further clarifying these contradictory statements would be useful future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 BROADER IMPACT</head><p>Broadly speaking, any generative model will reflect the biases of the datasets they are trained on. If deployed without careful consideration, generative models (including but not limited to VAEs) trained on research datasets like ImageNet, CIFAR-10, and FFHQ may inadvertently cause harm by propagating or otherwise reinforcing harmful biases in the dataset. Further work is required to improve and debias research benchmark datasets to mitigate this source of negative impact. Some VAEs are distinguished from other generative models by their fast synthesis of new data examples. Generative models with fast synthesis can allow for realtime synthesis of high dimensional data, such as music, speech, and video. These models could be used to augment human creativity and lead to a number of helpful applications in real-time media applications. Such models could also be used for compression, which could assist in delivering content to bandwidth-constrained regions of the world. They can also be used for spreading disinformation, generally making it less possible to distinguish real from generated data. An additional potential harm is that fast, high quality synthesis of data could end up economically displacing individuals who rely upon creative work, such as musicians, visual artists, and more. VAEs also are distinguished by their usage of latent variables. Generative models with useful latent variables could have positive impacts in scientific domains, where density estimation could lead to novel insights about chemical, physical, or biological data. Latent variable representations of data could also be helpful in efforts to debias, interpret, or otherwise increase understandibility of models and their representations.   Figure 11: FFHQ-1024 samples. These are generated with reduced temperature (top) and temperature 1.0 (bottom). The model we train has similar capacity to smaller ones we use on 32x32, 64x64, and 256x256 images, and so fails to capture the intricacies of this more complex distribution well. A larger model, trained for longer, may achieve better sample quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>A diagram of our top-down VAE architecture. Residual blocks are similar to bottleneck ResNet blocks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 :</head><label>7</label><figDesc>Non-cherrypicked, temperature 1.0 samples on FFHQ-256. Cover images were each cherrypicked from a batch of 16 (unadjusted temperature) samples. Here we show a random batch of 16 images for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>Non-cherrypicked, temperature 0.85 samples on FFHQ-256. Lower temperature samples result in greater regularity in images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :</head><label>9</label><figDesc>Non-cherrypicked, temperature 0.60 samples on FFHQ-256. We visualize temperature 0.60 samples for comparison withVahdat &amp; Kautz (2020)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>ImageNet-32 (left) and ImageNet-64 (right) reconstructions and samples. Reconstructions of validation images from various stages in the latent hierarchy (top), and unconditional samples from the model at temperature 1.0 (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Our main results on standard benchmark datasets. Very deep VAEs outperform PixelCNN-based autoregressive models with fewer parameters while maintaining fast sampling. "Depth" refers to the number of stochastic layers for hierarchical VAEs (although BIVA and IAFbased networks have additional statistical dependencies). Sampling refers to the number of network evaluations per sample, and D designates the dimensionality of the data. An asterisk ( * ) denotes our estimate of parameters. Samples for ImageNet and CIFAR-10 are in the Appendix.</figDesc><table><row><cell>Model type Params Depth Sampling NLL</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Effects of scaling residual initialization on very deep VAEs. We trained networks with varying depths for 80k iterations. Scaling the last layer in the residual block by 1 √ N results in higher losses for shallower networks, but lower losses and greater stability for deeper networks. The number of updates which are skipped because the gradient norm would destabilize the network is significantly reduced with scaling.</figDesc><table><row><cell cols="2">Depth Without scaling</cell><cell></cell><cell>With scaling</cell><cell></cell></row><row><cell></cell><cell>Loss</cell><cell cols="2">Skipped Updates Loss</cell><cell>Skipped Updates</cell></row><row><cell>15</cell><cell>2.50</cell><cell>13</cell><cell>2.51</cell><cell>0</cell></row><row><cell>30</cell><cell>2.36</cell><cell>41</cell><cell>2.38</cell><cell>1</cell></row><row><cell>45</cell><cell>2.31</cell><cell>48</cell><cell>2.30</cell><cell>0</cell></row><row><cell>60</cell><cell>2.30</cell><cell>76</cell><cell>2.29</cell><cell>1</cell></row><row><cell>75</cell><cell>Diverged</cell><cell>-</cell><cell>2.28</cell><cell>0</cell></row><row><cell cols="5">Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder</cell></row><row><cell cols="5">variational autoencoders. In Advances in neural information processing systems, pp. 3738-3746,</cell></row><row><cell>2016.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep</cell></row><row><cell cols="4">learning in nlp. arXiv preprint arXiv:1906.02243, 2019.</cell><cell></cell></row><row><cell cols="5">Benigno Uria, Iain Murray, and Hugo Larochelle. Rnade: The real-valued neural autoregressive</cell></row><row><cell cols="5">density-estimator. In Advances in Neural Information Processing Systems, pp. 2175-2183, 2013.</cell></row><row><cell cols="5">Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. arXiv preprint</cell></row><row><cell cols="2">arXiv:2007.03898, 2020.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Con-</cell></row><row><cell cols="5">ditional image generation with pixelcnn decoders. In Advances in neural information processing</cell></row><row><cell cols="2">systems, pp. 4790-4798, 2016.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without</cell></row><row><cell cols="4">normalization. arXiv preprint arXiv:1901.09321, 2019.</cell><cell></cell></row><row><cell cols="5">Shengjia Zhao, Jiaming Song, and Stefano Ermon. Learning hierarchical features from generative</cell></row><row><cell cols="3">models. arXiv preprint arXiv:1702.08396, 2017.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Key hyperparameters for experiments. We detail here the main hyperparameters used in training. FFHQ-1024 has reduced hidden size for higher resolutions; see code for details.</figDesc><table><row><cell>Parameter</cell><cell cols="5">CIFAR-10 ImageNet-32 ImageNet-64 FFHQ-256 FFHQ-1024</cell></row><row><cell>Num layers</cell><cell>45</cell><cell>78</cell><cell>75</cell><cell>62</cell><cell>72</cell></row><row><cell>Hidden size</cell><cell>384</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>Varies</cell></row><row><cell>Bottleneck size</cell><cell>96</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>Varies</cell></row><row><cell>Latent dim per layer</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell></row><row><cell>Batch size</cell><cell>32</cell><cell>256</cell><cell>128</cell><cell>32</cell><cell>32</cell></row><row><cell>Learning rate</cell><cell>0.0002</cell><cell>0.00015</cell><cell>0.00015</cell><cell>0.00015</cell><cell>0.00007</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Skip threshold</cell><cell>400</cell><cell>300</cell><cell>380</cell><cell>180</cell><cell>500</cell></row><row><cell>Weight Decay</cell><cell>0.01</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>EMA rate</cell><cell>0.0002</cell><cell>0.00015</cell><cell>0.00015</cell><cell>0.00015</cell><cell>0.00015</cell></row><row><cell>Training iterations</cell><cell>1.1M</cell><cell>1.7M</cell><cell>1.6M</cell><cell>1.7M</cell><cell>1.7M</cell></row><row><cell>GPUs</cell><cell>2 x V100</cell><cell>32 x V100</cell><cell>32 x V100</cell><cell>32 x V100</cell><cell>32 x V100</cell></row><row><cell>Training time</cell><cell>6 days</cell><cell>2.5 weeks</cell><cell>2.5 weeks</cell><cell>2.5 weeks</cell><cell>2.5 weeks</cell></row><row><cell>Parameters</cell><cell>39M</cell><cell>119M</cell><cell>125M</cell><cell>115M</cell><cell>115M</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Aditya Ramesh, Pranav Shyam, Johannes Otterbach, Heewoo Jun, Mark Chen, Prafulla Dhariwal, Alec Radford, Yura Burda, Bowen Baker, Raul Puri, and Ilya Sutskever for helpful discussions. We also thank the anonymous reviewers for helping improve our work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02731</idno>
		<title level="m">Ilya Sutskever, and Pieter Abbeel. Variational lossy autoencoder</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pixelsnail: An improved autoregressive generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09763</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Diagnosing and enhancing vae models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05789</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The helmholtz machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="904" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<title level="m">Jukebox: A generative model for music</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><forename type="middle">Ali</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pixelvae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05013</idno>
		<title level="m">A latent variable model for natural images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Flow++: Improving flowbased generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00275</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint arxiv</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learnable explicit density for continuous latent space and variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Touati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02248</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00779</idno>
		<title level="m">Neural autoregressive flows</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stochastic gradient vb and the variational auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An introduction to variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02691</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pixelcnn models with auxiliary variables for natural image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1905" to="1914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Biva: A very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Liévin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6548" to="6558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01608</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio</title>
		<meeting><address><addrLine>Alex Graves</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">Jimenez</forename><surname>Rezende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02762</idno>
		<title level="m">Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03664</idno>
		<title level="m">Parallel multiscale autoregressive density estimation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05517</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
