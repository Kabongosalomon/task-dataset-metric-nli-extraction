<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">In Defense of Grid Features for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
							<email>hzjiang@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UMass Amherst</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
							<email>imisra@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UMass Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
							<email>xinleic@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">In Defense of Grid Features for Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Popularized as 'bottom-up' attention [2], bounding box (or region) based visual features have recently surpassed vanilla grid-based convolutional features as the de facto standard for vision and language tasks like visual question answering (VQA). However, it is not clear whether the advantages of regions (e.g. better localization) are the key reasons for the success of bottom-up attention. In this paper, we revisit grid features for VQA, and find they can work surprisingly well -running more than an order of magnitude faster with the same accuracy (e.g. if pre-trained in a similar fashion). Through extensive experiments, we verify that this observation holds true across different VQA models (reporting a state-of-the-art accuracy on VQA 2.0 test-std, 72.71), datasets, and generalizes well to other tasks like image captioning. As grid features make the model design and training process much simpler, this enables us to train them end-to-end and also use a more flexible network design. We learn VQA models end-to-end, from pixels directly to answers, and show that strong performance is achievable without using any region annotations in pre-training. We hope our findings help further improve the scientific understanding and the practical application of VQA. Code and features will be made available. * This work was done when Huaizu Jiang was an intern at FAIR. <ref type="bibr" target="#b0">1</ref> We use the terms 'region' and 'bounding box' interchangeably. <ref type="bibr" target="#b0">1</ref> In the MCAN paper, the model is trained for 13 epochs, where each epoch contains 17,967 iterations. <ref type="bibr" target="#b1">2</ref> We use the implementation provided in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>After the introduction of deep learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b43">44]</ref> and attention mechanisms <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> to multi-modal vision and language research, perhaps one of the most significant developments was the discovery of 'bottom-up' attention <ref type="bibr" target="#b1">[2]</ref>. Unlike normal attention that uses 'top-down' linguistic inputs to focus on specific parts of the visual input, bottom-up attention uses pre-trained object detectors <ref type="bibr" target="#b32">[33]</ref> to identify salient regions based solely on the visual input itself. As a result, images are represented by a collection of bounding box or region 1 -based features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref>-in contrast to vanilla grid convolutional feature maps from ConvNets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b14">15]</ref>  <ref type="figure">Figure 1</ref>: We revisit grid-based convolutional features for VQA, and find they can match the accuracy of the dominant region-based features from bottom-up attention <ref type="bibr" target="#b1">[2]</ref>, provided that one closely follow the pre-training process on Visual Genome <ref type="bibr" target="#b21">[22]</ref>. As computing grid features skips the expensive region-related steps (shown in colors), it leads to significant speedups (all modules run on GPU; timed in the same environment).</p><p>for follow-up tasks. These region features have since then gained wide popularity and dominated vision and language leader boards <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b49">50]</ref> for major tasks like visual question answering (VQA). So what makes these region features successful? Naturally, one would assume a major reason is better localization of individual objects, as the regions are direct bounding box outputs from detectors. Another plausible answer is that a number of regions can easily capture both the coarse-level information and fine-grained details in the image -even if they overlap. However, do these potential advantages actually demonstrate that region features are superior to grids?</p><p>Surprisingly, we discovered that grid features extracted from exactly the same layer of the pre-trained detector can perform competitively against their region-based counterparts for VQA. Moreover, with simple modifications during training, the same grid features can be made even more effective and that they consistently achieve comparable and sometimes better VQA accuracy than region features. In fact, our ablative analysis suggests that the key factors which contributed to the high accuracy of existing bottom-up attention features are: 1) the large-scale object and attribute annotations collected in the Visual Genome (VG) <ref type="bibr" target="#b21">[22]</ref> dataset used for pre-training; and 2) the high spatial resolution of the input images used for computing features. As for the feature format itself -region or grid -it only affects accuracy minimally. Through a comprehensive set of experiments, we verified that our observations generalize across different network backbones, different VQA models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b49">50]</ref>, different VQA benchmarks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>, and even to other relevant tasks (e.g. image captioning <ref type="bibr" target="#b3">[4]</ref>).</p><p>Our findings have important consequences for the design of future multi-modal vision and language models. The immediate benefit of switching to grids is inference speed, as we can now skip all of the region-related steps in the existing VQA pipeline <ref type="figure">(Fig. 1</ref>). For example, using a ResNet-50 <ref type="bibr" target="#b14">[15]</ref> backbone, we find the overall running time drops from 0.89s to 0.02s per image -40+ times faster with slightly better accuracy! In fact, extracting region features is so time-consuming that most state-of-the-art models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b49">50]</ref> are directly trained and evaluated on cached visual features. This practice not only imposes unnecessary constraints on model designs, but also limits potential applications of existing vision and language systems.</p><p>Empowered by grid features, we therefore take an initial step to train VQA models end-to-end from pixels directly to answers. Note that end-to-end training with region features is challenging, since fine-tuning region locations likely requires additional grounding annotations <ref type="bibr" target="#b12">[13]</ref> that are computationally expensive and difficult to acquire. In contrast, grid features can be readily optimized for the final objective (e.g. to answer questions correctly) without extra grounding. The grid-feature pipeline also allows us to explore more effective designs for VQA (e.g. pyramid pooling module <ref type="bibr" target="#b53">[54]</ref>) and enables networks pre-trained with zero regionlevel annotations to greatly reduce the gap in accuracy with VG models (trained on bounding boxes) -indicating strong VQA models can be achieved without any explicit notion of regions. These results further strengthen our defense of grid features for VQA. We hope our discovery can open up new opportunities for vision and language research in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual features for vision and language tasks. Features have played a key role in the advancement of vision and language tasks. For example, deep learning features led to remarkable improvements in image captioning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b7">8]</ref>. While a complete review of visual features used for vision and language tasks is beyond the scope of this paper, we note that the accuracies of modern VQA models are dependent on the underlying visual features used, including VGG <ref type="bibr" target="#b34">[35]</ref> and ResNet <ref type="bibr" target="#b14">[15]</ref> grid features, which were later dominated by bottom-up attention region features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref>. Today, most state-of-the-art VQA models focus on fusing schemes <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50]</ref> and are built with region features asis <ref type="bibr" target="#b48">[49]</ref>; whereas our work revisits grid features, and shows that they can be equally effective and lead to remarkable speed-ups -often greater than an order of magnitude! Pre-training for VQA. Most VQA methods use two separately pre-trained models: vision models trained on Ima-geNet <ref type="bibr" target="#b5">[6]</ref> and VG <ref type="bibr" target="#b21">[22]</ref>; and word embeddings <ref type="bibr" target="#b30">[31]</ref> for linguistic features. As these separately trained features may not be optimal for joint vision and language understanding, a recent hot topic is to develop jointly pre-trained models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b4">5]</ref> for vision and language tasks. A common scheme for such methods is to view regions and words as 'tokens' for their respective domain, and pre-train a variant of BERT <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42]</ref> for 'masked' token prediction. Complementary to that direction, our work delves specifically into the 'format' of visual tokens and can be potentially combined with such methods for mutual benefits (e.g. trade-off between speed and accuracy).</p><p>Regions vs. grids. The debate between region features and grid features carries some inherent connections to object detection: the dominance of the R-CNN based detection models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14]</ref> demonstrates that a region (the 'R' in R-CNN) based refinement stage is beneficial for object detection. On the other hand, one-stage detectors <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref> approach the detection task without the need for explicit region-level computation and show that grid features can be competitive for object detection. In our work, we also use grid features no regions for the VQA task. To minimize changes from bottom-up attention paper <ref type="bibr" target="#b1">[2]</ref>, we pre-train the features with Faster R-CNN <ref type="bibr" target="#b32">[33]</ref>. However, during inference, we discard the region-related steps from the detector and use only the grid convolutional features. This in fact gives us a stronger defense for grids, as we show that VQA can operate on a 'single' feature map, instead of feature maps of 'multiple' scales that one-stage detectors <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref> thrive on.</p><p>It is also worth noting that while region features are effective on benchmarks like VQA <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref> and COCO captions <ref type="bibr" target="#b3">[4]</ref>, for benchmarks that diagnose a model's reasoning abilities when answering visual questions (e.g. CLEVR <ref type="bibr" target="#b16">[17]</ref>), simple methods based on grids <ref type="bibr" target="#b31">[32]</ref> have shown strong performance. We hope that our discovery that grid features also work well for the general VQA task can bridge the gap between these two lines of work <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">From Regions to Grids</head><p>In this section, we explain our approach to obtaining grid features that are just as effective as region features, with the constraint that they have been pre-trained with the same task. In Sec. 7, we show that the 'same pre-training' constraint can be lifted and grid features can still close the gap to regions with end-to-end training on down-stream tasks. We first briefly review the region features from bottom-up attention <ref type="bibr" target="#b1">[2]</ref>.  <ref type="bibr" target="#b1">[2]</ref> back to the ResNet <ref type="bibr" target="#b14">[15]</ref> grid feature extractor for the same layer (see Sec. 3.2, weights in blue are transferred), and find it works surprisingly well for VQA <ref type="bibr" target="#b10">[11]</ref>. Right: We build a detector based on 1×1 RoIPool while keeping the output architecture fixed for grid features (see Sec. 3.3), and the resulting grid features consistently perform at-par with region features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bottom-Up Attention with Regions</head><p>The bottom-up attention method <ref type="bibr" target="#b1">[2]</ref> uses a Faster R-CNN <ref type="bibr" target="#b32">[33]</ref> detection model. The detector is trained on a cleaned version of Visual Genome <ref type="bibr" target="#b21">[22]</ref>, with thousands of object categories and hundreds of attributes with bounding box (region) annotations.</p><p>In order to obtain bottom-up attention features for tasks like VQA, two region-related steps are needed: Region selection. As Faster R-CNN is a two-stage detector, region selection happens twice in the pipeline. The first is through a region proposal network <ref type="bibr" target="#b32">[33]</ref>, which deforms and selects prominent candidate 'anchors' as Regions of Interest (RoIs). Another selection is done as post-processing to aggregate top N boxes in a per-class manner. In both steps, non-maximal suppression (NMS) is used, which keeps the region with the highest classification score and removes other near-duplicates in a local neighborhood. Region feature computation. Given regions from the first stage (up to thousands), RoIPool operations <ref type="bibr" target="#b32">[33]</ref> are used to extract the initial region-level features. Additional network layers then compute the output representation of regions separately. Finally, region features that survive both rounds of selection are stacked together as the bottom-up features to represent an image.</p><p>It is important to note that due to the complexity of the VG dataset (e.g. thousands of classes) and the specific Faster R-CNN detector used <ref type="bibr" target="#b1">[2]</ref> (described next), both steps are computationally intensive. In contrast, directly using grid features can skip or accelerate these steps and offer potentially significant speed-ups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Grid Features from the Same Layer</head><p>The simplest way to convert region features to grids is to see if one can directly compute outputs of the same network layer, but in a shared, fully convolutional manner. To this end, we take a closer look at the specific Faster R-CNN architecture used by the original bottom-up attention <ref type="bibr" target="#b1">[2]</ref>.</p><p>The Faster R-CNN is a variant of the c4 model <ref type="bibr" target="#b14">[15]</ref> with an extra branch for attribute classification. It divides the weights from a ResNet <ref type="bibr" target="#b14">[15]</ref> into two separate sets: given an input image, it first computes feature maps using the lower blocks of ResNet up to C 4 . This feature map is shared among all regions. Then, separately, per-region feature computations are performed by applying the C 5 block on the 14×14 RoIPool-ed features. The output of C 5 is then AvgPool-ed to a final vector for each region as the bottom-up features <ref type="bibr" target="#b1">[2]</ref>. Since all the final region features are from C 5 , it is easy to convert the detector back to the ResNet classifier and take the same C 5 layer as our output grid features. <ref type="figure" target="#fig_0">Fig. 2</ref> (left) illustrates our conversion process.</p><p>As our experiments will show, directly using the converted C 5 output already works surprisingly well. Any performance drop from doing so may be because Faster R-CNN is highly optimized for region-based object detection, and likely not so much for grids. Therefore, we next see if some minimal adjustments to the model can be made to improve grid features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">1×1 RoIPool for Improved Grid Features</head><p>Our idea is to simply use 1×1 RoIPool. This means representing each region with a single vector, rather than a three-dimensional tensor in Faster R-CNN. At first glance, it may seem counter-intuitive, as the two additional spatial dimensions (height and width) are useful to characterize different parts of objects in 2D -indeed, we find this modification negatively affects object detection performance on VG. But importantly, using 1×1 RoIPool regions also means each vector on the grid feature map is forced to cover all the information for a spatial region alone, which can potentially result in stronger grid features. However, directly applying 1×1 RoIPool on the original model is problematic, likely because C 5 consists of several ImageNet pre-trained convolutional layers that work best with inputs of particular spatial dimensions. To resolve this, we follow recent developments in object detection and use the entire ResNet up to C 5 as the backbone for shared feature computation <ref type="bibr" target="#b55">[56]</ref>; and for region-level computation place two 1024D fully-connected (FC) layers on the top, which by default accept vectors as inputs.</p><p>To reduce the effect of low resolutions when training the detector with features pooled from C 5 (C 5 has stride 32, whereas C 4 has 16), the stride-2 layers are replaced with stride-1 layers, and the remaining layers are dilated with a factor of 2 <ref type="bibr" target="#b55">[56]</ref>. For grid feature extraction, we remove this dilation and convert it back to the normal ResNet. <ref type="figure" target="#fig_0">Fig. 2</ref> (right) summarizes the changes we made to improved grids. Note that compared to the original model (left), we only made necessary modifications to the region related components during training. Since all such computations are removed during feature extraction, our grid feature extractor is kept untouched during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Main Comparison: Regions vs. Grids</head><p>From this section on, we report our experimental results comparing regions with grids. We choose VQA (2.0) <ref type="bibr" target="#b10">[11]</ref> as our main task of interest, since it is currently a major benchmark for evaluating joint vision and language understanding and has clear metrics for evaluation. For all our comparisons, we denote methods using region features with the tag 'R', and methods using grid features with 'G'. In this section, we focus on reporting our main findings from converting regions to grids as described in Sec. 3. We begin by briefly describing our experimental setups (more details in the supplementary material). Note that our goal here is to make the conclusion meaningful by controlled comparisons, and not necessarily to optimize for absolute performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Faster R-CNN. For analysis, we use Faster R-CNN with a ResNet-50 backbone pre-trained on ImageNet by default 2 . Closely following bottom-up attention <ref type="bibr" target="#b1">[2]</ref>, the detector is then trained on the VG dataset <ref type="bibr" target="#b21">[22]</ref> with regionlevel annotations for 1600 object categories and 400 attribute classes. For attributes, an additional branch is added with loss weight 0.5. The model is trained with '1x' schedule <ref type="bibr" target="#b13">[14]</ref>. Notably, input images are resized to have a maximum shorter side of 600 pixels (longest 1000) when keeping aspect ratio fixed. For region features, we set N =100. VQA split. Unless otherwise specified, we use the default train set for training. To assist our analysis, we create a local validation set, vqa-dev, out of the standard val set to select the best model during training for evaluation. It contains randomly sampled 8.4K images and their corresponding questions, with 66K pairs in total. The rest of the original val set (named vqa-eval) is reserved for testing, on which we report results. VQA model. We use the co-attention model <ref type="bibr" target="#b51">[52]</ref> implemented in Pythia <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36]</ref>. This model fuses visual features (either region or grid) with textual representations of questions, and outputs the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>Our main results are summarized in <ref type="table" target="#tab_1">Table 1</ref>  to the VQA model. We report the average accuracy and standard deviation across 5 independent runs on the VQA 2.0 vqa-eval set. We observe that the VQA accuracy of region features saturates around 200 regions. In contrast, the grid features benefit from a larger N (translates from a larger input size) and in this case stays better than regions even when N is the same (608). regions) to represent 'R', and the 4 th row (best for grids) to represent 'G', to perform a more in-depth study and fair comparison between the two through the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Number of Regions</head><p>Apart from architectural differences in training, another factor that can affect VQA accuracy is the number of feature vectors N used to represent images. Our region model from Pythia <ref type="bibr" target="#b15">[16]</ref> has a default setting that uses the top 100 boxes to represent region features, increasing it from the original 36 boxes in <ref type="bibr" target="#b1">[2]</ref> to improve the accuracy. On the other hand, since grid features are convolutional feature maps for a preset layer, the number of features is determined by the input size to the network. As our largest input size is 600×1000, a 32-stride feature map (C 5 ) results in 608 grid featuresmuch larger than the number of region features. To understand how these different numbers of region features affect the accuracy, we ran experiments with varying number of features N and show the results in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>As for the region features, we observe an improvement in accuracy as the number of regions increases from 30 to 200, beyond which the accuracy saturates. Interestingly, our grid features are better even when compared to the highest number of regions 3 . Thus, the higher number of feature vectors used in our grid method compared to the baseline region method, is not the reason for its improved VQA accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Test Accuracy and Inference Time</head><p>We now report results on the VQA 2.0 test-dev set to quantify the difference in performance between region  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Comparison</head><p>We visualize attention maps over input images from the top-down attention module <ref type="bibr" target="#b1">[2]</ref>, together with answers from both regions and grids in <ref type="figure" target="#fig_2">Fig. 4</ref>. Source images are taken from COCO <ref type="bibr" target="#b26">[27]</ref> on which VQA 2.0 <ref type="bibr" target="#b10">[11]</ref> benchmark is built. To obtain the attention map, we propagate the attention value of each region or grid to its corresponding pixels, and then average the attention value for each pixel (normalizing them individually to [0, 1]). As can be seen, both types of features are able to capture relevant concepts in input images (e.g., snowfield in the top left). Naturally, attention maps of region features tend to cover object-like regions, while for grid features the attention does not necessarily cover the full area the supporting concept (e.g., the snowfield), which can be used to answer the question. However, both features are able to answer visual questions well, suggesting that localization is important, but accurate object detection of individual objects is not crucial for VQA <ref type="bibr" target="#b10">[11]</ref>.</p><p>We show failure cases of region and grid features in <ref type="figure" target="#fig_2">Fig. 4</ref> </p><formula xml:id="formula_0">(b)(c)(d).</formula><p>In most examples, the models attend to the supporting concepts but still give wrong answers. In the cases where both region and grid features fail, specifically designed modules may be needed (e.g., counting module <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b40">41]</ref> in the bottom right example) to answer the question correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Why do Our Grid Features Work?</head><p>As we mentioned in Sec.   <ref type="table">Table 3</ref>: Comparison between the conventional ImageNet pre-trained and our proposed grid features on the VQA 2.0 vqa-eval set. Besides VQA accuracy, we list two major differences between the two: 1) pretraining task and 2) input image size. pared to the previous attempts at grid features, why do our grid features work well? In <ref type="table">Table 3</ref> we show the performance of grid-based methods (ResNet-50 C 5 features) for different settings and find that there are two major factors: 1) input image size; 2) pre-training task. We study both these factors next and report results on the vqa-eval set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Factor 1: Input Image Size</head><p>The standard image size used during feature extraction for ImageNet pre-trained models is 448×448 <ref type="bibr" target="#b9">[10]</ref> discarding the aspect ratio; whereas for VG detection in bottom-up attention <ref type="bibr" target="#b1">[2]</ref>, the default size is 600×1000 while keeping the aspect ratio intact. Therefore, we experimented with different combinations and reported results for all of them in <ref type="table" target="#tab_7">Table 4</ref>. We note that for grid features, a larger input size means more features for the VQA model.</p><p>From the table, we find that grid features benefit from larger images as input, indicating this factor is indeed important. However, input size has a different effect for models pre-trained on ImageNet vs. VG. For ImageNet models which are pre-trained on smaller images <ref type="bibr" target="#b14">[15]</ref>, the performance saturates around 600×1000. Interestingly, the performance of VG models improves with the input size and continues to increase even at 800×1333. We still use 600×1000 for the rest of the paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Factor 2: Pre-Training Task</head><p>We now study the difference in VQA accuracy due to the pre-training task in the ImageNet (classification) and VG (detection) <ref type="bibr" target="#b3">4</ref> . To understand these differences better, we introduce an additional pre-trained model in each setting. For classification, we include a model trained on YFCC <ref type="bibr" target="#b39">[40]</ref>, which has 92M images with image tags. For detection, we include a standard model from COCO <ref type="bibr" target="#b26">[27]</ref> which only has object annotations (no attributes). All models use a ResNet-50 backbone for fair comparison.</p><p>The results are shown in <ref type="table">Table 5</ref>. In the image classification pre-trained setting, the YFCC model (trained on weak image level tags), performs better than the ImageNet model, possibly because it is trained on two orders of magnitude more data. For detection based pre-training, the VG model (trained with objects and attributes) gives better results than  <ref type="table">Table 5</ref>: Choice of pre-training task. We explore the impact of the type of pre-training task on the final performance while keeping the input size fixed at 600×1000. Results reported on vqa-eval. We broadly characterize the pre-training tasks into two types -object detection ('det') and image classification ('cls').  the COCO model. The larger number of categories in VG compared to COCO (1600 vs. 80) or the additional attribute annotations it has are two possible reasons for the improved performance. We study the impact of attributes next.</p><p>Attributes. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the impact of the attribute loss weight on VQA accuracy. Setting the attribute loss weight to zero during pre-training on VG, results in a drop in VQA performance. In fact, the VQA accuracy in this case matches the accuracy from a pre-trained COCO model suggesting that attributes in the pre-training task are a major reason for the better performance of VG models. We also note that the grid features consistently outperform the region features for all values of the attribute loss weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Generalization of Grid Features</head><p>We now study whether our findings about grid features are more broadly applicable to other tasks and models. In this section, we study generalization across: 1) different backbones; 2) different VQA models; 3) different VQA tasks; 4) other tasks. For all the studies, we set the attribute loss weight to 0.2, and compare both the accuracy and speed. For regions we use top N =100 ones. Detailed hyper-parameters are in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different backbone. We train Faster R-CNN models with</head><p>ResNeXt-101-32x8d <ref type="bibr" target="#b45">[46]</ref> backbone on VG and use the same Pythia setting from Section 4.5. Results on VQA 2.0 test-dev split are reported in <ref type="table" target="#tab_10">Table 6a</ref>. We find that our grid features are competitive to the region features even on this more powerful backbone model. Speed-wise, grid features still run substantially faster (23.8×) than region ones.</p><p>Different VQA model. We further test our features obtained from the previous ResNeXt-101 backbone with the state-of-the-art VQA model, MCAN <ref type="bibr" target="#b49">[50]</ref> (2019 VQA Challenge winner). We use the open-sourced implementation <ref type="bibr" target="#b4">5</ref> to train the large version of the model. The results on VQA 2.0 test-dev set are in <ref type="table" target="#tab_10">Table 6b</ref>, where our own region features perform better than the results reported in <ref type="bibr" target="#b49">[50]</ref> due to stronger backbone. On top of that, our grid features work even better than regions, leading to significant improvement over results reported in MCAN <ref type="bibr" target="#b49">[50]</ref> (+1.66). This final model reports a state-of-the-art test-std result of 72.71 (single-model performance) for future reference.</p><p>Different VQA task. We use the VizWiz VQA dataset <ref type="bibr" target="#b11">[12]</ref>, which is a real world dataset of pictures taken with cellphones by visually-impaired users. It is more challenging due to poor image quality, conversation-style questions, and unanswerable questions, etc. Pythia <ref type="bibr" target="#b15">[16]</ref> model is used (2018 challenge winner). Results on the test-dev set of VizWiz are reported in <ref type="table" target="#tab_10">Table 6c</ref>, where our grid features achieve comparable results to the regions. It is worth pointing out that our grid features run much faster (23×), which provides great potential to be deployed in practice, e.g., on cell phones, to better assist the visually-impaired.</p><p>Image captioning. We train the bottom-up attention model <ref type="bibr" target="#b1">[2]</ref> implemented in Pythia <ref type="bibr" target="#b15">[16]</ref> taking our features as input for image captioning on COCO <ref type="bibr" target="#b3">[4]</ref>. No CIDEr <ref type="bibr" target="#b42">[43]</ref> optimization <ref type="bibr" target="#b1">[2]</ref> is used for fair comparison. Quantitative results on the test set of Karpathy split <ref type="bibr" target="#b17">[18]</ref> are reported in <ref type="table" target="#tab_10">Table 6d</ref>. We use standard evaluation metrics including BLEU4 <ref type="bibr" target="#b29">[30]</ref>, METEOR <ref type="bibr" target="#b22">[23]</ref>, CIDEr, and SPICE <ref type="bibr" target="#b0">[1]</ref>. Similar to the VQA task, our grid features achieve comparable results to bottom-up region ones for image captioning while being significantly faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Towards End-to-end VQA</head><p>Although pre-training on VG, ImageNet, or YFCC provides useful feature representations for VQA, there are still potential domain shifts between the pre-training tasks and the downstream tasks. For example, YFCC contains a lot of outdoor images <ref type="bibr" target="#b39">[40]</ref>, which are not present in the VQA dataset. Instead of using pre-computed fixed feature representations, end-to-end training, where the initial feature representations will be fine-tuned, provides a natural solution to reducing such domain gaps. Empowered by the dramatic simplification of grid features for the VQA pipeline, we take an initial step towards this goal.   Training details. We adopt the 22K learning rate schedule <ref type="bibr" target="#b15">[16]</ref> to train both the ResNet-50 model and the Pythia VQA model jointly, with errors from the answering accuracy directly back-propagated to the grid convolutional feature maps. We fix the first two residual blocks and finetune the rest of the model. Since the visual representations are computed online (not stored on disk), it allows us to perform data augmentation including color jitter and affine transformation over the input images to reduce chance of over-fitting. For more details see supplementary material.</p><p>Results. We experiment with three models pre-trained on VG, ImageNet, and YFCC. Note that while VG uses region-level annotations, both ImageNet and YFCC only use image-level ones (human labels or noisy image tags). As can be seen from <ref type="table" target="#tab_11">Table 7</ref>, end-to-end training (denoted as 'e2e') can boost accuracy for all three pre-trained models, with the biggest improvements for ImageNet models.</p><p>Flexible network design. As we now have the ability to train our models end-to-end in a simple manner, it allows us to introduce more flexible architectural designs for vision and language tasks <ref type="bibr" target="#b28">[29]</ref>. Specifically, on top of the grid features from the ResNet-50 model, we add a Pyramid Pooling Module (PPM, a component widely used for semantic segmentation; details in supplementary material) <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b44">45]</ref> to aggregate visual information from grid features of different spatial resolutions. After adding this module to different pre-trained models ( <ref type="table" target="#tab_11">Table 7</ref>, 'PPM'), the VQA accuracy can be further improved. Remarkably, for ImageNet and YFCC pre-trained models, a combination of end-to-end training and PPM results in close or even better performance than a VG pre-trained model using pre-computed region features. This result is particularly desirable as it indicates good VQA accuracy can be achieved even with zero use of explicit region (bounding box) annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we revisit grid features as an alternative to the widely used bottom-up region features <ref type="bibr" target="#b1">[2]</ref> for vision and language tasks. We show they can in fact achieve on-par results in terms of accuracy over different VQA tasks and models and even on captioning. As a result of skipping the computationally expensive region-related bottlenecks in the pipeline, we see remarkable speed-ups -often more than an order of magnitude -to the existing systems that rely on regions. Our experiments show that rather than the 'format' of features (region vs. grids), the semantic content that features represent is more critical for their effectiveness. Such effective representation, per our experiment, can be achieved either by pre-training on object and attribute datasets such as VG, or more importantly, by end-to-end training of grid features directly for the end-task. Note that while easy with grid-features, end-to-end training is not trivial with regions. Even with limited exploration in this direction, we already find that given a more flexible design space, grid features pre-trained without any region-level annotations can in fact achieve strong performance on VQA. While we are aware that for tasks like referring expressions <ref type="bibr" target="#b18">[19]</ref> where the output itself is a region, modeling region is likely unavoidable, we hope our grid features can potentially offer new perspectives for vision and language research in general.   <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>, the default initial learning rate is 0.01. We found 0.002 leads to slightly better results. For the end-to-end trained Pythia (e2e Pythia in the last row), we use initial learning rate of 0.002 and a larger value of 1 for the gradient clip when fine-tuning the ResNet model for feature extraction.</p><p>Hyper-parameters of different models are summarized in <ref type="table" target="#tab_13">Table 8</ref>. For the SGD optimizer, the momentum is 0.9 and weight decay is 0.0001. For the Adamax optimizer, β 1 and β 2 are 0.9 and 0.999, respectively. No weight decay is used. For the Adam optimizer used in MCAN <ref type="bibr" target="#b49">[50]</ref>, β 1 and β 2 are 0.9 and 0.98, respectively. No weight decay is used.</p><p>We follow the default setting of hyperparameters for most of models. For the image captioning model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>, the default initial learning rate is 0.01. We found 0.002 leads to slightly better results. For the end-to-end trained Pythia (e2e Pythia in the last row), we use an initial learning rate of 0.002 and a larger value of 1 for the gradient clip when fine-tuning the ResNet model for feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Region Features from FPN</head><p>In the Pythia implementation <ref type="bibr" target="#b15">[16]</ref>, of bottom-up attention <ref type="bibr" target="#b1">[2]</ref>, a Feature Pyramid Network (FPN) model <ref type="bibr" target="#b24">[25]</ref> is used to compute region features. This is different from the original Faster R-CNN model <ref type="bibr" target="#b32">[33]</ref> used, and it is commonly believed that FPN can offer better object detection quality. Therefore, to reach a more solid conclusion, in this appendix we show extended results from the main paper to compare our grid features with FPN region features. The FPN model uses an entire ResNet model as the backbone, where the multi-scale feature maps of different blocks of the ResNet model are fused in a feature pyramid. Two randomly initialized fully-connect layers (denoted as fc6 and fc7 for simplicity) are added to predict object category, bounding box regression offsets, and attribute labels for each bounding box proposal. We follow the strategy used in <ref type="bibr" target="#b15">[16]</ref> to compute region features. Specifically, we use the output of the fc6 layer as input to a VQA or image captioning model, where the fc7 layer is also used and fine-tuned during VQA training.</p><p>Accuracy on the VQA 2.0 test-dev set and breakdown inference time of the FPN model, using a ResNet50 as the backbone, are summarized in <ref type="table" target="#tab_15">Table 9</ref>. Different from the trend observed in object detection <ref type="bibr" target="#b24">[25]</ref>, we find the FPN model, when used to provide region features for VQA, does not show clear advantage over the original C4 model <ref type="bibr" target="#b1">[2]</ref>, which in turn gives on-par results to our grid features. Speed-wise, despite the lighter pre-region computation, we find the region-related steps with FPN are still very expensive, and the efficiency advantage of our grid features is even more significant.</p><p>We also test the top 100 (N =100) regions using different backbones, VQA models, VQA tasks, and image captioning task, as we have done in Section 6 in the paper. Results are reported in <ref type="table" target="#tab_1">Table 10a</ref>, 10b, 10c, and 10d. For the accuracy on the VQA 2.0 test-dev set and VizWiz, the FPN model's accuracy is lower than the results reported in <ref type="bibr" target="#b15">[16]</ref>, because grid features (from an ImageNet pre-trained ResNet-152 <ref type="bibr" target="#b14">[15]</ref> model) are used in addition to the region features <ref type="bibr" target="#b15">[16]</ref>. Using the MCAN model <ref type="bibr" target="#b49">[50]</ref>, the FPN model achieves better results than reported in <ref type="bibr" target="#b49">[50]</ref> but still performs worse than C4 and our grid features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Details of PPM</head><p>In Section 7 of the main paper, we introduce end-to-end training of the Pythia VQA model <ref type="bibr" target="#b15">[16]</ref> with PPM (Pyramid Pooling Module) <ref type="bibr" target="#b53">[54]</ref>. A detailed illustration of this module is provided in <ref type="figure" target="#fig_5">Fig. 6</ref>. Given a grid convolution feature map from a ResNet model, adaptive average pooling operations are performed at three different spatial resolutions: 1×1, 4×4, and 8×8. Three separate convolution layers (followed by batch normalization and ReLU) are added, where the kernel sizes are    <ref type="table" target="#tab_10">Table 6</ref> in the main paper for generalization experiments. From left to right: (a) Different backbone. We use a ResNeXt-101-32x8d instead of a ResNet-50 as the backbone. (b) Different VQA model. We use MCAN <ref type="bibr" target="#b49">[50]</ref> implementation which is the state-of-the-art VQA model. (c) Accuracy on VizWiz using the same VQA models <ref type="bibr" target="#b15">[16]</ref>. all set to 1 and output dimensions are all 512. Finally, the original grid feature map is concatenated together with the three ones obtained from PPM as the input for VQA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>From regions to grids. Left: We convert the original region feature extractor used by bottom-up attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>VQA accuracy vs. number of features N as input</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>2, grid features are not new -in fact, they were widely used in vision and language tasks before the introduction of bottom-up attention features. Com-Q: Which devices do you see? Q: Has the pizza been eaten? Q: What color are the curtains? Q: What is the cat laying on? GT-A: phones GT-A: no GT-A: red and white GT-A: suitcase A(R): phones A(G): phones A(R): no A(G): yes A(R): red A(G): red and white A(R): shoes A(G): shoe Q: Is the plate white? Q: What breed of dog is this? Q: What is the person doing? Q: How many boats do you see? GT-A: yes GT-A: pug GT-A: cutting GT-A: 7 A(R): yes A(G): yes A(R): pug A(G): bulldog A(R): texting A(G): cutting AVisualizations of attention maps overlaid on images produced by VQA models [16]. Source images taken from COCO [27] to compare against bottom-up attention [2] on VQA 2.0 [11]. We show questions (Q), ground-truth answers (GT-A), and side-by-side predictions (attention maps, answers) of region (R) and grid (G) features. From left to right: (a) both region and grid features give correct answers, (b) region features give correct answers but grid features fail, (c) region features fail but grid features give correct answers, and (d) both region and grid features fail. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Analysis on attribute loss weights when pre-training grid features on Visual Genome (VG). All results on VQA 2.0 vqa-eval set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>(d) Image captioning on COCO Karpathy test split. Abbreviations: BLEU4 (B4), BLEU3 (B3), BLEU2 (B2), BLEU1 (B1), ROUGE L (RL), METEOR (M), CIDEr (C), and SPICE (S). Our grid features generalize well by achieving results at-par with bottom-up region features while being significantly faster. Illustration of PPM (Pyramid Pooling Module) [54] experimented in the end-to-end model for VQA. See Section C for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>-</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>region</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>selection</cell><cell></cell><cell></cell></row><row><cell>Pipeline</cell><cell>Bottom-Up Ours</cell><cell>(region) (grid)</cell><cell>image image</cell><cell>grid features grid features</cell><cell>region features</cell><cell>VQA VQA</cell></row><row><cell>Running Time</cell><cell>Bottom-Up Ours</cell><cell>(66.13) (66.27)</cell><cell>0.02s</cell><cell></cell><cell></cell><cell>0.89s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Main comparison. 'R' stands for region features as in bottomup attention<ref type="bibr" target="#b1">[2]</ref>. 'G' stands for grid features. All results reported on VQA 2.0 vqa-eval. We show that: 1) by simply extracting grid features from the same layer C 5 of the same model, the VQA accuracy is already much</figDesc><table><row><cell></cell><cell></cell><cell cols="2">VG detection pre-train</cell><cell></cell><cell>VQA</cell><cell></cell></row><row><cell cols="4"># feature RoIPool region layers</cell><cell>AP</cell><cell>accuracy</cell><cell>∆</cell></row><row><cell>1 2</cell><cell>R [2]</cell><cell>14×14 1×1</cell><cell>C 5 [15] 2-FC</cell><cell>4.07 2.90</cell><cell>64.29 63.94</cell><cell>--0.35</cell></row><row><cell>3</cell><cell></cell><cell>14×14</cell><cell>C 5</cell><cell>4.07</cell><cell>63.64</cell><cell>-0.65</cell></row><row><cell>4</cell><cell>G</cell><cell>1×1</cell><cell>2-FC</cell><cell>2.90</cell><cell>64.37</cell><cell>0.08</cell></row><row><cell>5</cell><cell></cell><cell cols="2">ImageNet pre-train</cell><cell></cell><cell>60.76</cell><cell>-3.53</cell></row></table><note>closer to bottom-up attention than ImageNet pre-trained ones (row 1,3 &amp; 5); 2) 1×1 RoIPool based detector pre-training improves the grid fea- tures accuracy while the region features get worse (row 1,2 &amp; 4). Last col- umn is the gap compared to the original bottom-up features (underlined).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. We make two observations: First, compared with the widely used bottom-up region features (row 1), directly extracting outputs from C 5 with the same model (row 3) works surprisingly well (64.29 vs. 63.64 accuracy). In contrast, the standard ResNet-50 model pre-trained on ImageNet<ref type="bibr" target="#b5">[6]</ref> shows much worse performance -60.76 accuracy, a gap of more than 3% with the bottom-up features.Second, while our 1×1 RoIPool-based variant hurts the object detection performance (average precision<ref type="bibr" target="#b26">[27]</ref> on VG drops from 4.07 to 2.90), it helps VQA -boosting the accuracy by 0.73% (row3 &amp; 4)  and as a result slightly outperforms the original region-based features. On the other hand, our RoI-based variant does not help the region features method and drops the accuracy of region features to 63.94. This indicates the original model used by bottomup attention favors regions; while our design works better for grids. Thus, we use the setting of the 1 st row (best for</figDesc><table><row><cell></cell><cell></cell><cell>VQA2 vqa-eval set</cell></row><row><cell></cell><cell>64.4</cell><cell></cell></row><row><cell></cell><cell>64.2</cell><cell></cell></row><row><cell>Accuracy</cell><cell>64.0</cell><cell></cell></row><row><cell></cell><cell>63.8</cell><cell></cell></row><row><cell></cell><cell>63.6</cell><cell></cell><cell>region features grid features</cell></row><row><cell></cell><cell>100</cell><cell>200 Maximum Number of Regions/Grids as Input for VQA 300 400 500</cell><cell>600</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Region vs. grid features on the VQA 2.0 test-dev with accuracy and inference time breakdown measured in milliseconds per image. Our grid features achieve comparable VQA accuracy to region features while being much faster without region feature computation and region selection.</figDesc><table /><note>and grid features. Note that different from previous setups, we use trainval+vqa-eval for training. We report the VQA accuracy and the inference time breakdown in Ta- ble 2. Unlike our grid features which directly use convo- lutional feature maps, region features involve additional op- erations of region selection and region feature computation. These additional operations take 98.3% of the total infer- ence time for a region-based model. As a result, the VQA model that takes our grid features as input runs 48× faster than its counterpart using bottom-up region features.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Impact of input image size on the VQA 2.0 vqa-eval set. Grid features benefit from larger input image sizes. For an ImageNet pretrained model, the accuracy saturates around 600×1000 but the VG model makes a better use of larger input image sizes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Generalizations of grid features. From left to right: (a) Different backbone. We use a ResNeXt-101-32x8d instead of a ResNet-50 as the backbone. (b) Different VQA model. We use MCAN<ref type="bibr" target="#b49">[50]</ref> implementation which is the state-of-the-art VQA model. (c) Accuracy on VizWiz using the same VQA models<ref type="bibr" target="#b15">[16]</ref>. (d) Image captioning on COCO Karpathy test split.</figDesc><table><row><cell>Abbreviations: BLEU4 (B4), METEOR (M), CIDEr (C), and SPICE (S). Our</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Results of end-to-end trained VQA models with grid features on the VQA 2.0 test-dev set. End-to-end learning boosts accuracy for all models and more for ones trained on ImageNet and YFCC. Adding PPM<ref type="bibr" target="#b53">[54]</ref> further improves accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Summary of hyperparameters. We follow the default setting for most of the models. For the image captioning model</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>This table extends Table 2in the main paper for speed and accuracy comparisons with added rows for region features with FPN. Results are reported on VQA 2.0 test-dev with accuracy and inference time breakdown measured in milliseconds per image. Despite the advantages which FPN features have that 1) pools features from higher-resolution feature maps; and 2) fine-tunes the fc7 layer<ref type="bibr" target="#b15">[16]</ref> when training VQA; our grid features achieve comparable VQA accuracy to all region features and are much faster. 46.8 60.4 76.4 56.5 27.7 113.9 20.8 1101 R, w/ FPN 35.7 46.5 60.3 76.6 56.4 27.5 113.1 20.6 1099 G 36.4 47.3 61.1 76.7 56.6 27.4 113.8 20.7 240</figDesc><table><row><cell></cell><cell></cell><cell cols="3">VQA 2.0 accuracy</cell><cell></cell><cell>time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">VQA 2.0 accuracy</cell><cell>time</cell></row><row><cell></cell><cell>Yes/No</cell><cell cols="2">Number</cell><cell>Other</cell><cell>Overall</cell><cell>(ms)</cell><cell></cell><cell></cell><cell></cell><cell>Yes/No</cell><cell cols="2">Number</cell><cell>Other</cell><cell>Overall</cell><cell>(ms)</cell></row><row><cell>[16]</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>68.31</cell><cell>-</cell><cell></cell><cell>[50]</cell><cell></cell><cell>87.39</cell><cell cols="2">52.78</cell><cell>60.98</cell><cell>70.93</cell><cell>-</cell></row><row><cell>R</cell><cell>84.73</cell><cell>46.88</cell><cell></cell><cell>58.98</cell><cell>68.21</cell><cell>929</cell><cell></cell><cell>R</cell><cell></cell><cell>88.19</cell><cell cols="2">54.38</cell><cell>62.19</cell><cell>72.01</cell><cell>963</cell></row><row><cell>R, w/ FPN</cell><cell>83.88</cell><cell>45.13</cell><cell></cell><cell>58.12</cell><cell>67.26</cell><cell>1069</cell><cell></cell><cell>R, w/ FPN</cell><cell></cell><cell>87.77</cell><cell cols="2">54.72</cell><cell>62.16</cell><cell>71.87</cell><cell>1100</cell></row><row><cell>G</cell><cell>84.13</cell><cell>45.98</cell><cell></cell><cell>58.76</cell><cell>67.76</cell><cell>39</cell><cell></cell><cell>G</cell><cell></cell><cell>88.46</cell><cell cols="2">55.68</cell><cell>62.85</cell><cell>72.59</cell><cell>72</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell></cell><cell cols="5">VizWiz accuracy Yes/No Number Other Un. Ans. Overall</cell><cell>time (ms)</cell><cell></cell><cell>B4</cell><cell>B3</cell><cell>B2</cell><cell>B1</cell><cell>RL</cell><cell>M</cell><cell>C</cell><cell>S</cell><cell>time (ms)</cell></row><row><cell>[16]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.22</cell><cell>-</cell><cell>[2]</cell><cell>36.2</cell><cell>-</cell><cell>-</cell><cell cols="3">77.2 56.4 27.0 113.5 20.3</cell><cell>-</cell></row><row><cell cols="2">R R, w/ FPN 73.00 73.17 G 75.17</cell><cell cols="2">28.89 83.63 27.11 82.02 24.89 83.68 (c)</cell><cell>35.62 33.59 35.35</cell><cell cols="2">54.28 874 52.50 1051 54.17 38</cell><cell>R</cell><cell cols="4">36.2 (d)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>This table extends</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/facebookresearch/ maskrcnn-benchmark</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Since NMS is used in selecting regions, the maximum number N varies across images. Therefore we 1) cannot directly set it to the same number as grids and 2) report maximum N instead (zero paddings are used for images with fewer regions).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Strictly speaking, VG also uses ImageNet classification for pretraining, because the detector is fine-tuned from a standard ImageNet pretrained model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/MILVLG/mcan-vqa</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank Larry Zitnick, Duy-Kien Nguyen, Devi Parikh, and Amanpreet Singh for helpful discussions. ELM acknowledges support from AFRL and DARPA (#FA8750-18-2-0126). The U.S. government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the AFRL and DARPA or the U.S. government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Details of Hyperparameters</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NAACL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploring nearest neighbor approaches for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04467</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vizwiz grand challenge: Answering visual questions from blind people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danna</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigale</forename><forename type="middle">J</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The symbol grounding problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stevan</forename><surname>Harnad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pythia v0.1: the winning entry to the vqa challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09956</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhaya</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Answer them all! toward universal visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robik</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards vqa models that can read</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visuallinguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning crossmodality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Xiaodong He, and Anton van den Hengel. Tips and tricks for visual question answering: Learnings from the 2017 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Interpretable counting for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Trott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural-symbolic VQA: disentangling reasoning from vision and language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pushmeet Kohli, and Josh Tenenbaum</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multimodal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to count objects in natural images for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><forename type="middle">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam Prügel-</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unified vision-language pre-training for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11059</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
