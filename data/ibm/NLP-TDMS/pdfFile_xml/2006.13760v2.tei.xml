<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The NetHack Learning Environment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">University of Oxford * New York University # Imperial College London ! University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">University of Oxford * New York University # Imperial College London ! University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">=</forename><surname>Alexander</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">University of Oxford * New York University # Imperial College London ! University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Miller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">University of Oxford * New York University # Imperial College London ! University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename><surname>Raileanu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">University of Oxford * New York University # Imperial College London ! University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Selvatici</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">University of Oxford * New York University # Imperial College London ! University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">#</forename><forename type="middle">Edward</forename><surname>Grefenstette</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">University of Oxford * New York University # Imperial College London ! University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">University of Oxford * New York University # Imperial College London ! University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The NetHack Learning Environment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Progress in Reinforcement Learning (RL) algorithms goes hand-in-hand with the development of challenging environments that test the limits of current methods. While existing RL environments are either sufficiently complex or based on fast simulation, they are rarely both. Here, we present the NetHack Learning Environment (NLE), a scalable, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular single-player terminalbased roguelike game, NetHack. We argue that NetHack is sufficiently complex to drive long-term research on problems such as exploration, planning, skill acquisition, and language-conditioned RL, while dramatically reducing the computational resources required to gather a large amount of experience. We compare NLE and its task suite to existing alternatives, and discuss why it is an ideal medium for testing the robustness and systematic generalization of RL agents. We demonstrate empirical success for early stages of the game using a distributed Deep RL baseline and Random Network Distillation exploration, alongside qualitative analysis of various agents trained in the environment. NLE is open source and available at https://github.com/facebookresearch/nle.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent advances in (Deep) Reinforcement Learning (RL) have been driven by the development of novel simulation environments, such as the Arcade Learning Environment (ALE) <ref type="bibr" target="#b8">[9]</ref>, StarCraft <ref type="bibr" target="#b62">[64,</ref><ref type="bibr" target="#b67">69]</ref>, BabyAI <ref type="bibr" target="#b15">[16]</ref>, Obstacle Tower <ref type="bibr" target="#b36">[38]</ref>, Minecraft <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">35]</ref>, and Procgen Benchmark <ref type="bibr" target="#b17">[18]</ref>. These environments introduced new challenges for state-of-the-art methods and demonstrated failure modes of existing RL approaches. For example, Montezuma's Revenge highlighted that methods performing well on other ALE tasks were not able to successfully learn in this sparse-reward environment. This sparked a long line of research on novel methods for exploration [e.g., <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b64">66,</ref><ref type="bibr" target="#b51">53]</ref> and learning from demonstrations [e.g., <ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b5">6]</ref>. However, this progress has limits: the current best approach on this environment, Go-Explore <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, overfits to specific properties of ALE and Montezuma's Revenge. While Go-Explore is an impressive solution for Montezuma's Revenge, it exploits the determinism of environment transitions, allowing it to memorize sequences of actions that lead to previously visited states from which the agent can continue to explore.</p><p>We are interested in surpassing the limits of deterministic or repetitive settings and seek a simulation environment that is complex and modular enough to test various open research challenges such as exploration, planning, skill acquisition, memory, and transfer. However, since state-of-the-art RL approaches still require millions or even billions of samples, simulation environments need to be fast to allow RL agents to perform many interactions per second. Among attempts to surpass the limits of deterministic or repetitive settings, procedurally generated environments are a promising path towards testing systematic generalization of RL methods [e.g., <ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b17">18]</ref>. Here, the game state is generated programmatically in every episode, making it extremely unlikely for an agent to visit the exact state more than once during its lifetime. Existing procedurally generated RL environments are either costly to run [e.g., <ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b36">38]</ref> or are, as we argue, of limited complexity [e.g., <ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>To address these issues, we present the NetHack Learning Environment (NLE), a procedurally generated environment that strikes a balance between complexity and speed. It is a fully-featured Gym environment <ref type="bibr" target="#b10">[11]</ref> around the popular open-source terminal-based single-player turn-based "dungeon-crawler" game, NetHack <ref type="bibr" target="#b41">[43]</ref>. Aside from procedurally generated content, NetHack is an attractive research platform as it contains hundreds of enemy and object types, it has complex and stochastic environment dynamics, and there is a clearly defined goal (descend the dungeon, retrieve an amulet, and ascend). Furthermore, NetHack is difficult to master for human players, who often rely on external knowledge to learn about strategies and NetHack's complex dynamics and secrets. <ref type="bibr" target="#b0">1</ref> Thus, in addition to a guide book <ref type="bibr" target="#b56">[58,</ref><ref type="bibr" target="#b57">59]</ref> released with NetHack itself, many extensive community-created documents exist, outlining various strategies for the game [e.g., <ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>In summary, we make the following core contributions: (i) we present NLE, a fast but complex and feature-rich Gym environment for RL research built around the popular terminal-based game, NetHack, (ii) we release an initial suite of tasks in the environment and demonstrate that novel tasks can be added easily, (iii) we introduce baseline models trained using IMPALA <ref type="bibr" target="#b23">[24]</ref> and Random Network Distillation (RND) <ref type="bibr" target="#b12">[13]</ref>, a popular exploration bonus, resulting in agents that learn diverse policies for early stages of NetHack, and (iv) we demonstrate the benefit of NetHack's symbolic observation space by presenting in-depth qualitative analyses of trained agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NetHack: a Frontier for Reinforcement Learning Research</head><p>In traditional so-called roguelike games (e.g., Rogue, Hack, NetHack, and Dungeon Crawl Stone Soup) the player acts turn-by-turn in a procedurally generated grid-world environment, with game dynamics strongly focused on exploration, resource management, and continuous discovery of entities and game mechanics <ref type="bibr">[IRDC, 2008]</ref>. These games are designed to provide a steep learning curve and a constant level of challenge and surprise to the player. They are generally extremely difficult to win even once, let alone to master, i.e., win regularly and multiple times in a row.</p><p>As advocated by <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b17">18]</ref>, procedurally generated environments are a promising direction for testing systematic generalization of RL agents. We argue that such environments need to be both sufficiently complex and fast to run to serve as a challenging long-term research testbed. In Section 2.1, we illustrate that NetHack contains many desirable properties, making it an excellent candidate for driving long-term research in RL. We introduce NLE in Section 2.2, an initial suite of tasks in Section 2.3, an evaluation protocol for measuring progress towards solving NetHack in Section 2.4, as well as baseline models in Section 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">NetHack</head><p>NetHack is one of the oldest and most popular roguelikes, originally released in 1987 as a successor to Hack, an open-source implementation of the original Rogue game. At the beginning of the game, the player takes the role of a hero who is placed into a dungeon and tasked with finding the Amulet of Yendor to offer it to an in-game deity. To do so, the player has to descend to the bottom of over 50 procedurally generated levels to retrieve the amulet and then subsequently escape the dungeon, unlocking five extremely challenging final levels (the four Elemental Planes and the Astral Plane).</p><p>Many aspects of the game are procedurally generated and follow stochastic dynamics. For example, the overall structure of the dungeon is somewhat linear, but the exact location of places of interest (e.g., the Oracle) and the structure of branching sub-dungeons (e.g., the Gnomish Mines) are determined randomly. The procedurally generated content of each level makes it highly unlikely that a player will ever experience the exact same situation more than once. This provides a fundamental challenge to learning systems and a degree of complexity that enables us to more effectively evaluate an agent's ability to generalize. It also disqualifies current state-of-the-art exploration methods such as Go-Explore <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> that are based on a goal-conditioned policy to navigate to previously visited That door is closed.    <ref type="figure" target="#fig_0">Figure 11</ref> in the appendix.</p><formula xml:id="formula_0">--------------- ---------- |........|....| |...=....| ------ |.............| #################*####.........| |&lt;...| |.............| |..!.?...| |...=-###%#+ . |.%...........| -------.-- |....| # --+-------.---- ############ # |@...| ### # # ## #----- # # -+---- # ##################### # ###....| 0 # # # # # # ### #|...| ### ## ######## # |...| ### ## --|--- ....-### -.----.-- # |.(..| |...| # |(......| # |.....#0##-----## |.......| # |....| ########### ### |.......| # |.....### #### |.......| # |....| ###........| # ------ |.....).| +# --------- Agent6850</formula><formula xml:id="formula_1">.--More-- -- -.-- --- ------.... |.------......| |...---.......- |............-| --+---+--.............| -.........|..........-- -- -----..|.--).)...---.-- --+- |..------.....---...-- |.. |...----.......%...| |. |...&gt;- |...%......-- -----.----------.--..| -..........- |..G.[@............|---|............| ---.G....---------.| --...----...-- |G%.-....| .| --..| -..| |%!.|....+ .| ---- -.| -----....| | -- .....| ------- Agent61322</formula><formula xml:id="formula_2">Legend " --Amulet ) --Weapon [ --Armor ! --Potion ? --Scroll / --Wand = --Ring + --Spellbook * --Gem ( --Tool O --Boulder $ --Gold % --Comestible</formula><p>states. Moreover, states in NetHack are composed of hundreds of possible symbols, resulting in an enormous combinatorial observation space. <ref type="bibr" target="#b1">2</ref> It is an open question how to best project this symbolic space to a low-dimensional representation appropriate for methods like Go-Explore. For example, Ecoffet et al.'s heuristic of downsampling images of states to measure their similarity to be used as an exploration bonus will likely not work for large symbolic and procedurally generated environments. NetHack provides further variation by different hero roles (e.g., monk, valkyrie, wizard, tourist), races (human, elf, dwarf, gnome, orc) and random starting inventories (see Appendix A for details). Consequently, NetHack poses unique challenges to the research community and requires novel ways to determine state similarity and, likely, entirely new exploration frameworks.   <ref type="figure">Figure 2</ref>: The hero (@) has to cross water (}) to get past Medusa (@, out of the hero's line of sight) down the staircase (&gt;) to the next level.</p><p>To provide a glimpse into the complexity of NetHack's environment dynamics, we closely follow the educational example given by "Mr Wendal" on YouTube. <ref type="bibr" target="#b2">3</ref> At a specific point in the game, the hero has to get past Medusa's Island (see <ref type="figure">Figure 2</ref> for an example). Medusa's Island is surrounded by water } that the agent has to cross. Water can rust and corrode the hero's metallic weapons ) and armor [. Applying a can of grease ( prevents rusting and corrosion. Furthermore, going into water will make a hero's inventory wet, erasing scrolls ? and spellbooks + that they carry. Applying a can of grease to a bag or sack ( will make it a waterproof container for items. But the sea can also contain a kraken ; that can grab and drown the hero, leading to instant death. Applying a can of grease to a hero's armor prevents the kraken from grabbing the hero. However, a cursed can of grease will grease the hero's hands instead and they will drop their weapon and rings. One can use a towel ( to wipe off grease. To reach Medusa @, the hero can alternatively use magic to freeze the water and turn it into walkable ice .. Wearing snow boots [ will help the hero not to slip. When Medusa is in the hero's line of sight, her gaze will petrify and instantly kill-the hero should use a towel to cover their eyes to fight Medusa, or even apply a mirror ( to petrify her with her own gaze.</p><p>There are many other entities a hero must learn to face, many of which appear rarely even across multiple games, especially the most powerful monsters. These entities are often compositional, for example a monster might be a wolf d, which shares some characteristics with other in-game canines such as coyotes d or hell hounds d. To help a player learn, NetHack provides in-game messages describing many of the hero's interactions (see the top of <ref type="figure" target="#fig_0">Figure 1</ref>). <ref type="bibr" target="#b3">4</ref> Learning to capture these interesting and somewhat realistic albeit abstract dynamics poses challenges for multi-modal and language-conditioned RL <ref type="bibr" target="#b44">[46]</ref>. NetHack is an extremely long game. Successful expert episodes usually last tens of thousands of turns, while average successful runs can easily last hundreds of thousands of turns, spawning multiple days of play-time. Compared to testbeds with long episode horizons such as StarCraft and Dota 2, NetHack's "episodes" are one or two orders of magnitude longer, and they wildly vary depending on the policy. Moreover, several official conducts exist in NetHack that make the game even more challenging, e.g., by not wearing any armor throughout the game (see Appendix A for more).</p><p>Finally, in comparison to other classic roguelike games, NetHack's popularity has attracted a larger number of contributors to its community. Consequently, there exists a comprehensive game wiki <ref type="bibr" target="#b48">[50]</ref> and many so-called spoilers <ref type="bibr" target="#b24">[25]</ref> that provide advice to players. Due to the randomized nature of NetHack, this advice is general in nature (e.g., explaining the behavior of various entities) and not a step-by-step guide. These texts could be used for language-assisted RL along the lines of <ref type="bibr" target="#b70">[72]</ref>. Lastly, there is also a large public repository of human replay data (over five million games) hosted on the NetHack Alt.org (NAO) servers, with hundreds of finished games per day on average <ref type="bibr" target="#b45">[47]</ref>. This extensive dataset could spur research advances in imitation learning, inverse RL, and learning from demonstrations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The NetHack Learning Environment</head><p>The NetHack Learning Environment (NLE) is built on NetHack 3.6.6, the 36th public release of NetHack, which was released on March 8th, 2020 and is the latest available version of the game at the time of publication of this paper. NLE is designed to provide a common, turn-based (i.e., synchronous) RL interface around the standard terminal interface of NetHack. We use the game as-is as the backend for our NLE environment, leaving the game dynamics unchanged. We added to the source code more control over the random number generator for seeding the environment, as well as various modifications to expose the game's internal state to our Python frontend.</p><p>By default, the observation space consists of the elements glyphs, chars, colors, specials, blstats, message, inv_glyphs, inv_strs, inv_letters, as well as inv_oclasses. The elements glyphs, chars, colors, and specials are tensors representing the (batched) 2D symbolic observation of the dungeon; blstats is a vector of agent coordinates and other character attributes ("bottom-line stats", e.g., health points, strength, dexterity, hunger level; normally displayed in the bottom area of the GUI), message is a tensor representing the current message shown to the player (normally displayed in the top area of the GUI), and the inv_* elements are padded tensors representing the hero's inventory items. More details about the default observation space and possible extensions can be found in Appendix B.</p><p>The environment has 93 available actions, corresponding to all the actions a human player can take in NetHack. More precisely, the action space is composed of 77 command actions and 16 movement actions. The movement actions are split into eight "one-step" compass directions (i.e., the agent moves a single step in a given direction) and eight "move far" compass directions (i.e., the agent moves in the specified direction until it runs into some entity). The 77 command actions include eating, opening, kicking, reading, praying as well as many others. We refer the reader to Appendix C as well as to the NetHack Guidebook <ref type="bibr" target="#b57">[59]</ref> for the full table of actions and NetHack commands. NLE comes with a Gym interface <ref type="bibr" target="#b10">[11]</ref> and includes multiple pre-defined tasks with different reward functions and action spaces (see next section and Appendix E for details). We designed the interface to be lightweight, achieving competitive speeds with Gym-based ALE (see Appendix D for a rough comparison). Finally, NLE also includes a dashboard to analyze NetHack runs recorded as terminal tty recordings. This allows NLE users to analyze replays of the agent's behavior at an arbitrary speed and provides an interface to visualize action distributions and game events (see Appendix H for details).   <ref type="figure" target="#fig_0">Figure 12</ref> in the appendix.  </p><formula xml:id="formula_3">-------.-- |....| # --+-------.---- ############ # |@...| ### # # ## #----- # # -+---- # ##################### # ###....| 0 # # # # # # ### #|...| ### ## ######## # |...| ### ## --|--- ....-### -.----.-- # |.(..| |...| # |(......| # |.....#0##-----## |.......| # |....| ########### ### |.......| # |.....### #### |.......| # |....| ###........| # ------ |.....).| +# ---------</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tasks</head><p>NLE aims to make it easy for researchers to probe the behavior of their agents by defining new tasks with only a few lines of code, enabled by NetHack's symbolic observation space as well as its rich entities and environment dynamics. To demonstrate that NetHack is a suitable testbed for advancing RL, we release a set of initial tasks for tractable subgoals in the game: navigating to a staircase down to the next level, navigating to a staircase while being accompanied by a pet, locating and eating edibles, collecting gold, maximizing in-game score, scouting to discover unseen parts of the dungeon, and finding the oracle. These tasks are described in detail in Appendix E, and, as we demonstrate in our experiments, lead to unique challenges and diverse behaviors of trained agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluation Protocol</head><p>We lay out a protocol and provide guidance for evaluating future work on NLE in a reproducible manner. The overall goal of NLE is to train agents that can solve NetHack. An episode in the full game of NetHack is considered solved if the agent retrieves the Amulet of Yendor and offers it to its co-aligned deity in the Astral Plane, thereby ascending to demigodhood. We declare NLE to be solved once agents can be trained to consecutively ascend (ten episodes without retry) to demigodhood on unseen seeds given a random role, race, alignment, and gender combination. Since the environment is procedurally generated and stochastic, evaluating on held-out unseen seeds ensures we test systematic generalization of agents. As of October 2020, NAO reports the longest streak of human ascensions on NetHack 3.6.x to be 61; the role, race, etc. are not necessarily randomized for these ascension streaks. Since we believe that this goal is out of reach for machine learning approaches in the foreseeable future, we recommend comparing models on the score task in the meantime. Using NetHack's in-game score as the measure for progress has caveats. For example, expert human players can solve NetHack while minimizing the score [see 50, "Score" entry, for details]. NAO reports ascension scores for NetHack 3.6.x ranging from the low hundreds of thousands to tens of millions. Although we believe training agents to maximize the in-game score is likely insufficient for solving the game, the in-game score is still a sensible proxy for incremental progress on NLE as it is a function of, among other things, the dungeon depth that the agent reached, the number of enemies it killed, the amount of gold it collected, as well as the knowledge it gathered about potions, scrolls, and wands.</p><p>When reporting results on NLE, we require future work to state the full character specification (e.g., mon-hum-neu-mal), all NetHack options that were used (e.g., whether or not autopickup was used), which actions were allowed (see <ref type="table" target="#tab_3">Table 1</ref>), which actions or action-sequences were hardcoded (e.g., engraving [see 50, "Elbereth" as an example]) and how many different seeds were used during training. We ask to report the average score obtained on 1000 episodes of randomly sampled and previously unseen seeds. We do not impose any restrictions during training, but at test time any save scumming (i.e., saving and loading previous checkpoints of the episode) or manipulation of the random number generator [e.g., 2] is forbidden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Baseline Models</head><p>For our baseline models, we encode the multi-modal observation o t as follows. Let the observation o t at time step t be a tuple (g t , z t ) consisting of the 21 × 79 matrix of glyph identifiers and a 21dimensional vector containing agent stats such as its (x, y)-coordinate, health points, experience level, and so on. We produce three dense representations based on the observation (see <ref type="figure" target="#fig_3">Figure 3</ref>). For every of the 5991 possible glyphs in NetHack (monsters, items, dungeon features, etc.), we learn a k-dimensional vector embedding. We apply a ConvNet (red) to all visible glyph embeddings as well as another ConvNet (blue) to the 9×9 crop of glyphs around the agent to create a dedicated egocentric representation for improved generalization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b69">71]</ref>. We found this egocentric representation to be an important component during preliminary experiments. Furthermore, we use an MLP to encode the hero's stats (green). These vectors are concatenated and processed by another MLP to produce a low-dimensional latent representation o t of the observation. Finally, we employ a recurrent policy parameterized by an LSTM <ref type="bibr" target="#b32">[33]</ref> to obtain the action distribution. For baseline results on the tasks above, we use a reduced action space that includes the movement, search, kick, and eat actions.</p><p>For the main experiments, we train the agent's policy for 1B steps in the environment using IM-PALA <ref type="bibr" target="#b23">[24]</ref> as implemented in TorchBeast <ref type="bibr" target="#b42">[44]</ref>. Throughout training, we change NetHack's seed for procedurally generating the environment after every episode. To demonstrate NetHack's variability based on the character configuration, we train with four different agent characters: a neutral human male monk (mon-hum-neu-mal), a lawful dwarf female valkyrie (val-dwa-law-fem), a chaotic elf male wizard (wiz-elf-cha-mal), and a neutral human female tourist (tou-hum-neu-fem). More implementation details can be found in Appendix F.</p><p>In addition, we present results using Random Network Distillation (RND) <ref type="bibr" target="#b12">[13]</ref>, a popular exploration technique for Deep RL. As previously discussed, exploration techniques which require returning to previously visited states such as Go-Explore are not suitable for use in NLE, but RND does not have this restriction. RND encourages agents to visit unfamiliar states by using the prediction error of a fixed random network as an intrinsic exploration reward, which has proven effective for hard exploration games such as Montezuma's Revenge <ref type="bibr" target="#b11">[12]</ref>. The intrinsic reward obtained from RND can create "reward bridges" between states which provide sparse extrinsic environmental rewards, thereby enabling the agent to discover new sources of extrinsic reward that it otherwise would not have reached. We replace the baseline network's pixel-based feature extractor with the symbolic feature extractor described above for the baseline model, and use the best configuration of other RND hyperparameters documented by the authors (see Appendix G for full details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We present quantitative results on the suite of tasks included in NLE using a standard distributed Deep RL baseline and a popular exploration method, before additionally analyzing agent behavior qualitatively. For each model and character combination, we present results of the mean episode return over the last 100 episodes averaged for five runs in <ref type="figure" target="#fig_7">Figure 5</ref>. We discuss results for individual tasks below (see <ref type="table" target="#tab_6">Table 5</ref> in the appendix for full details).</p><p>Staircase: Our agents learning to navigate the dungeon to the staircase &gt; with a success rate of 77.26% for the monk, 50.42% for the tourist, 74.62% for the valkyrie, and 80.42% for the wizard. What surprised us is that agents learn to reliably kick in locked doors. This is a costly action to explore as the agent loses health points and might even die when accidentally kicking against walls. Similarly, the agent has to learn to reliably search for hidden passages and secret doors. Often, this involves using the search action many times in a row, sometimes even at many locations on the map (e.g., around all walls inside a room). Since NLE is procedurally generated, during training agents might encounter easier environment instances and use the acquired skills to accelerate learning on the harder ones <ref type="bibr" target="#b58">[60,</ref><ref type="bibr" target="#b17">18]</ref>. With a small probability, the staircase down might be generated near the agent's starting position. Using RND exploration, we observe substantial gains in the success rate for the monk (+13.58pp), tourist (+6.52pp) and valkyrie (+16.34pp) roles, while lower results for wizard roles (−12.96pp).</p><p>Pet: Finding the staircase while taking care of the hero's pet (e.g., the starting kitten f or little dog d) is a harder task as the pet might get killed or fall into a trap door, making it impossible for the agent to successfully complete the episode. Compared to the staircase task, the agent success rates are generally lower (62.02% for monk, 25.66% for tourist, 63.30% for valkyrie, and wizard 66.80%). Again, RND exploration provides consistent and substantial gains.</p><p>Eat: This tasks highlights the importance of testing with different character classes in NetHack. The monk and tourist start with a number edible items (e.g., food rations %, apples % and oranges %).  sub-optimal strategy is to consume all of these comestibles right at the start of the episode, potentially risking choking to death. In contrast, the other roles have to hunt for food, which our agents learn to do slowly over time for the valkyrie and wizard roles. By having more pressure to quickly learn a sustainable food strategy, the valkyrie learns to outlast other roles and survives the longest in the game (on average 1713 time steps). Interestingly, RND exploration leads to consistently worse results for this task.</p><formula xml:id="formula_4">CNN mon-hum-neu-mal CNN tou-hum-neu-fem CNN val-dwa-law-fem CNN wiz-elf-cha-mal RND mon-hum-neu-mal RND val-dwa-law-fem RND wiz-elf-cha-mal RND tou-hum-neu-fem</formula><p>Gold: Locating gold $ in NetHack provides a relatively sparse reward signal. Still, our agents learn to collect decent amounts during training and learn to descend to deeper dungeon levels in search for more. For example, monk agents reach dungeon level 4.2 on average for the CNN baseline and even 5.0 using RND exploration.</p><p>Score: As discussed in Section 2.4, we believe this task is the best candidate for comparing future methods regarding progress on NetHack. However, it is questionable whether a reward function based on NetHack's in-game score is sufficient for training agents to solve the game. Our agents average at a score of 748 for monk, 11 for tourist, 573 for valkyrie, and 314 for wizard, with RND exploration again providing substantial gains (e.g. increasing the average score to 780 for monk). The resulting agents explore much of the early stages of the game, reaching dungeon level 5.4 on average for the monk with the deepest descent to level 11 achieving a high score of 4260 while leveling up to experience level 7 (see <ref type="table">Table 6</ref> in the appendix).</p><p>Scout: The scout task shows a trend that is similar to the score task. Interestingly, we observe a lower experience level and in-game score, but agents descend, on average, similarly deep into the dungeon (e.g. level 5.5 for monk). This is sensible, since a policy that avoids to fight monsters, thereby lowering the chances of premature death, will not increase the in-game score as fast or level up the character as quickly, thus keeping the difficulty of spawned monsters low. We note that delaying to level up in order to avoid encountering stronger enemies early in the game is a known strategy human players adopt in NetHack [e.g. 50, "Why do I keep dying?" entry, January 2019 version].</p><p>Oracle: None of our agents find the Oracle @ (except for one lucky valkyrie episode). Locating the Oracle is a difficult exploration task. Even if the agent learns to make its way down the dungeon levels, it needs to search many, potentially branching, levels of the dungeon. Thus, we believe this task serves as a challenging benchmark for exploration methods in procedurally generated environments in the short term. Long term, many tasks harder than this (e.g., reaching Minetown, Mines' End, Medusa's Island, The Castle, Vlad's Tower, Moloch's Sanctum etc.) can be easily defined in NLE with very few lines of code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generalization Analysis</head><p>Akin to <ref type="bibr" target="#b17">[18]</ref>, we evaluate agents trained on a limited set of seeds while still testing on 100 held-out seeds. We find that test performance increases monotonically with the size of the set of seeds that the agent is trained on. <ref type="figure" target="#fig_5">Figure 4</ref> shows this effect for the score and staircase tasks. Training only on a limited number of seeds leads to high training performance, but poor generalization. The gap between training and test performance becomes narrow when training with at least 1000 seeds, indicating that at that point agents are exposed to sufficient variation during training to make memorization infeasible. We also investigate how model capacity affects performance by comparing agents with five different hidden sizes for the final layer (of the architecture described in Section 2.5). <ref type="figure" target="#fig_11">Figure 7</ref> in the appendix shows that increasing the model capacity improves results on the score but not on the staircase task, indicating that it is an important hyperparameter to consider, as also noted by <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Qualitative Analysis</head><p>We analyse the cause for death of our agents during training and present results in <ref type="figure" target="#fig_14">Figure 9</ref> in the appendix. We notice that starvation and traps become a less prominent cause of death over time, most likely because our agents, when starting to learn to descend dungeon levels and fight monsters, are more likely to die in combat before they starve or get killed by a trap. In the score and scout tasks, our agents quickly learn to avoid eating rotten corpses, but food poisoning becomes again prominent towards the end of training.</p><p>We can see that gnome lords G, gnome kings G, chameleons :, and even mind flayers h become a more prominent cause of death over time, which can be explained with our agents leveling up and descending deeper into the dungeon. Chameleons are a particularly interesting entity in NetHack as they regularly change their form to a random animal or monster, thereby adversarially confusing our agent with rarely seen symbols for which it has not yet learned a meaningful representation (similar to unknown words in natural language processing). We release a set of high-score recordings of our agents (see Appendix J on how to view them via a browser or terminal).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Progress in RL has historically been achieved both by algorithmic innovations as well as development of novel environments to train and evaluate agents. Below, we review recent RL environments and delineate their strengths and weaknesses as testbeds for current methods and future research.</p><p>Recent Game-Based Environments: Retro video games have been a major catalyst for Deep RL research. ALE <ref type="bibr" target="#b8">[9]</ref> provides a unified interface to Atari 2600 games, which enables testing of RL algorithms on high-dimensional visual observations quickly and cheaply, resulting in numerous Deep RL publications over the years <ref type="bibr" target="#b3">[4]</ref>. The Gym Retro environment <ref type="bibr" target="#b49">[51]</ref> expands the list of classic games, but focuses on evaluating visual generalization and transfer learning on a single game, Sonic The Hedgehog.</p><p>Both StarCraft: BroodWar and StarCraft II have been successfully employed as RL environments <ref type="bibr" target="#b62">[64,</ref><ref type="bibr" target="#b67">69]</ref> for research on, for example, planning <ref type="bibr" target="#b50">[52,</ref><ref type="bibr" target="#b47">49]</ref>, multi-agent systems <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b61">63]</ref>, imitation learning <ref type="bibr" target="#b68">[70]</ref>, and model-free reinforcement learning <ref type="bibr" target="#b68">[70]</ref>. However, the complexity of these games creates a high entry barrier both in terms of computational resources required as well as intricate baseline models that require a high degree of domain knowledge to be extended.</p><p>3D games have proven to be useful testbeds for tasks such as navigation and embodied reasoning.</p><p>Vizdoom <ref type="bibr" target="#b40">[42]</ref> modifies the classic first-person shooter game Doom to construct an API for visual control; DeepMind Lab <ref type="bibr" target="#b6">[7]</ref> presents a game engine based on Quake III Arena to allow for the creation of tasks based on the dynamics of the original game; Project Malmo <ref type="bibr" target="#b35">[37]</ref>, MineRL <ref type="bibr" target="#b28">[29]</ref> and CraftAssist <ref type="bibr" target="#b33">[35]</ref> provide visual and symbolic interfaces to the popular Minecraft game. While Minecraft is also procedurally generated and has complex environment dynamics that an agent needs to learn about, it is much more computationally demanding than NetHack (see <ref type="table" target="#tab_5">Table 4</ref> in the appendix). As a consequence, the focus has been on learning from demonstrations <ref type="bibr" target="#b28">[29]</ref>.</p><p>More recent work has produced game-like environments with procedurally generated elements, such as the Procgen Benchmark <ref type="bibr" target="#b17">[18]</ref>, MazeExplorer <ref type="bibr" target="#b29">[30]</ref>, and the Obstacle Tower environment <ref type="bibr" target="#b36">[38]</ref>. However, we argue that, compared to NetHack or Minecraft, these environments do not provide the depth likely necessary to serve as long-term RL testbeds due to limited number of entities and environment interactions that agents have to learn to master. In contrast, NetHack agents have to acquire knowledge about complex environment dynamics of hundreds of entities (dungeon features, items, monsters etc.) to do well in a game that humans often take years of practice to solve.</p><p>In conclusion, none of the current benchmarks combine a fast simulator with a procedurally generated environment, a hard exploration problem, a wide variety of complex environment dynamics, and numerous types of static and interactive entities. The unique combination of challenges present in NetHack makes NLE well-suited for driving research towards more general and robust RL algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Roguelikes as Reinforcement Learning Testbeds:</head><p>We are not the first to argue for roguelike games to be used as testbeds for RL. Asperti et al. <ref type="bibr" target="#b4">[5]</ref> present an interface to Rogue, the very first roguelike game and one of the simplest roguelikes in terms of game dynamics and difficulty. They show that policies trained with model-free RL algorithms can successfully learn rudimentary navigation. Similarly, Kanagawa and Kaneko <ref type="bibr" target="#b39">[41]</ref> present an environment inspired by Rogue that provides a parameterizable generation of Rogue levels. Like us, Dannenhauer et al. <ref type="bibr" target="#b19">[20]</ref> argue that roguelike games could be a useful RL testbed. They discuss the roguelike game Dungeon Crawl Stone Soup, but their position paper provides neither an RL environment nor experiments to validate their claims.</p><p>Most similar to our work is gym_nethack <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, which offers a Gym environment based on NetHack 3.6.0. We commend the authors for introducing NetHack as an RL environment, and to the best of our knowledge they were the first to suggest the idea. However, there are several design choices that limit the impact and longevity of their version as a research testbed. First, they heavily modified NetHack to enable agent interaction. In the process, gym_nethack disables various crucial game mechanics to simplify the game, its environment dynamics, and the resulting optimal policies. This includes removing obstacles like boulders, traps, and locked doors as well as all item identification mechanics, making items much easier to employ and the overall environment much closer to its simpler predecessor, Rogue. Additionally, these modifications tie the environment to a particular version of the game. This is not ideal as (i) players tend to use new versions of the game as they are released, hence, publicly available human data becomes progressively incompatible, thereby limiting the amount of data that can be used for learning from demonstrations; (ii) older versions of NetHack tend to include well-documented exploits which may be discovered by agents (see Appendix I for exploits used in programmatic bots). In contrast, NLE is designed to make the interaction with NetHack as close as possible to the one experienced by humans playing the full game. NLE is the only environment exposing the entire game in all its complexity, allowing for larger-scale experimentation to push the boundaries of RL research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>The NetHack Learning Environment is a fast, complex, procedurally generated environment for advancing research in RL. We demonstrate that current state-of-the-art model-free RL serves as a sensible baseline, and we provide an in-depth analysis of learned agent behaviors.</p><p>NetHack provides interesting challenges for exploration methods given the extremely large number of possible states and wide variety of environment dynamics to discover. Previously proposed formulations of intrinsic motivation based on seeking novelty <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b12">13]</ref> or maximizing surprise <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b55">57]</ref> are likely insufficient to make progress on NetHack given that an agent will constantly find itself in novel states or observe unexpected environment dynamics. NetHack poses further challenges since, in order to win, an agent needs to acquire a wide range of skills such as collecting resources, fighting monsters, eating, manipulating objects, casting spells, or taking care of their pet, to name just a few. The multilevel dependencies present in NetHack could inspire progress in hierarchical RL and long-term planning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b66">68]</ref>. Transfer to unseen game characters, environment dynamics, or level layouts can be evaluated <ref type="bibr" target="#b65">[67]</ref>. Furthermore, its richness and constant challenge make NetHack an interesting benchmark for lifelong learning <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b46">48]</ref>. In addition, the extensive documentation about NetHack can enable research on using prior (natural language) knowledge for learning, which could lead to improvements in generalization and sample efficiency <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b70">72,</ref><ref type="bibr" target="#b34">36]</ref>. Lastly, NetHack can also drive research on learning from demonstrations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> since a large collection of replay data is available. In sum, we argue that the NetHack Learning Environment strikes an excellent balance between complexity and speed while encompassing a variety of challenges for the research community.</p><p>For future versions of the environment, we plan to support NetHack 3.7 once it is released, as it will further increase the variability of observations via Themed Rooms. This version will also introduce scripting in the Lua language, which we will leverage to enable users to create their custom sandbox tasks, directly tapping into NetHack and its rich universe of entities and their complex interactions to define custom RL tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Broader Impact</head><p>To bridge the gap between the constrained world of video and board games, and the open and unpredictable real world, there is a need for environments and tasks which challenge the limits of current Reinforcement Learning (RL) approaches. Some excellent challenges have been put forth over the years, demanding increases in the complexity of policies needed to solve a problem or scale needed to deal with increasingly photorealistic, complex environments. In contrast, our work seeks to be extremely fast to run while still testing the generalization and exploration abilities of agents in an environment which is rich, procedurally generated, and in which reward is sparse. The impact of solving these problems with minimal environment-specific heuristics lies in the development of RL algorithms which produce sample efficient, robust, and general policies capable of more readily dealing with the uncertain and changing dynamics of "real world" environments. We do not solve these problems here, but rather provide the challenge and the testbed against such improvements can be produced and evaluated.</p><p>Auxiliary to this, and in line with growing concerns that progress in Deep RL is more the result of industrial labs having privileged access to the resources required to run environments and agents on a massive scale, the environment presented here is computationally cheap to run and to collect data in. This democratizes access for researchers in more resource-constrained labs, while not sacrificing the difficulty and richness of the environment. We hope that as a result of this, and of the more general need to develop sample-efficient agents with fewer data, the environmental impact of research using our environment will be reduced compared to more visually sophisticated ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Further Details on NetHack</head><p>Character options The player may choose (or pick randomly) the character from thirteen roles (archaeologist, barbarian, cave(wo)man, healer, knight, priest(ess), ranger, rogue, samurai, tourist, valkyrie, and wizard), five races (human, elf, dwarf, gnome, and orc), three moral alignments (neutral, lawful, chaotic), and two genders (male or female). Each choice determines some of the character's features, as well as how the character interacts with other entities (e.g., some species of monsters may not be hostile depending on the character race; priests of a particular deity may only help religiously aligned characters).</p><p>The hero's interaction with several game entities involves pre-defined stochastic dynamics (usually defined by virtual dice tosses), and the game is designed to heavily punish careless exploration policies. <ref type="bibr" target="#b4">5</ref> This makes NetHack an ideal environment for evaluating exploration methods such as curiosity-driven learning <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b11">12]</ref> or safe reinforcement learning <ref type="bibr" target="#b27">[28]</ref>.</p><p>Learning and planning in NetHack involves dealing with partial observability. The game, by default, employs Fog of War to hide information based on a simple 2D light model (see for example the difference between white . and gray . room tiles in <ref type="figure" target="#fig_0">Figure 1</ref> or <ref type="figure" target="#fig_0">Figure 11</ref>), requiring the player not only to discover the topology of the level (including searching for hidden doors and passages), but to also condition their policy on a world that might change, e.g., due to monsters spawning and interacting outside of the visible range.</p><p>On top of the standard ASCII interface, NetHack supports many official and unofficial graphical user interfaces. <ref type="figure" target="#fig_8">Figure 6</ref> shows a screenshot of Lu Wang's BrowserHack 6 as an example. Conducts While winning NetHack by retrieving and ascending with the Amulet of Yendor is already immensely challenging, experienced NetHack players like to challenge themselves even more by imposing additional restrictions on their play. The game tracks some of these challenges with the #conduct command <ref type="bibr" target="#b57">[59]</ref>. These official challenges include eating only vegan or vegetarian food, or not eating at all, or playing the game in "pacifist" mode without killing a single monster. While very experienced players often try to adhere to several challenges at once, even moderately experienced players often limit their use of certain polymorph spells (e.g., "polypiling"-changing the form of several objects at once in the hope of getting better ones) or they try to beat the game while minimizing the in-game score. We believe this established set of conducts will supply the RL community with a steady stream of extended challenges once the standard NetHack Learning Environment is solved by future methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Observation Space</head><p>The Gym environment is implemented by wrapping a more low-level NetHack Python object into a Python class responsible for the featurization, reward schedule and end-of-episode dynamics. While the low-level NetHack object gives access to a large number of NetHack game internals, the Gym wrapper exposes by default only a part of this data as numerical observation arrays, namely the observation tensors glyphs, chars, colors, specials, blstats, message, inv_glyphs, inv_strs, inv_letters, and inv_oclasses.</p><p>Glyphs, Chars, Colors, Specials: NetHack supports non-ASCII graphical user interfaces, dubbed window-ports (see <ref type="figure" target="#fig_8">Figure 6</ref> for an example). To support displaying different monsters, objects and floor types in the NetHack dungeon map as different tiles, NetHack internally defines glyphs as ids in the range 0, . . . , MAX_GLYPH, where MAX_GLYPH = 5991 in our build 7 . The glyph observation is an integer array of shape (21, 79) of these game glyph ids. <ref type="bibr" target="#b7">8</ref> In NetHack's standard terminal-based user interface, these glyphs are mapped into ASCII characters of different colors which we return as the chars, colors, and specials observations, both all which are of shape <ref type="bibr" target="#b20">(21,</ref><ref type="bibr">79)</ref>; chars are ASCII bytes in the range 0, . . . , 127 wheras colors are in range 0, . . . , 15. For additional highlighting (e.g., flipping background and foreground colors for the hero's pet), NetHack also computes xor'ed values which we return as the specials tensor.</p><p>Blstats: "Bottom line statistics", a integer vector of length 25, containing the (x, y) coordinate of the hero and the following 23 character stats that typically appear in the bottom line of the ASCII interface: strength_percentage, strength, dexterity, constitution, intelligence, wisdom, charisma, score, hitpoints, max_hitpoints, depth, gold, energy, max_energy, ar-mor_class, monster_level, experience_level, experience_points, time, hunger_state, carrying_capacity, dungeon_number, and level_number.</p><p>Message: A padded byte vector of length 256 representing the current message shown to the player, normally displayed in the top area of the GUI. We support different padding strategies and alphabet sizes, but by default we choose an alphabet size of 96, where the last character is used for padding.</p><p>Inventory: In NetHack's default ASCII user interface, the hero's inventory can be opened and closed during the game. Other user interfaces display a permanent inventory at all times. NLE follows that strategy. The inventory observations consist of the following four arrays: inv_glyphs: an integer vector of length 55 of glyph ids, padded with MAX_GLYPH; inv_strs: A padded byte array of shape (55, 80) describing the inventory items; inv_letters: A padded byte vector of length 55 with the corresponding ASCII character symbol; inv_oclasses: An integer vector of shape 55 with ids describing the type of inventory objects, padded with MAXOCLASSES = 18.</p><p>The low-level NetHack Python object has some additional methods to query and modify NetHack's game state, e.g. the current RNG seeds. We refer to the source code to describe these. 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Action Space</head><p>The game of NetHack uses ASCII inputs, i.e., individual keyboard presses including modifiers like Ctrl and Meta. NLE pre-defines 98 actions, 16 of which are compass directions and 82 of which are command actions. <ref type="table" target="#tab_3">Table 1</ref> gives a list of command actions, including their ASCII value and the corresponding key binding in NetHack, while <ref type="table" target="#tab_4">Table 3</ref> lists the 16 compass directions. For a detailed description of these actions, as well as other NetHack commands, we refer the reader to the NetHack guide book <ref type="bibr" target="#b57">[59]</ref>. Not all actions are sensible for standard RL training on NLE. E.g., the VERSION or QUIT actions are unlikely to be useful for direct input from the agent. NLE defines a list of USEFUL_ACTIONS that includes a subset of 76 actions; however, what is useful depends on the circumstances. In addition, even though an action like SAVE is unlikely to be useful in most game situations it corresponds to the letter S, which may be assigned to an inventory item or some other in-game menu entry such that it does become a useful action in that context.</p><p>By default, NLE will auto-apply the MORE action in situations where the game waits for input to display more messages.  <ref type="bibr" target="#b8">9</ref> See, e.g., the nethack.py as well as pynethack.cc files in the NLE repository. <ref type="bibr" target="#b9">10</ref> The descriptions are mostly taken from the cmd.c file in the NetHack source code.  <ref type="table" target="#tab_5">Table 4</ref> shows a comparison between popular Gym environments and NLE. All environments were controlled with a uniformly random policy using reset on terminal states. The tests were conducted on a MacBook Pro equipped with an Intel Core i7 2.9 GHz, 16GB of RAM, MacOS Mojave, Python 3.7, Conda 4.7.12, and latest available packages as of May 2020. ObstacleTowerEnv was instantiated with (retro=False, real_time=False). Note that this data does not necessarily reflect performance of these environments with better-or worse-policies, as each environment has computational dynamics that depend on its state. However, we expect the difference in terms of magnitude to remain mostly unchanged across these environments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Task Details</head><p>For all tasks described below, we add a penalty of −0.001 to the reward function if the agent's action did not advance the in-game timer, which, for example, happens when the agent tries to move against a wall or navigates menus. For all tasks, except the Gold task, we disable NetHack's autopick option <ref type="bibr" target="#b57">[59]</ref>. Furthermore, we disable so-called bones files that would otherwise lead to agents occasionally discovering the remains and ghosts of previous agents, considerably increasing the variance across episodes.</p><p>Staircase The agent has to find the staircase down &gt; to the next dungeon level. This task is already challenging, as there is often no direct path to the staircase. Instead  boulders O that obstruct a passage. The agent receives a reward of 100 once it reaches the staircase down and the the episode terminates after 1000 agent steps.</p><p>Pet Many successful strategies for NetHack rely on taking good care of the hero's pet (e.g., the little dog d or kitten f that the hero starts with). Pets are controlled by the game, but their behavior is influenced by the agent's actions. In this task, the agent only receives a positive reward of 100 when it reaches the staircase while the pet is next to the agent.</p><p>Eat To survive in NetHack, players have to make sure their character does not starve to death. There are many edible objects in the game, for example food rations %, tins, and monster corpses. In this task, the agent receives the increase of nutrition as determined by the in-game "Hunger" status as reward [see 50, "Nutrition" entry for details]. A steady source of nutrition are monster corpses, but for that the agent has to learn to locate and to kill monsters while avoiding to consume rotten corpses, poisonous monster corpses such as Kobolds k or acidic monster corpses such as Acid Blobs b.</p><p>Gold Throughout the game, the player can collect gold $ to, for example, trade for useful items with shopkeepers. The agent receives the amount of gold it collects as reward. This incentivizes the agent to explore dungeon maps fully and to descend dungeon levels to discover new sources of gold. There are many advanced strategies for obtaining large amounts of gold such as finding, identifying and selling gems; stealing from or killing shopkeepers; or hunting for vaults or leprechaun halls. To make this task easier for the agent, we enable NetHack's autopickup option for gold.</p><p>Scout An important part of the game is exploring dungeon levels. Here, we reward the agent (+1) for uncovering previously unknown tiles in the dungeon, for example by entering a new room or following a newly discovered passage. Like the previous task, this incentivizes the agent to explore dungeon levels and to descend.</p><p>Score In this task, the agent receives the increase of the in-game score between two time steps as reward. The in-game score is governed by a complex calculation, but in early stages of the game it is dominated by killing monsters and the number of dungeon levels that the agent descends [see 50, "Score" entry for details].</p><p>Oracle While levels are procedurally generated, there are a number of landmarks that appear in every game of NetHack. One such landmark is the Oracle @, which is randomly placed between levels five and nine of the dungeon. Reliably finding the Oracle is difficult, as it requires the agent to go down multiple staircases and often to exhaustively explore each level. In this task, the agent receives a reward of 1000 if it manages to reach the Oracle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Baseline CNN Details</head><p>As embedding dimension of the glyphs we use 32 and for the hidden dimension for the observation o t and the output of the LSTM h t , we use 128. For encoding the full map of glyphs as well as the 9 × 9 crop, we use a 5-layer ConvNet architecture with filter size 3 × 3, padding 1 and stride 1. The input channel of the first layer of the ConvNet is the embedding size of the glyphs <ref type="bibr" target="#b31">(32)</ref>. Subsequent layers have an input and output channel dimension of 16. We employ a gradient norm clipping of 40 and clip rewards using r c = tanh(r/100). We use RMSProp with a learning rate of 0.0002 without momentum and with ε RMSProp = 0.000001. Our entropy cost is set to 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Random Network Distillation Details</head><p>For RND hyperparameters we mostly follow the recommendations by the authors <ref type="bibr" target="#b12">[13]</ref>:</p><p>• we initialize the weights according to the original paper, using an orthogonal distribution with a gain of √ 2 • we use a two-headed value function rather than merely summing the intrinsic and extrinsic reward • we use a discounting factor of 0.999 for the extrinsic reward and 0.99 for the intrinsic reward • we use non-episodic intrinsic reward and episodic extrinsic reward • we use reward normalization for the intrinsic reward, dividing it by a running estimate of its standard deviation</p><p>We modify a few of the parameters for use in our setting:</p><p>• we use exactly the same feature extraction architecture as the baseline model instead of the pixel-based convolutional feature extractor • we do not use observation normalization, again due to the symbolic nature of our observation space</p><p>• before normalizing, we divide the intrinsic reward by ten so that it has less weight than the extrinsic reward</p><p>• we clip intrinsic rewards in the same way that we clip extrinsic rewards, i.e., using r c = tanh(r/100), so that the intrinsic and extrinsic rewards are on a similar scale</p><p>We downscale the forward modeling loss by a factor of 0.01 to slow down the rate at which the model becomes familiar with a given state, since the intrinsic reward often collapsed quickly despite the reward normalization. We determined these settings during a set of small-scale experiments.</p><p>We also tried using subsets of the full feature set (only the embedding of the full display of glyphs, or only the embedding of the crop of glyphs around the agent) as well as the exact architecture used by the original authors, but with the pixel input replaced by a random 8-dimensional embedding of the symbolic observation space. However, we did not observe this improved results.</p><p>We tried using intrinsic reward only as the authors did in the original RND paper, but we found that agents trained in this way made no significant progress through the dungeon, even on a single fixed seed. This indicates that this form of intrinsic reward is not sufficient to make progress on NetHack. As noted in Section 3, the intrinsic reward did help in some tasks for some characters when combined with the extrinsic reward. Crucially, RND exploration is not sufficient for agents to learn to find the Oracle, which leaves this as a difficult challenge for future exploration techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Dashboard</head><p>We release a web dashboard built with NodeJS (see <ref type="figure" target="#fig_0">Figure 10</ref>) to visualize experiment runs and statistics for NLE, including replaying episodes that were recorded as tty files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I NetHack Bots</head><p>Since the early stages of the development of NetHack, players have tried to build bots to play and solve the game. Notable examples are TAEB, BotHack, and Saiph <ref type="bibr" target="#b63">[65,</ref><ref type="bibr" target="#b48">50]</ref>. These bot frameworks largely rely on search heuristics and common planning methods, without generally making use of any statistical learning methods. An exception is SWAGGINZZZ <ref type="bibr" target="#b1">[2]</ref> which uses lookups, exhaustive simulation and manipulation of the random number generator.</p><p>Successful bots have made use of exploits that are no longer present in recent versions of NetHack. For example, BotHack employs the "pudding farming" strategy [see 50, "Pudding farming" entry] to level up and to create items for the character by spawning and killing a large number of black puddings P. This enabled the bot to become quite strong, which rendered late-game fights considerably easier. This strategy was disabled by the NetHack DevTeam with a patch that is incorporated into versions of NetHack above 3.6.0. Likewise, the random number generator manipulations employed in SWAGGINZZZ are no longer possible.</p><p>We believe that it is very unlikely that in the future we will see a hand-crafted bot solving NetHack in the way we defined it in Section 2.4. In fact, the creator of SWAGGINZZZ remarked that "[e]ven with RNG manipulation, writing a bot that 99% ascends NetHack is extremely complicated. So much stuff can go wrong, and there is no shortage of corner cases" <ref type="bibr" target="#b1">[2]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Viewing Agent Videos</head><p>We have uploaded some agent recordings to https://asciinema.org/~nle. These can be either watched on the Asciinema portal, or on a terminal by running asciinema play -s 0.2 url (asciinema itself is available as a pip package at https://pypi.org/project/asciinema).</p><p>The -s flag regulates the speed of the recordings, which can also be modified on the web interface by pressing &gt; (faster) or &lt; (slower).  <ref type="table">Table 6</ref>: Top five of the last 1000 episodes in the score task.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Annotated example of an agent at two different stages in NetHack (Left: a procedurally generated first level of the Dungeons of Doom, right: Gnomish Mines). A larger version of this figure is displayed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The gelatinous cube eats a scroll! }}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}} }}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}} }}.}}}}}P.}}}}}......}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}..".}}}...}}}}} }...}}.....}}}}}....}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}...............} }....}}}}}}}}}}....}}}..}}}}}}}}}}}.......}}}}}}}}}}}}}}}}..}}.....}}}...}} }....}}}}}}}}b....}}}}.[}}}}}}%................}}}}}}}}}}}.}}}}.....}}...}} }....}}}}}}}}}}}}.}}}}.}}}}}}M-----------------.}}}}}}}}}}}}}}}}}.........} }....}}}}}}}}}}}}}}}}}}.}}}...|.......^.......|...}}}}}}}}}}}}}}}}}}}....}} }.....}.}}s...}}}}}}}}}.}}....--------+--------....}}}}}}..}}}}}}}}}}}...}} }..oo..}}}}.%}}}}}}}}}}}}}........|.......|........}}}}}....}}}}}}}}}}}}}}} }.....}}}}}}}}}}}}}}}}}}}}........|.&gt;.....|^....^..}}}}}...}}}}}}}}}.}}}}}} }.....}}}}}}}}}}}}}}}}}}}}....--------+--------....}}}}}}.}.}}}}}}}}}}}}}}} }....@.}}}}}}}}}}}}}}}}}}}}...|.......^.......|...}}}}}}}}}}}}}}}}}.}}}}}}} }.......}}}}}}}..}}}}}}}}}}}}.-----------------.}}}}}}}}}}}}}}}}}....</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>NLE is available under an open source license at https://github.com/facebookresearch/nle. 4 An example interaction after applying a figurine of an Archon: "You set the figurine on the ground and it transforms. You get a bad feeling about this. The Archon hits! You are blinded by the Archon's radiance! You stagger. . . It hits! You die. . . But wait. . . Your medallion feels warm! You feel much better! The medallion crumbles to dust! You survived that attempt on your life." That door is closed. -------------------------|........|....| |...=....| ------|.............| #################*####.........| |&lt;...| |.............| |..!.?...| |...=-###%#+ . |.%...........|</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Agent6850 the Candidate St:18/03 Dx:11 Co:12 In:8 Wi:13 Ch:11 Neutral S: Dlvl:1 $:7 HP:14(14) Pw:5(5) AC:4 Xp:1/17 T:835 Hungry 1 Overview of the core architecture of the baseline models released with NLE. A larger version of this figure is displayed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Training and test performance when training on restricted sets of seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Mean return of the last 100 episodes averaged over five runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Screenshot of BrowserHack showing NetHack with a graphical user interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>MONSTER 237 M</head><label>237</label><figDesc>-m use monster's special ability MORE 13 C-m read the next message MOVE 109 m Prefix: move without picking up objects/fighting MOVEFAR 77 M Prefix: run without picking up objects/fighting OFFER 239 M-o offer a sacrifice to the gods an accessory (ring, amulet, etc) RIDE 210 M-R mount or dismount a saddled steed RUB 242 M-r rub a lamp or a stone RUSH 103 g Prefix: rush until something interesting is seen</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>, the agent has to learn to reliably open doors +, kick-in locked doors, search for hidden doors and passages #, avoid traps^, or move</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Mean episode return of the last 100 episodes for models with different hidden sizes averaged over five runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>steps CNN mon-hum-neu-mal CNN tou-hum-neu-fem CNN val-dwa-law-fem CNN wiz-elf-cha-mal RND mon-hum-neu-mal RND val-dwa-law-fem RND wiz-elf-cha-mal RND tou-hum-neu-fem</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Mean score, dungeon level reached, experience level achieved, and steps performed in the environment in the last 100 episodes averaged over five runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>CNN mon-hum-neu-mal CNN tou-hum-neu-fem CNN val-dwa-law-fem CNN wiz-elf-cha-mal RND mon-hum-neu-mal RND val-dwa-law-fem RND wiz-elf-cha-mal RND tou-hum-neu-fem Analysis of different causes of death during training, averaged over the last 1000 episodes and over five runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 : 1 inFigure 11 :Figure 12 :</head><label>1011112</label><figDesc>Screenshot of the web dashboard included in the NetHack Learning Environment. That door is closed. -------------------------|........|....| |...=....| ------|.............| #################*####.........| |&lt;...| |.............| |..!.?...| |...=-###%#+ . |.%..........St:18/03 Dx:11 Co:12 In:8 Wi:13 Ch:11 Neutral S: Dlvl:1 $:7 HP:14(14) Pw:5(5) AC:4 Xp:1/17 TYou kill the dwarf! Welcome to experience level 5.--More-------......| |...---.......-|............-| --+---+--.............| -.........|..........---------..|.--).)...---.----+-|..------.....---...--|.. |...----.......%...| |. |...&gt;-|...%......-------.----------.--..| -..........-|..G.[@............|---|............| ---.G....---------.| --...----...St:18/02 Dx:12 Co:12 In:11 Wi:13 Ch:8 Neutral S: Dlvl:5 $:0 HP:37(39) Pw:25(25) AC:5 Xp:5/168 T:768 Hungry Annotated example of an agent at two different stages in NetHack (Left: a procedurally generated first level of the Dungeons of Doom, right: Gnomish Mines). That door is closed. -------------------------|........|....| |...=....| ------|.............| #################*####.........| |&lt;...| |.............| |..!.?..Overview of the core architecture of the baseline models released with NLE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>You kill the dwarf! Welcome to experience level 5</figDesc><table><row><cell>agent</cell><cell></cell><cell></cell><cell>ring</cell></row><row><cell cols="2">room</cell><cell>potion</cell><cell>scroll</cell></row><row><cell></cell><cell></cell><cell>boulder</cell></row><row><cell>closed door</cell><cell></cell><cell></cell></row><row><cell>tool</cell><cell></cell><cell></cell></row><row><cell>hidden passage</cell><cell></cell><cell></cell></row><row><cell>the Candidate</cell><cell cols="3">St:18/03 Dx:11 Co:12 In:8 Wi:13 Ch:11 Neutral S:</cell></row><row><cell cols="3">Dlvl:1 $:7 HP:14(14) Pw:5(5) AC:4 Xp:1/17 T:835 Hungry</cell></row><row><cell></cell><cell>1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>}}}}}} }....f..p}}.}}...U}}}}}}}}}}}}.......Y.........}}}}}..}}}}}}}}}.......}}}}} }.......}}}}}}}......}}}}}}}}}}}}}}..v....}}}}}}}}}..C..}}}}}}...}}..}}}}}} }.....}}}}}}}}}}}.Y...}}}}}}}}}}}}}}}}}}}}}}.}}}}}}}..B}}}}}}}}}....}}}}}}} }}..}}}}}}}}}}}}}....}}}}}}}}}}}}}}}}}}}}}}...}}..}}}}}}}.}}.}}}}.^}}}}}}}}</figDesc><table><row><cell cols="2">}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}</cell></row><row><cell cols="2">}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}</cell></row><row><cell>Georg XIII the Thaumaturge</cell><cell>St:7 Dx:14 Co:17 In:19 Wi:10 Ch:10 Neutral S:</cell></row><row><cell cols="2">Dlvl:6 $:0 HP:52(52) Pw:28(73) AC:5 Xp:7/921 T:7451 Hungry</cell></row><row><cell></cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Command actions.<ref type="bibr" target="#b9">10</ref> </figDesc><table><row><cell>Name</cell><cell cols="2">Value Key Description</cell></row><row><cell>EXTCMD</cell><cell>35 #</cell><cell>perform an extended command</cell></row><row><cell>EXTLIST</cell><cell cols="2">191 M-? list all extended commands</cell></row><row><cell>ADJUST</cell><cell cols="2">225 M-a adjust inventory letters</cell></row><row><cell>ANNOTATE</cell><cell cols="2">193 M-A name current level</cell></row><row><cell>APPLY</cell><cell>97 a</cell><cell>apply (use) a tool (pick-axe, key, lamp...)</cell></row><row><cell>ATTRIBUTES</cell><cell cols="2">24 C-x show your attributes</cell></row><row><cell>AUTOPICKUP</cell><cell>64 @</cell><cell>toggle the pickup option on/off</cell></row><row><cell>CALL</cell><cell>67 C</cell><cell>call (name) something</cell></row><row><cell>CAST</cell><cell>90 Z</cell><cell>zap (cast) a spell</cell></row><row><cell>CHAT</cell><cell cols="2">227 M-c talk to someone</cell></row><row><cell>CLOSE</cell><cell>99 c</cell><cell>close a door</cell></row><row><cell>CONDUCT</cell><cell cols="2">195 M-C list voluntary challenges you have maintained</cell></row><row><cell>DIP</cell><cell cols="2">228 M-d dip an object into something</cell></row><row><cell>DOWN</cell><cell>62 &gt;</cell><cell>go down (e.g., a staircase)</cell></row><row><cell>DROP</cell><cell>100 d</cell><cell>drop an item</cell></row><row><cell>DROPTYPE</cell><cell>68 D</cell><cell>drop specific item types</cell></row><row><cell>EAT</cell><cell>101 e</cell><cell>eat something</cell></row><row><cell>ESC</cell><cell cols="2">27 C-[ escape from the current query/action</cell></row><row><cell>ENGRAVE</cell><cell>69 E</cell><cell>engrave writing on the floor</cell></row><row><cell>ENHANCE</cell><cell cols="2">229 M-e advance or check weapon and spell skills</cell></row><row><cell>FIRE</cell><cell>102 f</cell><cell>fire ammunition from quiver</cell></row><row><cell>FIGHT</cell><cell>70 F</cell><cell>Prefix: force fight even if you don't see a monster</cell></row><row><cell>FORCE</cell><cell cols="2">230 M-f force a lock</cell></row><row><cell>GLANCE</cell><cell>59 ;</cell><cell>show what type of thing a map symbol corresponds to</cell></row><row><cell>HELP</cell><cell>63 ?</cell><cell>give a help message</cell></row><row><cell>HISTORY</cell><cell>86 V</cell><cell>show long version and game history</cell></row><row><cell>INVENTORY</cell><cell>105 i</cell><cell>show your inventory</cell></row><row><cell>INVENTTYPE</cell><cell>73 I</cell><cell>inventory specific item types</cell></row><row><cell>INVOKE</cell><cell cols="2">233 M-i invoke an object's special powers</cell></row><row><cell>JUMP</cell><cell cols="2">234 M-j jump to another location</cell></row><row><cell>KICK</cell><cell cols="2">4 C-d kick something</cell></row><row><cell>KNOWN</cell><cell>92 \</cell><cell>show what object types have been discovered</cell></row><row><cell>KNOWNCLASS</cell><cell>96 '</cell><cell>show discovered types for one class of objects</cell></row><row><cell>LOOK</cell><cell>58 :</cell><cell>look at what is here</cell></row></table><note>LOOT 236 M-l loot a box on the floor</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Compass direction actions.</figDesc><table><row><cell></cell><cell cols="2">one-step</cell><cell cols="2">move far</cell></row><row><cell>Direction</cell><cell cols="4">Value Key Value Key</cell></row><row><cell>North East South West North East South East South West North West</cell><cell>107 108 106 104 117 110 98 121</cell><cell>k l j h u n b y</cell><cell>75 76 74 72 85 78 66 89</cell><cell>K L J H U N B Y</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison between NLE and popular environments when using their respective Python Gym interface. SPS stands for "environment steps per second". All environments but ObstacleTowerEnv were run via gym with standard settings (and headless when possible), for 60 seconds.</figDesc><table><row><cell>Environment</cell><cell>SPS</cell><cell cols="2">steps episodes</cell></row><row><cell>NLE (score)</cell><cell>14.4K</cell><cell>868.75K</cell><cell>477</cell></row><row><cell>CartPole-v1</cell><cell cols="2">76.88K 4612.65K</cell><cell>207390</cell></row><row><cell>ALE (MontezumaRevengeNoFrameskip-v4)</cell><cell>0.90K</cell><cell>53.91K</cell><cell>611</cell></row><row><cell>Retro (Airstriker-Genesis)</cell><cell>1.31K</cell><cell>78.56K</cell><cell>52</cell></row><row><cell>ProcGen (procgen-coinrun-v0)</cell><cell>13.13K</cell><cell>787.98K</cell><cell>1283</cell></row><row><cell>ObstacleTowerEnv</cell><cell>0.06K</cell><cell>3.61K</cell><cell>6</cell></row><row><cell>MineRLNavigateDense-v0</cell><cell>0.06K</cell><cell>3.39K</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Metrics averaged over last 1000 episodes for each task.</figDesc><table><row><cell>Task</cell><cell cols="2">Model Character</cell><cell cols="4">Score Time Exp Lvl Dungeon Lvl</cell><cell>Win</cell></row><row><cell cols="2">staircase CNN</cell><cell>mon-hum-neu-mal</cell><cell>20</cell><cell>252</cell><cell>1.0</cell><cell cols="2">1.0 77.26</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>6</cell><cell>288</cell><cell>1.0</cell><cell cols="2">1.0 50.42</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell>19</cell><cell>329</cell><cell>1.0</cell><cell cols="2">1.0 74.62</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell>20</cell><cell>253</cell><cell>1.0</cell><cell cols="2">1.0 80.42</cell></row><row><cell></cell><cell>RND</cell><cell>mon-hum-neu-mal</cell><cell>26</cell><cell>199</cell><cell>1.0</cell><cell cols="2">1.0 90.84</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>8</cell><cell>203</cell><cell>1.0</cell><cell cols="2">1.0 56.94</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell>25</cell><cell>242</cell><cell>1.0</cell><cell cols="2">1.0 90.96</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell>20</cell><cell>317</cell><cell>1.0</cell><cell cols="2">1.0 67.46</cell></row><row><cell>pet</cell><cell>CNN</cell><cell>mon-hum-neu-mal</cell><cell>20</cell><cell>297</cell><cell>1.0</cell><cell cols="2">1.1 62.02</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>6</cell><cell>407</cell><cell>1.0</cell><cell cols="2">1.0 25.66</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell>18</cell><cell>379</cell><cell>1.0</cell><cell cols="2">1.0 63.30</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell>16</cell><cell>273</cell><cell>1.0</cell><cell cols="2">1.0 66.80</cell></row><row><cell></cell><cell>RND</cell><cell>mon-hum-neu-mal</cell><cell>33</cell><cell>319</cell><cell>1.1</cell><cell cols="2">1.0 74.38</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>10</cell><cell>336</cell><cell>1.0</cell><cell cols="2">1.0 49.38</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell>28</cell><cell>311</cell><cell>1.0</cell><cell cols="2">1.0 81.56</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell>20</cell><cell>278</cell><cell>1.0</cell><cell cols="2">1.0 70.48</cell></row><row><cell>eat</cell><cell>CNN</cell><cell>mon-hum-neu-mal</cell><cell cols="2">36 1254</cell><cell>1.1</cell><cell>1.2</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>7</cell><cell>423</cell><cell>1.0</cell><cell>1.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell cols="2">75 1713</cell><cell>1.5</cell><cell>1.1</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell cols="2">50 1181</cell><cell>1.3</cell><cell>1.1</cell><cell>-</cell></row><row><cell></cell><cell>RND</cell><cell>mon-hum-neu-mal</cell><cell cols="2">36 1102</cell><cell>1.0</cell><cell>1.2</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>9</cell><cell>404</cell><cell>1.0</cell><cell>1.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell cols="2">55 1421</cell><cell>1.2</cell><cell>1.1</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell>14</cell><cell>808</cell><cell>1.0</cell><cell>1.1</cell><cell>-</cell></row><row><cell>gold</cell><cell>CNN</cell><cell>mon-hum-neu-mal</cell><cell>307</cell><cell>947</cell><cell>1.8</cell><cell>4.2</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>71</cell><cell>788</cell><cell>1.0</cell><cell>2.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell cols="2">245 1032</cell><cell>1.6</cell><cell>3.5</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell>162</cell><cell>780</cell><cell>1.3</cell><cell>2.7</cell><cell>-</cell></row><row><cell></cell><cell>RND</cell><cell>mon-hum-neu-mal</cell><cell cols="2">403 1006</cell><cell>2.2</cell><cell>5.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>92</cell><cell>816</cell><cell>1.0</cell><cell>2.2</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell>298</cell><cell>998</cell><cell>1.8</cell><cell>4.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell>217</cell><cell>789</cell><cell>1.5</cell><cell>3.3</cell><cell>-</cell></row><row><cell>score</cell><cell>CNN</cell><cell>mon-hum-neu-mal</cell><cell>748</cell><cell>932</cell><cell>3.2</cell><cell>5.4</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>11</cell><cell>795</cell><cell>1.0</cell><cell>1.1</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell>573</cell><cell>908</cell><cell>2.8</cell><cell>4.8</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell>314</cell><cell>615</cell><cell>1.6</cell><cell>3.5</cell><cell>-</cell></row><row><cell></cell><cell>RND</cell><cell>mon-hum-neu-mal</cell><cell>780</cell><cell>863</cell><cell>3.1</cell><cell>5.4</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>219</cell><cell>490</cell><cell>1.1</cell><cell>2.6</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell>647</cell><cell>857</cell><cell>2.8</cell><cell>5.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell>352</cell><cell>585</cell><cell>1.6</cell><cell>3.5</cell><cell>-</cell></row><row><cell>scout</cell><cell>CNN</cell><cell>mon-hum-neu-mal</cell><cell>372</cell><cell>838</cell><cell>2.2</cell><cell>5.3</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>105</cell><cell>580</cell><cell>1.0</cell><cell>2.7</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell>331</cell><cell>852</cell><cell>1.9</cell><cell>5.1</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell>222</cell><cell>735</cell><cell>1.5</cell><cell>3.8</cell><cell>-</cell></row><row><cell></cell><cell>RND</cell><cell>mon-hum-neu-mal</cell><cell>416</cell><cell>924</cell><cell>2.3</cell><cell>5.5</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>119</cell><cell>599</cell><cell>1.0</cell><cell>2.8</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell cols="2">304 1021</cell><cell>1.8</cell><cell>4.6</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell>231</cell><cell>719</cell><cell>1.5</cell><cell>3.8</cell><cell>-</cell></row><row><cell>oracle</cell><cell>CNN</cell><cell>mon-hum-neu-mal</cell><cell>24</cell><cell>876</cell><cell>1.0</cell><cell>1.1</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>9</cell><cell>674</cell><cell>1.0</cell><cell>1.1</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell cols="2">18 1323</cell><cell>1.0</cell><cell>1.1</cell><cell>0.02</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell>10</cell><cell>742</cell><cell>1.0</cell><cell>1.1</cell><cell>0.00</cell></row><row><cell></cell><cell>RND</cell><cell>mon-hum-neu-mal</cell><cell>32</cell><cell>967</cell><cell>1.0</cell><cell>1.1</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell>tou-hum-neu-fem</cell><cell>13</cell><cell>811</cell><cell>1.0</cell><cell>1.1</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell>val-dwa-law-fem</cell><cell cols="2">26 1353</cell><cell>1.0</cell><cell>1.1</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell>wiz-elf-cha-mal</cell><cell>14</cell><cell>791</cell><cell>1.0</cell><cell>1.1</cell><cell>0.00</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">"NetHack is largely based on discovering secrets and tricks during gameplay. It can take years for one to become well-versed in them, and even experienced players routinely discover new ones."<ref type="bibr" target="#b25">[26]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Information about the over 450 items and 580 monster types, as well as environment dynamics involving these entities can be found in the NetHack Wiki<ref type="bibr" target="#b48">[50]</ref> and to some extent in the NetHack Guidebook<ref type="bibr" target="#b57">[59]</ref>.<ref type="bibr" target="#b2">3</ref> youtube.com/watch?v=SjuTyJlgLJ8</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Occasionally dying because of simple, avoidable mistakes is so common in the game that the online community has defined an acronym for it: Yet Another Stupid Death (YASD).<ref type="bibr" target="#b5">6</ref> Playable online at https://coolwanglu.github.io/BrowserHack/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The exact number of monsters in NetHack depends on compile-time options as well as the target operating system. For instance, the mail daemon &amp; is only available on Unix-like operating systems, where it delivers email in the form of a NetHack scroll if the system is configured to host a Unix mailbox.<ref type="bibr" target="#b7">8</ref> NetHack's set of glyph ids is not necessarily well suited for machine learning. For example, more than half of all glyph ids are of type "swallow", most of which are guaranteed not to show up in any actual game of NetHack. We provide additional tooling to determine the type of a given glyph id to process this observation further.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the NetHack DevTeam for creating and continuously extending this amazing game over the last decades. We thank Paul Winner, Bart House, M. Drew Streib, Mikko Juola, Florian Mayer, Philip H.S. Torr, Stephen Roller, Minqi Jiang, Vegard Mella, Eric Hambro, Fabio Petroni, Mikayel Samvelyan, Vitaly Kurin, Arthur Szlam, Sebastian Riedel, Antoine Bordes, Gabriel Synnaeve, Jeremy Reizenstein, as well as the NeurIPS 2020, ICML 2020, and BeTR-RL 2020 reviewers and area chairs for their valuable feedback. Nantas Nardelli is supported by EPSRC/MURI grant EP/N019474/1. Finally, we would like to pay tribute to the 863,918,816 simulated NetHack heroes who lost their lives in the name of science for this project (thus far).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Apprenticeship learning via inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aransentin</forename><surname>Breggan Hampe Pellsson</surname></persName>
		</author>
		<ptr target="https://pellsson.github.io/" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2020" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of robot learning from demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenna</forename><surname>Argall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><forename type="middle">M</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Browning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="469" to="483" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning: A brief survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Peter</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil Anthony</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="26" to="38" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Crawling in rogue&apos;s dungeons with deep reinforcement techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Asperti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Cortesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><forename type="middle">De</forename><surname>Pieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianmaria</forename><surname>Pedrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Sovrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Games</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Playing hard exploration games by watching youtube</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lefrancq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Víctor</forename><surname>Valdés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg</pubPlace>
		</imprint>
	</monogr>
	<note>and Stig Petersen. Deepmind lab. CoRR, abs/1612.03801</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unifying count-based exploration and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yavar</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to win by reading manuals in a monte-carlo framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno>abs/1606.01540</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>OpenAI Gym. CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale study of curiosity-driven learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploration by random network distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning combat in NetHack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>Verbrugge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIIDE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploration in NetHack with secret discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>Verbrugge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Games</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Babyai: A platform to study the sample efficiency of grounded language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Chevalier-Boisvert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salem</forename><surname>Lahlou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Minimalistic Gridworld Environment for OpenAI Gym</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Boisvert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Pal</surname></persName>
		</author>
		<ptr target="https://github.com/maximecb/gym-minigrid" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Leveraging procedural generation to benchmark reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01588</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quantifying generalization in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dungeon crawl stone soup as an evaluation domain for artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Dannenhauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Floyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Decker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Games and Simulations for Artificial Intelligence, AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feudal reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Ecoffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Huizinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10995</idno>
		<title level="m">Go-Explore: A New Approach for Hard-exploration Problems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">First return then explore. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Ecoffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Huizinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">List of Nethack spoilers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Myers</surname></persName>
		</author>
		<ptr target="https://sites.google.com/view/evasroguelikegamessite/list-of-nethack-spoilers" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2020" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reinforcement Learning for roguelike type games (eliteMod v0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan Manuel Sanchez</forename><surname>Fernandez</surname></persName>
		</author>
		<ptr target="https://kcir.pwr.edu.pl/~witold/aiarr/2009_projekty/elitmod/" />
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stabilising experience replay for deep multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comprehensive survey on safe reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Garcıa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The MineRL competition on sample efficient reinforcement learning using human priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cayden</forename><surname>Guss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Codel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noboru</forename><surname>Houghton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Kuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharada</forename><surname>Milani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><forename type="middle">Perez</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Liebana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS Competition Track</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mazeexplorer: A customisable 3d benchmark for assessing generalisation in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Harries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaroslaw</forename><surname>Rzepecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Games</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep q-learning from demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Vecerík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Sendonaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Emergent systematic generalization in a situated agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">K</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Craftassist instruction parsing: Semantic parsing for a minecraft assistant. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavya</forename><surname>Srinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Language as an abstraction for hierarchical deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The malmo platform for artificial intelligence experimentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Hutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bignell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Juliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent-Pierre</forename><surname>Berges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ervin</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hunter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Crespi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Illuminating generalization in deep reinforcement learning through procedural level generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><surname>Justesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><forename type="middle">Rodriguez</forename><surname>Torrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bontrager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Risi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10729</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reinforcement learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew W</forename><surname>Michael L Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rogue-gym: A new challenge for generalization in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Kanagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoyuki</forename><surname>Kaneko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Games</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Vizdoom: A doom-based ai research platform for visual reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michał</forename><surname>Kempka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Wydmuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Runc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Toczek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Jaśkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computational Intelligence and Games</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">NetHack Home Page</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Lorber</surname></persName>
		</author>
		<ptr target="https://nethack.org" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2020" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Selvatici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viswanath</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno>abs/1910.03552</idno>
		<title level="m">TorchBeast: A PyTorch Platform for Distributed RL. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A survey of reinforcement learning informed by natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Public NetHack server at alt.org (NAO)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Drew</forename><surname>Streib</surname></persName>
		</author>
		<ptr target="https://alt.org/nethack/,2020" />
		<imprint>
			<biblScope unit="page" from="2020" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unicorn: Continual learning with a universal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustin</forename><surname>Zídek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schaul</surname></persName>
		</author>
		<idno>abs/1802.08294</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Value propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nethack</forename><surname>Wiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nethackwiki</surname></persName>
		</author>
		<ptr target="https://nethackwiki.com/,2020" />
		<imprint>
			<biblScope unit="page" from="2020" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03720</idno>
		<title level="m">Gotta learn fast: A new benchmark for generalization in rl</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A survey of real-time strategy game ai research and competition in starcraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Uriarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Richoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Churchill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Preuss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Aäron van den Oord, and Rémi Munos. Count-based exploration with neural density models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bellemare</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><forename type="middle">I</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">L</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Reinforcement learning with hierarchies of machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Curiosity-driven exploration by self-supervised prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">RIDE: Rewarding impact-driven exploration for procedurally-generated environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">A Guide to the Mazes of Menace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Raymond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">A Guide to the Mazes of Menace: Guidebook for NetHack. NetHack DevTeam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Stephenson</surname></persName>
		</author>
		<ptr target="http://www.nethack.org/download/3.6.5/nethack-365-Guidebook.pdf" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Procedural content generation: From automatically generating game levels to increasing generality in machine learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Risi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Experience replay for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Wayne</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning montezuma&apos;s revenge from a single demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1812.03381</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The StarCraft Multi-Agent Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikayel</forename><surname>Samvelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tabish</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schroeder De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nantas</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Man</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Auvolat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Richoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00625</idno>
		<title level="m">TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeb</forename><surname>Documentation</surname></persName>
		</author>
		<ptr target="https://taeb.github.io/bots.html" />
		<imprint>
			<date type="published" when="2015" />
			<publisher>Other Bots</publisher>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main"># Exploration: A study of count-based exploration for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Stooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openai Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Deturck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Transfer learning for reinforcement learning domains: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Machine Learning Research</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Feudal networks for hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Sasha</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Manfred</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otto</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petko</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Sasha</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04782</idno>
		<title level="m">StarCraft II: A New Challenge for Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Grandmaster level in StarCraft II using multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petko</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georgiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Rotation, translation, and cropping for zero-shot generalization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bontrager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">RTFM: Generalising to new environment dynamics via reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
