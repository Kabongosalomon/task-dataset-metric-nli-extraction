<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Spectral Clustering Using Random Binning Fea-tures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 19-23. 2018. 2018. August 19-23. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
							<email>pin-yu.chen@ibm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>En-Hsu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangli</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
							<email>yinglong.xia.2010@ieee.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>En-Hsu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangli</forename><surname>Yen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">College of William and Mary</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Huawei Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">IBM Research AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Spectral Clustering Using Random Binning Fea-tures</title>
					</analytic>
					<monogr>
						<title level="j" type="main">KDD</title>
						<meeting> <address><addrLine>London, United Kingdom Aggarwal; London, United Kingdom</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">18</biblScope>
							<date type="published">August 19-23. 2018. 2018. August 19-23. 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3219819.3220090</idno>
					<note>ACM ISBN 978-1-4503-5552-0/18/08. . . $15.00 ACM, New York, NY, USA, 10 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Artificial intelligence</term>
					<term>Ma- chine learning</term>
					<term>Unsupervised learning</term>
					<term>Cluster analysis</term>
					<term>KEYWORDS Spectral clustering</term>
					<term>Graph Construction</term>
					<term>Random Binning Features</term>
					<term>Large-Scale Graph</term>
					<term>PRIMME</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spectral clustering is one of the most effective clustering approaches that capture hidden cluster structures in the data. However, it does not scale well to large-scale problems due to its quadratic complexity in constructing similarity graphs and computing subsequent eigendecomposition. Although a number of methods have been proposed to accelerate spectral clustering, most of them compromise considerable information loss in the original data for reducing computational bottlenecks. In this paper, we present a novel scalable spectral clustering method using Random Binning features (RB) to simultaneously accelerate both similarity graph construction and the eigendecomposition. Specifically, we implicitly approximate the graph similarity (kernel) matrix by the inner product of a large sparse feature matrix generated by RB. Then we introduce a state-of-the-art SVD solver to effectively compute eigenvectors of this large matrix for spectral clustering. Using these two building blocks, we reduce the computational cost from quadratic to linear in the number of data points while achieving similar accuracy. Our theoretical analysis shows that spectral clustering via RB converges faster to the exact spectral clustering than the standard Random Feature approximation. Extensive experiments on 8 benchmarks show that the proposed method either outperforms or matches the state-of-the-art methods in both accuracy and runtime. Moreover, our method exhibits linear scalability in both the number of data samples and the number of RB features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Clustering is one of the most fundamental problems in machine learning and data mining tasks. In the past two decades, spectral clustering (SC) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref> has shown great promise for learning hidden cluster structures from data. The superior performance of SC roots in exploiting non-linear pairwise similarity between data instances, while traditional methods like K-means heavily rely on Euclidean geometry and thus place limitations on the shape of the clusters <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35]</ref>. However, SC methods are typically not the first choice for large-scale clustering problems since modern datasets exhibit great challenges in both computation and memory consumption for computing the pairwise similarity matrix W ∈ R N ×N , where N denotes the number of data points. In particular, given a data matrix X ∈ R N ×d whose underlying data distribution can be represented as K weakly inter-connected clusters, it requires O(N 2 ) space to store the matrix and O(N 2 d) complexity to compute W, and at least takes O(KN 2 ) or O(N 3 ) complexity to compute K eigenvectors of the corresponding graph Laplacian matrix L, depending on whether an iterative or a direct eigensolver is used. To accelerate SC, many efforts have been devoted to address the following computational bottlenecks: 1) pairwise similarity graph construction of W from the raw data X, and 2) eigendecomposition of the graph Laplacian matrix L.</p><p>A number of methods have been proposed to accelerate the eigendecomposition, e.g., randomized sketching and power method <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref>, sequential reduction algorithm toward an early-stop strategy <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>, and graph filtering of random signals <ref type="bibr" target="#b26">[27]</ref>. However, these approaches only partially reduce the computation cost of the eigendecomposition, since the construction of similarity graph matrix W still requires quadratic complexity for both computation and memory consumption.</p><p>Another family of research is the use of Landmarks or representatives to jointly improve the computation efficiency of the similarity matrix W and the eigendecomposition of L. One strategy is performing random sampling or K-means on the dataset to select a small number of representative data points and then employing SC on the reduced dataset <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35]</ref>. Another strategy <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref> is approximating the similarity matrix W by a low rank affinity matrix Z ∈ R N ×R , which is computed via either random projection or a bipartite graph between all data points and selected anchor points <ref type="bibr" target="#b19">[20]</ref>. Furthermore, a heuristic that only selects a few nearest anchor points has been applied to build a KNN-based sparse graph similarity matrix. Despite promising results in accelerating SC, these approaches disregard considerable information in the raw data and may lead to degrading clustering performance. More importantly, there is no guarantee that these heuristic methods can approach the results of standard SC.</p><p>Another line of research <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> focuses on leveraging various kernel approximation techniques such as Nyström <ref type="bibr" target="#b28">[29]</ref>, Random Fourier <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref>, and random sampling to accelerate similarity matrix construction and the eigendecomposition at the same time. The pairwise similarity (kernel) matrix W (a weighted fully-connected graph) is then directly approximated by an inner product of the feature matrix Z computed from the raw data. Although a KNN-based graph construction allows efficient sparse representation, pairwise method takes full advantage of more complete information in the data and takes into account the long-range connections <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref>. The drawback of pairwise methods is the high computational costs in requiring the similarity between every pair of data points. Fortunately, we present an approximation technique applicable to SC that significantly alleviates this computational bottleneck. As our work focuses on enabling the scalability of SC using RB, the case of robust spectral clustering on noisy data, such as <ref type="bibr" target="#b0">[1]</ref>, could be applied but is beyond the scope of this paper.</p><p>In this paper, inspired by recent advances in the fields of kernel approximation and numerical linear algebra <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>, we present a scalable spectral clustering method and theoretic analysis to circumvent the two computational bottlenecks of SC in largescale datasets. We highlight the following main contributions:</p><p>(1) We present for the first time the use of Random Binning features (RB) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32]</ref> for scaling up the graph construction of similarity matrix in SC, which is implicitly approximated by the inner product of the RB sparse feature matrix Z ∈ R N ×D derived from the raw dataset, where each row has nnz(Z(i, :)) = R. To this end, we reduce the computational complexity of the pairwise graph construction from O(N 2 d) to O(N Rd) and memory consumption from O(N 2 ) to N R. (2) We further show how to make full use of state-of-the-art near-optimal eigensolver PRIMME <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref> to efficiently compute the eigenvectors of the corresponding graph Laplacian L without explicit formulation. As a result, the computational complexity of the eigendecomposition is reduced from </p><formula xml:id="formula_0">O(KN 2 m) to O(KN Rm),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SPECTRAL CLUSTERING AND RANDOM BINNING</head><p>We briefly introduce the SC algorithms and then illustrate RB, an important building block to our proposed method. Here are some notations we will use throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spectral Clustering</head><p>Given a set of N data points [x 1 , . . . , x N ] = X N ×d , with x i ∈ R d , the SC algorithm constructs a similarity matrix W ∈ R N ×N representing an affinity matrix G = (V , E), where the node v i ∈ V represents the data point x i and the edge E i j represents the similarity between x i and x j . The goal of SC is to use W to partition x 1 , . . . , x N into K clusters. There are several variants of SC. Without lose of generality, we consider the widely used normalized spectral clustering <ref type="bibr" target="#b20">[21]</ref>. To fully utilize complete similarity information, we consider a fully connected graph instead of a KNN-based graph for SC. An example similarity (kernel) function is the Gaussian Kernel:</p><formula xml:id="formula_1">k(x i , x j ) = exp − ∥x i − x j ∥ 2 2σ 2 (1)</formula><p>where σ is the bandwidth parameter. The normalized graph Laplacian matrix L is defined as:</p><formula xml:id="formula_2">L = D −1/2 (D − W)D −1/2 = I − D −1/2 WD −1/2<label>(2)</label></formula><p>where D ∈ R N ×N is the degree matrix with each diagonal element D ii = N j=1 W i j . The objective function of normalized SC can be defined as <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_3">min U∈R N ×K ,U T U=I trace(U T LU),<label>(3)</label></formula><p>where trace(·) denotes the matrix trace, I denotes the identity matrix, and the constraint U T U = I implies orthonormality on the columns of U. We further denote U * ∈ R N ×K as the optimizer of the minimization problem in (3), where the columns of U * are the K smallest eigenvectors of L. Finally, the K-means method is applied on the rows of U * to obtain the clusters. The high computational costs of the similarity matrix O(N 2 d) and the eigendecomposition O(KN 2 ) make SC hardly scalable to large-scale problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Random Binning Features</head><p>RB features are first introduced in <ref type="bibr" target="#b21">[22]</ref> and rediscovered in <ref type="bibr" target="#b31">[32]</ref> to yield a faster convergence compared to other Random Features methods for scaling up large-scale kernel machine. It considers a feature map of the form</p><formula xml:id="formula_4">k(x 1 , x 2 ) = ∫ ω p(ω)ϕ B ω (x 1 ) T ϕ B ω (x 2 ) dω<label>(4)</label></formula><p>where a set of bins B ω defines a random grid that are determined by ω = (ω 1 , u 1 , ..., ω d , u d ) drawn from a distribution p(ω), and (ω i , u i ) represents width and bias in the i-th dimension of a grid. Then for any bin b ∈ B ω , the feature vector ϕ B ω (x) has </p><formula xml:id="formula_5">ϕ b (x i ) = 1, if b = (⌊ x (1) i − u 1 ω 1 ⌋, ..., ⌊ x (d ) i − u d ω d ⌋)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 RB Features Generation</head><p>Input: Given a kernel function k(</p><formula xml:id="formula_6">x i , x j ) = d l =1 k l (|x (l ) i −x (l ) j |). Let p j (ω) ∝ ωk ′′ j (ω) be a distribution over ω. Output: RB feature matrix Z N ×D for raw data X 1: for j = 1, . . . , R do 2: Draw ω i from p j (ω) and u i ∈ [0, ω i ], for ∀i ∈ [d] 3:</formula><p>Compute feature values z j (x i ) as the indicator vector of bin</p><formula xml:id="formula_7">index (⌊ x (1) i −u 1 ω 1 ⌋, ..., ⌊ x (d ) i −u d ω d ⌋), for ∀i ∈ [N ]. 4: end for 5: Z i,: = 1 √ R [z 1 (x i ); ...; z D (x i )], for ∀i ∈ [N ]</formula><p>if the data point x i locates in the bin b ∈ B ω . Since a data point can only locate in one bin, ϕ b (x i ) = 0 for any other bins. Note for each grid B ω the number of bins |B ω | is countably infinite, therefore ϕ B ω (x) has infinite dimensions but only 1 non-zero entry (at the bin x lies in). <ref type="figure" target="#fig_0">Figure 1</ref> illustrates an RB example when the data dimension d = 2. When two data points x 1 , x 2 fall in the same bin, the collision probability for this to happen is proportional to the kernel value k(x 1 , x 2 ). Note that for a given grid multiple non-empty bins (features) can be produced and thus RB essentially generates a large sparse binary matrix (for more details, please refer to <ref type="bibr" target="#b31">[32]</ref>). In practice, in order to obtain a good kernel approximation matrix Z, a simple Monte Carlo method is often leveraged to approximate (4) by averaging over R grids</p><formula xml:id="formula_8">{B ω i } R i=1</formula><p>, where each grid's parameter ω i is drawn from p(ω). Algorithm 1 summarizes the procedure for generating a number R of RB features from the original dataset X. The resulting feature matrix Z ∈ R N ×D , where D is determined jointly by both the number of grids R and the kernel parameter σ . However, for each row the number of nonzero entries nnz(Z(i, :)) = R and thus the total number of nnz(Z) = N R, which is the same as other random feature methods <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SCALABLE SPECTRAL CLUSTERING USING RB FEATURES</head><p>In this section, we introduce our proposed scalable SC method, called SC_RB, based on RB and a state-of-the-art sparse eigensolver (PRIMME) to effectively tackle the two computational bottlenecks: 1) Pairwise graph construction; and 2) Eigendecomposition of the graph Laplacian matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pairwise graph construction</head><p>The first step of SC is to build a similarity graph. A fully connected graph entailing complete similarity information of the original data offers great flexibility in the underlying kernel function to define the affinities between data points. However, constructing a pairwise graph essentially computes a similarity (kernel) matrix between each pair of samples, which is computationally intensive due to its quadratic complexity O(N 2 d). Thus, we propose to use RB features Z to approximate the pairwise kernel matrix W, resulting in the following approximate spectral clustering objective:</p><formula xml:id="formula_9">min U∈R N ×K ,U T U=I trace(U T LU)<label>(5)</label></formula><p>where</p><formula xml:id="formula_10">L := I − D −1/2 W D −1/2 = I − D −1/2 ZZ T D −1/2</formula><p>and Z is a large sparse N × D matrix generated from Algorithm 1.</p><p>To apply RB to SC, it is necessary to compute the degree matrix D, the row sum of W. Luckily, we can compute it without explicitly computing ZZ T since</p><formula xml:id="formula_11">D = diag( W1) = diag(Z(Z T 1))<label>(6)</label></formula><p>where diag(x) is a diagonal matrix with the vector x on its main diagonal, and 1 represents a column vector of ones. Therefore, we can simply compute D by two matrix-vector multiplication without the explicit form of W. With D, we define Z = D −1/2 Z and thus we approximate the graph Laplacian matrix with L = I − Z Z T in linear complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effective eigendecomposition using PRIMME</head><p>After constructing the pairwise graph implicitly using Z, we compute the largest left singular vectors of Z, which is equivalent to computing the smallest eigenvectors of L := I − Z Z T in Equation <ref type="formula" target="#formula_9">(5)</ref>, satisfying</p><formula xml:id="formula_12">Zv i = σ i u i , i = 1, . . . , K, K ≪ N ,<label>(7)</label></formula><p>where the singular values are labeled in descending order,</p><formula xml:id="formula_13">σ 1 ≥ σ 2 ≥ . . . ≥ σ K . The matrices U = [u 1 , . . . , u K ] ∈ R N ×K and V = [v 1 , .</formula><p>. . , v n ] ∈ R D×K are the left and right singular vectors respectively, where U is the low-dimensional embedding associated with K clusters. However, Z, a weighted RB feature matrix of Z, is a very large sparse matrix of the size N × D, making it a challenging task for any standard SVD solver. Specifically, one has to resort to a powerful iterative sparser eigensolver that is capable to handle two difficulties for large-scale matrix: 1) slow convergence of eigenvalues when the eigenvalue gaps are not well separated, a common case when N is large; 2) low memory footprint yet near-optimal convergence for seeking a small number of eigenpairs.</p><p>To overcome these challenges, we leverage current state-of-theart eigenvalue and SVD solver <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>, named PReconditioned Iterative MultiMethod Eigensolver (PRIMME). It implements two near-optimal eigenmethods GD+K and JDQMR that are default Algorithm 2 Scalable SC method based on RB Input: Data matrix X, number of clusters K, number of girds R, kernel parameter σ . Output: K clusters and membership matrix M 1: Construct a fully connected graph using a sparse feature matrix Z ∈ R N ×D generated by RB using Algorithm 1.</p><p>2: Compute degree matrix D using Equation <ref type="bibr" target="#b5">6</ref> and obtain Z = D −1/2 Z using Equation 5.</p><p>3: Compute K largest left singular vectors U of Z using state-ofthe-art iterative sparse SVD solver (e.g., PRIMME). 4: Obtain the matrix U from U by row normalization. <ref type="bibr">5:</ref> Cluster the rows of U into K clusters using K-means and obtain the corresponding membership matrix M.</p><p>methods for seeking a small portion of extreme eigenpairs under limited memory. Unlike Lanczos methods (like Matlab svds function), these eigenmethods are in the classes of Generalized Davidson, which enjoy benefits for advanced subspace restarting and preconditioning techniques to accelerate the convergence.</p><p>Once the left singular vectors U are obtained, following <ref type="bibr" target="#b20">[21]</ref>, we obtain U by normalizing each row of U to unit norm. Then K-means method is applied to the rows of U to obtain the final K clusters and the binary membership matrix M ∈ {0, 1} N ×K .</p><p>Computation analysis. Algorithm 2 summarizes the procedure for the proposed scalable SC method based on RB and PRIMME. Using these two important building blocks, the computational complexity has been substantially reduced from O(N 2 d) to O(N Rd + N R) for computing the feature matrix Z from RB in pairwise graph construction, and from at least O(KN 2 m) to O(KN Rm) for the subsequent SVD computation, where m is the number of iterations of the underlying SVD solver. At the same time, the memory consumption has been reduced from O(N 2 ) to O(N R). In addition to these two key steps, the final K-means also takes O(N K 2 t), where t is the number of iterations of K-means. Therefore, the total computational complexity and memory consumption are O(N Rd +N KRm +N K 2 t) and O(N R). The linear complexity in the number N of data points render SC scalable to large-scale problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICAL ANALYSIS</head><p>The convergence of Random Feature approximation has been studied since it was first proposed in <ref type="bibr" target="#b21">[22]</ref>, where a sampling approximation analysis was employed to show the convergence of the approximation to exact kernel. Such analysis was adopted by most of its follow-up works. More recently, a new approach of analysis based on infinite-dimensional optimization is proposed in <ref type="bibr" target="#b36">[37]</ref>, which achieves a faster convergence rate than the previous approach, and was employed further in <ref type="bibr" target="#b31">[32]</ref> to explain the superior convergence of Random Binning Features than other types of random features in the context of classification.</p><p>Here we further adapt the analysis in <ref type="bibr" target="#b31">[32]</ref> to study the convergence of Spectral Clustering under RB approximation. We first recall the well-known connection between SC and kernel K-means <ref type="bibr" target="#b11">[12]</ref>, stating the equivalence of (3) to the following objective</p><formula xml:id="formula_14">min U∈R N ×K :U T U=I min M 1 2N N i=1 ∥ϕ(x i ) − MU T i,: ∥ 2 (8)</formula><p>where ϕ(.) is a possibly infinite-dimensional feature map in (4) from the normalized kernel, M is the matrix of means with k columns, each of which has the same dimension to the feature map ϕ(.).</p><p>Dropping constants that are neither related to U nor related to M, the objective <ref type="formula">(8)</ref> becomes</p><formula xml:id="formula_15">f (U, M) := 1 N N i=1 −⟨ϕ(x i ), MU T i,: ⟩ + 1 2N ∥MU i,: ∥ 2 .<label>(9)</label></formula><p>Let ( U, M) be the clustering from the RB approximation:</p><formula xml:id="formula_16">( U, M) := arg min U, M 1 N N i=1 −⟨z(x i ), MU T i,: ⟩ + 1 2N ∥MU i,: ∥ 2 (10)</formula><p>Let (U * , M * ) be the exact minimizer of (9). Our goal is to show that</p><formula xml:id="formula_17">f ( U, M) ≤ f (U * , M * ) + ϵ<label>(11)</label></formula><p>as long as the number of Random Binning grids satisfies R = Ω( 1 κϵ ), where κ is an estimate of the number of non-empty bins per grid. The quantity κ is crucial in the our analysis, as under the same computational budget, RB generates κ more features in expectation and converges κ-times faster than other types of random features. Note the computational cost is not κ-times more because of the sparse structure of RB-only one of κ features is non-zero for each sample x. The formal definition of κ is as follows. Definition 1. Define the collision probability of data X on bin b ∈ B δ as:</p><formula xml:id="formula_18">ν b := |{n ∈ [N ] | ϕ b = 1}| N .<label>(12)</label></formula><p>Let ν δ := max b ∈B δ ν b be an upper bound on <ref type="bibr" target="#b11">(12)</ref>, and κ δ := 1/ν δ be a lower bound on the number of non-empty bins of grid B δ . Then</p><formula xml:id="formula_19">κ := E δ [κ δ ] ≥ 1<label>(13)</label></formula><p>is the expected number of non-empty bins.</p><p>The proof of (11) contains two parts. In the first part, we show that f (U, M) ≤ f (U, M * ) + ϵ for any given U. This is obtained from the insight that the RB approximation z(x) (from Algorithm 1) is a subset of coordinates from the feature map ϕ(x). Therefore, M can be interpreted as a solution obtained from R iterations of Theorem 1. Let R be the number of grids generated by Algorithm 1. For any given U, let M * and M be the minimizers of (8) and (10) respectively. We have</p><formula xml:id="formula_20">E[f (U, M)] − f (U, M * ) ≤ ∥M * ∥ 2 F κ(R − c)<label>(14)</label></formula><p>for R &gt; c, where c is a small constant.</p><p>Proof. Let Φ := [ϕ(x 1 ), ..., ϕ(x N )]. Given U that satisfies U T U = I, the objective f (U, M) can be written as</p><formula xml:id="formula_21">f (U, M) = −1 N ⟨Φ, MU T ⟩ + 1 2N trace(MU T UM T ) = −1 N ⟨ΦU, M⟩ + 1 2N ∥M ∥ 2 F = K k =1 д(U :,k , M :,k ), where д(U :,k , M :,k ) is defined as 1 N N i=1 −⟨ϕ(x i )u ik , M :,k ⟩ + 1 2N ∥M :,k ∥ 2 .<label>(15)</label></formula><p>In other words, given U, f (U, M) can be separated as K independent subproblems, each solving a column of M. Let the first term of (15) be the loss function and the second term be the regularizer. Then <ref type="bibr" target="#b14">(15)</ref> satisfies the form of a convex, smooth empirical loss minimization problem studied in <ref type="bibr" target="#b31">[32]</ref>. By Theorem 1 of <ref type="bibr" target="#b31">[32]</ref>, the minimizer of (15) satisfies</p><formula xml:id="formula_22">E[д(U :,k , M :,k )] − д(U :,k , M * :,k ) ≤ ∥M * :,k ∥ 2 κ(R − c k )<label>(16)</label></formula><p>with</p><formula xml:id="formula_23">c k := ⌈ 2κ(д(U :,k ,0)−д(U :,k , M * :, k )) ∥ M * :,k ∥ 2 ⌉. Summing (16) over k = 1...K,</formula><p>we have</p><formula xml:id="formula_24">E[f (U, M)] − f (U, M * ) ≤ ∥M * ∥ 2 F κ(R − c) where c := max K k =1 c k . □ Theorem 1 implies that f (U, M) ≤ f (U, M * ) + ϵ for R RB ≥ ∥M * ∥ 2 F κϵ + c.<label>(17)</label></formula><p>As noted by the earlier work <ref type="bibr" target="#b31">[32]</ref>, this convergence rate is κ times faster than that of other Random Features under the same analysis framework. More specifically, if applying Theorem 2 of <ref type="bibr" target="#b36">[37]</ref> instead of Theorem 1 of <ref type="bibr" target="#b31">[32]</ref> in the proof of Theorem 1, one would have obtained a κ-times slower convergence rate for a general Random Feature method that generates a single feature at a time, which requires</p><formula xml:id="formula_25">R RF ≥ ∥M * ∥ 2 F ϵ + c ′</formula><p>number of features to guarantee an ϵ suboptimality. This is owing to RB's ability to generate a block of κ expected number of features at a time.</p><p>In the second part of the proof, we show that the spectral clustering U obtained from the RB approximation converges to U * in the objective. Theorem 2. Let R be the number of grids generated by Algorithm 1, and let U * , U be the spectral clusterings obtained from (8), (10) respectively. We have</p><formula xml:id="formula_26">E[f ( U, M)] − f (U * , M * ) ≤ ϵ (18) for R ≥ ∥M * ∥ 2 F κϵ + c</formula><p>where c is a small constant (defined in Theorem 1).</p><p>Proof. Note the problem (10) can be solved with global optimal guarantee by finding minimum eigenvalues and eigenvectors of (5). Therefore, let M( U), M(U * ) be the minimizers of (10) under U, U * respectively. We have</p><formula xml:id="formula_27">f ( U, M( U)) ≤ f (U * , M(U * ))<label>(19)</label></formula><p>by the optimality of ( U, M( U)) under the approximate feature map z(x). In addition, from Theorem 1 we have</p><formula xml:id="formula_28">f (U * , M(U * )) − f (U * , M * ) ≤ ∥M * ∥ 2 F κ(R − c)<label>(20)</label></formula><p>for R &gt; c. Combining <ref type="formula" target="#formula_15">(19)</ref> and <ref type="formula" target="#formula_2">(20)</ref> leads to the result. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We conduct experiments to demonstrate the effectiveness and efficiency of the proposed method, and compare against 8 baselines on 8 benchmarks. Our code 1 is implemented in Matlab and we use C Mex functions for computationally expensive components of RB 2 and of PRIMME eigensolver 3 . All computations are carried out on a linux machine with Intel Xeon CPU at 3.3GHz for a total of 16 cores and 500 GB main memory. Datasets. As shown in <ref type="table" target="#tab_1">Table 1</ref>, we choose 8 datasets from Lib-SVM <ref type="bibr" target="#b1">[2]</ref>, where 5 of them overlap with the datasets used in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref>. We summarize them as follows: 1) pendigits. A collection of handwritten digit data set consisting of 250 samples from 44 writers where sampled coordination information are used to generate 16 features per sample;</p><p>2) letter. A collection of images for 26 capital letters in the English alphabet where 16 character image features are generated;</p><p>3) minst. A popular collection of handwritten digit data set distributed by Yann LeCun, where each image is represented by a 784 dimensional vector; 4) acoustic. A collection of time-series data from various sensors in the moving vehicles for measuring the acoustic modality where a 50 dimensional feature vector is generated by using FFT for each time-series; 5) ijcnn1. A collection of time-series data from IJCNN 2001 Challenge, where 22 attributes are generated as a feature vector; 6) cod_rna. A collection of non-coding RNA sequences, where the total 8 features are generated by counting the frequencies of 'A', 'U', 'C' of sequences 1 and 2 as well as the length of the shorter sequence and deltaG_total value; 7) covtype-mult. A collection of samples for predicting the forest cover type from cartographic variables, where the total 54 feature vector is generated for representing a sample; 8) poker. A collection of poker record samples where each hand consisting of five playing cards drawn from a standard deck of 52 generates a feature vector of 10 attributes.</p><p>Baselines. We compare against 8 random feature based SC or approximation SC methods: 1) SC_Nys [13]: a fast SC method based on Nyström method; 2) SC_LSC [9]: approximate SC for KNN-based bipartite graph between raw data and anchor points selected by K-means;</p><p>3) SV_RF [11]: fast kernel K-means using singular vectors of the RF feature matrix (approximating similarity matrix W); 4) SC_RF: we modify SV_RF method to become a fast SC method based on RF feature matrix (approximating Laplacian matrix L); 5) KK_RF <ref type="bibr" target="#b10">[11]</ref>: another kernel K-means approximation method directly using the RF feature matrix; 6) KK_RS [10]: an approximate Kernel K-means by a random sampling approach; 7) SC [21]: Exact SC method; 8) K-means <ref type="bibr" target="#b14">[15]</ref>: standard K-means method applied on original dataset.</p><p>Evaluation metrics. We use 4 commonly used clustering metrics for cluster quality evaluation, which has been advocated and discussed in <ref type="bibr" target="#b37">[38]</ref>. Let {C k } K k =1 and {C ′ k } K k=1 denote the K clusters found by a clustering algorithm and the true cluster labels, respectively. The considered clustering metrics are: 1) Normalized mutual information (NMI):</p><formula xml:id="formula_29">NMI = 2 · I ({C k }, {C ′ k }) |H ({C k }) + H ({C ′ k })| ,</formula><p>where I is the mutual information between {C k } K k =1 and {C ′ k } K k =1 , and H is the entropy of clusters.</p><p>2) Rand index (RI):</p><formula xml:id="formula_30">RI = T P + T N T P + T N + F P + F N ,</formula><p>where T P, T N , F P and F N represent true positive, true negative, false positive, and false negative decisions, respectively.</p><p>3) F-measure (FM):</p><formula xml:id="formula_31">FM = 1 K K k =1 F-measure k , where F-measure k = 2·P REC k ·RECALL k P REC k +RECALL k</formula><p>, and PREC k and RECALL k are the precision and recall values for cluster C k . 4) Accuracy (Acc):</p><formula xml:id="formula_32">Acc = 1 N N i=1 δ ({C k } K k =1 , {C ′ k } K k=1 )</formula><p>where N is the total number of samples and the best mapping function δ (x, y) is the delta function that equals 1 if x = y and equals 0 otherwise between cluster labels {C k } K k =1 and the true labels {C ′ k } K k=1 for each sample. These metrics are all scaled between 0 and 1, and higher value means better clustering.</p><p>Average rank score. To combine multiple clustering metrics for performance evaluation of different SC methods, we adopt the methodology proposed in <ref type="bibr" target="#b35">[36]</ref> and use the average rank score of all clustering metrics as the final performance metric. Therefore, lower average rank score means better clustering performance.</p><p>Parameter selection. We use the RBF kernel for all similarity (Kernel) based methods on all datasets, where the kernel parameter σ is obtained through cross-validation within typical range [0.01 100]. All methods use the same kernel parameters to promote a fair comparison. For other parameters, we use the recommended settings in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Clustering accuracy and computational time on all datasets</head><p>Setup. We first compare against 8 aforementioned baselines in terms of both average rank score and computational time. We use the methodology proposed in <ref type="bibr" target="#b35">[36]</ref> to compute the average ranking score among 4 different metrics NMI, RI, FM, and Acc. Although the rank R has different meanings in each method but similar effects on the performance, we choose R = 1024 for all methods to promote a fair comparison. In addition, we use PRIMME_SVDS to accelerate SVD decomposition for all methods except SC_LSC. For the final step of SC, we use Matlab's internal K-means function with 10 replicates. All methods use same random seeds so the difference caused by randomness is minimized.</p><p>Results. <ref type="table" target="#tab_2">Table 2</ref> shows that SC_RB consistently outperforms or matches state-of-the-art SC methods in terms of average ranking score on 5 out of 8 datasets (except pendigits, mnist, poker). The first highlight in the table is that SC_LSC has quite good performance in the majority of datasets, especially for pendigits and mnist, owing to the sparse low rank approximation using AnchorGraph technique <ref type="bibr" target="#b19">[20]</ref>. However, we would like to point out that in SC_LSC the similarity matrix is built on a KNN-based graph, which is essentially different from other SC methods which use a fully connected graph. This explains why SC_LSC has even better performance than the exact SC method in these two datasets. For poker, all methods have very close numbers in four different metrics, which leads to quite similar average ranking score for all methods. Secondly, the SC type methods such as SC_Nys, SC_RF, and SC_RB generally achieve better ranking scores compared to similarity-based on methods such as KK_RF and SV_RF. This is because the SC type methods are built on a Normalized Cuts formulation that yields a better performance guarantee in terms of the graph clustering. Finally, the improved performance of SC_RB in the majority of datasets stems from the fact that it directly approximate a pairwise similarity matrix, which utilizes all information from the raw data. Its faster convergence allows it to retrieve more information with a similar rank R compared to other methods. <ref type="table" target="#tab_3">Table 3</ref> illustrates that SC_RB can achieve similar computational time to the other methods despite of a very large sparse matrix generated from RB, due to an important factor -near-optimal eigensolver PRIMME. One should not be surprised that the empirical  runtime of various scalable SC methods has relatively large range of differences. It is because that the constant factor in the computation complexity may vary with different datasets but the total computational costs are still bounded by O(N Rd + KN Rm + N K 2 t).</p><p>This constant factor typically depends on different characteristics of various datasets and specific method. For instance, KK_RF often needs more computational time than other methods since it needs firstly compute a dense feature matrix Z of N × R size and applied K-means directly on Z . When R is relatively large, the computation of K-means requiring N RKt complexity may start dominating the total complexity, which is observed in the <ref type="table" target="#tab_3">Table 3</ref>. Similarly, the computational time on covtype-mult with SC_RB is substantially heavier than those of other methods since the eigenvalues of the corresponding Laplacian matrix is very clustered making the number of iterations m much more than the usual (typically 10 -100 iterations) in other cases. Nevertheless, both complexity analysis and empirical runtime corroborate that SC_RB is computationally as efficient as other random features based SC methods and approximation Kernel K-means methods in most of cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effects of RB on runtime and convergence</head><p>Setup. The first goal here is to investigate the scalability of SC_RB over the vanilla SC method in terms of runtime while achieving the similar performance. The second goal is to study the behavior of various scalable SC and approximate Kernel K-means methods based on different random features. We limit our comparisons among two random features (RF and RB) based SC or Kernel K-means type methods. We choose the mnist dataset since it has been widely studied for convergence analysis of approximation in the literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>. We report runtime and commonly used Accuracy (Acc)   as our measurement metric when varying the rank R from 16 to 4096 (except SC_RB from 16 to 1024).</p><p>Results. We investigate how the performance of different methods changes when the number R of random features (RF and RB) increases from 16 to 4096. <ref type="figure" target="#fig_3">Fig. 2b</ref> illustrates that despite a large sparse feature matrix generated by RB, the computational time of SC_RB is orders of magnitudes less expensive compared to that of exact SC, and is comparable to other SC methods based on RF features. This is the desired feature of SC_RB that it can achieve higher accuracy than other efficient SC methods without comprising the computation time. As shown in <ref type="figure" target="#fig_3">Fig. 2a</ref>, we can see that the clustering accuracy (Acc) of all methods generally converge to that of exact SC but with different convergence rates. More importantly, SC_RB yields faster convergence compared to other scalable SC methods based on RF features, which confirms our analysis in Theorem 2. For instance, SC_RB with R = 1024 has already reached the same accuracy as the exact SC method while SC_RF converges relatively slower to the exact SC and get close to SC with R = 4096. Interestingly, SV_RF and KK_RF are not competitive in Accuracy, indicating that approximating the graph Laplacian matrix L is more beneficial than these that approximating the similarity matrix W in some cases. Setup. We perform experiments to study the effects of various SVD solvers on runtime for SC_RB. We choose the covtypemult dataset due to two reasons: 1) RB generates a very large sparse matrix Z having the size of half millions in the number of data points and tens of millions in the number of sparse features, which challenges any existing SVD solver in a single machine; 2) the convergence of iterative eigensolver largely depends on the well-separation of the desired eigenvalues. Unfortunately, the gap between the largest eigenvalues of covtype-mult is very small O(1E − 5), making it a difficult eigenvalue problem. We compare PRIMME_SVDS with Matlab SVDS function, a widely used SVD solver routine in the research community. We also set stopping tolerance 1E-5 to yield faster convergence for both solvers. We vary R for SC_RB from 16 to 128 and record the Accuracy (Acc) and Runtime (in Seconds) as our performance metrics for this set of experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effects of SVD solvers on runtime</head><p>Results. <ref type="figure" target="#fig_4">Fig. 3</ref> shows how the accuracy (Acc) and runtime changes for SC_RB using these two different SVD solvers when varying the rank R on covtype-mult dataset. Interestingly, SC_RB with PRIMME_SVDS delivers more consistent accuracy than that with SVDS. This may be because Matlab SVDS function has a difficult time to converge to multiple very close singular values, showing an warning message "reach default maximum iterations". However, as shown in <ref type="figure" target="#fig_4">Fig. 3b</ref>, less accurate singular triplets (from Matlab SVDS) takes significantly more computational time compared to PRIMME_SVDS, especially when R increases. In contrast, the computational time of eigendecomposition using PRIMME_SVDS changes slowly with increased R. Thanks to the power of PRIMME, the proposed SC_RB could achieve good clustering performance based on high-quality singular vectors while managing attractable computational time for very large sparse matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Scalability of SC_RB when varying the number of data samples N</head><p>Setup. Our goal in this experiment is to assess the scalability of SC_RB when varying the number of data samples N on poker dataset and another large dataset SUSY 4 . We vary the number of data samples in the range of N = [100 1, 000, 000] on poker and N = [4000 4, 000, 000] on the synthetic dataset. We use the same hyperparameters as the previous experiments and fix R = 256. Since RB can be easily parallelized, we accelerate its computation using 4 threads. Matlab also automatically parallelize the matrix-vector operations for other solvers. We report the runtime for generating random binning feature matrix, computing partial eigendecomposition using state-of-the-art eigensolver <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>, performing K-means, and the overall runtime, respectively.</p><p>Results. <ref type="figure" target="#fig_5">Figure 4</ref> clearly shows that SC_RB indeed scales linearly with the increase in the number of data samples. Note that even for large datasets consisting of millions of samples, the computation time of SC_RB is still less than 500 seconds. These results suggest that: (i) SC_RB exhibits linear scalability in N and is comtenant of handling large datasets in a reasonable time frame; (ii) with the stateof-the-art eigensolver <ref type="bibr" target="#b29">[30]</ref>, the complexity of computing a few of eigenvectors for spectral clustering is indeed linearly proportional to the matrix size N .</p><p>The key factor that contributes to the competitive computation time and linear scalability of SC_RB in the data size N is that we take into account the end-to-end spectral clustering pipeline consisting of the build blocks, RB generation, eigensolver and Kmeans, and our approach ensures each component takes a similar (linear) computation complexity, as analyzed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Scalability of SC_RB when varying the number of RB features R</head><p>Setup. We further investigate the scalablity of various randomfeature-based, sampling-based SC methods and approximate Kernel K-means methods when varying the latent feature size R. One of our goal is to investigate whether the latent matrix rank R is linearly proportional to N . If this is true, then the total complexity of an approximation method is still bounded by O(N 2 ), which is an unfavorable property for large-scale data. Therefore, we study the computational time of 8 baselines when varying R from 16 to 1024 on 4 datasets. The other settings are same as before.   Results. <ref type="figure" target="#fig_7">Figure 5</ref> shows that the computation of SC_RB is as efficient as other approximation methods. There are several comments worth making here. First, compared to the quadratic complexity of exact SC in <ref type="figure" target="#fig_7">Figures 5a and 5b</ref>, various approximation methods except KK_RF require much less computational time. It means that the total complexity of various methods are respecting to O(N R) where R could be somehow treated as a constant as long as R is significantly smaller than N . Remarkably, <ref type="figure" target="#fig_7">Figures 5c and 5d</ref> show that most of approximation methods including SC_RB can even behave as efficient as K-means on the original dataset. Obviously, most of these methods exhibit clear linear relation with R, indicating that these methods are not tightly associated with N . In other words, if the low rank R in any method is respecting with N , then the total complexity of the method is proportional to O(N 2 ), which should yield non-linear scalability respecting to R. The only exception is the KK_RF method, which consistently requires much more runtime compared to other methods, making it less attractable than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have presented a scalable end-to-end spectral clustering method based on RB features (SC_RB) for overcoming two computational bottlenecks -similarity graph construction and eigendecomposition of the graph Laplacian. By leveraging RB features, the pairwise similarity matrix can be approximated implicitly by the inner product of the RB feature matrix, which significantly reduces the computational complexity from quadratic to linear in terms of the number of data samples. We further show how to effectively and directly apply SVD on the weighted RB feature matrix and introduce a state-of-the-art sparse SVD solver to efficiently manage the SVD computation for a very large sparse matrix. Our theoretical analysis shows that by drawing R grids with at least κ number of non-empty bins per grid, SC_RB can guarantee convergence to exact spectral clustering with a rate of O(1/(κR)) under the same pairwise graph construction process, which is much faster than other Random Features based SC methods. Our extensive experiments on 8 benchmarks over 4 performance metrics demonstrate that SC_RB either outperforms or matches 8 baselines in both accuracy and computational time, and corroborate that SC_RB indeed exhibits linear scalability in terms of the number of data samples and the number of RB features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example of generating RB features when d = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Randomized Block Coordinate Descent on f (U, M) w.r.t. M, which results in R non-zero blocks of rows in M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Clustering accuracy and runtime when varying R on mnist for random features based SC methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Clustering accuracy and runtime when varying R on covtype-mult using PRIMME_SVDS and Matlab SVDS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Linear scalability of SC_RB when varying the number of samples N . Linear and quadratic complexity are also plotted for easy comparisons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Scalability of SC_RB and other methods on 4 datasets when varying the number of latent features R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Properties of the datasets.</figDesc><table><row><cell>Name</cell><cell cols="3">K: Classes d: Features N : Samples</cell></row><row><cell>pendigits</cell><cell>10</cell><cell>16</cell><cell>10,992</cell></row><row><cell>letter</cell><cell>26</cell><cell>16</cell><cell>15,500</cell></row><row><cell>mnist</cell><cell>10</cell><cell>780</cell><cell>70,000</cell></row><row><cell>acoustic</cell><cell>3</cell><cell>50</cell><cell>98,528</cell></row><row><cell>ijcnn1</cell><cell>2</cell><cell>22</cell><cell>126,701</cell></row><row><cell>cod_rna</cell><cell>2</cell><cell>8</cell><cell>321,054</cell></row><row><cell>covtype-mult</cell><cell>7</cell><cell>54</cell><cell>581,012</cell></row><row><cell>poker</cell><cell>10</cell><cell>10</cell><cell>1,025,010</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average rank scores comparing SC_RB against others methods using R = 1024.</figDesc><table><row><cell>Dataset</cell><cell cols="9">K-means SC KK_RS KK_RF SV_RF SC_LSC SC_Nys SC_RF SC_RB</cell></row><row><cell>pendigits</cell><cell>3.00</cell><cell>4.75</cell><cell>2.00</cell><cell>7.75</cell><cell>8.5</cell><cell>1.00</cell><cell>4.75</cell><cell>7.25</cell><cell>5.00</cell></row><row><cell>letter</cell><cell>8.50</cell><cell>5.75</cell><cell>5.50</cell><cell>7.50</cell><cell>4.75</cell><cell>3.25</cell><cell>4.75</cell><cell>3.75</cell><cell>1.25</cell></row><row><cell>mnist</cell><cell>5.00</cell><cell>4.25</cell><cell>5.00</cell><cell>9.00</cell><cell>8.00</cell><cell>1.00</cell><cell>3.25</cell><cell>6.75</cell><cell>2.75</cell></row><row><cell>acoustic</cell><cell>4.75</cell><cell>-</cell><cell>4.25</cell><cell>6.25</cell><cell>5.75</cell><cell>3.50</cell><cell>4.75</cell><cell>5.75</cell><cell>1.00</cell></row><row><cell>ijcnn1</cell><cell>4.50</cell><cell>-</cell><cell>5.75</cell><cell>2.00</cell><cell>4.00</cell><cell>6.75</cell><cell>4.75</cell><cell>7.25</cell><cell>1.00</cell></row><row><cell>cod_rna</cell><cell>5.75</cell><cell>-</cell><cell>3.50</cell><cell>5.00</cell><cell>7.75</cell><cell>5.50</cell><cell>4.00</cell><cell>2.75</cell><cell>1.75</cell></row><row><cell>covtype-mult</cell><cell>3.75</cell><cell>-</cell><cell>5.25</cell><cell>5.75</cell><cell>6.50</cell><cell>2.50</cell><cell>4.75</cell><cell>5.75</cell><cell>1.75</cell></row><row><cell>poker</cell><cell>4.33</cell><cell>-</cell><cell>4.00</cell><cell>3.33</cell><cell>4.67</cell><cell>5.67</cell><cell>5.00</cell><cell>4.33</cell><cell>4.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Computational time (seconds) comparing SC_RB against others methods using R = 1024.</figDesc><table><row><cell>Dataset</cell><cell>K-means</cell><cell>SC</cell><cell cols="7">KK_RS KK_RF SV_RF SC_LSC SC_Nys SC_RF SC_RB</cell></row><row><cell>pendigits</cell><cell>0.8</cell><cell>25.0</cell><cell>10.7</cell><cell>10.4</cell><cell>1.0</cell><cell>7.6</cell><cell>2.5</cell><cell>1.4</cell><cell>1.8</cell></row><row><cell>letter</cell><cell>5.9</cell><cell>171.4</cell><cell>17.1</cell><cell>36.9</cell><cell>8.9</cell><cell>27.1</cell><cell>14.6</cell><cell>10.0</cell><cell>7.7</cell></row><row><cell>mnist</cell><cell>278.1</cell><cell>2661</cell><cell>79.1</cell><cell>312.4</cell><cell>22.6</cell><cell>25.5</cell><cell>31.0</cell><cell>20.5</cell><cell>25.9</cell></row><row><cell>acoustic</cell><cell>10.2</cell><cell>-</cell><cell>34.7</cell><cell>83.7</cell><cell>6.3</cell><cell>16.7</cell><cell>20.1</cell><cell>7.0</cell><cell>10.7</cell></row><row><cell>ijcnn1</cell><cell>4.2</cell><cell>-</cell><cell>44.2</cell><cell>89.6</cell><cell>5.1</cell><cell>9.9</cell><cell>18.5</cell><cell>5.5</cell><cell>34.7</cell></row><row><cell>cod_rna</cell><cell>6.7</cell><cell>-</cell><cell>88.2</cell><cell>190.0</cell><cell>8.6</cell><cell>8.9</cell><cell>46.8</cell><cell>13.0</cell><cell>24.2</cell></row><row><cell>covtype-mult</cell><cell>60.7</cell><cell>-</cell><cell>180.2</cell><cell>220.0</cell><cell>40.5</cell><cell>181.1</cell><cell>99.1</cell><cell>41.5</cell><cell>1593</cell></row><row><cell>poker</cell><cell>102.4</cell><cell>-</cell><cell>363.1</cell><cell>5812</cell><cell>254.5</cell><cell>337.4</cell><cell>340.6</cell><cell>293.3</cell><cell>538.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>pendigits: Computational Time</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>10 3</cell><cell cols="2">letter: Computational Time</cell><cell></cell><cell>10 2</cell><cell cols="3">acoustic: Computational Time</cell><cell></cell><cell>ijcnn1: Computational Time</cell></row><row><cell>SC RB SC RF</cell><cell></cell><cell></cell><cell>SC RB SC RF</cell><cell></cell><cell></cell><cell></cell><cell>SC RB SC RF</cell><cell></cell><cell></cell><cell>10 2</cell><cell>SC RB SC RF</cell></row><row><cell>SC Nys SC LSC SV RF KK RF KK RS K-means SC</cell><cell>Runtime (Seconds)</cell><cell>10 1 10 2</cell><cell>SC Nys SC LSC SV RF KK RF KK RS K-means SC</cell><cell></cell><cell>Runtime (Seconds)</cell><cell>10 1</cell><cell>SC Nys SC LSC SV RF KK RF KK RS K-means</cell><cell></cell><cell>Runtime (Seconds)</cell><cell>10 0 10 1</cell><cell>SC Nys SC LSC SV RF KK RF KK RS K-means</cell></row><row><cell></cell><cell></cell><cell>10 0</cell><cell>10 1</cell><cell>10 2</cell><cell>10 3</cell><cell>10 0</cell><cell>10 1</cell><cell>10 2</cell><cell>10 3</cell><cell>10 1</cell><cell>10 2</cell><cell>10 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Varying R</cell><cell></cell><cell></cell><cell></cell><cell>Varying R</cell><cell></cell><cell></cell><cell>Varying R</cell></row><row><cell>(a) pendigits</cell><cell></cell><cell></cell><cell></cell><cell>(b) letter</cell><cell></cell><cell></cell><cell></cell><cell>(c) acoustic</cell><cell></cell><cell></cell><cell>(d) ijcnn1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/IBM/SpectralClustering_RandomBinning 2 https://github.com/teddylfwu/RandomBinning 3 https://github.com/primme/primme</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">SUSY is a large dataset in the LIBSVM data collections<ref type="bibr" target="#b1">[2]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust Spectral Clustering for Noisy Data: Modeling Sparse Corruptions Improves Latent Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Matkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="737" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on intelligent systems and technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast spectral clustering of data using sequential matrix compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient one-vs-one kernel ridge regression for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Ramabhadrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2454" to="2458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Phase transitions and a model order selection criterion for spectral graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><forename type="middle">O</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03159</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Revisiting Spectral Graph Clustering with Generative Community Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incremental eigenpair computation for graph Laplacian matrices: theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baichuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Al</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Network Analysis and Mining</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parallel spectral clustering in distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="568" to="586" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large Scale Spectral Clustering with Landmark-Based Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Approximate kernel k-means: Solution to large scale kernel clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radha</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Havens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="895" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient kernel clustering using random fourier features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radha</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kernel k-means: spectral clustering and normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="551" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Spectral grouping using the Nyström method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="214" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Approximate spectral clustering via randomized sketching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Gittens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhanjan</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Boutsidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ebay/IBM Research Technical Report</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithm AS 136: A k-means clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Manchek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable Sequential Spectral Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1809" to="1815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Power iteration clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning</title>
		<meeting>the 27th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="655" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale spectral clustering on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Danilevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third international joint conference on Artificial Intelligence</title>
		<meeting>the Twenty-Third international joint conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1486" to="1492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast large-scale spectral clustering by sequential shrinkage optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huai-Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Information Retrieval</title>
		<imprint>
			<biblScope unit="page" from="319" to="330" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large graph construction for scalable semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning</title>
		<meeting>the 27th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="679" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast Spectral Clustering with Random Projection and Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Imiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLDM</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="372" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spectral Clustering for a Large Data Set by Reducing the Similarity Matrix Size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shinnou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minoru</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PRIMME: preconditioned iterative multimethod eigensolverâĂŤmethods and software description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James R Mccombs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Compressive spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1002" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using the Nyström method to speed up kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="682" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PRIMME_SVDS: A high-performance preconditioned SVD solver for accurate large-scale computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eloy</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stathopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="248" to="271" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A preconditioned hybrid svd method for accurately computing singular triplets of large matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stathopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="365" to="388" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Revisiting random binning features: Fast convergence and strong parallelizability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>En-Hsu Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Ravikuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04956</idno>
		<title level="m">D2KE: From Distance to Kernel and Embedding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Random Warping Series: A Random Features Method for Time-Series Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>En-Hsu Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="793" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast approximate spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="907" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Defining and evaluating network communities based on ground-truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="181" to="213" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sparse random feature algorithm as coordinate descent in hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>En-Hsu Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shou-De</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2456" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Data mining and analysis: fundamental concepts and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wagner</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wagner</forename><surname>Meira</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
