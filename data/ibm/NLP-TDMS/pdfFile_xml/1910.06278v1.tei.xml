<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distribution-Aware Coordinate Representation for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
							<email>zhangfengwcy@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
							<email>daihanbin.ac@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distribution-Aware Coordinate Representation for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While being the de facto standard coordinate representation in human pose estimation, heatmap is never systematically investigated in the literature, to our best knowledge. This work fills this gap by studying the coordinate representation with a particular focus on the heatmap. Interestingly, we found that the process of decoding the predicted heatmaps into the final joint coordinates in the original image space is surprisingly significant for human pose estimation performance, which nevertheless was not recognised before. In light of the discovered importance, we further probe the design limitations of the standard coordinate decoding method widely used by existing methods, and propose a more principled distribution-aware decoding method. Meanwhile, we improve the standard coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) by generating accurate heatmap distributions for unbiased model training. Taking the two together, we formulate a novel Distribution-Aware coordinate Representation of Keypoint (DARK) method. Serving as a model-agnostic plugin, DARK significantly improves the performance of a variety of state-of-the-art human pose estimation models. Extensive experiments show that DARK yields the best results on two common benchmarks, MPII and COCO, consistently validating the usefulness and effectiveness of our novel coordinate representation idea. The project page is at https://ilovepose.github.io/coco/ 1 The label representation is for encoding the label annotations (e.g. 1,000 one-hot vectors for 1,000 object class labels in Ima-geNet), totally different from the data representation for encoding the data samples (e.g. the object images from ImageNet).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Human pose estimation is a fundamental computer vision problem that aims to detect the spatial location (i.e. coordinate) of human body joints in unconstrained images <ref type="bibr" target="#b0">(Andriluka et al. 2014)</ref>. It is a non-trivial task as the appearance of body joints vary dramatically due to diverse styles of clothes, arbitrary occlusion, and unconstrained background contexts, whilst it is needed to identify the fine-grained joint coordinates. As strong image processing models, convolutional neural networks (CNNs) excel at this task <ref type="bibr" target="#b13">(LeCun et al. 1998)</ref>. Existing works typically focus on designing the CNN architecture tailored particularly for human pose inference <ref type="bibr" target="#b18">(Newell, Yang, and Deng 2016;</ref><ref type="bibr" target="#b24">Sun et al. 2019</ref>).</p><p>Analogous to the common one-hot vectors as the object class label representation in image classification, a human  <ref type="figure">Figure 1</ref>: Pipeline of a human pose estimation system. For efficiency, resolution reduction is often applied on the original person detection bounding boxes as well as the groundtruth heatmap supervision. That is, the model operates in a low-resolution image space. At test time, a corresponding resolution recovery is therefore necessary in order to obtain the joint coordinate prediction in the original image space.</p><p>pose CNN model also requires a label representation for encoding the body joint coordinate labels, so that the supervised learning loss can be quantified and computed during training and the joint coordinates can be inferred properly 1 . The de facto standard label representation is coordinate heatmap, generated as a 2-dimensional Gaussian distribution/kernel centred at the labelled coordinate of each joint <ref type="bibr" target="#b27">(Tompson et al. 2014)</ref>. It is obtained from a coordinate encoding process, from coordinate to heatmap. Heatmap is characterised by giving spatial support around the groundtruth location, considering not only the contextual clues but also the inherent target position ambiguity. Importantly, this may effectively reduce the model overfitting risk in training, in a similar spirit of the class label smoothing regularisation <ref type="bibr" target="#b25">(Szegedy et al. 2016)</ref>. Come as no surprise, the state-of-the-art pose models <ref type="bibr" target="#b18">(Newell, Yang, and Deng 2016;</ref><ref type="bibr" target="#b30">Xiao, Wu, and Wei 2018;</ref><ref type="bibr" target="#b24">Sun et al. 2019</ref>) are based on the heatmap coordinate representation. With the heatmap label representation, one major obstacle is that, the computational cost is a quadratic function of the input image resolution, preventing the CNN models from processing the typically high-resolution raw imagery data. To be computationally affordable, a standard strategy (see <ref type="figure">Fig. 1</ref>) is to downsample all the person bounding box images at arbitrarily large resolutions into a prefixed small resolution with a data preprocessing procedure, before being fed into a human pose estimation model. Aiming to predict the joint location in the original image coordinate space, after the heatmap prediction a corresponding resolution recovery is required for transforming back to the original coordinate space. The final prediction is considered as the location with the maximal activation. We call this process as coordinate decoding, from heatmap to coordinate. It is worthy noting that quantisation error can be introduced during the above resolution reduction. To alleviate this problem, during the existing coordinate decoding process a hand-crafted shifting operation is usually performed according to the direction from the highest activation to the second highest activation <ref type="bibr" target="#b18">(Newell, Yang, and Deng 2016)</ref>.</p><p>In the literature, the problem of coordinate encoding and decoding (i.e. denoted as coordinate representation) gains little attention, although being indispensable in model inference. In contrast to the current research focus on designing more effective CNN structures, we reveal a surprisingly important role the coordinate representation plays on the model performance, much more significant than expected. For instance, with the state-of-the-art model HRNet-W32 <ref type="bibr" target="#b24">(Sun et al. 2019)</ref>, the aforementioned shifting operation of coordinate encoding brings as high as 5.7% AP on the challenging COCO validation set <ref type="table" target="#tab_0">(Table 1)</ref>. It is noteworthy to mention that, this gain is already much more significant than those by most individual art methods. But it is never well noticed and carefully investigated in the literature to our best knowledge.</p><p>Contrary to the existing human pose estimation studies, in this work we dedicatedly investigate the problem of joint coordinate representation including encoding and decoding. Moreover, we recognise that the heatmap resolution is one major obstacle that prevents the use of smaller input resolution for faster model inference. When decreasing the input resolution from 256×192 to 128×96, the model performance of HRNet-W32 drops significantly from 74.4% to 66.9% on the COCO validation set, although the model inference cost falls from 7.1×10 9 to 1.8×10 9 FLOPs.</p><p>In light of the discovered significance of coordinate representation, we conduct in-depth investigation and recognise that one key limitation lies in the coordinate decoding process. Whilst existing standard shifting operation has shown to be effective as found in this study, we propose a principled distribution-aware representation method for more accurate joint localisation at sub-pixel accuracy. Specifically, it is de-signed to comprehensively account for the distribution information of heatmap activation via Taylor-expansion based distribution approximation. Besides, we observe that the standard method for generating the ground-truth heatmaps suffers from quantisation errors, leading to imprecise supervision signals and inferior model performance. To solve this issue, we propose generating the unbiased heatmaps allowing Gaussian kernel being centred at sub-pixel locations.</p><p>The contribution of this work is that, we discover the previously unrealised significance of coordinate representation in human pose estimation, and propose a novel Distribution-Aware coordinate Representation of Keypoint (DARK) method with two key components: (1) efficient Taylor-expansion based coordinate decoding, and (2) unbiased sub-pixel centred coordinate encoding. Importantly, existing human pose methods can be seamlessly benefited from DARK without any algorithmic modification. Extensive experiments on two common benchmarks (MPII and COCO) show that our method provides significant performance improvement for existing state-of-the-art human pose estimation models <ref type="bibr" target="#b24">(Sun et al. 2019;</ref><ref type="bibr" target="#b30">Xiao, Wu, and Wei 2018;</ref><ref type="bibr" target="#b18">Newell, Yang, and Deng 2016)</ref>, achieving the best single model accuracy on COCO and MPII. DARK favourably enables the use of smaller input image resolutions with much smaller performance degradation, whilst dramatically boosting the model inference efficiency therefore facilitating lowlatency and low-energy applications as required in embedded AI scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>There are two common coordinate representation designs in human pose estimation: direct coordinate and heatmap. Both are used as the regression targets for model training.</p><p>Coordinate regression Directly taking the coordinates as model output target is straightforward and intuitive. But only a handful of existing methods adopt this design <ref type="bibr" target="#b28">(Toshev and Szegedy 2014;</ref><ref type="bibr" target="#b8">Fan et al. 2015;</ref><ref type="bibr" target="#b2">Carreira et al. 2016;</ref>. One plausible reason is that, this representation lacks the spatial and contextual information, making the learning of human pose model extremely challenging due to the intrinsic visual ambiguity in joint location. Heatmap regression The heatmap representation elegantly addresses the above limitations. It was firstly introduced in <ref type="bibr" target="#b27">(Tompson et al. 2014</ref>) and rapidly became the most commonly used coordinate representation. Generally, the mainstream research focus is on designing network architectures for more effectively regressing the heatmap supervision. Representative design improvements include sequential modelling (Gkioxari, Toshev, and Jaitly 2016; Belagiannis and Zisserman 2017), receptive field expansion <ref type="bibr" target="#b29">(Wei et al. 2016)</ref>, position voting <ref type="bibr" target="#b14">(Lifshitz, Fetaya, and Ullman 2016)</ref>, intermediate supervision <ref type="bibr" target="#b18">(Newell, Yang, and Deng 2016;</ref><ref type="bibr" target="#b29">Wei et al. 2016)</ref>, pairwise relations modelling (Chen and Yuille 2014), tree structure modelling <ref type="bibr" target="#b7">(Chu et al. 2016b;</ref><ref type="bibr" target="#b6">Chu et al. 2016a;</ref><ref type="bibr" target="#b22">Sun et al. 2017;</ref><ref type="bibr" target="#b26">Tang, Yu, and Wu 2018)</ref>, pyramid residual learning , cascaded pyramid learning ), knowledge-guided learning <ref type="bibr" target="#b19">(Ning, Zhang, and He 2017)</ref>, ac-tive learning <ref type="bibr" target="#b16">(Liu and Ferrari 2017)</ref>, adversarial learning <ref type="bibr" target="#b4">(Chen et al. 2017)</ref>, deconvolution upsampling <ref type="bibr" target="#b30">(Xiao, Wu, and Wei 2018)</ref>, multi-scale supervision <ref type="bibr" target="#b12">(Ke et al. 2018</ref>), attentional mechanism <ref type="bibr" target="#b17">(Liu et al. 2018;</ref><ref type="bibr" target="#b21">Su et al. 2019)</ref>, and high-resolution representation preserving <ref type="bibr" target="#b24">(Sun et al. 2019)</ref>.</p><p>In contrast to all previous works, we instead investigate the issues of heatmap representation on human pose estimation, a largely ignored perspective in the literature. Not only do we reveal a big impact of resolution reduction in the process of using heatmap but also we propose a principled coordinate representation method for significantly improving the performance of existing models. Crucially, our method can be seamlessly integrated without model design change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>We consider the coordinate representation problem including encoding and decoding in human pose estimation. The objective is to predict the joint coordinates in a given input image. To that end, we need to learn a regression model from the input image to the output coordinates, and the heatmap is often leveraged as coordinate representation during both model training and testing. Specifically, we assume access to a training set of images. To facilitate the model learning, we encode the labelled ground-truth coordinate of a joint into a heatmap as the supervised learning target. During testing, we then need to decode the predicted heatmap into the coordinate in the original image coordinate space.</p><p>In the following we first describe the decoding process, focusing on the limitation analysis of the existing standard method and the development of a novel solution. Then, we further discuss and address the limitations of the encoding process. Lastly, we describe the integration of existing human pose estimation models with the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coordinate Decoding</head><p>Despite being considered as an insignificant component of the model testing pipeline, as we found in this study, coordinate decoding turns out to be one of the most significant performance contributors for human pose estimation in images (see <ref type="table" target="#tab_0">Table 1</ref>). Specifically, this is a process of translating a predicted heatmap of each individual joint into a coordinate in the original image space. Suppose the heatmap has the same spatial size as the original image, we only need to find the location of the maximal activation as the joint coordinate prediction, which is straightforward and simple. However, this is often not the case as interpreted above. Instead, we need to upsample the heatmaps to the original image resolution by a sample-specific unconstrained factor λ ∈ R + . This involves a sub-pixel localisation problem. Before introducing our method, we first revisit the standard coordinate decoding method used in existing pose estimation models.</p><p>The standard coordinate decoding method is designed empirically according to model performance <ref type="bibr" target="#b18">(Newell, Yang, and Deng 2016)</ref>. Specifically, given a heatmap h predicted by a trained model, we first identify the coordinates of the maximal (m) and second maximal (s) activation. The joint location is then predicted as</p><formula xml:id="formula_0">p = m + 0.25 s − m s − m 2 (1)</formula><p>where · 2 defines the magnitude of a vector. This means that the prediction is as the maximal activation with a 0.25 pixel (i.e. sub-pixel) shifting towards the second maximal activation in the heatmap space. The final coordinate prediction in the original image is computed as:</p><formula xml:id="formula_1">p = λp (2)</formula><p>where λ is the resolution reduction ratio.</p><p>Remarks The aim of the sub-pixel shifting in Eq. (1) is to compensate the quantisation effect of image resolution downsampling. That being said, the maximum activation in the predicted heatmap does not correspond to the accurate position of the joint in the original coordinate space, but only to a coarse location. As we will show, this shifting surprisingly brings a significant performance boost <ref type="table" target="#tab_0">(Table 1)</ref>. This may partly explain why it is often used as a standard operation in model test. Interestingly, to our best knowledge no specific work has delved into the effect of this operation on human pose estimation performance. Therefore, its true significance has never been really recognised and reported in the literature. While this standard method lacks intuition and interpretation in design, no dedicated investigation has been carried out for improvement. We fill this gap by presenting a principled method for shifting estimation and finally more accurate human pose estimation. The proposed coordinate decoding method explores the distribution structure of the predicted heatmap to infer the underlying maximum activation. This differs dramatically to the standard method above relying on a hand-designed offset prediction, with little design justification and rationale.</p><p>Specifically, to obtain the accurate location at the degree of sub-pixel, we assume the predicted heatmap follows a 2D Gaussian distribution, same as the ground-truth heatmap. Therefore, we represent the predicted heatmap as</p><formula xml:id="formula_2">G(x; µ, Σ) = 1 (2π)|Σ| 1 2 exp − 1 2 (x − µ) T Σ −1 (x − µ) (3)</formula><p>where x is a pixel location in the predicted heatmap, µ is the Gaussian mean (centre) corresponding to the to-beestimated joint location. The covariance Σ is a diagonal matrix, same as that used in coordinate encoding:</p><formula xml:id="formula_3">Σ = σ 2 0 0 σ 2<label>(4)</label></formula><p>where σ is the standard deviation same for both directions.</p><p>In the log-likelihood optimisation principle <ref type="bibr" target="#b11">(Goodfellow, Bengio, and Courville 2016)</ref>, we transform G through logarithm to facilitate inference while keeping the original location of the maximum activation as: Our objective is to estimate µ. As an extreme point in the distribution, it is well-known that the first derivative at the location µ meets a condition as:</p><formula xml:id="formula_4">P(x; µ, Σ) = ln(G) = − ln(2π) − 1 2 ln(|Σ|) (5) − 1 2 (x − µ) T Σ −1 (x − µ) (b) Distribution-aware Maximum Re-localization</formula><formula xml:id="formula_5">D (x) x=µ = ∂P T ∂x x=µ = −Σ −1 (x − µ) x=µ = 0</formula><p>(6) To explore this condition, we adopt the Taylor's theorem. Formally, we approximate the activation P(µ) by a Taylor series (up to the quadratic term) evaluated at the maximal activation m of the predicted heatmap as</p><formula xml:id="formula_6">P(µ) = P(m)+D (m)(µ−m)+ 1 2 (µ−m) T D (m)(µ−m)<label>(7)</label></formula><p>where D (m) denotes the second derivative (i.e. Hessian) of P evaluated at m, formally defined as:</p><formula xml:id="formula_7">D (m) = D (x) x=m = −Σ −1<label>(8)</label></formula><p>The intuition of selecting m to approximate µ is that it represents a good coarse joint prediction that approaches µ. Taking Eq. (6), (7), and (8) together, we eventually obtain</p><formula xml:id="formula_8">µ = m − D (m) −1 D (m)<label>(9)</label></formula><p>where D (m) and D (m) can be estimated efficiently from the heatmap. Once obtaining µ, we also apply Eq.</p><p>(2) to predict the coordinate in the original image space.</p><p>Remarks In contrast to the standard method considering the second maximum activation alone in heatmap, the proposed coordinate decoding fully explores the heatmap distributional statistics for revealing the underlying maximum more accurately. In theory, our method is based on a principled distribution approximation under a trainingsupervision-consistent assumption that the heatmap is in a Gaussian distribution. Crucially, it is very efficient computationally as it only needs to compute the first and second derivative of one location per heatmap. Consequently, existing human pose estimation approaches can be readily benefited without any computational cost barriers. Heatmap distribution modulation As the proposed coordinate decoding method is based on a Gaussian distribution assumption, it is necessary for us to examine how well this condition is satisfied. We found that, often, the heatmaps predicted by a human pose estimation model do not exhibit good-shaped Gaussian structure compared to the training heatmap data. As shown in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>, the heatmap usually presents multiple peaks around the maximum activation. This may cause negative effects to the performance of our decoding method. To address this issue, we propose modulating the heatmap distribution beforehand.</p><p>Specifically, to match the requirement of our method we propose exploiting a Gaussian kernel K with the same variation as the training data to smooth out the effects of multiple peaks in the heatmap h, formally as</p><formula xml:id="formula_9">h = K h<label>(10)</label></formula><p>where specifies the convolution operation. To preserve the original heatmap's magnitude, we finally scale h so that its maximum activation is equal to that of h, via the following transformation:</p><formula xml:id="formula_10">h = h − min(h ) max(h ) − min(h ) * max(h)<label>(11)</label></formula><p>where max() and min() return the maximum and minimum values of an input matrix, respectively. In our experimental analysis, it is validated that this distribution modulation further improves the performance of our coordinate decoding method <ref type="table" target="#tab_2">(Table 3)</ref>, with the resulting visual effect and qualitative evaluation demonstrated in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. Summary We summarise our coordinate decoding method in <ref type="figure" target="#fig_1">Fig. 2</ref>. Specifically, a total of three steps are involved in a sequence: (a) Heatmap distribution modulation (Eq. (10), <ref type="figure">Figure 4</ref>: Illustration of quantisation error in the standard coordinate encoding process. The blue point denotes the accurate position (g ) of a joint. With the floor based coordinate quantisation, an error (indicated by red arrow) is introduced. Other quantisation methods share the same problem.</p><p>(11)), (b) Distribution-aware joint localisation by Taylor expansion at sub-pixel accuracy (Eq.</p><p>(3)- <ref type="formula" target="#formula_8">(9)</ref>), (c) Resolution recovery to the original coordinate space (Eq. <ref type="formula">(2)</ref>). None of these steps incur high computational costs, therefore being able to serve as an efficient plug-in for existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coordinate Encoding</head><p>The previous section has addressed the problem with coordinate decoding, rooted at resolution reduction. As a similar process, coordinate encoding shares the same limitation. Specifically, the standard coordinate encoding method starts with downsampling original person images into the model input size. So, the ground-truth joint coordinates require to be transformed accordingly before generating the heatmaps. Formally, we denote by g = (u, v) the ground-truth coordinate of a joint. The resolution reduction is defined as:</p><formula xml:id="formula_11">g = (u , v ) = g λ = ( u λ , v λ )<label>(12)</label></formula><p>where λ is the downsampling ratio. Conventionally, for facilitating the kernel generation, we often quantise g :</p><formula xml:id="formula_12">g = (u , v ) = quantise(g ) = quantise( u λ , v λ ) (13)</formula><p>where quantise() specifies a quantisation function, with the common choices including floor, ceil and round. Subsequently, the heatmap centred at the quantised coordinate g can be synthesised through:</p><formula xml:id="formula_13">G(x, y; g ) = 1 2πσ 2 exp − (x − u ) 2 + (y − v ) 2 2σ 2<label>(14)</label></formula><p>where (x, y) specifies a pixel location in the heatmap, and σ denotes a fixed spatial variance. Obviously, the heatmaps generated in the above way are inaccurate and biased due to the quantisation error <ref type="figure">(Fig. 4)</ref>. This may introduce sub-optimal supervision signals and result in degraded model performance, particularly for the case of accurate coordinate encoding as proposed in this work.</p><p>To address this issue, we simply place the heatmap centre at the non-quantised location g which represents the accurate ground-truth coordinate. We still apply Eq. (14) but replacing g with g . We will demonstrate the benefits of this unbiased heatmap generation method <ref type="table" target="#tab_2">(Table 3)</ref>.  <ref type="bibr" target="#b24">(Sun et al. 2019</ref>) and Simple-Baseline (Xiao, Wu, and Wei 2018), we followed the same learning schedule and epochs as in the original works. For Hourglass <ref type="bibr" target="#b18">(Newell, Yang, and Deng 2016)</ref>, the base learning rate was fine-tuned to 2.5e-4, and decayed to 2.5e-5 and 2.5e-6 at the 90-th and 120-th epoch. The total number of epochs is 140. We used three different input sizes <ref type="bibr">(128 × 96, 256 × 192, 384 × 288)</ref> in our experiments. We adopted the same data preprocessing as in <ref type="bibr" target="#b24">(Sun et al. 2019</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluating Coordinate Representation</head><p>As the core problem in this work, the effect of coordinate representation on model performance was firstly examined, with a connection to the input image resolution (size). In this test, by default we used HRNet-W32 <ref type="bibr" target="#b24">(Sun et al. 2019</ref>) as the  backbone model and 128 × 96 as the input size, and reported the accuracy results on the COCO validation set. (i) Coordinate decoding We evaluated the effect of coordinate decoding, in particular, the shifting operation and distribution modulation. The conventional biased heatmaps were used. In this test, we compared the proposed distributionaware shifting method with no shifting (i.e. directly using the maximal activation location), and the standard shifting <ref type="figure">(Eq. (1)</ref>). We make two major observations in <ref type="table" target="#tab_0">Table 1</ref>: (i) The standard shifting gives as high as 5.7% AP accuracy boost, which is surprisingly effective. To our best knowledge, this is the first reported effectiveness analysis in the literature, since this problem is largely ignored by previous studies. This reveals previously unseen significance of coordinate decoding to human pose estimation. (ii) Despite the great gain by the standard decoding method, the proposed model further improves AP score by 1.5%, among which the distribution modulation gives 0.3% as shown in <ref type="table" target="#tab_1">Table 2</ref>. This validates the superiority of our decoding method. (ii) Coordinate encoding We tested how effective coordinate encoding can be. We compared the proposed unbiased encoding with the standard biased encoding, along with both the standard and our decoding method. We observed from <ref type="table" target="#tab_2">Table 3</ref> that our unbiased encoding with accurate kernel centre brings positive performance margin, regardless of the coordinate decoding method. In particular, unbiased encoding contributes consistently over 1% AP gain in both cases.  This suggests the importance of coordinate encoding, which again is neglected by previous investigations.</p><p>(iii) Input resolution We examined the impact of input image resolution/size by testing a number of different sizes, considering that it is an important factor relevant to model inference efficiency. We compared our DARK model (HRNet-W32 as backbone) with the original HRNet-W32 using the biased heatmap supervision for training and the standard shifting for testing. From <ref type="table" target="#tab_3">Table 4</ref> we have a couple of observations: (a) With reduced input image size, as expected the model performance consistently degrades whilst the inference cost drops clearly. (b) With the support of DARK, the model performance loss can be effectively mitigated, especially in case of very small input resolution (i.e. very fast model inference). This facilitates the deployment of human pose estimation models on low-resource devices, highly desired in the emerging embedded AI.</p><p>(iv) Generality Besides the state-of-the-art HRNet, we also tested other two representative human pose estimation models under varying CNN architectures: SimpleBaseline <ref type="bibr" target="#b30">(Xiao, Wu, and Wei 2018)</ref> and Hourglass <ref type="bibr" target="#b18">(Newell, Yang, and Deng 2016)</ref>. The results in <ref type="table" target="#tab_5">Table 5</ref> show that DARK provides significant performance gain to the existing models in most cases. This suggests a generic usefulness of our approach. We showed qualitative evaluation in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p><p>(v) Complexity We tested the inference efficiency impact by our method in HRNet-W32 at input size of 128 × 96. On a Titan V GPU, the running speed is reduced from 360 fps to 320 fps in the low-efficient python environment, i.e. a drop of 11%. We consider this extra cost is rather affordable.     (ii) Evaluation on MPII We compared DARK with HRNet-W32 on the MPII validation set. The comparisons in <ref type="table" target="#tab_7">Table 7</ref> show a consistent performance superiority of our method over the best competitor. Under the more strict accuracy measurement PCKh@0.1, the performance margin of DARK is even more significant. Note, MPII provides significantly smaller training data than COCO, suggesting that our method generalises across varying training data sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we for the first time systematically investigated the largely ignored yet significant problem of coordinate representation (including encoding and decoding) for human pose estimation in unconstrained images. We not only revealed the genuine significance of this problem, but also presented a novel distribution-aware coordinate representation of keypoint (DARK) for more discriminative model training and inference. Serving as a ready-to-use plug-in component, existing state-of-the-art models can be seamlessly benefited from our DARK method without any algorithmic adaptation at a neglectable cost. Apart from demonstrating empirically the importance of coordinate representation, we validated the performance advantages of DARK by conducting extensive experiments with a wide spectrum of contemporary models on two challenging datasets. We also provided a sequence of in-depth component analysis for giving insights on the design rationale of our model formulation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed distribution aware coordinate decoding method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of heatmap distribution modulation. (a) Predicted heatmap; (b) Modulated heatmap distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative evaluation of DARK (red) vs. HRNet-W32 (cyan) on COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>1 42.7 42.0 41.6 17.9 29.9 31.0 37.7 DARK 55.2 47.8 47.4 45.2 20.1 33.4 35.4 42.0 efficient model (Integral Pose Regression), DARK(HRNet-W32) achieves an AP gain of 2.2% (70.0-67.8) whilst only needing 16.4% (1.8/11.0 GFLOPs) execution cost. These suggest the advantages and flexibility of DARK on top of existing models in terms of both accuracy and efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effect of coordinate decoding on the COCO validation set. Model: HRNet-W32; Input size: 128 × 96. Decoding AP AP 50 AP 75 AP M AP L AR No Shifting 61.2 88.1 72.3 59.0 66.3 68.7 Standard Shifting 66.9 88.7 76.3 64.6 72.3 73.7 Ours 68.4 88.6 77.4 66.0 74.0 74.9Integration with State-of-the-Art Models</figDesc><table><row><cell>Our DARK method is model-agnostic, seamlessly integrable</cell></row><row><cell>with any existing heatmap based pose models. Importantly,</cell></row><row><cell>this does not involve any algorithmic changes to previous</cell></row><row><cell>methods. In particular, during training the only change is</cell></row><row><cell>the ground-truth heatmap data generated based on the ac-</cell></row><row><cell>curate joint coordinates. At test time, we take as input the</cell></row><row><cell>predicted heatmaps predicted by any model such as HRNet</cell></row><row><cell>(Sun et al. 2019), and output more accurate joint coordinates</cell></row><row><cell>in the original image space. In the whole lifecycle, we keep</cell></row><row><cell>an existing model intact as the original design. This allows</cell></row><row><cell>to maximise the generality and scalability of our method.</cell></row><row><cell>Experiments</cell></row><row><cell>Datasets We used two popular human pose estimation</cell></row><row><cell>datasets, COCO and MPII. The COCO keypoint dataset (Lin</cell></row><row><cell>et al. 2014) presents naturally challenging imagery data with</cell></row><row><cell>various human poses, unconstrained environments, differ-</cell></row><row><cell>ent body scales and occlusion patterns. The entire objec-</cell></row><row><cell>tive involves both detecting person instances and localis-</cell></row><row><cell>ing the body joints. It contains 200,000 images and 250,000</cell></row><row><cell>person samples. Each person instance is labelled with 17</cell></row><row><cell>joints. The annotations of training and validation sets are</cell></row><row><cell>publicly benchmarked. In evaluation, we followed the com-</cell></row><row><cell>monly used train2017/val2017/test-dev2017 split. The MPII</cell></row><row><cell>human pose dataset (Andriluka et al. 2014) contains 40k per-</cell></row><row><cell>son samples, each labelled with 16 joints. We followed the</cell></row><row><cell>standard train/val/test split as in (Tompson et al. 2014).</cell></row><row><cell>Evaluation metrics We used Object Keypoint Similar-</cell></row><row><cell>ity (OKS) for COCO and Percentage of Correct Keypoints</cell></row><row><cell>(PCK) for MPII to evaluate the model performance.</cell></row><row><cell>Implementation details For model training, we used the</cell></row><row><cell>Adam optimiser. For HRNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Effect of distribution modulation (DM) on the COCO val set. Backbone: HRNet-W32; Input size: 128×96. DM AP AP 50 AP 75 AP M AP L AR</figDesc><table><row><cell>68.1 88.5</cell><cell>77.1</cell><cell>65.8</cell><cell>73.7 74.8</cell></row><row><cell>68.4 88.6</cell><cell>77.4</cell><cell>66.0</cell><cell>74.0 74.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Effect of coordinate encoding on the COCO validation set. Model: HRNet-W32; Input size: 128 × 96. Encode Decode AP AP 50 AP 75 AP M AP L AR</figDesc><table><row><cell cols="2">Biased Standard 66.9 88.7 76.3 64.6 72.3 73.7</cell></row><row><cell cols="2">Unbiased Standard 68.0 88.9 77.0 65.4 73.7 74.5</cell></row><row><cell>Biased</cell><cell>Ours 68.4 88.6 77.4 66.0 74.0 74.9</cell></row><row><cell cols="2">Unbiased Ours 70.7 88.9 78.4 67.9 76.6 76.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Effect of input image size on the COCO validation set. DARK uses HRNet-W32 (HRN32) as backbone. Method Input size GFLOPs AP AP 50 AP 75 AP M AP L AR</figDesc><table><row><cell>HRN32 128×96 DARK</cell><cell>1.8</cell><cell>66.9 88.7 76.3 64.6 72.3 73.7 70.7 88.9 78.4 67.9 76.6 76.7</cell></row><row><cell>HRN32 256×192 DARK</cell><cell>7.1</cell><cell>74.4 90.5 81.9 70.8 81.0 79.8 75.6 90.5 82.1 71.8 82.8 80.8</cell></row><row><cell>HRN32 384×288 DARK</cell><cell>16.0</cell><cell>75.8 90.6 82.5 72.0 82.7 80.9 76.6 90.7 82.8 72.7 83.9 81.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Evaluating the generality of our DARK method to varying state-of-the-art models on the COCO validation set.DARK BaselineInput size #Params GFLOPs AP AP 50 AP 75 AP M AP L AR</figDesc><table><row><cell>Hourglass (4 Blocks)</cell><cell>128 × 96</cell><cell>13.0M</cell><cell>2.7</cell><cell>66.2 69.6</cell><cell>87.6 87.8</cell><cell>75.1 77.0</cell><cell>63.8 67.0</cell><cell>71.4 72.8 75.4 75.7</cell></row><row><cell>Hourglass (8 Blocks)</cell><cell>128 × 96</cell><cell>25.1M</cell><cell>4.9</cell><cell>67.6 70.8</cell><cell>88.3 87.9</cell><cell>77.4 78.3</cell><cell>65.2 68.3</cell><cell>73.0 74.0 76.4 76.6</cell></row><row><cell>SimpleBaseline-R50</cell><cell>128 × 96</cell><cell>34.0M</cell><cell>2.3</cell><cell>59.3 62.6</cell><cell>85.5 86.1</cell><cell>67.4 70.4</cell><cell>57.8 60.4</cell><cell>63.8 66.6 67.9 69.5</cell></row><row><cell cols="2">SimpleBaseline-R101 128 × 96</cell><cell>53.0M</cell><cell>3.1</cell><cell>58.8 63.2</cell><cell>85.3 86.2</cell><cell>66.1 71.1</cell><cell>57.3 61.2</cell><cell>63.4 66.1 68.5 70.0</cell></row><row><cell cols="2">SimpleBaseline-R152 128 × 96</cell><cell>68.6M</cell><cell>3.9</cell><cell>60.7 63.1</cell><cell>86.0 86.2</cell><cell>69.6 71.6</cell><cell>59.0 61.3</cell><cell>65.4 68.0 68.1 70.0</cell></row><row><cell>HRNet-W32</cell><cell>128 × 96</cell><cell>28.5M</cell><cell>1.8</cell><cell>66.9 70.7</cell><cell>88.7 88.9</cell><cell>76.3 78.4</cell><cell>64.6 67.9</cell><cell>72.3 73.7 76.6 76.7</cell></row><row><cell>HRNet-W48</cell><cell>128 × 96</cell><cell>63.6M</cell><cell>3.6</cell><cell>68.0 71.9</cell><cell>88.9 89.1</cell><cell>77.4 79.6</cell><cell>65.7 69.2</cell><cell>73.7 74.7 78.0 77.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison with the state-of-the-art human pose estimation methods on the COCO test-dev set. Method Backbone Input size #Params GFLOPs AP AP 50 AP 75 AP M AP L AR</figDesc><table><row><cell>G-RMI</cell><cell>ResNet-101</cell><cell cols="2">353 × 257 42.6M</cell><cell cols="2">57.0 64.9 85.5 71.3 62.3 70.0 69.7</cell></row><row><cell>Integral Pose Regression</cell><cell>ResNet-101</cell><cell cols="2">256 × 256 45.1M</cell><cell cols="2">11.0 67.8 88.2 74.8 63.9 74.0 -</cell></row><row><cell>CPN</cell><cell cols="2">ResNet-Inception 384 × 288</cell><cell>-</cell><cell>-</cell><cell>72.1 91.4 80.0 68.7 77.2 78.5</cell></row><row><cell>RMPE</cell><cell>PyraNet</cell><cell cols="2">320 × 256 28.1M</cell><cell cols="2">26.7 72.3 89.2 79.1 68.0 78.6 -</cell></row><row><cell>CFN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.6 86.1 69.7 78.3 64.1 -</cell></row><row><cell>CPN (ensemble)</cell><cell cols="2">ResNet-Inception 384 × 288</cell><cell>-</cell><cell>-</cell><cell>73.0 91.7 80.9 69.5 78.1 79.0</cell></row><row><cell>SimpleBaseline</cell><cell>ResNet-152</cell><cell cols="2">384 × 288 68.6M</cell><cell cols="2">35.6 73.7 91.9 81.1 70.3 80.0 79.0</cell></row><row><cell>HRNet</cell><cell>HRNet-W32</cell><cell cols="2">384 × 288 28.5M</cell><cell cols="2">16.0 74.9 92.5 82.8 71.3 80.9 80.1</cell></row><row><cell>HRNet</cell><cell>HRNet-W48</cell><cell cols="2">384 × 288 63.6M</cell><cell cols="2">32.9 75.5 92.5 83.3 71.9 81.5 80.5</cell></row><row><cell>DARK</cell><cell>HRNet-W32</cell><cell cols="2">128 × 96 28.5M</cell><cell>1.8</cell><cell>70.0 90.9 78.5 67.4 75.0 75.9</cell></row><row><cell>DARK</cell><cell>HRNet-W48</cell><cell cols="2">384 × 288 63.6M</cell><cell cols="2">32.9 76.2 92.5 83.6 72.5 82.4 81.1</cell></row><row><cell>G-RMI (extra data)</cell><cell>ResNet-101</cell><cell cols="2">353 × 257 42.6M</cell><cell cols="2">57.0 68.5 87.1 75.5 65.8 73.3 73.3</cell></row><row><cell>HRNet (extra data)</cell><cell>HRNet-W48</cell><cell cols="2">384 × 288 63.6M</cell><cell cols="2">32.9 77.0 92.7 84.5 73.4 83.1 82.0</cell></row><row><cell>DARK (extra data)</cell><cell>HRNet-W48</cell><cell cols="2">384 × 288 63.6M</cell><cell cols="2">32.9 77.4 92.6 84.6 73.6 83.7 82.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison on the MPII validation set. DARK uses HRNet-W32 (HRN32) as backbone. Input size: 256×256. Single-scale model performance is considered. Method Head Sho. Elb. Wri. Hip Kne. Ank. Mean PCKh@0.5 HRN32 97.1 95.9 90.3 86.5 89.1 87.1 83.3 90.3 DARK 97.2 95.9 91.2 86.7 89.7 86.7 84.0 90.6</figDesc><table><row><cell>PCKh@0.1</cell></row><row><cell>HRN32 51.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Belagiannis and Zisserman</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crf-cnn: Modeling structured information in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="316" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengio</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Courville ; Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fetaya</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Active learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4363" to="4372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A cascaded inception of inception network with attention modulated feature fusion for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deng ; Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledge-guided deep fractal neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Papandreou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with enhanced channel-wise and spatial information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Sun et al. 2018</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Sun et al. 2019</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu ; Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Toshev and Szegedy</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei ;</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
