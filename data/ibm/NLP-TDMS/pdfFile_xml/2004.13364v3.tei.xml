<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gradient-Induced Co-Saliency Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>Jun Xu 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
							<email>zzhang@mail.nankai.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Gradient-Induced Co-Saliency Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">Jun Xu 1</date>
						</imprint>
					</monogr>
					<note>1 TKLNDST, CS, Nankai University</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Co-saliency detection</term>
					<term>new dataset</term>
					<term>gradient inducing</term>
					<term>jig- saw training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Co-saliency detection (Co-SOD) aims to segment the common salient foreground in a group of relevant images. In this paper, inspired by human behavior, we propose a gradient-induced co-saliency detection (GICD) method. We first abstract a consensus representation for a group of images in the embedding space; then, by comparing the single image with consensus representation, we utilize the feedback gradient information to induce more attention to the discriminative co-salient features. In addition, due to the lack of Co-SOD training data, we design a jigsaw training strategy, with which Co-SOD networks can be trained on general saliency datasets without extra pixel-level annotations. To evaluate the performance of Co-SOD methods on discovering the cosalient object among multiple foregrounds, we construct a challenging CoCA dataset, where each image contains at least one extraneous foreground along with the co-salient object. Experiments demonstrate that our GICD achieves state-of-the-art performance. Our codes and dataset are available at https://mmcheng.net/gicd/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Co-Saliency Detection (Co-SOD) aims to discover the common and salient objects by exploring the inherent connection of multiple relevant images. It is a challenging computer vision task due to complex variations on the co-salient objects and backgrounds. As a useful task for understanding correlations in multiple images, Co-SOD is widely employed as a pre-processing step for many vision tasks, such as weakly-supervised semantic segmentation <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b46">47]</ref>, image surveillance <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>, and video analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, etc.</p><p>Previous researches study the Co-SOD problem from different aspects <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>. At the early stage, researchers explored the consistency among a group of relevant images using handcrafted features, e.g., SIFT <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>, color and texture <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>, or multiple cues fusion <ref type="bibr" target="#b3">[4]</ref>, etc. These shallow features are not discriminative enough to separate co-salient objects in real-world scenarios. Recently, learning-based methods achieve encouraging Co-SOD performance by exploring the semantic connection within a group of images, via deep learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43]</ref>, self-paced learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b51">52]</ref>, metric learning <ref type="bibr" target="#b15">[16]</ref>, or graph learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b54">55]</ref>, etc. However, these methods suffer from the inherent discrepancy in features, due to varying viewpoints, appearance, and positions of the common objects. How to better utilize the connections of relevant images is worth deeper investigation.</p><p>How do humans segment co-salient objects from a group of images? Generally, humans first browse the group of images, summarize the shared attributes of the co-salient objects with "general knowledge" <ref type="bibr" target="#b32">[33]</ref>, and then segment the common objects in each image with these attributes. This process is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Inspired by human behavior, we design an end-to-end network with corresponding two stages. To obtain the shared attributes of the common objects as humans do, we calculate the consensus representation of multiple relevant images in a high-dimensional space with a learned embedding network. Once the consensus representation is obtained, for each image, we propose a Gradient Inducing Module (GIM) to imitate the human behavior of comparing a specific scene with the consensus description to feedback matching information.</p><p>In GIM, the similarity between the single and consensus representations can be measured first. As high-level convolutional kernels with different semantic awareness <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b55">56]</ref>, we can find out the kernels that are more related to the consensus representation and enhance them to detect co-salient objects. To this end, by partially back-propagating, we calculate the gradients of the similarity with respect to the top convolution layer as the feedback information. High gradient values mean corresponding kernels have a positive impact on the similarity results; thus, by assigning more weight to these kernels, the model will be induced to focus on the co-salient related features. Moreover, to better discriminate the co-salient object in each level of the top-down decoder, we propose an Attention Retaining Module (ARM) to connect the corresponding encoderdecoder pairs of our model. We call this two-stage framework with GIM and ARM as Gradient-Induced Co-saliency Detection (GICD) network. Experiments on benchmark datasets demonstrate the advantages of our GICD over previous Co-SOD methods.</p><p>Without sufficient labels, existing Co-SOD networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref> are trained with semantic segmentation datasets, e.g., Microsoft COCO <ref type="bibr" target="#b26">[27]</ref>. However, the annotated objects in segmentation datasets are not necessarily salient. In this Pipeline of our Gradient-Induced Co-saliency Detection (GICD) method. GIM denotes the Gradient Inducing Module, while ARM means the Attention Retaining Module. "•", " × ○", " + ○", and " s ○" represent the inner product, element-wise production, element-wise addition, and the sigmoid function, respectively.</p><p>paper, we introduce a novel jigsaw strategy to extend existing salient object detection (SOD) datasets, without extra pixel-level annotating, for training Co-SOD networks.</p><p>In addition, to better evaluate the Co-SOD methods' ability of discovering co-salient object(s) among multiple foregrounds, most images in an evaluation dataset should contain at least one unrelated salient foreground except for the co-salient object(s). As can be seen in <ref type="figure">Fig. 3</ref>, this is ignored by the current Co-SOD datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51]</ref>. To alleviate the problem, we meticulously construct a more challenging dataset, named Common Category Aggregation (CoCA).</p><p>In summary, our major contributions are as follows:</p><p>-We propose a gradient-induced co-saliency detection (GICD) network for Co-SOD. Specifically, we propose a gradient inducing module (GIM) to pay more attention to the discriminative co-salient features, and an attention retaining module (ARM) to keep the attention during the Different from traditional salient object detection (SOD) task <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>, Co-SOD aims to automatically segment the common salient objects in a group of relevant images. Early Co-SOD methods assume that the co-salient objects in multiple images share low-level consistency <ref type="bibr" target="#b51">[52]</ref>. For instance, Li et al. <ref type="bibr" target="#b23">[24]</ref> introduced a co-multi-layer graph by exploring color and texture properties. Fu et al. <ref type="bibr" target="#b12">[13]</ref> explored the contrast, spatial, and corresponding cues to enforce global association constraint by clustering. Cao et al. <ref type="bibr" target="#b3">[4]</ref> integrated multiple saliency cues by a self-adaptive weighting manner. Tsai et al. <ref type="bibr" target="#b38">[39]</ref> extracted co-salient objects by solving an energy minimization problem over a graph.</p><p>Recently, many deep learning-based methods have been proposed to explore high-level features for the Co-SOD task <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref>. These methods can be divided into two categories. One is a natural deep extension from traditional low-level consistency. It explores the high-level similarity to enhance the similar candidate regions among multiple images. For example, Zhang et al. <ref type="bibr" target="#b50">[51]</ref> jointly investigated inter-group separability and intra-group consistency depending on highlevel CNN features. Hsu et al. <ref type="bibr" target="#b16">[17]</ref> proposed an unsupervised method by maximizing the similarity among multiple foregrounds and minimizing the similarity between foregrounds and backgrounds with graphical optimization. Jiang et al. <ref type="bibr" target="#b19">[20]</ref> explored the superpixel-level similarity by intra-and inter-graph learning using the graph convolution network. Zhang et al. <ref type="bibr" target="#b52">[53]</ref> proposed a mask-guided network to obtain coarse Co-SOD results and then refined the results by multilabel smoothing. The second category of deep methods is based on joint feature extracting. They often extract the common feature for a group of images, and then fuse it with each single image feature. For instance, Wei et al. <ref type="bibr" target="#b42">[43]</ref> learn a shared feature for every five images with a group learning branch, then concatenate the shared feature with every single feature to get the final prediction. Li et al. <ref type="bibr" target="#b22">[23]</ref> extend this idea with a sequence model to process variable length input. Wang et al. <ref type="bibr" target="#b39">[40]</ref> and Zha et al. <ref type="bibr" target="#b47">[48]</ref> learn a category vector for an image group to concatenate with each spatial position of a single image feature on multiple levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Co-SOD Datasets</head><p>Current Co-SOD datasets include mainly MSRC <ref type="bibr" target="#b44">[45]</ref>, iCoseg <ref type="bibr" target="#b1">[2]</ref>, CoSal2015 <ref type="bibr" target="#b50">[51]</ref>, and CoSOD3k <ref type="bibr" target="#b10">[11]</ref>, etc. In <ref type="figure">Fig. 3</ref>, we show some examples of these datasets and our CoCA dataset. MSRC <ref type="bibr" target="#b44">[45]</ref> is mainly for recognizing objects from images. In <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b50">51]</ref>, they select 233 images of seven groups from MSRC-v1 for evaluating detection accuracy. iCoseg <ref type="bibr" target="#b1">[2]</ref> contains 643 images of 38 groups in invariant scenes. In the above datasets, the co-salient objects are mostly the same in similar scenes, and consistent in appearance. CoSal2015 <ref type="bibr" target="#b50">[51]</ref> and CoSOD3k <ref type="bibr" target="#b10">[11]</ref> are two large-scale datasets containing 2015 and 3316 images, respectively. In the two datasets, some target objects belong to the same category differ greatly in iCoseg <ref type="bibr" target="#b1">[2]</ref> MSRC <ref type="bibr" target="#b44">[45]</ref> CoSOD3k <ref type="bibr" target="#b10">[11]</ref> CoSal2015 <ref type="bibr" target="#b50">[51]</ref> Common Category Aggregation (CoCA) dataset <ref type="figure">Fig. 3</ref>. Examples of current popular datasets and our proposed CoCA dataset. In CoCA, except for the co-salient object(s), each image contains at least one extraneous salient object, which enables the dataset to better evaluate the models' ability of discovering co-salient object(s) among multiple foregrounds. appearance, which makes them more challenging datasets. However, these above datasets are not well-designed for evaluating the Co-SOD algorithms because they only have a single salient object in most images. Taking the athlete of the iCoseg in <ref type="figure">Fig. 3</ref> as an example, although the athlete is co-salient in different images, these data can be easily processed by a SOD method because there is no other extraneous salient foreground interference. Although this awkwardness has been avoided in some groups in CoSal2015 and CoSOD3k, it is not guaranteed in most cases. As discovering the co-salient object(s) among multiple foregrounds is the primary pursuit of a Co-SOD method in real-world applications <ref type="bibr" target="#b49">[50]</ref>, to evaluate this ability better, we construct a challenging CoCA dataset, where each image contains at least one extraneous salient object. network. Our backbone is the widely used Feature Pyramid Network (FPN) <ref type="bibr" target="#b25">[26]</ref>. For the Co-SOD task, we incorporate it with two proposed modules: the gradient inducing module (GIM), and the attention retaining module (ARM). GICD detects co-salient objects in two stages. It first receives a group of images as input for exploring a consensus representation in a high-dimensional space with a learned embedding network. The representation describes the common patterns of the co-salient objects within the group. Then, it turns back to segment the co-salient object(s) for each sample. In this stage, for inducing the attention of the model on co-salient regions, we utilize GIM to enhance the features closely related to co-salient object by comparing single and consensus representation in the embedding space. In order to retain the attention during the top-down decoding, we use ARM to connect each encoder-decoder pairs. We train the GICD network with jigsaw training strategy, where the Co-SOD models can be trained on SOD dataset without extra pixel-level annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Consensus Representation</head><p>Given a group of images I = {I n } N n=1 , to locate the co-salient object(s) in each image, we should first know what patterns the co-salient objects have based on prior knowledge. To this end, we propose to learn a consensus representation with a pre-trained embedding network, for the co-salient objects of the image group I. Deep classifiers can be naturally utilized for representation learning <ref type="bibr" target="#b33">[34]</ref>, where the prior knowledge of semantic attribute can be transformed from the parameters pre-trained on ImageNet <ref type="bibr" target="#b6">[7]</ref>. In this case, we employ a pre-trained classified network F(·), such as VGG-16, as our embedding network by removing the softmax layer. It first extracts the representation e n = F(I n ) ∈ R d of each image I n , where d is the dimension of the last full connection layer. The consensus representation e † can be calculated by e † = Softmax N n=1 e n , to describe the common attributes of this image group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gradient Inducing Module</head><p>After obtaining the consensus representation e † of the group I, for each image, we focus on how to find the discriminative features that match the consensus description. As demonstrated in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b55">56]</ref>, high-level convolutional layers naturally possess semantic-specific spatial information. We denote the five convolutional feature blocks of F(·) as {F 1 , F 2 , . . . , F 5 }. In <ref type="figure" target="#fig_3">Fig. 4</ref>, we show the feature maps of the last convolutional layer F 5 . The input image (1st column) contains a pocket watch and blue gloves, and the convolutional kernels focus on different regions (2nd to 4th columns). If assigning more importance to these kernels which closely concern about the co-salient objects, the model will tend to segment the co-salient objects (pocket watch) by decoding the induced features. As indicated in <ref type="bibr" target="#b35">[36]</ref>, the discriminability of features in neural networks can be measured by the gradient obtained by optimizing objectives. Therefore, we propose a gradient inducing module (GIM) for enhancing the discriminative feature by exploring the feedback gradient information. As the encoder of our FPN backbone shares the fixed parameters with the consensus embedding network, it can also embed each image into the same space as consensus representation e † . For the extracted representation e n of the n-th image, the similarity c n between e n and its consensus representation e † can be defined by inner product, i.e., c n = e n e † . Then we compute the positive gradient G n flowing back into the last convolutional layer F 5 ∈ R w×h×c to select discriminative features in F 5 n , specifically,</p><formula xml:id="formula_0">G n = ReLU ∂c n ∂F 5 n ∈ R w×h×c .<label>(1)</label></formula><p>In this partial backpropagation, the positive gradient G n reflects the sensitivity of the corresponding position to the final similarity score; that is, increasing activation value with a larger gradient will make the specific representation e n more consistent with the consensus one e † . Therefore, the importance of a convolution kernel for a particular object can be measured by the mean of its feature gradients. Specifically, the channel-wise importance values can be calculated by global average pooling (GAP), namely w n = GAP(G n ) = 1 wh i j G n , where i = 1, ..., w and j = 1, ..., h. Once obtaining the weight, we can induce the highlevel feature F 5 n by assigning the importance value to each kernelF 5 n = F 5 n ⊗ w n , where ⊗ denotes the element-wise production. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, we visualize the mean heat-maps of F 5 n andF 5 n . without our GIM module, the kernels will averagely focus on both objects. One can see that the kernels more relevant to the co-salient category have higher gradient weights (marked with green numbers), and the attention of the network has shifted to the co-salient object(s) after gradient inducing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention Retaining Module</head><p>In GIM, the high-level features have been induced by the gradient. However, topdown decoder is built upon the bottom-up backbone, and the induced high-level features will be gradually diluted when transmitted to lower layers. To this end, we propose an attention retaining module (ARM) to connect the corresponding encoder-decoder pairs of our GICD network. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, for each ARM, the feature of encoder used for skip-connection is guided by the higher-level prediction. Through top-down iterative reminding, the network will focus the detail recovery of the co-salient regions without being interfered by other irrelevant objects. We take the channel-wise mean ofF 5 n as the first low-resolution guide map S 5 n , and reduceF 5 n to feature P 5 n containing 64 channels. The decoding process with ARM is as follows: where (·) ↑ is the up-sampling operation. R i (·) consists of two convolutional layers, and reduces the enhanced featuresF i n to 64 channels. E i (·) is the corresponding two convolutional layers, with 64 kernels, in decoder. D i (·) is applied for deep supervision, and outputs a prediction by two convolutional layers followed by a sigmoid layer. The last S 1 n is the final output. To validate the effectiveness of our ARM, in <ref type="figure" target="#fig_4">Fig. 5</ref>, we show the intermediate features in different levels of the decoder with ARM (1st row) and without ARM (2nd row). We observe that, through GIM, both locate the co-salient object (i.e., Teddy bear) successfully, while our GICD w/o ARM is gradually interfered by other salient objects during upsampling and produces inaccurate detection results. These results show that our ARM can effectively hold the attention on the co-salient objects in relevant images.</p><formula xml:id="formula_1">    F i n = S i+1 n ↑ F i n P i n = E i P i+1 n ↑ +R i F i n , S i n = D i P i n , i ∈ {4, 3, 2, 1},<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Jigsaw Training Strategy</head><p>Strategy. One important problem in Co-SOD task is that current SOD datasets, e.g., DUTS <ref type="bibr" target="#b40">[41]</ref> and MSRA-B <ref type="bibr" target="#b28">[29]</ref>, are not suitable for the training of Co-SOD networks. The reasons are two-fold: 1) they do not have class information, so it is impossible to train models in groups; 2) most samples in them only contain one salient foreground object. It is difficult to enable the network to distinguish the co-salient object(s) among multiple foreground objects. Recent Co-SOD methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref> are trained on semantic segmentation datasets <ref type="bibr" target="#b26">[27]</ref>. This suffers from two problems: 1) the label of such a semantic segmentation dataset is relatively rough, so the ability of recovery details of the trained network is not ideal, which cannot meet the accuracy requirements of downstream tasks; 2) the objects in such datasets are not necessarily salient. To alleviate these problems, we design a jigsaw strategy to transform SOD datasets into suitable training data for Co-SOD models: Step 1: we employ a classifier <ref type="bibr" target="#b30">[31]</ref> to classify every SOD dataset into multiple categories since it has no category information.</p><p>Step 2: we splice the samples of one category with the samples of other ones to form a new jigsaw, as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. This step is to ensure that an input image contains not only a co-salient foreground but also extraneous foreground objects. Through the above steps, existing SOD datasets can be seamlessly utilized to train Co-SOD networks without additional pixel-level annotations.</p><p>Loss function. Considering the most important goal for co-saliency detection is to find the position of the common foreground objects correctly, we employ the soft intersection over union (IoU) loss <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35]</ref> for GICD, specifically,</p><formula xml:id="formula_2">L (S, G) = 1 − c S (c) G (c) c [S (c) + G (c) − S (c) G (c)] ,<label>(3)</label></formula><p>where S is the prediction, and G denotes ground-truth. c represents each pixel position in the image. The loss function of our model can be expressed as</p><formula xml:id="formula_3">L total = N n=1 4 i=1 L S i n , G n .<label>(4)</label></formula><p>4 Proposed CoCA Dataset Construction guidelines. We construct our CoCA dataset under four guidelines. G1: each image should contain at least one extraneous foreground, excluding the co-salient object(s). G2: in each image group, the co-salient objects are better to be different. G3: the dataset needs to be misaligned with the categories of the common training set, to explore the ability of the model on handling unseen categories. The guideline G1 reflects whether or not the model can detect the co-salient objects, rather than only segmenting the foreground and background. G2 can evaluate whether the model is robust to the intragroup differences. G3 ensures that the model can be evaluated for its ability to detect co-salient objects from unknown categories robustly.</p><p>Construction procedures. With the above guidelines, we collect images from pixabay 3 . We divide them into 80 categories, covering everyday indoor and outdoor scenes. It is worth noting that these categories are outright staggered with Microsoft COCO <ref type="bibr" target="#b26">[27]</ref>, which is often used for the training of co-saliency models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref>. Most importantly, with manual screening, the images in our dataset include at least one extraneous salient object, excluding the co-salient object(s). We provide four levels of annotations: class level, bounding box level, object level, and instance level. The high-quality object-level annotations are applicable to the co-saliency detection task in this paper. Different levels of annotations of our dataset corresponds to different tasks, such as co-localization <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38]</ref>, few-shot object segmentation <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b53">54]</ref>, and instance co-segmentation <ref type="bibr" target="#b36">[37]</ref>.</p><p>Dataset statistics. Our CoCA dataset consists of 80 categories with 1295 images. As shown in <ref type="figure">Fig. 3</ref>, these images are challenging in occlusion, clutter background, extraneous object interference etc. The number of images in each category is different, varying from 8 to 40. This diversity is helpful in evaluating the ability of the model for different image set sizes. The number of co-salient instances in an image is also diverse. 336 images have more than two co-salient instances. The diversity of the number of instances can help to evaluate the robustness of the model to multi-object scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>We train our GICD network on the training set of DUTS <ref type="bibr" target="#b40">[41]</ref> with our jigsaw training strategy. These samples are classified into 291 groups, which contains 8250 images with removing the noisy samples. Each sample will be combined with others to form three jigsaws as supplementary samples, as shown in <ref type="figure" target="#fig_5">Fig.  6</ref>; thus, the candidate training data is quadrupled. In each training epoch, we randomly select at most 20 samples from each group. The Adam optimizer <ref type="bibr" target="#b21">[22]</ref> is used with an initial learning rate of 0.0001, β 1 = 0.9, and β 2 = 0.99. The learning rate is divided by 10 at the 50-th epoch. We train our GICD for 100 epochs in total. To accommodate the input images with our FPN backbone (VGG network), we resize them to 224 × 224 during the training and test stage, and the output saliency maps are resized back to the original size for evaluating. Our GICD is implemented in PyTorch <ref type="bibr" target="#b31">[32]</ref>, and runs at ∼ 55 FPS on an NVIDIA GeForce RTX 2080Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Datasets and Metrics</head><p>Datasets. We employ two challenging datasets to evaluate the performance of various methods. The first dataset is CoSal2015 <ref type="bibr" target="#b50">[51]</ref>. In some image groups, e.g. baseball, it is challenging in the interference of extraneous salient objects. The other is our CoCA, where most images possess more than one irrelevant salient objects besides the co-salient target.</p><p>Metrics. We employ five widely used metrics as suggested by <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53]</ref>: mean F-measure (F avg ) <ref type="bibr" target="#b0">[1]</ref>, maximum F-measure (F max ) <ref type="bibr" target="#b2">[3]</ref>, Precision-Recall (PR) curve, S-measure (S α ) <ref type="bibr" target="#b8">[9]</ref>, and mean E-measure (E ξ ) <ref type="bibr" target="#b9">[10]</ref>. and CoCA datasets. "↑" means that the higher the numerical value, the better the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with State-of-the-Arts</head><p>Comparison methods. We compare our GICD with seven state-of-the-art methods, including four Co-SOD method: RCAN <ref type="bibr" target="#b22">[23]</ref>, CSMG <ref type="bibr" target="#b52">[53]</ref>, GW <ref type="bibr" target="#b41">[42]</ref>, and CBCD <ref type="bibr" target="#b12">[13]</ref>, as well as three SOD mehtods: BASNet (ResNet-34) <ref type="bibr" target="#b34">[35]</ref>, PoolNet (ResNet-50) <ref type="bibr" target="#b27">[28]</ref>, and SCRN (ResNet-50) <ref type="bibr" target="#b45">[46]</ref>.</p><p>Quantitative evaluation. In the Tab. 1, we illustrate the quantitative results of our GICD and other state-of-the-art methods on the CoSal2015 and our CoCA datasets. As can be seen, our GICD achieves better performance. The results show some interesting phenomena. On the CoSal2015, the SOD methods outperform most Co-SOD methods except GICD. The reason is that a large part of the images in CoSal2015 have only one salient object, which can be solved by SOD algorithms. The advantages of Co-SOD algorithms cannot be fully reflected on this data, and these detail-oriented SOD methods easily surpass their performance. However, in our newly proposed CoCA dataset, this phenomenon is no longer obvious, because the salient objects in an image contain many objects that are not co-salient. This is why our CoCA dataset is more suitable for evaluating Co-SOD algorithms. Nevertheless, our GICD still surpasses the SOD methods on CoSal2015. It brings 11.4% improvement in terms of mean F-meansure compared with the best Co-SOD method, 5.7% improvement compared with the SOD method. In our CoCA dataset, GICD brings 3.1% improvement in terms of S-measure compared with the best Co-SOD method, 4.6% improvement compared with the SOD method. Seen from <ref type="figure">Fig. 7</ref>, our method also outperforms other methods on the PR curve and F-measure curve. The trend of the curves demonstrates our method is less affected by the threshold, because it predicts the result with high confidence. This can avoid the problem of how to select the appropriate threshold in the subsequent practical applications.</p><p>CoSal2015 <ref type="bibr" target="#b50">[51]</ref> CoCA (Ours) CoSal2015 <ref type="bibr" target="#b50">[51]</ref> CoCA (Ours) <ref type="figure">Fig. 7</ref>. Precision-Recall (PR) and F-measure curves of our GICD and seven state-of-the-art methods on the CoSal2015 and CoCA datasets. The node on each PR curve denotes the precision and recall value used for calculating maximum F-measure.</p><p>Qualitative results. In <ref type="figure" target="#fig_6">Fig. 8</ref>, we show some saliency maps produced by GICD and other compared methods for intuitive comparison. The samples we illustrate are challenging because the salient objects in each input include not only co-salient object(s) but also interference from other extraneous foreground(s). This is also reflected in the prediction results of SOD algorithms, which over segmented many unrelated regions. From the overall results, our GICD has high confidence in the prediction maps, even at the edge, while most of the other methods suffer from uncertain regions. Back to specific examples, baseball is the most challenging subset of the CoSal2015 <ref type="bibr" target="#b50">[51]</ref>, because it varies greatly in size across images and is interfered by other salient objects. Results show that our method successfully handles the tiny size and the occlusions. In CoCA, boots class faces the interference of background color, and strawberry class has multiple segmentation targets. Nevertheless, GICD locates the target object accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>In order to explore the contribution and mechanism of gradient inducing module (GIM), attention retaining module (ARM), and jigsaw training (JT) to our GICD network, we evaluated all possible combinations of the three candidates. Note that, these three candidates are interdependent and are not recommended to be used alone. As shown in Tab. 2, "A" is the baseline without JT, GIM, and ARM. It is actually a SOD model because it does not take into account any relationship between images.</p><p>Effectiveness of GIM. GIM is the core module of our GICD. With GIM, the variants of "C", "E", "G", and our GICD can be regarded as a Co-SOD network; while, without GIM, the variants of "A", "B", "D", and "F" have limited ability on discovering co-salient objects. By directly applying GIM, the variant "C" shifts the attention to co-salient objects in the high-level features, and achieves a certain performance improvement compared to the baseline. However, in this case, the training set is ill-posed for co-saliency detection task without JT, and the attention will be disturbed during the decoding without the help of ARM. These factors limit its performance. By introducing JT or ARM (variants "E", "G" and our GICD), the effect of GIM is further enhanced.</p><p>CoSal2015 <ref type="bibr" target="#b50">[51]</ref> CoCA (Ours) CoSal2015 <ref type="bibr" target="#b50">[51]</ref> CoCA (Ours) <ref type="figure">Fig. 7</ref>. Precision-Recall (PR) and F-measure curves of our GICD and seven state-of-the-art methods on the CoSal2015 and CoCA datasets. The node on each PR curve denotes the precision and recall value used for calculating maximum F-measure.</p><p>Qualitative results. In <ref type="figure" target="#fig_6">Fig. 8</ref>, we show some saliency maps produced by GICD and other compared methods for intuitive comparison. The samples we illustrate are challenging because the salient objects in each input include not only co-salient object(s) but also interference from other extraneous foreground(s). This is also reflected in the prediction results of SOD algorithms, which over segmented many unrelated regions. From the overall results, our GICD has high confidence in the prediction maps, even at the edge, while most of the other methods suffer from uncertain regions. Back to specific examples, baseball is the most challenging subset of the CoSal2015 <ref type="bibr" target="#b50">[51]</ref>, because it varies greatly in size across images and is interfered by other salient objects. Results show that our method successfully handles the tiny size and the occlusions. In CoCA, boots class faces the interference of background color, and strawberry class has multiple segmentation targets. Nevertheless, GICD locates the target object accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>In order to explore the contribution and mechanism of gradient inducing module (GIM), attention retaining module (ARM), and jigsaw training (JT) to our GICD network, we evaluated all possible combinations of the three candidates. Note that, these three candidates are interdependent and are not recommended to be used alone. As shown in Tab. 2, "A" is the baseline without JT, GIM, and ARM. It is actually a SOD model because it does not take into account any relationship between images.</p><p>Effectiveness of GIM. GIM is the core module of our GICD. With GIM, the variants of "C", "E", "G", and our GICD can be regarded as a Co-SOD network; while, without GIM, the variants of "A", "B", "D", and "F" have limited ability on discovering co-salient objects. By directly applying GIM, the variant "C" shifts the attention to co-salient objects in the high-level features, and achieves a certain performance improvement compared to the baseline. However, in this case, the training set is ill-posed for co-saliency detection task without JT, and the attention will be disturbed during the decoding without the help of ARM. These factors limit its performance. By introducing JT or ARM (variants "E", "G" and our GICD), the effect of GIM is further enhanced. Effectiveness of ARM. ARM plays a role in retaining high-level prediction information during the top-down decoding. As shown in variant "D", using ARM alone does not improve Co-SOD performance. The reason is that, without inducing by GIM, the prediction in high-level is actually the salient objects rather than co-salient objects. When cooperating with GIM in variant "G", although trained on ill-posed data, it still compulsorily keeps the inducing information of GIM to an extend; thus, "G" achieves significantly better performance than the variant "C". "E" is a variant with our GIM and ARM modules. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, without ARM, it is easy to be interfered by irrelevant foreground when recovering object details. Therefore, its performance is inferior to our GICD.</p><p>Effectiveness of JT. The jigsaw training (JT) helps turn SOD datasets into Co-SOD ones, and serves as a useful strategy for training Co-SOD networks. In Tab. 2, without GIM, the variant model "B" and "F" are SOD models, not Co-SOD ones. Since no interactive cue between images is considered, a SOD model trained on Co-SOD dataset is unable to discover group-wise co-salient  <ref type="table">Table 2</ref>. Ablation study of the proposed GICD on the CoCA and CoSal2015 datasets. The candidates are jigsaw training (JT), gradient inducing module (GIM), and attention retaining module (ARM). Note that, the variants "A", "B", "D", and "F", without GIM, are actually SOD models rather than Co-SOD models. The experiments reflect the interaction mechanism of our three contributions.</p><p>connections, and the generated JT labels will bring meaningless predictions in this ill-posed scene; therefore, the JT does not work in these cases. When working with GIM in variant "E", JT improves the effect in the challenging CoCA dataset. Similarly, this improvement can also be seen through the comparison between our GICD and variant "G". In summary, our three contributions of the GIM, ARM, and JT candidates are mutually reinforced for better co-saliency detection performance, as validated through comprehensive experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, inspired by the mechanism of how human behaves on the Co-SOD task, we proposed an end-to-end Gradient-Induced Co-saliency Detection (GICD) method. In GICD, the gradient information, which highlights the discrimination of features, is generated from the comparison between single and consensus representations. Induced by the gradient, GICD pays more attention to discriminative convolutional kernels, enabling our model to locate the cosalient regions. Due to the lack of Co-SOD training data, we designed a novel jigsaw training strategy, with which we trained Co-SOD models on a general SOD dataset without extra pixel-level annotations. In addition, we constructed a challenging CoCA dataset for Co-SOD evaluation, to prosper the subsequent research on exploring real-world Co-SOD scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>arXiv:2004.13364v3 [cs.CV] 12 Dec 2020 GIM GICD Human behavior inspired GICD. GIM is the gradient inducing module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. Pipeline of our Gradient-Induced Co-saliency Detection (GICD) method. GIM denotes the Gradient Inducing Module, while ARM means the Attention Retaining Module. "•", " × ○", " + ○", and " s ○" represent the inner product, element-wise production, element-wise addition, and the sigmoid function, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>shows the flowchart of our gradient-induced co-saliency detection (GICD)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of high-level features induced by GIM. In the six small images on the left, the kernels above are not sensitive to the target object, while the kernels below are related to the target object. Their corresponding important values based on gradients are marked with green numbers on the top-left corners. The mean values of F 5 n and inducedF 5 n are shown in the form of orange heat maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Image w/ ARM | &lt;---------Top-Down Features w/ ARM --------| GT w/o ARM | &lt;---------Top-Down Features w/o ARM -------| Visualization of attention retaining with ARM. The first row shows the multiple level intermediate features with (w/) our ARM, and the second row shows salient maps without (w/o) ARM. The prediction w/ ARM (second column, up) is more accurate than that w/o ARM (second column, down), since our ARM pays stronger attention to the co-salient regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Demo of jigsaw training. A sample (cat), together with the samples from other categories, constitute new jigsaws for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Visual comparison of our GICD with 7 state-of-the-arts (4 Co-SOD methods and 3 SOD methods) on the CoSal2015 [51] and our CoCA datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>JT GIM ARM Favg ↑ Fmax ↑ Sα ↑ E ξ ↑ Favg ↑ Fmax ↑ Sα ↑ E ξ ↑</figDesc><table><row><cell>Variant</cell><cell>Candidate</cell><cell cols="2">CoCA</cell><cell></cell><cell cols="2">CoSal2015 [51]</cell></row><row><cell>A</cell><cell>0.420</cell><cell>0.430</cell><cell>0.601 0.627</cell><cell>0.788</cell><cell>0.800</cell><cell>0.818 0.852</cell></row><row><cell>B</cell><cell>0.424</cell><cell>0.430</cell><cell>0.602 0.655</cell><cell>0.750</cell><cell>0.759</cell><cell>0.782 0.821</cell></row><row><cell>C</cell><cell>0.446</cell><cell>0.462</cell><cell>0.618 0.643</cell><cell>0.809</cell><cell>0.824</cell><cell>0.833 0.868</cell></row><row><cell>D</cell><cell>0.429</cell><cell>0.437</cell><cell>0.607 0.628</cell><cell>0.800</cell><cell>0.809</cell><cell>0.829 0.860</cell></row><row><cell>E</cell><cell>0.470</cell><cell>0.478</cell><cell>0.631 0.689</cell><cell>0.795</cell><cell>0.803</cell><cell>0.808 0.850</cell></row><row><cell>F</cell><cell>0.436</cell><cell>0.442</cell><cell>0.612 0.654</cell><cell>0.762</cell><cell>0.770</cell><cell>0.795 0.832</cell></row><row><cell>G</cell><cell>0.471</cell><cell>0.480</cell><cell>0.636 0.667</cell><cell>0.826</cell><cell>0.835</cell><cell>0.845 0.879</cell></row><row><cell>GICD</cell><cell>0.504</cell><cell>0.513</cell><cell>0.658 0.701</cell><cell>0.835</cell><cell>0.844</cell><cell>0.844 0.883</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://pixabay.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Ming-Ming Cheng is the corresponding author. Zhao Zhang and Wenda Jin are the joint first authors. This research was supported by Major Project for New Generation of AI under Grant No. 2018AAA0100400, NSFC (61922046), Tianjin Natural Science Foundation (18ZXZNGX00110), and the Fundamental Research Funds for the Central Universities, Nankai University (63201169).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">iCoseg: Interactive cosegmentation with intelligent scribble guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-adaptively weighted co-saliency detection via rank constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4175" to="4186" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From co-saliency to co-segmentation: An efficient and fully unsupervised energy minimization model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2129" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Preattentive co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1117" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI. pp</title>
		<imprint>
			<biblScope unit="page" from="698" to="704" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Taking a deeper look at the co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">BBS-Net: RGB-D salient object detection with a bifurcated backbone strategy network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cluster-based co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3766" to="3778" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Highly efficient salient object detection with 100k parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Trustful internet of surveillance things based on deeply-represented visual co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">H C</forename><surname>De Albuquerque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A unified metric learning-based framework for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2473" to="2483" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised CNNbased co-saliency detection with graphical optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="485" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">CARS: Co-saliency activated tracklet selection for video co-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Jerripothula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="187" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient video object co-localization with co-saliency activated tracklets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Jerripothula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="744" to="755" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A unified multiple graph learning and convolutional network model for co-saliency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1375" to="1382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient image and video co-localization with frank-wolfe algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="253" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detecting robust co-saliency with recurrent co-attention neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI. pp</title>
		<imprint>
			<biblScope unit="page" from="818" to="825" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A co-saliency model of image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3365" to="3375" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Interactive image segmentation with latent diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-camera saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2057" to="2070" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graded modality-specific specialisation in semantics: A computational account of optic aphasia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Plaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive neuropsychology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="603" to="639" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Grad-CAM: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning deep match kernels for image-set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6240" to="6249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Co-localization in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1464" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image co-saliency detection and co-segmentation via progressive joint optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="71" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust deep co-saliency detection with group semantic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8917" to="8924" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Group-wise deep co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">E F</forename><surname>Bourahla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI. pp</title>
		<imprint>
			<biblScope unit="page" from="3041" to="3047" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep group-wise fully convolutional network for co-saliency detection with graph propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">E F</forename><surname>Bourahla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2314" to="2320" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Object categorization by learned universal visual dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7264" to="7273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Joint learning of saliency detection and weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7223" to="7233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust deep co-saliency detection with group semantic and pyramid attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">CANet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5217" to="5226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A review of co-saliency detection algorithms: Fundamentals, applications, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TIST</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Detection of co-salient objects by looking deep and wide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a self-paced multipleinstance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Co-saliency detection via mask-guided fully convolutional networks with multi-scale label smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3095" to="3104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">SG-One: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A feature-adaptive semi-supervised framework for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="959" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
