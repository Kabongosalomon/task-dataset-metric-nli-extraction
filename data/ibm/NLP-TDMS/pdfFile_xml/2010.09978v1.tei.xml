<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 12-16, 2020. October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Song</surname></persName>
							<email>yifan.song@cripac.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
							<email>zzhang@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><forename type="middle">Caifeng</forename><surname>Caifeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<email>wangliang@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
							<email>shan@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><forename type="middle">2020</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stronger</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Institute of Automation, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">College of Electrical Engineering and Automation, Shandong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Artificial Intelligence Research, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Institute of Automation, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 28th ACM International Conference on Multimedia (MM &apos;20)</title>
						<meeting>the 28th ACM International Conference on Multimedia (MM &apos;20) <address><addrLine>Seattle, WA, USA; Seattle, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 12-16, 2020. October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413802</idno>
					<note>ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00 ACM Reference Format: Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition. In ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action Recognition</term>
					<term>Skeleton</term>
					<term>ResGCN</term>
					<term>Bottleneck</term>
					<term>Part Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One essential problem in skeleton-based action recognition is how to extract discriminative features over all skeleton joints. However, the complexity of the State-Of-The-Art (SOTA) models of this task tends to be exceedingly sophisticated and over-parameterized, where the low efficiency in model training and inference has obstructed the development in the field, especially for large-scale action datasets. In this work, we propose an efficient but strong baseline based on Graph Convolutional Network (GCN), where three main improvements are aggregated, i.e., early fused Multiple Input Branches (MIB), Residual GCN (ResGCN) with bottleneck structure and Part-wise Attention (PartAtt) block. Firstly, an MIB is designed to enrich informative skeleton features and remain compact representations at an early fusion stage. Then, inspired by the success of the ResNet architecture in Convolutional Neural Network (CNN), a ResGCN module is introduced in GCN to alleviate computational costs and reduce learning difficulties in model training while maintain the model accuracy. Finally, a PartAtt block is proposed to discover the most essential body parts over a whole action sequence and obtain more explainable representations for different skeleton action sequences. Extensive experiments on two large-scale datasets, i.e., NTU RGB+D 60 and 120, validate that the proposed baseline slightly outperforms other SOTA models and meanwhile requires much fewer parameters during training and inference procedures, e.g., at most 34 times less than DGNN, which is one of the best SOTA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Activity recognition and understanding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the past decade, human action recognition becomes increasingly crucial and achieves promising progress in various applications, such as video surveillance, human-computer interaction, video retrieval and so on <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29]</ref>. One essential problem in human action recognition is how to extract discriminative and rich features to fully describe the spatial configurations and temporal dynamics in human actions. <ref type="bibr" target="#b0">1</ref> Currently, skeleton-based representations have been very popular for human action recognition, as human skeletons provide a compact data form to depict dynamic changes in human body movements <ref type="bibr" target="#b9">[10]</ref>. Skeleton data is a time series of 3D coordinates of multiple skeleton joints, which can be either estimated from 2D images by pose estimation methods <ref type="bibr" target="#b4">[5]</ref> or directly collected by multimodal sensors such as Kinect <ref type="bibr" target="#b36">[37]</ref>. Moreover, skeleton-based representations are more robust to the variations of illuminations, camera viewpoints and other background changes. These merits inspire researchers to develop various methods to explore informative features from skeleton motion sequences for action recognition.</p><p>The current development of skeleton-based action recognition can be divided mainly into two phrases. In early years, conventional methods adopt Recurrent Neural Network (RNN)-based or CNN-based models to analyze skeleton sequences. For example, Du et al. <ref type="bibr" target="#b5">[6]</ref> employ a hierarchical bidirectional RNN to capture rich dependencies between different body parts. And Li et al. <ref type="bibr" target="#b11">[12]</ref> design a simple yet effective CNN architecture for action classification from trimmed skeleton sequences. In recent years, due to the greatly expressive power for depicting structural data, graph-based models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref> have been proposed for modeling dynamic skeleton sequences. Yan et al. <ref type="bibr" target="#b30">[31]</ref> firstly propose the Spatial Temporal Graph Convolutional Networks (ST-GCN) for skeleton-based action recognition, after that increasing studies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36]</ref> are reported based on GCN models.</p><p>Nevertheless, for learning discriminative and rich features from skeleton sequences, the current SOTA models are often exceedingly sophisticated and over-parameterized, where the network often contains a multi-stream architecture with a large number of model parameters, which leads to a hard training procedure and high computational costs (low inference speed). For example, the 2s-AGCN in <ref type="bibr" target="#b22">[23]</ref> contains about 6.94 million parameters, and takes nearly 4 GPU-days for model training on the NTU RGB+D 60 dataset <ref type="bibr" target="#b20">[21]</ref>. And the DGNN <ref type="bibr" target="#b21">[22]</ref> contains more than 26 million parameters which is very hard for parameter tuning on large-scale datasets. Thus, the high model complexity has seriously limited the development of skeleton-based action recognition, while there are few literatures on this issue. Moreover, the explainability issue of conventional models still lacks of considerations in current studies. Although some studies <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref> have utilized attention models to discover informative skeleton joints and explain the differences between action categories, they commonly exploit the relationships among individual joints and frames, which often suffers from the noisy skeleton joints in sensor inputs or inaccurate estimations.</p><p>To tackle the above problems, we make three main improvements in this paper to build a new efficient baseline for skeletonbased action recognition. Firstly, an early fused Multiple Input Branches (MIB) architecture is proposed to capture rich spatial configurations and temporal dynamics from skeleton data, where the three branches include joint positions (relative and absolute), bone features (lengths and angles) and motion velocities (one or two temporal steps) respectively, which are subsequently fused in the early stage of the whole model for reducing the model complexity. Secondly, inspired by the success of the ResNet <ref type="bibr" target="#b7">[8]</ref> in CNN-based image classification, we introduce a Residual GCN (ResGCN) module, where the residual links make the model optimization earlier than the original unreferenced feature projection and the bottleneck structure can significantly alleviate the amount of parameter tuning costs. Finally, instead of existing joint-wise attentions in previous models, a Part-wise Attention (PartAtt) module is proposed to discover the most essential body parts over a whole action sequence, and thereby enhance the explainability and stability of the learned representations for different action sequences.</p><p>The whole pipeline of the newly proposed baseline is shown in the top of <ref type="figure" target="#fig_0">Fig. 1</ref>, where the three input sequences (Joint, Velocity and Bone) are initially extracted from the original skeleton sequence. Next, each input sequence is sent to an input branch consisting of some ResGCN modules. Then, the three branches will be concatenated and sent to several PA-ResGCN modules, where each PA-ResGCN module contains a sequential execution of a ResGCN module, followed by a PartAtt block. Finally, the GCN features of all joints are concatenated and feed into a fully-connected (FC) layer for action classification. In this paper, two types of baselines are provided, i.e., a baseline with high performance (PA-ResGCN) and a baseline with high efficiency (ResGCN, without PartAtt blocks). Compared with the most popular GCN baseline, i.e., ST-GCN <ref type="bibr" target="#b30">[31]</ref>, the PA-ResGCN achieves over 10% and 20% relative performance increases with the similar model size on the two datasets, NTU RGB+D 60 <ref type="bibr" target="#b20">[21]</ref> and 120 <ref type="bibr" target="#b15">[16]</ref>. Besides, the PA-ResGCN obtains the SOTA performance on the NTU 120 dataset, while it also achieves competitive performance to other SOTA methods on the NTU 60 dataset. Furthermore, when considering the model size and computational cost, the ResGCN with bottleneck structure obtains a slightly lower accuracy than other SOTA models, while it only contains 0.77 million parameters, nearly 34 times less than DGNN <ref type="bibr" target="#b21">[22]</ref>, which is one of the best SOTA methods. The main contributions of the proposed baseline can be summarized as follows:</p><p>• An early fused multi-branch architecture is designed to take inputs from three individual spatio-temporal feature sequences (Joint, Velocity and Bone) obtained from raw skeleton data, which enables the baseline model to extract sufficient structural features. • To further enhance the efficiency of our model, a residual bottleneck structure is introduced in GCN, where the residual links reduce the difficulties in model training and the bottleneck structure reduces the computational costs in parameter tuning and model inference. • A part-wise attention block is proposed to compute attention weights for different human body parts to further improve the discriminative capability of the features, which meanwhile provides an explanation for the classification results through visualizing the class activation maps. • Extensive experiments are conducted on two large-scale skeleton action datasets, i.e., NTU RGB+D 60 and 120, where the PA-ResGCN can achieve the SOTA performance, and the ResGCN with bottleneck structure obtains competitive performance with much fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Skeleton-based Action Recognition. Due to its compactness to the RGB-based representations, action recognition based on skeleton data has received increasing attentions. In an earlier work <ref type="bibr" target="#b12">[13]</ref>, a convolutional co-occurrence feature learning framework is proposed, where a hierarchical methodology is employed to gradually aggregate different levels of contextual information. The study in <ref type="bibr" target="#b33">[34]</ref> designs a view adaptive model to automatically regulate observation viewpoints during the occurrence of an action, so as to obtain the view invariant representations of human actions.</p><p>Inspired by the booming graph-based methods, Yan et al. <ref type="bibr" target="#b30">[31]</ref> firstly introduce GCN into the skeleton-based action recognition task, and propose the ST-GCN to model the spatial configurations and temporal dynamics of skeletons synchronously. Following this work, Song et al. <ref type="bibr" target="#b26">[27]</ref> aims to solve the occlusion problem in this task, and propose a multi-stream GCN to extract rich features from more activated skeleton joints. Shi et al. <ref type="bibr" target="#b22">[23]</ref> utilize a Non-local method into a two-stream GCN model, which significantly improves the model's accuracy. However, these well-performance models are usually based on multi-stream structures, which need to tune a larger amount of parameters with higher computational costs. Therefore, how to reduce the complexity of the GCN model is still a challenging problem for 3D skeleton based action recognition. Efficient Models. Some existing studies have been considering the model complexity problem. The study <ref type="bibr" target="#b31">[32]</ref> constructs a lightweight network with CNN-based blocks, which is not as accurate as GCN models. The work <ref type="bibr" target="#b34">[35]</ref> adopts a complex data preprocessing strategy, whose inputs include positions, velocities, frame indexes and joint types. This data preprocessing module enables the model to recognize actions with a shallow model, thereby achieves a very fast inference speed with 188 sequences/(second*GPU), yet its performance is obviously lower than other SOTA models.</p><p>Attention Models. Attention mechanisms have become an integral part of compelling sequence modeling in various tasks, such as action recognition. Baradel et al. <ref type="bibr" target="#b1">[2]</ref> introduce the attention mechanism into an RGB-based action recognition model, which uses human pose to calculate spatial and temporal attentions. The study in <ref type="bibr" target="#b25">[26]</ref> firstly introduces attention modules into skeleton-based action recognition, where a spatial-temporal attention Long Short-Term Memory (LSTM) is built to allocate different levels of attention to the discriminative joints within each frame. Si et al. <ref type="bibr" target="#b23">[24]</ref> also incorporate attention modules within LSTM units. Both the two models apply attention modules for each frame individually, which may attend to some unstable noisy features. Besides, the traditional attention module is usually implemented by a multi-layer perception, which does not consider the intensive local dependency for temporal attention and the part dependency for spatial attention.</p><p>Part-based Models. Human skeleton is a natural graph with five main body parts, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Thus, part-based methods are often designed by researchers to extract the features of body parts individually. Du et al. <ref type="bibr" target="#b5">[6]</ref> propose a bidirectional RNN to hierarchically concatenate the features of body parts. Thakkar et al. <ref type="bibr" target="#b27">[28]</ref> utilize GCN to model different body parts, then aggregate them together to recognize actions. Recently, Huang et al. <ref type="bibr" target="#b8">[9]</ref> propose a part-based skeleton model, which is capable to synchronously explore discriminative features from joints and body parts. All these part-based models aim to extract features from body parts individually, while our work focuses on discovering the most informative parts with attention mechanisms.  In this section, we will illustrate the proposed ResGCN/PA-ResGCN. Firstly, the GCN operation will be briefly introduced. Next, we will discuss the details of ResGCN, which can be constructed by stacking some basic or bottleneck blocks. Then, the multiple input branches (MIB) with data preprocessing will be presented. Finally, the new PartAtt block is proposed to enhance the model performance and explainability. An example of the baseline with bottleneck structure is displayed in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Convolutional Network</head><p>According to <ref type="bibr" target="#b30">[31]</ref>, the spatial GCN operation for each frame in a skeleton sequence is formulated as</p><formula xml:id="formula_0">= ∑︁ =0 (Λ − 1 2 Λ − 1 2 ⊗ ),<label>(1)</label></formula><p>where is a predefined maximum graph distance, and denote the input and output feature maps, ⊗ means element-wise multiplication, represents the -th order adjacency matrix that marks the pairs of joints with a graph distance , and Λ is used to normalize . and are both learnable parameters, which are utilized to implement the convolution operation and tune the importance of each edge, respectively.</p><p>For temporal feature extraction, an × 1 convolutional layer is designed to aggregate the contextual features embedded in adjacent frames. In this operation, is a predefined hyper-parameter, defining the length of temporal windows. Both spatial and temporal convolutional layers are followed by a BatchNorm layer and a ReLU layer, and totally construct a basic block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ResGCN</head><p>Bottleneck. He et al. <ref type="bibr" target="#b7">[8]</ref> suggest a subtle block structure named bottleneck, which inserts two 1 × 1 convolutional layers before and after the common convolution layer, respectively, in order to reduce the number of feature channels with a reduction rate in convolution calculation.</p><p>In this paper, we replace spatial and temporal basic blocks with the bottleneck structure, and obtains a significantly faster implementation of model training and inference. Suppose that the input and output channels are both 256, and the channel reduction rate is 4, the temporal window size is 9. Then, the basic block contains 256 × 256 × 9 = 589824 parameters, while the bottleneck block only contains 256 × 64 + 64 × 64 × 9 + 64 × 256 = 69632 parameters, nearly 8.5 times less than the basic block. In <ref type="figure" target="#fig_1">Fig. 2</ref>, each module in ResGCN contains a sequential execution of one spatial block and one temporal block respectively.</p><p>Residual Links. Based on the spatial and temporal blocks mentioned above, it is easy to construct a ResGCN module after adding residual links over the blocks. There are three types of residual links, i.e., block, module and dense, displayed in the bottom-right of <ref type="figure" target="#fig_1">Fig. 2</ref>. As we can see, the block residual link connects the features before and after each block, while the module link jumps the whole module. It seems that the dense link possesses both advantages of the other two links, but more links may harm the compactness of the model and need more memory costs. Therefore, it is necessary to select an appropriate type of residual links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multiple Input Branches</head><p>Model Architecture. <ref type="figure" target="#fig_1">Fig. 2</ref> gives an example of the MIB architecture, which can be summarized by a set of hyper-parameters [B1,N2,N3,N3]. The first parameter denotes that we use one Res-GCN module with basic (B) blocks to process the initial input data. And the other three parameters represent the ResGCN modules with bottleneck (N) blocks, while the differences locate at the number of input and output channels. Every ResGCN module in the third and the fourth parts is followed by a PartAtt block. In addition, at the beginning module of the third and the fourth parts, a temporal stride of 2 is used to further reduce the complexity, which is also found useful for avoiding over-fitting in our experiments.</p><p>Furthermore, it should be noticed that current high accuracy models usually apply a multi-stream architecture with various input data. For example, Shi et al. <ref type="bibr" target="#b22">[23]</ref> take the joints data and bones data as input for feeding to the same model separately, and eventually choose the fusion results of two streams as the final decision. This is an effective way to augment the input data and enhance the model performance. However, a multi-stream network often means high computation costs and difficulties of parameter turning on large-scale datasets. Thus, we fuse the three input branches at the early stage of our model, and apply one main stream to extract discriminative features after the concatenation of three branches of features. This architecture not only retains the rich input features, but also significantly suppresses the model complexity, and makes the training procedure easier to converge.</p><p>Data Preprocessing. Data preprocessing is essential for skeletonbased action recognition, according to the previous studies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>. In this work, the input features after various preprocessing methods are mainly divided into three classes: 1) joint positions, 2) motion velocities and 3) bone features.</p><p>Suppose that the original 3D coordinate set of an action sequence is X = { ∈ R × × }, where , , denote the coordinate, frame and joint, respectively. Then the relative position set R = { | = 1, 2, · · · , }, where = [:, :, ] − [:, :, ], [:, :, ] represents the center joint of a skeleton (center spine). These two sets are concatenated into a single sequence, and sent to the first branch as the input of joint positions. Moreover, it is easy to obtain the two sets of motion velocities F = { | = 1, 2, · · · , } and S = { | = 1, 2, · · · , } with the following definitions: </p><formula xml:id="formula_1">+ 2 , + 2 , ),<label>(2)</label></formula><p>where ∈ { , , } denotes the 3D coordinates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Part-wise Attention</head><p>Previous part-based models usually aim to extract feature from body parts individually, while we focus on discovering the importance of different body parts over the whole action sequences. Inspired by Split Attention (SplitAtt) in the ResNeSt model <ref type="bibr" target="#b32">[33]</ref>, the PartAtt block is designed as <ref type="figure" target="#fig_3">Fig. 3</ref>. Firstly, five individual body parts are obtained from the input features by manually selecting corresponding joints (seen as <ref type="figure" target="#fig_0">Fig. 1</ref>). Then, the features of all parts are concatenated and average pooled in temporal dimension, and then passed through a fully connected layer with a BatchNorm layer and a ReLU function. Subsequently, five fully connected layers are adopted to calculate the attention matrices and a Softmax function is utilized to determine the most essential body parts. Finally, the features of five parts are concatenated as an integral skeleton representation with different attention weights. This PartAtt block can be formulated as</p><formula xml:id="formula_2">= ( ) ⊗ ( ( ( ) ) ) (3) = ({ | = 1, 2, · · · , })<label>(4)</label></formula><p>where and denote input and output feature maps, ⊗ means element-wise multiplication, (·) denotes temporal avg-pool and part-pool operations, (·) and (·) represent part-level Softmax and ReLU activation functions. And ∈ R × , ∈ R × are both learnable parameters, where is shared by all parts for dimension reduction and is specific to each part for calculating the final attention weights.</p><p>The main difference between PartAtt and SplitAtt is that the cardinal groups of SplitAtt are obtained by separating feature channels, while the cardinal groups in PartAtt correspond to various body parts from the spatial view. Compared to other attention models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>, there are two obvious differences between our PartAtt and their methods. On one hand, we employ this block to work on body parts, while their attention blocks concentrate on joints. On the other hand, traditional spatial attention blocks work for each frame individually, while our spatial attention block is based on the global contextual feature maps obtained by average pooling over the whole temporal sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>In this section, we evaluate the performance of the proposed PA-ResGCN and ResGCN (without PartAtt blocks) on two large-scale datasets NTU RGB+D 60 <ref type="bibr" target="#b20">[21]</ref> and NTU RGB+D 120 <ref type="bibr" target="#b15">[16]</ref>. Ablation studies are also performed to validate the contributions of each component in our model. For simplicity, all experiments in ablation studies choose ResGCN with [B1,N2,N3,N3] structure as the base model (seen as <ref type="figure" target="#fig_1">Fig. 2</ref>), which can be denoted as ResGCN-N51, where N51 means there are 51 convolutional or FC layers within the model. Similarly, the model with the same architecture and basic blocks can be denoted as ResGCN-B19. Finally, result analyses are reported to prove the effectiveness of the proposed PartAtt block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>NTU RGB+D 60 Dataset. This large-scale indoor captured dataset is provided in <ref type="bibr" target="#b20">[21]</ref>, which contains 56680 human action videos collected by three Kinect v2 cameras. These actions consist of 60 classes, where the last 10 classes are all interactions between two subjects. For simplicity, the input frame number is set to 300, and the sequences with less than 300 frames are padded by 0 at the end. Each frame contains no more than 2 skeletons, and each skeleton is composed of 25 joints. The authors of this dataset recommend two benchmarks: 1) cross-subject (X-sub) contains 40320 training videos and 16560 evaluation videos divided by splitting the 40 subjects into two groups. 2) cross-view (X-view) recognizes the videos collected by cameras 2 and 3 as training samples (37920 videos), while the videos collected by camera 1 are treated as evaluation samples (18960 videos). Note that there are 302 wrong samples selected by <ref type="bibr" target="#b15">[16]</ref> that need to be ignored.</p><p>NTU RGB+D 120 Dataset. This is the currently largest indoor action recognition dataset <ref type="bibr" target="#b15">[16]</ref>, which is an extended version of the NTU RGB+D 60. It totally contains 114480 videos performed by 106 Model Conf.</p><p>Speed Param. X-sub X-view X-sub120 X-set120 HBRNN <ref type="bibr">[</ref>  subjects from 155 viewpoints. These videos consist of 120 classes, extended from the 60 classes of the previous dataset. Similarly, two benchmarks are suggested: 1) cross-subject (X-sub120) is divided subjects into two groups, to construct training and evaluation sets (63026 and 50922 videos respectively). 2) cross-setup (X-set120) contains 54471 videos for training and 59477 videos for evaluation, which are separated based on the distance and height of their collectors. According to <ref type="bibr" target="#b15">[16]</ref>, 532 bad samples of this dataset should be ignored in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>In our experiments, the maximum graph distance and the temporal window size mentioned in Section 3.1 are set to 2 and 9, respectively. The maximum number of training epochs is set to 70. The initial learning rate is set to 0.1 and decays by 10 at the 20-th and 50-th epochs. Moreover, a warmup strategy <ref type="bibr" target="#b7">[8]</ref> is utilized at the first 10 epochs to make the training procedure more stable. The stochastic gradient descent (SGD) with the Nesterov momentum of 0.9 and the weight decay of 0.0001 is employed to tune the parameters. Other structural parameters are defined as <ref type="figure" target="#fig_1">Fig. 2</ref>. In addition, the dropout layer in original ST-GCN model <ref type="bibr" target="#b30">[31]</ref> is removed. All our experiments are performed on two GTX TITAN X GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with SOTA Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">NTU RGB+D 60 Dataset.</head><p>vs. SOTA Models. From Tab. 1, the PA-ResGCN-B19 obtains an excellent performance, 90.9% for X-sub benchmark and 96.0% for Xview benchmark. When removing the PartAtt blocks and replacing basic blocks with bottleneck blocks, the ResGCN-N51 is built, and its recognition accuracies are 89.1% and 93.5% for the two benchmarks, respectively, but with only a quarter amount of model parameters compared to PA-ResGCN-B19. Here, three typical methods should be noticed. 1) The first one is ST-GCN <ref type="bibr" target="#b30">[31]</ref>, which is the currently most popular backbone model for skeleton-based action recognition. Compared with ST-GCN, our ResGCN-N51 outperforms by 7.6% on X-sub benchmark and 5.2% on X-view benchmark. 2) 2s-AGCN <ref type="bibr" target="#b22">[23]</ref> is another popular baseline in skeleton-based action recognition. The proposed baseline ResGCN-N51 outperforms 2s-AGCN in both accuracy and efficiency.</p><p>3) The third one is DGNN <ref type="bibr" target="#b21">[22]</ref>, which is the current SOTA method with GCN technique. The ResGCN-N51 is slightly lower than DGNN in accuracy, while ResGCN-N51 only requires 0.77 million parameters, about 34 times less than that of DGNN. With respect to PA-ResGCN-B19, our model achieves SOTA performance on NTU 60 dataset with only 1/8 parameters of DGNN. This gap of model complexity is caused by the multistream architecture in DGNN, while ResGCN only contains one main stream. These results imply that the proposed ResGCN is an efficient baseline with competitive performance to SOTA methods.</p><p>vs. Efficient Models. In order to verify the efficiency of our model with bottleneck blocks, we compare ResGCN-N51 with other methods in accuracy and inference speed on X-sub benchmark. The inference speed is defined as the number of sequences successfully evaluated by the model in one second with one GPU. The inference speeds are demonstrated in Tab. 1. From this table, it can be found that ResGCN-N51 greatly improves the inference speed compared with the ST-GCN model <ref type="bibr" target="#b30">[31]</ref>. For LSTM-based models such as VAfusion <ref type="bibr" target="#b33">[34]</ref> and SR-TSL <ref type="bibr" target="#b24">[25]</ref>, the inference speeds are very slow, because of the high computational cost of the LSTM technique. SGN <ref type="bibr" target="#b34">[35]</ref> is a lightweight model which contains five GCN or CNN layers and obtains an extremely fast inference speed. However, it only achieves an accuracy of 86.6% on X-sub benchmark, significantly worse than the ResGCN-N51. Therefore, the ResGCN-N51 is a considerable model that balances the performance and efficiency.</p><p>vs. Attention Enhanced Models. STA-LSTM <ref type="bibr" target="#b25">[26]</ref> and AGC-LSTM <ref type="bibr" target="#b23">[24]</ref> are also enhanced by attention blocks. However, there are obvious differences between PA-ResGCN and these two models, e.g., our attention block works for the whole sequence and body part, while their models use the attention block for each frame and each joint. The performance of PA-ResGCN greatly exceeds STA-LSTM over 10% on the two benchmarks, and outperforms AGC-LSTM by 1.7% and 1.0% on X-sub and X-view benchmarks, respectively.</p><p>vs. Part-based Models. There are three part-based models, i.e., HBRNN <ref type="bibr" target="#b5">[6]</ref>, PB-GCN <ref type="bibr" target="#b27">[28]</ref> and PL-GCN <ref type="bibr" target="#b8">[9]</ref>, which often incorporate the part-based operation into the RNN or GCN blocks for a more informative representation, while PA-ResGCN calculates part-wise attentions to discover the key body parts. As shown in Tab. 1, the proposed PA-ResGCN outperforms PL-GCN in terms of the two benchmarks. As to HBRNN and PB-GCN, our approach has superior performance because the model architectures of HBRNN and PB-GCN are too simple to sufficiently explore discriminative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">NTU RGB+D 120</head><p>Dataset. The NTU RGB+D 120 dataset is proposed by Liu et al. <ref type="bibr" target="#b15">[16]</ref> recently. As a newly released dataset, there is a little work performed on the new dataset. For more convincing, four popular models, i.e., ST-GCN <ref type="bibr" target="#b30">[31]</ref>, RA-GCN <ref type="bibr" target="#b26">[27]</ref>, AS-GCN <ref type="bibr" target="#b13">[14]</ref> and 2s-AGCN <ref type="bibr" target="#b22">[23]</ref>, are implemented by ourselves, based on their released codes. The right column of Tab. 1 presents the experimental results, from which we can find a huge gap between the proposed ResGCN/PA-ResGCN and other models. For example, PA-ResGCN-B19 outperforms 2s-AGCN by 4.8% and 4.1% for the two benchmarks. We consider that this phenomenon is caused by the capability of PartAtt blocks to discovering features from the most informative body parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>Bottleneck Block. In Section 3.2, we introduce the bottleneck structure into the GCN model, for reducing the model size and computational cost. There is a hyper-parameter in the bottleneck structure, i.e., the reduction rate , which determines the number of channels in middle layers. The top part of Tab. 2 illustrates the influence of the bottleneck structure, from which our ResGCN with basic blocks achieves an excellent performance on X-sub benchmark. After introducing the bottleneck structure, the performance of our model is slightly decreased by about 1%. Except the very large reduction rate ( = 8), the ResGCN-N51 obtains competitive accuracies but only with a half or even a quarter amount of model   Residual Links. From <ref type="figure" target="#fig_1">Fig. 2</ref>, three types of residual links are demonstrated. The bottom part of Tab. 2 displays the recognition accuracies of different residual links. As shown in Tab. 2, the ResGCN-N51 with block residual link achieves the best performance, while the model with the module residual link obtains the worst accuracy. As to the dense residual link, it is not as accurate as expected, which implies that the uses of the block residual link and the module residual link simultaneously may produce somehow inconsistency in feature learning. Therefore, we use the block residual link to construct the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIB</head><p>Module. The proposed model contains three input branches, which are defined in Section 3.3. Tab. 3 presents the ablation studies of the input data. As shown in the table, the models with only one input branch are significantly worse than the others. In contrast, the model with all input data gets the best accuracy. This implies that each input branch is necessary to the model, and our model takes a huge benefit from the MIB architecture.</p><p>Model Architecture. Previous studies <ref type="bibr" target="#b7">[8]</ref> tell us that a deeper model usually means a better performance, as well as a harder training procedure. In this part, we will discuss which model architecture has high performance/cost ratio. Tab. 4 gives the accuracies of four example models. It is described that the ResGCN-N51 achieves the best performance on X-sub benchmark, while the ResGCN-N75 obtains the best accuracy on the larger NTU 120 dataset.   <ref type="table">Table 5</ref>: Comparison with different attentions on X-sub and X-sub120 benchmarks in accuracy (%) and parameter number (million).</p><p>in <ref type="figure" target="#fig_1">Fig. 2</ref>. This phenomenon that the deeper network does not bring the better performance here is mainly due to the limitation of variations in the NTU 60 dataset. A bigger and more difficult dataset may need a deeper ResGCN model, such as the NTU 120 dataset.</p><p>PartAtt Block. To illustrate the advantages of PartAtt blocks, we design three comparative blocks, i.e.ChannelAtt, FrameAtt and Join-tAtt, according to previous studies <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>. Experimental results in Tab. 5 show that the proposed PartAtt achieves the best accuracies on both benchmarks. Especially on the larger benchmark X-sub120, the gaps between PartAtt and other attention blocks are more obvious. This is mainly because the PartAtt is more robust to the noisy skeleton joints in sensor input or inaccurate pose estimations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion and Failure Cases</head><p>Activation Map. To show how our model works, the activation maps of some action sequences are calculated by class activation map technique <ref type="bibr" target="#b37">[38]</ref>, as presented in <ref type="figure" target="#fig_4">Fig. 4</ref>, in which the activated joints in several sampled frames are displayed. From this figure, we can find that the PA-ResGCN-B19 model successfully concentrates on the most informative body parts, i.e., left arm for the two actions. Besides, compared with the ResGCN-B19 model, the PA-ResGCN-B19 pays obviously higher attention to the left arm, while ResGCN-B19 shows nearly equal attention to the whole upper body. This significant difference implies that the proposed PartAtt block is more explainable than the joint-based methods.</p><p>Failure Cases. Although PA-ResGCN receives promising results on the large-scale datasets, there are still two actions which are difficult to recognize (accuracies less than 70%). They are the actions reading and writing. It is easy to find that, both the two actions are performed by two hands, and are extremely similar with each other. However, there are only two joints are recorded for each hand in the two datasets. Therefore, it is still challenging for our model to capture such subtle movements of two hands. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have proposed an efficient but strong baseline based on the MIB, residual bottleneck blocks and PartAtt blocks. Different from other attention enhanced models, the proposed Par-tAtt block concentrates more on the essential body parts, instead of joints, which makes the model avoid focusing on some superfluous even interferential features. In order to save the training and inference time, we utilize the bottleneck technique into the GCN model, which significantly reduces the number of learnable parameters, at most 34 times less than other models. On the challenging datasets, NTU RGB+D 60 &amp; 120, the proposed PA-ResGCN achieves the SOTA performance, while its inference speed is obviously higher than other models. Thus, the new baseline will have huge potential for some complex extensions. In the future, we will extend the proposed baseline with the object appearance, which is responsible for the recognition of the extremely similar actions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) is the overall pipeline of our approach and (b) is an illustration of five manually designed body parts. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An example of the ResGCN model with bottleneck structure. The structural parameters of this example are [B1,N2,N3,N3], which correspond to the type and the number of modules in different model parts. Concretely, B1 denotes one ResGCN module with basic (B) blocks and N2/N3 denotes two/three ResGCN modules with bottleneck (N) blocks. Each module in the network is composed of a spatial block, a temporal block and a residual link. In addition, this figure illustrates three types of residual links, i.e., Block residual, Module residual and Dense residual, shown at the bottom-right corner. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 ,</head><label>2</label><figDesc>= [:, + 2, :] − [:, , :] and = [:, + 1, :] − [:, , :]. And the input of motion velocities is acquired by concatenating F and S for each joint to obtain a 6-d feature vector at each time. Finally, the input of bone features consists of the bone lengths L = { | = 1, 2, · · · , } and the bone angles A = { | = 1, 2, · · · , }. To obtain these two sets, the displacement of each bone is calculated by = [:, :, ] − [:, :, ], where means the adjacent joint of the -th joint. Next, the angle of each bone is calculated by , = ( , √︃</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The overview of the proposed PartAtt block, where denotes the number of input channels, = 4 is utilized to reduce the computational cost, = 5 represents five individual body parts, ⊗ means the element-wise multiplication and Part-level Softmax means to calculate Softmax at part level. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Activated joints in several contextual frames of ResGCN-B19 and PA-ResGCN-B19 for the sample actions throwing and drinking water. The red joints denote the activated joints, while blue means non-activated joints. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>These results are provided by the authors or calculated according to their released codes.</figDesc><table><row><cell>6]</cell><cell>CVPR15</cell><cell>-</cell><cell>-</cell><cell>59.1</cell><cell>64.0</cell><cell>-</cell><cell>-</cell></row><row><cell>ST-LSTM [17]</cell><cell>ECCV16</cell><cell>-</cell><cell>-</cell><cell>69.2</cell><cell>77.7</cell><cell>55.0</cell><cell>57.9</cell></row><row><cell>TSRJI [3]</cell><cell>SIBGRAPI19</cell><cell>-</cell><cell>-</cell><cell>73.3</cell><cell>80.3</cell><cell>67.9</cell><cell>62.8</cell></row><row><cell>TSA [4]</cell><cell>AVSS19</cell><cell>-</cell><cell>-</cell><cell>76.5</cell><cell>84.7</cell><cell>67.7</cell><cell>66.9</cell></row><row><cell>VA-fusion [34]</cell><cell>TPAMI19</cell><cell>-</cell><cell>24.60</cell><cell>89.4</cell><cell>95.0</cell><cell>-</cell><cell>-</cell></row><row><cell>ST-GCN [31]</cell><cell>AAAI18</cell><cell>42.9 ★</cell><cell>3.10 ★</cell><cell>81.5</cell><cell>88.3</cell><cell>70.7  †</cell><cell>73.2  †</cell></row><row><cell>SR-TSL [25]</cell><cell>ECCV18</cell><cell cols="2">14.0 ★ 19.07 ★</cell><cell>84.8</cell><cell>92.4</cell><cell>-</cell><cell>-</cell></row><row><cell>PB-GCN [28]</cell><cell>BMVC18</cell><cell>-</cell><cell>-</cell><cell>87.5</cell><cell>93.2</cell><cell>-</cell><cell>-</cell></row><row><cell>RA-GCN [27]</cell><cell>ICIP19</cell><cell>18.7  †</cell><cell>6.21 ★</cell><cell>85.9</cell><cell>93.5</cell><cell>74.6  †</cell><cell>75.3  †</cell></row><row><cell>GR-GCN [7]</cell><cell>ACMMM19</cell><cell>-</cell><cell>-</cell><cell>87.5</cell><cell>94.3</cell><cell>-</cell><cell>-</cell></row><row><cell>AS-GCN [14]</cell><cell>CVPR19</cell><cell>-</cell><cell>6.99 ★</cell><cell>86.8</cell><cell>94.2</cell><cell>77.9  †</cell><cell>78.5  †</cell></row><row><cell>2s-AGCN [23]</cell><cell>CVPR19</cell><cell>22.3  †</cell><cell>6.94 ★</cell><cell>88.5</cell><cell>95.1</cell><cell>82.5  †</cell><cell>84.2  †</cell></row><row><cell>AGC-LSTM [24]</cell><cell>CVPR19</cell><cell>-</cell><cell>22.89 ★</cell><cell>89.2</cell><cell>95.0</cell><cell>-</cell><cell>-</cell></row><row><cell>DGNN [22]</cell><cell>CVPR19</cell><cell>-</cell><cell>26.24 ★</cell><cell>89.9</cell><cell>96.1</cell><cell>-</cell><cell>-</cell></row><row><cell>AS-GCN+DH-TCN [18]</cell><cell>arXiv19</cell><cell>-</cell><cell>-</cell><cell>85.3</cell><cell>92.8</cell><cell>78.3</cell><cell>79.8</cell></row><row><cell>SGN [35]</cell><cell>arXiv19</cell><cell>188.0</cell><cell>1.8</cell><cell>86.6</cell><cell>93.4</cell><cell>-</cell><cell>-</cell></row><row><cell>PL-GCN [9]</cell><cell>AAAI20</cell><cell>-</cell><cell>20.70 ★</cell><cell>89.2</cell><cell>95.0</cell><cell>-</cell><cell>-</cell></row><row><cell>NAS-GCN [19]</cell><cell>AAAI20</cell><cell>-</cell><cell>6.57 ★</cell><cell>89.4</cell><cell>95.7</cell><cell>-</cell><cell>-</cell></row><row><cell>ResGCN-N51 (Bottleneck)</cell><cell>ours</cell><cell>67.4</cell><cell>0.77</cell><cell>89.1</cell><cell>93.5</cell><cell>84.0</cell><cell>84.2</cell></row><row><cell>PA-ResGCN-N51</cell><cell>ours</cell><cell>54.8</cell><cell>1.14</cell><cell>90.3</cell><cell>95.6</cell><cell>86.6</cell><cell>87.1</cell></row><row><cell>ResGCN-B19 (Basic)</cell><cell>ours</cell><cell>44.0</cell><cell>3.26</cell><cell>90.0</cell><cell>94.8</cell><cell>85.2</cell><cell>85.7</cell></row><row><cell>PA-ResGCN-B19</cell><cell>ours</cell><cell>38.3</cell><cell>3.64</cell><cell>90.9</cell><cell>96.0</cell><cell>87.3</cell><cell>88.3</cell></row></table><note>★ :† : These results are implemented by ourselves, based on their released codes on the Github website.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparison with the SOTA methods on NTU RGB+D 60 &amp; 120 datasets in accuracy (%), inference speed (se- quences/(second*GPU)) and parameter number (million). The top part consists of several models without the GCN technique, while the middle part contains some graph-based models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: Comparison with different types of blocks and resid-</cell></row><row><cell cols="4">ual links on X-sub benchmark in accuracy (%) and parameter</cell></row><row><cell cols="3">number (million). means the reduction rate.</cell><cell></cell></row><row><cell>Architecture</cell><cell>Input data</cell><cell cols="2">Param. X-sub</cell></row><row><cell>Three-branch</cell><cell>ResGCN-N51</cell><cell>0.77</cell><cell>89.1</cell></row><row><cell></cell><cell>w/o Joint</cell><cell>0.71</cell><cell>88.2</cell></row><row><cell>Two-branch</cell><cell>w/o Velocity</cell><cell>0.71</cell><cell>88.0</cell></row><row><cell></cell><cell>w/o Bone</cell><cell>0.71</cell><cell>86.7</cell></row><row><cell></cell><cell>w/o Joint &amp; Bone</cell><cell>0.67</cell><cell>86.6</cell></row><row><cell cols="2">One-branch w/o Bone &amp; Velocity</cell><cell>0.67</cell><cell>86.1</cell></row><row><cell></cell><cell>w/o Joint &amp; Velocity</cell><cell>0.67</cell><cell>84.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison with different input data on X-sub benchmark in accuracy (%) and parameter number (million).parameters, compared to the ResGCN-B19 model. These results clearly indicate that the bottleneck structure with a proper reduction rate ( = 4) can reduce the model complexity effectively while maintains the model accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison with different model structures on Xsub and X-sub120 benchmarks in accuracy (%) and parameter number (million). ResGCN-Nx means that this model contains x conventional layers within the bottleneck blocks.</figDesc><table><row><cell>Model</cell><cell cols="3">Param. X-sub X-sub120</cell></row><row><cell>ResGCN-N51</cell><cell>0.77</cell><cell>89.1</cell><cell>84.0</cell></row><row><cell>+ ChannelAtt</cell><cell>0.89</cell><cell>89.1</cell><cell>85.4</cell></row><row><cell>+ FrameAtt</cell><cell>0.77</cell><cell>88.6</cell><cell>84.8</cell></row><row><cell>+ JointAtt</cell><cell>0.77</cell><cell>89.1</cell><cell>85.3</cell></row><row><cell>+ PartAtt</cell><cell>1.14</cell><cell>90.3</cell><cell>86.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The codes and pretrained models of the preposed ResGCN are available at here. arXiv:2010.09978v1 [cs.CV] 20 Oct 2020</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human action recognition: Pose-based attention draws focus to hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skeleton Image Representation for 3D Action Recognition Based on Tree Structure and Reference Joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Robson</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SkeleMotion: A New Representation of Skeleton Joint Sequences based on Motion Information for 3D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Sena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franeois</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Dos</forename><surname>Jefersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Robson</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimized skeleton-based action recognition via sparsified graph regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (ACMMM)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Part-Level Graph Convolutional Network for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia &amp; Expo Workshop (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><forename type="middle">Lisboa</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Kot</forename><surname>Chichung</surname></persName>
		</author>
		<title level="m">NTU RGB+ D 120: A Large-Scale Benchmark for 3D Human Activity Understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enjie</forename><surname>Ghorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamila</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ottersten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09745</idno>
		<title level="m">Vertex Feature Encoding and Hierarchical Temporal Modeling in a Spatial-Temporal Graph Convolutional Network for Action Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">NTU RGB+ D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Skeleton-Based Action Recognition with Directed Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Skeletonbased action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">RICHLY ACTIVATED GRAPH CONVOLUTIONAL NETWORK FOR ACTION RECOGNITION WITH INCOM-PLETE SKELETONS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Part-based Graph Convolutional Network for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpit</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A survey of visionbased methods for action representation, segmentation and recognition. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Boyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="224" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
	<note>Joon-Young Lee, and In So Kweon</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Make Skeletonbased Action Recognition Model Smaller</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09658</idno>
	</analytic>
	<monogr>
		<title level="m">Faster and Better</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">ResNeSt: Split-Attention Networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">View adaptive neural networks for high performance skeletonbased human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Jianru Xue, and Nanning Zheng</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01189</idno>
	</analytic>
	<monogr>
		<title level="m">Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph Edge Convolutional Neural Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
