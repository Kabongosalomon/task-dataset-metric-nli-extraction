<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OpenPifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
						</author>
						<title level="a" type="main">OpenPifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-composite fields</term>
					<term>pose estimation</term>
					<term>pose tracking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many image-based perception tasks can be formulated as detecting, associating and tracking semantic keypoints, e.g., human body pose estimation and tracking. In this work, we present a general framework that jointly detects and forms spatio-temporal keypoint associations in a single stage, making this the first real-time pose detection and tracking algorithm. We present a generic neural network architecture that uses Composite Fields to detect and construct a spatio-temporal pose which is a single, connected graph whose nodes are the semantic keypoints (e.g., a person's body joints) in multiple frames. For the temporal associations, we introduce the Temporal Composite Association Field (TCAF) which requires an extended network architecture and training method beyond previous Composite Fields. Our experiments show competitive accuracy while being an order of magnitude faster on multiple publicly available datasets such as COCO, CrowdPose and the PoseTrack 2017 and 2018 datasets. We also show that our method generalizes to any class of semantic keypoints such as car and animal parts to provide a holistic perception framework that is well suited for urban mobility such as self-driving cars and delivery robots.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The computer vision community has made tremendous progress in solving fine-grained perception tasks such as human body joints detection and tracking <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. We can cast these tasks as detecting, associating and tracking semantic keypoints. Examples of semantic keypoints are "left shoulders", "right knees" or the "left brake lights of vehicles". However, the performance of semantic keypoint tracking in live video sequences has been limited in accuracy and high in computational complexity and prevented applications to the transportation domain with real-time requirements like selfdriving cars and last-mile delivery robots. The majority of self-driving car accidents is caused by "robotic" driving where the self-driving car conducts an allowed but unexpected stop and a human driver crashes into the self-driving car <ref type="bibr" target="#b2">[3]</ref>. At their core, self-driving cars lack social intelligence. They are blind to the body language of surrounding pedestrians when every person is only perceived as a bounding box. Current pose detection and tracking methods are neither fast enough nor robust enough to occlusions to be viable for self-driving cars. Tracking human poses in real-time will enable self-driving cars to develop a finer-grained understanding of pedestrian behavior Sven Kreiss, Lorenzo Bertoni and Alexandre Alahi are at the VITA lab at EPFL, 1015 Lausanne, Switzerland, e-mail: sven.kreiss@epfl.ch <ref type="figure">Fig. 1</ref>.</p><p>A real-world scene from the perspective of a self-driving car. Schematically, all moving actors are detected with their poses and tracked so that they can be consistently quantified over time. We place particular emphasis on understanding humans but also show generalizations to animals and cars. Here, a car (tracked as 4) is running a red light while also swerving to the right to avoid a woman (tracked as 2) who is walking her dog (tracked as <ref type="bibr" target="#b2">3)</ref>. and with that a better conditioned reasoning for more natural driving.</p><p>The problem is to estimate and track multiple human, car and animal poses in image sequences, see <ref type="figure">Figure 1</ref>. The major challenges for tracking poses from the car perspective are (i) occlusions due to the viewing angle and (ii) prediction speed to be able to react to real-time changes in the environment. Our method must be fast enough to be viable for selfdriving cars and robust to real-world variations like lighting, weather and occlusions.</p><p>Although tracking has been studied extensively before human pose estimation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, a significant cornerstone that leverages poses are the works of Insafutdinov et al. <ref type="bibr" target="#b7">[8]</ref> and Iqbal et al. <ref type="bibr" target="#b8">[9]</ref> who pioneered multi-person pose tracking for an arbitrary number of people in the wild. Both methods use graph matching to track independent, single-frame poses over time. To improve the matching for tracking, Doering et al. <ref type="bibr" target="#b9">[10]</ref> introduced temporal flow fields that improve the cost function for matching. However, these works treat pose tracking as a multi-stage process: infer single-frame poseswhich is itself a multi-stage process for top-down methodsand connect poses from frame to frame. This prohibits any improvement to single-frame poses that could result from the temporal information available in tracking. Here, we address these challenges by introducing a new method that jointly solves pose detection and tracking with Composite Fields.</p><p>First, we review Composite Fields for single-image multi-person pose estimation <ref type="bibr" target="#b10">[11]</ref>. Second, we introduce a new method for pose tracking. While single-frame pose estimation can be viewed as a pose completion task starting at a seed joint, we treat pose tracking as a pose completion task starting with a pose from a previous frame and completing a spatiotemporal pose, which is a single, connected graph that spans space and time. The spatio-temporal pose consists of at least two single-frame poses and additional connections across the frames. The contributions of this paper are (i) a Temporal Composite Association Field (TCAF) which we use to form a spatiotemporal pose and (ii) a greedy decoder to jointly detect and track poses. To the best of our knowledge, this method is the first single-stage, bottom-up pose detection and tracking method. We outperform all previous methods in accuracy and speed on the CrowdPose dataset <ref type="bibr" target="#b11">[12]</ref> with its particularly crowded images. We perform on par with the state-of-the-art bottom-up method for single-image human pose estimation on the COCO [2] keypoint task in precision and are an order of magnitude faster in speed. Our model performs on par with the state-of-the-art method for human pose tracking on PoseTrack 2017 and 2018 <ref type="bibr" target="#b12">[13]</ref> while simultaneously being an order of magnitude faster during prediction. We also show that our method generalizes to car and animal poses which demonstrates its suitability for a holistic perception framework. Our method is implemented as an open source library, referred to as OpenPifPaf 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pose Estimation</head><p>State-of-the-art methods for pose estimation are based on Convolutional Neural Networks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. All approaches for human pose estimation can be grouped into bottom-up and top-down methods. The former estimates each body joint first and then groups them to form a unique pose. The latter runs a person detector first and estimates body joints within the detected bounding boxes. Bottom-up methods were pioneered, e.g., by Pishchulin et al. with DeepCut <ref type="bibr" target="#b24">[25]</ref>. In their work, the part association is solved with an integer linear program leading to processing times for one image of the order of hours. Newer methods use greedy decoders in combination with additional tools to reduce prediction time as in Part Affinity Fields <ref type="bibr" target="#b15">[16]</ref>, Associative Embedding <ref type="bibr" target="#b16">[17]</ref>, PersonLab <ref type="bibr" target="#b17">[18]</ref> and multi-resolution networks with associate embedding <ref type="bibr" target="#b23">[24]</ref>. PifPaf <ref type="bibr" target="#b10">[11]</ref> introduced composite fields for pose estimation that produces a more precise association between joints than OpenPose's Part Affinity Fields <ref type="bibr" target="#b15">[16]</ref> and PersonLab's midrange fields <ref type="bibr" target="#b17">[18]</ref>. In the next section, we will review composite fields and show that they generalize to tracking tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pose Tracking</head><p>Tracking algorithms can be grouped into top-down versus bottom-up approaches for the pose part and the tracking part. Doering et al. <ref type="bibr" target="#b9">[10]</ref> were the first to introduce a method that is bottom-up in both the spatial and the temporal part. They employ Part Affinity Fields <ref type="bibr" target="#b15">[16]</ref> for the single-frame poses in a Siamese architecture. The temporal flow fields (TFF) feed into an edge cost computation for bipartite graph matching for tracking. The idea is extended in MIPAL <ref type="bibr" target="#b25">[26]</ref> for tracking limbs instead of joints and in STAF <ref type="bibr" target="#b26">[27]</ref>.</p><p>Early work on multi-person pose tracking started with <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Recent work has shown excellent performance on the PoseTrack 2018 dataset including the top-down method openSVAI <ref type="bibr" target="#b27">[28]</ref> which decomposes the problem into three independent stages of human candidate detection, singleimage human pose estimation and pose tracking. Similarly, LightTrack <ref type="bibr" target="#b28">[29]</ref> also builds a strong top-down pipeline with interchangeable and independent modules. Miracle <ref type="bibr" target="#b29">[30]</ref> uses a strong single-image pose estimator with a cascaded pyramid network together with an IOU tracker. HRNet for human pose estimation <ref type="bibr" target="#b19">[20]</ref> leverages a multi-resolution backbone to produce high resolution feature maps that are context aware via HRNet's multi-scale fusion. In MSRA/FlowTrack <ref type="bibr" target="#b18">[19]</ref>, optical flow is used to improve top-down tracking of bounding boxes for tracking of human poses. Pose-Guided Grouping (PGG) <ref type="bibr" target="#b30">[31]</ref> proposes a part association method based on separate spatial and temporal embeddings. KeyTrack <ref type="bibr" target="#b31">[32]</ref> uses pose tokenization and a transformer network to associate poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Beyond Humans</head><p>While many state-of-the-art methods focused on human body pose detection and tracking, the research community has recently studied their performance on other classes such as animals and cars. Pose estimation research for animals and cars has to deal with additional challenges: limited labeled data <ref type="bibr" target="#b32">[33]</ref> and large number of self-occlusions <ref type="bibr" target="#b33">[34]</ref>.</p><p>For animals, datasets are usually small and include limited animal species <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>. To overcome this issue, DeepLabCut <ref type="bibr" target="#b39">[39]</ref> and WS-CDA <ref type="bibr" target="#b32">[33]</ref> have developed transfer learning techniques from humans to animals. Mu et al. <ref type="bibr" target="#b40">[40]</ref> have generated a synthetic dataset from CAD animal models and proposed a technique to bridge the real-synthetic domain gap. Another line of work has extended the human SMPL model <ref type="bibr" target="#b41">[41]</ref> to animals to learn simultaneously pose and shape of endangered animals <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>.</p><p>For cars, self-occlusions between keypoints are inevitable. A few methods improve performances by estimating 2D and 3D keypoints of vehicles together. Occlusion-net <ref type="bibr" target="#b33">[34]</ref> uses a 3D graph network with self-supervision to predict 2D and 3D keypoints of vehicles using the CarFusion dataset <ref type="bibr" target="#b45">[45]</ref>, while GSNet <ref type="bibr" target="#b46">[46]</ref> predicts 6DoF car pose and reconstructs dense 3D shape simultaneously. Without 3D information, the popular OpenPose <ref type="bibr" target="#b47">[47]</ref> shows qualitative results for vehicles and Simple Baseline <ref type="bibr" target="#b48">[48]</ref> extends a top-down pose estimator for cars on a custom dataset based on Pascal3D+ <ref type="bibr" target="#b49">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. COMPOSITE FIELDS</head><p>Our method relies on the Composite Fields formalism to jointly detect and track semantic keypoints. Hereafter, we briefly present them. (a) (b) <ref type="figure">Fig. 3</ref>. Visualizing the components of the CAF that associates left shoulder with left hip. This is one of the 18 CAF. Every location of the feature map is the origin of two vectors which point to the shoulders and hips to associate. The confidence of associations ac is shown at their origin in (3a) and the vector components for ac greater than 0.5 are shown in (3b).</p><p>(a) (b) (c) <ref type="figure">Fig. 4</ref>. Common association fields between two joints. Joints are visualized as gray circles. Part Affinity Fields (a) as used in OpenPose <ref type="bibr" target="#b15">[16]</ref> are unit vectors indicating a direction towards the next joint. Mid-range fields (b) as used in PersonLab <ref type="bibr" target="#b17">[18]</ref> are vectors originating in the vicinity of a source joint and point to the target joint. Our Composite Association Field (c) regresses both source and target points and additionally predicts their joint size which are visualized with blue squares. a) Field Notation: Fields are functions over locations (e.g., feature map cells) and their outputs are primitives like scalars or composites. Composite Fields as introduced in <ref type="bibr" target="#b10">[11]</ref> jointly predict multiple variables of interest, for example, the confidence, precise location and size of a semantic keypoint (e.g., body joint).</p><p>We will enumerate the spatial output coordinates of the neural network with i, j and reserve x, y for real-valued coordinates in the input image. A field over (i, j) is denoted with f ij and can have scalar, vector or composite values. For example, the composite field of scalars s and 2D vector</p><p>components v x , v y is {s, v x , v y } ij . This is equivalent to "overlaying" a confidence map with a vector field if the ground truth is aligned. This equivalence is trivial in this example but becomes more subtle when we discuss association fields below.</p><p>b) Composite Intensity Fields (CIF): The Composite Intensity Fields (CIF) characterize the intensity of semantic keypoints. The composite structure is based on <ref type="bibr" target="#b53">[53]</ref> with the extension of a scale σ to characterize the keypoint size. This is identical to the part intensity field in <ref type="bibr" target="#b10">[11]</ref>. We use the notation p ij J = {c, x, y, b, σ} ij J where J is a particular body joint type, c is the confidence, x and y are regressed coordinates, b is the uncertainty in the location and σ is the size of the joint. <ref type="figure" target="#fig_0">Figure 2</ref> shows the components of a CIF field and a high resolution accumulation of the predicted intensity. The field is coarse with a stride of 16 with respect to the input image but the accumulated intensity is at high resolution. The high resolution confidence map f (v, w) is a convolution of an unnormalized Gaussian kernel N with width σ over the regressed targets from the Composite Intensity Field x and y weighted by its confidence c: <ref type="figure">Fig. 5</ref>. Model architecture. The input is an image batch of size (H, W ) with three color channels, indicated by "x3". During training, the datasets produce image pairs whereas during evaluation they produce single images in a sequence. The neural network based encoder produces composite fields for M joints and N connections. An operation with stride two is indicated by "//2". The shared backbone is a ResNet <ref type="bibr" target="#b50">[50]</ref> or ShuffleNetV2 <ref type="bibr" target="#b51">[51]</ref> without max-pooling. The Feature Cache is only used during evaluation and injects for every image the previous feature map into the batch. We use a single 1 × 1 convolution in each head network. The TCAF head networks have a shared pre-processing step consisting of a feature reduction to 512 with a 1 × 1 convolution followed by ReLU, a concatenation of the two feature maps and another 1 × 1 convolution with ReLU activation. For optional spatial upsampling, we append a sub-pixel convolution layer <ref type="bibr" target="#b52">[52]</ref> to each head network. The decoder converts a set of composite fields into pose estimates. Each semantic keypoint is represented by a confidence score, real-valued (x, y) coordinates and a size estimate. accumulation incorporates information of the confidence c, the precisely regressed spatial location (x, y) and the predicted joint size σ. This map f J is used to seed the pose decoder and to rescore predicted CAF associations. c) Composite Association Fields (CAF): Efficiently forming associations is the core challenge for tracking multiple poses in a video sequence. The most difficult cases are crowded scenes and camera angles where people occlude other people -as is the case in the self-driving car perspective where pedestrians occlude other pedestrians. Top-down methods first estimate bounding boxes and then do single-person pose estimation per bounding box. This assumes non-overlapping bounding boxes which is not given in our scenario. Therefore, we focus on bottom-up methods.</p><p>In <ref type="bibr" target="#b10">[11]</ref>, we introduced Part Association Fields to connect joint locations together into poses. Here, we extend this field with joint-scale components and call it Composite Association Field (CAF) to distinguish it better from Part Affinity Fields introduced in <ref type="bibr" target="#b15">[16]</ref>. A graphical review of association fields is shown in <ref type="figure">Figure 4</ref> and shows that our CAF expresses the most detail about an association.</p><p>CAFs predict a confidence, two vectors to the two parts this association is connecting, two spreads b for the spatial precisions of the regressions (details in Section IV-A) and two joint sizes σ. CAFs are represented with a ij J1↔J2 = {c,</p><formula xml:id="formula_0">x 1 , y 1 , x 2 , y 2 , b 1 , b 2 , σ 1 , σ 2 } ij J1↔J2 where J 1 ↔ J 2</formula><p>is the association between body joints J 1 and J 2 . Predicted associations between left shoulders and left hips are shown for an example image in <ref type="figure">Figure 3</ref>. In our representation of an association, physically meaningful quantities are regressed to continuous variables and do not suffer from the discreteness of the feature map. In addition, it is important to represent associations between two joints that are at the same pixel location. Our representation is stable for these zero-distance associations -something that Part Affinity Fields <ref type="bibr" target="#b15">[16]</ref> cannot do -which becomes particularly important when we introduce our extension for tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHOD</head><p>We aim to present a method that can detect, associate and track semantic keypoints in videos efficiently. We place particular emphasis on urban and crowded scenes that are difficult for autonomous vehicles. Many previous methods struggle when object bounding boxes overlap. In bird-eye views from drones or security cameras, bounding boxes are more separated than in a car driver's perspective. Here, topdown methods struggle. Previous bottom-up methods have been trailing top down methods in accuracy without improving on performance either. Our bottom-up method is efficient, employs a stable field representation and has high accuracy and performance that even surpasses top-down methods. <ref type="figure">Figure 5</ref> presents our model architecture. It is a shared ResNet <ref type="bibr" target="#b50">[50]</ref> or ShuffleNetV2 <ref type="bibr" target="#b51">[51]</ref> base network without max-pooling. The head networks are shallow and not shared between datasets. In our examples, each dataset has a head network for joint intensities (Composite Intensity Field -CIF) and a head network for associations (Composite Association Field -CAF). Beyond CIF and CAF, additional head networks can be added. In Section IV-B, we introduce the new Temporal Composite Association Field (TCAF) which is predicted by an additional head network to facilitate pose tracking.</p><p>We will introduce a tracking method that is a direct extension of single-image pose estimation. Therefore, we first introduce our method for single-image pose estimation with particular emphasis on details that will be relevant for pose tracking.</p><p>A. Single-Image Pose Estimation a) Loss Functions for Composite Fields: Human pose estimation algorithms tend to struggle with the diversity of scales that a human pose can have in an image. While a localization error for the joint of a large person can be minor, that same absolute error might be a major mistake for a small person. Our loss is the logarithm of the probability that all components are "well" predicted, i.e., it is the sum of the logprobabilities for the individual components. Each component follows standard loss prescriptions. We use binary cross entropy (BCE) for classification with a Focal loss modification w <ref type="bibr" target="#b54">[54]</ref>. To regress locations in the image, we use the Laplace loss <ref type="bibr" target="#b55">[55]</ref> which is an L 1 -type loss that is attenuated by a predicted spreadb in the location. To regress additional scale components (keypoint sizes), we use a Laplace loss with a fixed spread b σ = 3. The CIF loss function is:</p><formula xml:id="formula_1">L CIF = mc w(c,ĉ)BCE(c,ĉ) (2) + mv 1 b L 2 (v,v, b min ) + logb (3) + ms 1 b s |1 −ŝ s |<label>(4)</label></formula><p>with its three parts for confidence (2), localization (3) and scale (4) and where:</p><formula xml:id="formula_2">L 2 (v,v, b min ) = (v 1 −v 1 ) 2 + (v 2 −v 2 ) 2 + b 2 min . (5)</formula><p>The sums are over masked feature cells m c , m v and m σ with i, j, J implied. The mask for confidence m c is almost the entire image apart from regions annotated as "crowd regions" <ref type="bibr" target="#b1">[2]</ref>.</p><p>The masks for localization m v and for scale m σ are only active in a 4 × 4 window around the ground truth keypoint. Per feature map cell, there is a ground truth confidence c and its predicted counterpartĉ. The predicted locationv = (v 1 ,v 2 ) is optimized with a Laplace loss with a predicted spread b for heteroscedastic aleatoric uncertainty <ref type="bibr" target="#b55">[55]</ref> with respect to the ground truth location v. A b min = 1px is added to prevent exploding losses when the spread becomes too small. For stability, we clip the BCE loss when it becomes larger than five. The CAF loss has the same structure but with two localization components (3) and two scale components (4). b) Self-Hidden Keypoint Suppression: The COCO evaluation metric treats visible and hidden keypoints in the same manner. As in <ref type="bibr" target="#b10">[11]</ref>, we include hidden keypoints in our training. However, when a visible and a hidden keypoint appear close together, we remove the hidden keypoint from the ground truth annotation so that this keypoint is not included in associations. In <ref type="figure" target="#fig_1">Figure 6</ref>, we show the effect of excluding these self-hidden keypoints from training and observe better pose reconstruction when a keypoint hides another keypoint of the same type. c) Greedy Decoder with Frontier: The composite fields are converted into sets of pose estimates with the greedy decoder introduced in <ref type="bibr" target="#b10">[11]</ref> and reviewed here. The CIF field and its high-resolution accumulation f (x, y) defined in equation 1 provide seed locations. Previously, new associations were formed starting at the joint that has currently the highest score without taking the CAF confidence of the association into account. Here, we introduce a frontier which is a priority queue of possible next associations. The frontier is ordered by the possible future joint scores which are a function of the previous joint score and the best CAF association:</p><formula xml:id="formula_3">max ij s(a ij J1↔J2 , x) = c exp − || x − (x 1 , y 1 )|| 2 σ 1 f J2 (x 2 , y 2 ) (6) where x is the source joint location, a ij J1↔J2 = (c, x 1 , y 1 , x 2 , y 2 , σ 1 , σ 2 )</formula><p>is the CAF field with implied sub-/superscripts on the components and f J2 is the high resolution confidence map of the target joint J 2 . An association is rejected when it fails reverse matching. To reduce jitter, we not only use the best CAF association in the above equation but a weighted mixture of the best two associations; similar to blended connections in <ref type="bibr" target="#b56">[56]</ref>. Only when all possible associations are added to the frontier, the connection is made to the highest priority in the frontier. This algorithm is fast and greedy. Once a connection to a new joint has been made, this decision is final. d) Instance Score and Non-Maximum Suppression (NMS): Once all poses are reconstructed, we apply NMS. Poses are first sorted by their instance score which is the weighted mean of the keypoint scores where the three highest keypoint scores are weighted three times higher. We run NMS at the keypoint level as in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The suppression radius is dynamic and based on the predicted joint size. We do not refine predictions. e) Denser Pose Skeletons: <ref type="figure">Figure 7</ref> gives an overview of the pose skeletons that are used in this paper. In particular, <ref type="figure">Figure 7b</ref> shows a modification of the standard COCO pose <ref type="bibr" target="#b1">[2]</ref> with additional associations. These denser associations are redundancies in case of occlusions. The additional associations are longer-range and therefore harder to predict. The frontier in our greedy decoder takes this difficulty into account and automatically prefers easier, confident associations when available. Qualitatively, the advantage of dense associations is shown in <ref type="figure" target="#fig_2">Figure 8</ref>. With the standard COCO skeleton, the single person's pose skeleton would be divided into two disconnected parts (left image) as indicated by the two white bounding boxes. With the additional denser associations, a single pose is formed (right image). <ref type="figure">Fig. 7</ref>.</p><formula xml:id="formula_4">(a) (b) (c) (d)</formula><p>A COCO person pose <ref type="bibr" target="#b1">[2]</ref> is shown in (a). Additional denser connections are shown in lighter colors in (b). The additional connections provide redundancies in case of occlusions. A pose skeleton as used in Posetrack with temporal connections is shown in (c). An example of a tracked pose is shown in (d). The first frame is captured with the right leg (blue) in front and the second frame one step later. For clarity, only connections that were used to decode the pose are shown and therefore only the temporal connection that is connecting the right ankle from the past frame to the current frame is visible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pose Tracking</head><p>In the previous section we introduced our method for bottom-up pose estimation in single images. We now generalize that method to tracking poses in videos with associations between images in the same bottom-up fashion. Our unified approach forms both spatial and temporal associations simultaneously. This even leads to improved single-image poses from the additional temporal information.</p><p>a) Temporal Composite Association Field (TCAF): During training, tracking data is fed into the base network as image pairs that are concatenated in the batch dimension, i.e., a batched input tensor of eight image pairs has the same shape as 16 individual images.</p><p>During prediction, the backbone processes one image at a time and each image only once. The resulting feature map is then concatenated with the previous feature map from the "Feature Cache" (see <ref type="figure">Figure 5</ref>). While there is still duplicate computation in the head networks, their computational complexity is small.</p><p>To form associations in image sequences, we introduce the Temporal Composite Association Field (TCAF). Its output structure is identical to a CAF field, but its input is based on pairs of feature maps that were created independently. To jointly process information from both feature maps, the TCAF head contains a preprocessing step of a 1×1 input convolution to reduce the feature size to 512 with ReLU non-linearity, a concatenation of these two feature maps to 1024 features, a 1 × 1 convolution with ReLU to process the two images jointly and a final 1×1 convolution to produce all components necessary for a composite association field. b) Spatio-Temporal Poses: <ref type="figure">Figure 7c</ref> shows a schematic of a person pose (17 joints and 18 associations) with additional temporal connections to all joints of the same kind in the previous frame. In our method, this is treated as a single pose with 2 × 17 joints (CIF) and 18 associations (CAF) within the same frame and an additional 17 associations (TCAF) between frames.</p><p>c) From Spatio-Temporal Poses to Tracks: Spatiotemporal poses create temporal associations in pairs of images. We now introduce our book-keeping method to go from pairs of images to image sequences. During evaluation and for a new frame t 0 , the decoder creates new tracking poses from existing tracks (poses in the previous frame t −1 ) or from single-image seeds in the current frame t 0 . These partial poses are then completed using the same greedy frontier decoder described for single images. Once all spatio-temporal poses are complete, the t 0 joints are extracted into single-frame poses. Every single-frame pose is already tagged with an existing track-id if the spatio-temporal pose was generated from an existing track or a new track-id if the spatio-temporal pose originated from a new seed in the current frame. The singleframe poses are then filtered with soft NMS <ref type="bibr" target="#b17">[18]</ref> and then either added to existing tracks or they become the first poses of new tracks.</p><p>Our method is bottom-up in both pose estimation and tracking and estimates temporal and spatial connections within a single stage. Most existing work -even other bottom-up tracking methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[26]</ref> -employ a two stage process where, first, spatial connections are estimated and, second, temporal connections are made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>Self-driving cars must perceive and predict pedestrians and other traffic participants robustly. One of the most challenging scenarios are crowded places. We will first show experiments on single-image human pose estimation in CrowdPose <ref type="bibr" target="#b11">[12]</ref> which contains particularly challenging scenarios and on the standardized and competitive COCO <ref type="bibr" target="#b1">[2]</ref> person keypoint benchmark. Then we will show results for pose tracking in videos on the PoseTrack 2017 <ref type="bibr" target="#b8">[9]</ref> and 2018 <ref type="bibr" target="#b12">[13]</ref> datasets. We have conducted extensive experiments to show the benefit of a unified bottom-up pose estimation and tracking method with spatio-temporal poses. To demonstrate the universality of our approach, we apply our method also to poses of cars and poses of animals.</p><p>A. Datasets a) CrowdPose: In <ref type="bibr" target="#b11">[12]</ref>, the CrowdPose dataset is proposed. It is a selection of images from other datasets with a particular emphasis on how crowded the images are. The crowd-index of an image represents the amount of overlap between person bounding boxes. The authors place particular emphasis on a uniform distribution of the crowd-index in all data partitions. Because this dataset is a composition of other datasets and to avoid contamination, our CrowdPose models are pretrained on ImageNet <ref type="bibr" target="#b57">[57]</ref> and then trained on CrowdPose only. The dataset comes with a split of 10,000 images for training, 2,000 for validation and 8,000 images for the test set. b) COCO: The de-facto standard for person keypoint prediction is the competitive COCO keypoint task <ref type="bibr" target="#b1">[2]</ref>. The test set is private and powers an active leaderboard via a protected challenge server. COCO contains 56,599 diverse training images with person keypoint annotations. The validation and testdev sets contain 5,000 and 20,288 images. c) ApolloCar3D: We generalize our approach to vehicle keypoints using the ApolloCar3D dataset <ref type="bibr" target="#b58">[58]</ref>, which contains 5,277 driving images at a resolution of 4K and over 60K car instances. The authors defined 66 semantic keypoints in the dataset and, for each car, they provided annotations for the visible ones. For clarity, we choose a subset of 24 semantic keypoints and show quantitative and qualitative results on this dataset. d) Animal Dataset: We evaluate the performances of our algorithm on the Animal-Pose Dataset <ref type="bibr" target="#b32">[33]</ref>, which provides annotations for five categories of animals: dog, cat, cow, horse, sheep for a total of 20 keypoints. The dataset includes 5,517 instances in more than 3,000 images. The majority of these images originally belong to the VOC dataset <ref type="bibr" target="#b59">[59]</ref>. e) PoseTrack 2017 and 2018: We conduct quantitative studies of our tracking performance on the PoseTrack 2017 <ref type="bibr" target="#b8">[9]</ref> and 2018 <ref type="bibr" target="#b12">[13]</ref> datasets. The datasets contain short video sequences of annotated and tracked human poses in diverse situations. The PoseTrack 2018 dataset contains 593 training scenes, 170 validation scenes and 375 test scenes. The test labels are private. PoseTrack 2017 is a subset of the 2018 dataset with 292 train, 50 validation and 208 test scenes. However, the 2018 leaderboard is frozen and new results are only updated for the 2017 leaderboard. Therefore, many recent methods present results on the older, smaller dataset. Here, we will report numbers for both 2017 and 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation a) Single-Image Multi-Person Poses:</head><p>Both CrowdPose and COCO follow COCO's keypoint evaluation method. The object keypoint similarity (OKS) score <ref type="bibr" target="#b1">[2]</ref> is used to assign a bounding box to each keypoint as a function of the person instance bounding box area. Similar to detection, the metric computes overlaps between ground truth and predicted bounding boxes to compute the standard detection metrics average precision (AP) and average recall (AR).</p><p>CrowdPose breaks down the test set at the image level into easy, medium and hard. The easy set contains images with a crowd index in [0, 0.1], the medium set in [0.1, 0.8] and the hard set in [0.8, 1.0]. Given the uniform crowd-index distribution, most images of the test set are in the medium category.</p><p>COCO breaks down the precision scores at the instance level for medium instances with a bounding box area of (32 px) 2 to (96 px) 2 and for large instances with a bounding box area larger than (96 px) 2 . For each image, pose estimators have to provide the 17 keypoint locations per pose and a total score for each pose. Only the top 20 scoring poses per image are considered for evaluation. b) Pose Tracks: A common metric to evaluate the tracking of human poses is the Multi Object Tracker Accuracy (MOTA) <ref type="bibr" target="#b60">[60]</ref>, <ref type="bibr" target="#b3">[4]</ref> which is also the main metric in PoseTrack challenges and leaderboards. It combines false positives, false negatives and ID switches into a single metric. We compare against the best methods that submitted to the PoseTrack 2017 and 2018 evaluation server which computes all metrics on private test sets. These methods include strong top-down methods as well as bottom-up methods for pose estimation and tracking.</p><p>C. Implementation Details a) Neural Network Configuration: All our models are based on ResNet <ref type="bibr" target="#b50">[50]</ref> or ShuffleNetV2 <ref type="bibr" target="#b51">[51]</ref> base networks and multiple head networks. The base networks have their input max-pooling operation removed as it destroys spatial information. The stride from input image to output feature map is 16 with 2048 features at each location. We apply no additional modifications to the standard ResNet models. We use the standard building blocks of ShuffleNetV2 backbones to construct our custom configurations which we denote ShuffleNetV2K16/K30. A ShuffleNetV2K16 model has the prediction accuracy of a ResNet50 with fewer parameters than a ResNet18. The configuration is specified by the number of output features of the five stages and the number of repetitions of the blocks in each stage. Our ShuffleNetV2K16 has output features (block repeats) of 24 (1), 348 (4), 696 <ref type="bibr" target="#b7">(8)</ref>, 1392 (4), 1392 (1) and our ShuffleNetV2K30 has 32 (1), 512 (8), 1024 <ref type="bibr" target="#b15">(16)</ref>, 2048 (6), 2048 <ref type="bibr" target="#b0">(1)</ref>. Spatial 3×3 convolutions are replaced with 5×5 convolutions which introduces only a small increase in the number of parameters because all spatial convolutions are depth-wise.</p><p>Each head network is a single 1×1 convolution followed by a sub-pixel convolution <ref type="bibr" target="#b52">[52]</ref> to double the spatial resolution bringing the total stride down to eight. Therefore, the spatial feature map size for an input image of 801px×801px is 101× 101. The confidence component of a field is normalized with a sigmoid non-linearity and the scale components for joint-sizes are enforced to be positive with a softplus <ref type="bibr" target="#b61">[61]</ref>. b) Augmentations: We apply the standard augmentations of random horizontal flipping, random rescaling with a rescaling factor r ∈ [0.5, 2.0], random cropping and padding to 385 × 385 followed by color jittering with 40% variation in brightness and saturation and 10% variation in hue. We also convert a random 1% of the images to grayscale and generate strong JPEG compression artifacts in 10% of the images.</p><p>The tracking task is similarly augmented. The random rescaling is adapted to an image width in [0.5×801, 1.5×801] and random cropping to a maximum image side of 385 px. Half of the image pairs are randomly reoriented (rotations by multiples of 90 • ). To increase the inter-frame variations, we add a small synthetic camera shift of maximum 30 px between image pairs. To further increase the variation, we form image pairs with a random interval of 4, 8 and 12 frames. In 20% of image pairs, we replace one of the images with a random image to provide a higher number of negative samples for tracking.</p><p>c) Single-Image Training: For ResNet <ref type="bibr" target="#b50">[50]</ref> backbones, we use ImageNet <ref type="bibr" target="#b57">[57]</ref> pretrained models. ShuffleNetV2 <ref type="bibr" target="#b51">[51]</ref> models are trained from random initializations. We use the SGD <ref type="bibr" target="#b62">[62]</ref> optimizer with Nesterov momentum [63] of 0.95, batch size of 32 and weight decay of 10 −5 . The learning rate is exponentially warmed up for one epoch from 10 −3 of its target value. At certain epochs (specified below), the learning rate is exponentially decayed over 10 epochs by a factor of 10. We employ model averaging <ref type="bibr" target="#b64">[64]</ref>, <ref type="bibr" target="#b65">[65]</ref> to extract stable models for validation. At each optimization step, we update an exponentially weighted version of the model parameters with a decay constant of 10 −2 .</p><p>On CrowdPose, which is a smaller dataset than COCO, we train for 300 epochs. We set the target learning rate to 10 −5 and decay at epochs 250 and 280.</p><p>On COCO, we use a target learning rate of 10 −4 and decay at epoch 130 and 140. The training time for 150 epochs of a ShuffleNetV2K16 on two V100 is approximately 37 hours. We do not use any additional datasets beyond the COCO keypoint annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d) Training for Tracking on PoseTrack:</head><p>We use the ShuffleNetV2k30 backbone for all our tracking experiments. PoseTrack 2018 is a video dataset which means that despite a large number of annotations, the variation is smaller than in single-image pose datasets. Therefore, we keep single-image pose estimation on the COCO dataset <ref type="bibr" target="#b1">[2]</ref> as an auxiliary task and train on PoseTrack and COCO simultaneously. The type of poses that are annotated in the two datasets are similar but not identical, e.g., one dataset annotates the eyes and the other does not. During training, we alternate the two tasks between batches. In one batch we feed pairs of images from the PoseTrack dataset and apply losses to the corresponding head networks and in the next batch we feed in single images from COCO and apply losses to the other head networks (see <ref type="figure">Figure 5</ref>). The COCO task is trained identical to the singleimage pose estimation discussed in the previous section, but converted from single images to pairs of tracked images via synthetic shifts of up to 30px. Starting from a trained singleimage pose backbone, we train on both datasets with SGD <ref type="bibr" target="#b62">[62]</ref> with the same configuration as for single images. We alternate the dataset every batch and only do an SGD-step every two batches. We train for 50 epochs where every epoch consists of 4994 batches. The training time is 55 minutes per epoch on two V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results</head><p>a) Crowded Single-Image Pose Estimation: In <ref type="figure" target="#fig_3">Figure 9</ref>, we show example pose predictions from the CrowdPose <ref type="bibr" target="#b11">[12]</ref> validation set. We show results in a diverse selection of sports disciplines and everyday settings. All shown images are from the hard subset with a crowd-index larger than 0.8.</p><p>In <ref type="table" target="#tab_0">Table I</ref>, we show a quantitative comparison of our performance with other methods. We are not only more precise across all precision metrics AP, AP 0.50 , AP 0.75 , AP easy , AP medium and AP hard but also predict faster than all previous top-performing methods at 13.7 FPS (frames-per-second) on a single GTX1080Ti. b) COCO: All state-of-the-art methods compare their performance on the well-established COCO keypoint task <ref type="bibr" target="#b1">[2]</ref>. Our quantitative results on the private 2017 test-dev set are shown in <ref type="table" target="#tab_0">Table II</ref> along with other bottom-up methods. This comparison includes field-based methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b10">[11]</ref> and methods based on associative embedding <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>. We perform on par with the best existing bottom-up method. We evaluate on rescaled images where the longer edge is 801 px which is the same image size that will be used for tracking below. We evaluate a single forward pass without horizontal flipping and without multi-scale evaluation because we aim for a fast method. The average time per image with a GTX1080Ti is 152 ms (63 ms on a V100) of which 29 ms is used for decoding. c) Pose Tracking: We want to track multiple human poses in videos. We train and validate on the PoseTrack 2018 dataset <ref type="bibr" target="#b12">[13]</ref>. <ref type="table" target="#tab_0">Table III</ref> shows our main results for pose tracking on both of the private test sets of Posetrack 2017 and 2018. All our results are produced in a single pass and online (without future frames). The frames per second (FPS) stated in <ref type="table" target="#tab_0">Table III</ref> refer to the single process, sequential evaluation.</p><p>Spatio-temporal poses on real-world examples are shown in <ref type="figure" target="#fig_4">Figure 10</ref>. They show challenging scenarios with occlusions. <ref type="figure" target="#fig_5">Figure 11</ref> highlights the ability of spatio-temporal poses to complete poses through time, i.e., even when a pose is partitioned because of occlusion in the current frame, multiple temporal connections (TCAF) form a single tracked pose. Similarly, for the poses 3, 4, 5 and 7 in <ref type="figure" target="#fig_0">Figure 12</ref>, the associations from shoulders to hips are often difficult because of the lighting condition. Depending on the predicted association confidences, the decoder determines automatically whether to connect to a keypoint with a spatial or temporal connection. In these difficult scenarios, the greedy decoder completed these poses with multiple temporal connections (TCAF).  d) Car and Animal Poses: A holistic perception framework for autonomous vehicles also needs to be able to generalize to other classes than humans. We show that we can predict poses of cars and animals with high accuracy in <ref type="figure">Figures 13  and 14</ref>.</p><p>On car instances, our model achieves an average precision (AP) of 76.1%. The AP metric follows the same protocol of human instances, but to the best of our knowledge no previous method has evaluated AP on ApolloCar3D <ref type="bibr" target="#b58">[58]</ref> without leveraging 3D information. Hence, we include a study <ref type="bibr">Fig. 12</ref>. Qualitative results from the Posetrack 2018 <ref type="bibr" target="#b12">[13]</ref> validation set. (a) indicates a connection that has been made spatially in a previous frame but for the last few frames the left leg of person 3 is connected to the rest of the body only through temporal connections. (b) shows a connection that is temporarily occluded by the arm of the person in front and also here our algorithm decided to connect the left leg via temporal connections instead of spatial ones. on the keypoint detection rate, which has been defined in the ApolloCar3D dataset <ref type="bibr" target="#b58">[58]</ref> and considers a keypoint to be correctly estimated if the error is less than 10 pixels. Our method achieves a detection rate of 86.1% compared to 75.4% of CPM <ref type="bibr" target="#b20">[21]</ref>. Notably, the authors of ApoolloCar3D <ref type="bibr" target="#b58">[58]</ref> also report the detection rate of the human labelers to be 92.4%.</p><p>On animal instances, our model achieves an AP of 47.8%, compared to 44.3% of WS-CDA, the baseline developed by the authors of the Animal-Pose dataset <ref type="bibr" target="#b32">[33]</ref>. Lower performances on animals are due to the smaller dataset size with just 4K training instances. Simultaneous training for humans and animals to achieve better generalization is left for future work. <ref type="figure">Fig. 13</ref>. Qualitative results from the KITTI <ref type="bibr" target="#b68">[68]</ref> and ApolloCar3D <ref type="bibr" target="#b58">[58]</ref> datasets. <ref type="figure">Fig. 14.</ref> Qualitative results from the Animal-Pose dataset <ref type="bibr" target="#b32">[33]</ref>. The left image was processed by a person model and an animal model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Studies</head><p>We study the impact of the backbone, the precise criteria for a keypoint, our proposed Frontier decoder, a memory efficient decoder, alternatives to TCAF and the impact of input image size. We start with studies for single images on the COCO val set <ref type="table" target="#tab_0">(Table IV)</ref> before moving to tracking studies for PoseTrack <ref type="table" target="#tab_2">(Table V)</ref>.</p><p>Our single-image studies are run with an option to force complete poses. This is the common practice as the COCO metric does not penalize false positive keypoints within poses. This option would not be used in most real-world settings. Without forcing complete poses, the decoding time and the total prediction time is reduced by about 10ms. a) Backbone: The reference backbone is a small Shuf-fleNetV2K16. We show comparisons to the larger ResNet50 and ShuffleNetV2K30 backbones and show how they improve precision (AP) and at what cost in timing. b) Keypoint Criterium: We try to illuminate why our precision and speed is significantly better than OpenPose <ref type="bibr" target="#b15">[16]</ref>. OpenPose first detects keypoints and then associates them. Therefore, every keypoint has to be detectable individually. In OpenPifPaf, new keypoint associations are generated from a source keypoint. These new keypoints are not previously known. They are discovered in the association. That allows OpenPifPaf to generate poses from a strong seed keypoint and connect to less confident keypoints. In "independent-only", we restrict the keypoints of OpenPifPaf to be all of the quality of an independent seed keypoint and observe a dramatic drop of 8.1% in AP. c) Frontier Decoder: Next, we study the impact of the Frontier decoder with respect to a simpler decoder without frontier. The standard pose is sparsely connected and, therefore, the frontier only has few alternatives to prioritize. For a denser pose ("dense"), the impact of the frontier (compare with "no-frontier and dense") is more pronounced (+0.3 AP). d) Memory Efficient Decoding: In the bottom part of <ref type="table" target="#tab_0">Table IV</ref>, we study the effect of removing the high-resolution accumulation map (HR) to reduce the memory footprint. This high resolution map is used in two places. First, to rescore the seeds and, second, to rescore the CAF. The impact of the seed rescoring is only 0.1 in AP but comes at a large cost in decoding time. As an alternative, we investigate a local non-maximum suppression (NMS) that selects a seed only if it is the highest confidence in a 3 × 3 window (introduced in CenterNet <ref type="bibr" target="#b69">[69]</ref>). This NMS reduces the decoding time but not back to the original speed. Independently, we study the impact of rescoring the CAF field which is about +1.0% in AP. Only when both the seed rescoring and the CAF rescoring are removed, the creation of the HR maps can be omitted. In that memory efficient configuration (bottom line in <ref type="table" target="#tab_0">Table IV</ref>), the AP dropped by 1.4% with respect to "original". This demonstrates the importance of the highresolution accumulation for speed and accuracy and which should only be removed when absolutely necessary. e) Tracking Baselines: We conducted detailed studies of our method on the Posetrack 2018 validation set that are shown in <ref type="table" target="#tab_2">Table V</ref>. First, we created two baselines ourselves. Both baselines first do single-image pose estimation and then use the Hungarian algorithm <ref type="bibr" target="#b70">[70]</ref> to track poses from frame to frame. Our first algorithm uses a simple Euclidean distance between joints to construct a pose similarity score. Our second method replaces the Euclidean distance with an OKS-based distance that is used in the COCO metric to compare predictions to ground truth. Both methods show a drop in MOTA of 1.5 and 2.0 while operating at about the same speed as our "original" model. This demonstrates that the overhead of our tracking network is comparable to the small overhead of the Hungarian algorithm with respect to the single-image model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>f) Tracking Ablation:</head><p>We studied the effect of input image size at the bottom of Table V. Our "original" model rescales the image width to 801px. Larger images do not show an improvement in accuracy (MOTA) while becoming significantly slower. Smaller input images decrease MOTA but at the same time can drastically increase speed. Most applications can probably tolerate an accuracy reduction by 0.9 in MOTA to improve speed by +37%. When the input image size is reduced to 513px, MOTA drops by 2.9 (still a great result) which comes with a speed improvement of +82% to a fast 22.2 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We have demonstrated a new method for bottom-up pose tracking for 2D human poses and shown its strength in crowded and occluded scenes that are relevant for perception in self-driving cars and social robots. We outperform previous state-of-the-art methods on CrowdPose and on PoseTrack2018. On PoseTrack2017 we are on par with the state-of-the-art but run an order of magnitude faster. We have also shown that our method generalizes to pose estimation of cars and animals to create a holistic perception framework for autonomous vehicles.</p><p>Alexandre Alahi is an Assistant Professor at EPFL. He spent five years at Stanford University as a Post-doc and Research Scientist after obtaining his Ph.D. from EPFL. His research enables machines to perceive the world and make decisions in the context of transportation problems and smart environments. He has worked on the theoretical challenges and practical applications of socially-aware Artificial Intelligence, i.e., systems equipped with perception and social intelligence. He was awarded the Swiss NSF early and advanced researcher grants for his work on predicting human social behavior. Alexandre has also co-founded multiple startups such as Visiosafe, and won several startup competitions. He was elected as one of the Top 20 Swiss Venture leaders in 2010.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Visualizing the components of the CIF for the "left shoulder" keypoint on a small image crop. The confidence map is shown in (2a). The vector field with joint-scale estimates is shown in (2b). Only locations with confidence &gt; 0.5 are drawn. The fused confidence, vector and scale components according to Equation 1 are shown in (2c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 6 .</head><label>6</label><figDesc>Effect of self-hidden keypoint suppression during training. The left image is without and the right image is with self-hidden keypoint suppression. The left hips of both soccer players collide in pixel space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 8 .</head><label>8</label><figDesc>Left: A sparse pose cannot connect the right arm to the facial keypoints leading to the detection of two separate person instances highlighted by the two white bounding boxes. Right: An additional dense connection between the nose and right shoulder leads to a correctly identified single pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 9 .</head><label>9</label><figDesc>Illustration of OpenPifPaf predictions on a diverse selection of sports disciplines (top) and professional and casual photos (bottom) from the CrowdPose<ref type="bibr" target="#b11">[12]</ref> val set with crowd-index hard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 10 .</head><label>10</label><figDesc>Qualitative results from the Posetrack 2018<ref type="bibr" target="#b12">[13]</ref> validation set. Images show tracks of spatio-temporal poses including their frame-to-frame associations where only connections that were used to construct the poses are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 11 .</head><label>11</label><figDesc>Qualitative results from the Posetrack 2018 [13] validation set. Left: Single-image detection. The person's left shoulder is not visible and therefore the left arm cannot be connected to the rest of the body. Right: Spatio-temporal pose. Multiple temporal connections allow to safely connect both left and right arm to the rest of the body.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EVALUATION</head><label>I</label><figDesc>ON THE CROWDPOSE TEST DATASET<ref type="bibr" target="#b11">[12]</ref>. OUR OPENPIFPAF RESULT IS BASED ON A RESNET50 BACKBONE WITH SINGLE-SCALE EVALUATION AT 641PX. * VALUES EXTRACTED FROM CROWDPOSE PAPER<ref type="bibr" target="#b11">[12]</ref>. + EMPLOYS MULTI-SCALE TESTING.TABLE II EVALUATION METRICS FOR THE COCO 2017 TEST-DEV DATASET FOR BOTTOM-UP METHODS. NUMBERS ARE EXTRACTED FROM THE RESPECTIVE PAPERS. OUR FPS IS DETERMINED ON A SINGLE V100 GPU. ONLY EVALUATING IMAGES WITH THREE PERSON INSTANCES.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AP</cell><cell>AP 0.50</cell><cell>AP 0.75</cell><cell>APeasy</cell><cell>AP medium</cell><cell>AP hard</cell><cell>FPS</cell></row><row><cell cols="3">Mask R-CNN  *  [15]</cell><cell></cell><cell></cell><cell>57.2</cell><cell>83.5</cell><cell>60.3</cell><cell>69.4</cell><cell>57.9</cell><cell>45.8</cell><cell>2.9</cell></row><row><cell cols="3">AlphaPose  *  [66]</cell><cell></cell><cell></cell><cell>61.0</cell><cell>81.3</cell><cell>66.0</cell><cell>71.2</cell><cell>61.4</cell><cell>51.1</cell><cell>10.9</cell></row><row><cell cols="4">HigherHRNet-W48 [24]</cell><cell></cell><cell>65.9</cell><cell>86.4</cell><cell>70.6</cell><cell>73.3</cell><cell>66.5</cell><cell>57.9</cell><cell>-</cell></row><row><cell cols="2">SPPE [12]</cell><cell></cell><cell></cell><cell></cell><cell>66.0</cell><cell>84.2</cell><cell>71.5</cell><cell>75.5</cell><cell>66.3</cell><cell>57.4</cell><cell>10.1</cell></row><row><cell cols="5">HigherHRNet-W48 + [24]</cell><cell>67.6</cell><cell>87.4</cell><cell>72.6</cell><cell>75.8</cell><cell>68.1</cell><cell>58.9</cell><cell>-</cell></row><row><cell cols="3">OpenPifPaf (ours)</cell><cell></cell><cell></cell><cell>70.5</cell><cell>89.1</cell><cell>76.1</cell><cell>78.4</cell><cell>72.1</cell><cell>63.8</cell><cell>13.7</cell></row><row><cell></cell><cell>AP</cell><cell>AP M</cell><cell cols="2">AP L</cell><cell>t [ms]</cell><cell></cell><cell></cell></row><row><cell>OpenPose [16]</cell><cell>61.8</cell><cell>57.1</cell><cell>68.2</cell><cell></cell><cell>100</cell><cell></cell><cell></cell></row><row><cell>Assoc. Emb. [17]</cell><cell>65.5</cell><cell>60.6</cell><cell>72.6</cell><cell></cell><cell>166</cell><cell></cell><cell></cell></row><row><cell>PersonLab [18]</cell><cell>68.7</cell><cell>64.1</cell><cell>75.5</cell><cell></cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>MultiPoseNet [23]</cell><cell>69.6</cell><cell>65.0</cell><cell>76.3</cell><cell></cell><cell>43  *</cell><cell></cell><cell></cell></row><row><cell>HigherHRNet [24]</cell><cell>70.5</cell><cell>66.6</cell><cell>75.8</cell><cell cols="2">&gt;1000</cell><cell></cell><cell></cell></row><row><cell>OpenPifPaf (ours)</cell><cell>70.9</cell><cell>67.1</cell><cell>76.8</cell><cell></cell><cell>95</cell><cell></cell><cell></cell></row><row><cell cols="3">TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">EVALUATION METRICS ON THE TEST SETS OF (A) POSETRACK 2018 [13]</cell><cell></cell></row><row><cell cols="7">AND (B) POSETRACK 2017 [9]. NUMBERS ARE EXTRACTED FROM THE</cell><cell></cell></row><row><cell cols="7">RESPECTIVE PAPERS AND THE LEADERBOARD. ALL METHODS ARE</cell><cell></cell></row><row><cell cols="6">ONLINE METHODS APART FROM DETTRACK [67].</cell><cell></cell><cell></cell></row><row><cell>PoseTrack 2018</cell><cell></cell><cell cols="2">MOTA</cell><cell>FPS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>openSVAI [28]</cell><cell></cell><cell>54.5</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MIPAL [26]</cell><cell></cell><cell>54.9</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Miracle [30]</cell><cell></cell><cell>57.4</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MSRA/FlowTrack [19]</cell><cell>61.4</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">OpenPifPaf (ours)</cell><cell>61.7</cell><cell cols="3">12.2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PoseTrack 2017</cell><cell></cell><cell cols="2">MOTA</cell><cell>FPS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>STAF [27]</cell><cell></cell><cell>53.8</cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MIPAL [26]</cell><cell></cell><cell>54.5</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MSRA/FlowTrack [19]</cell><cell>57.8</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HRNet [20]</cell><cell></cell><cell>57.9</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LightTrack [29]</cell><cell></cell><cell>58.0</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">OpenPifPaf (ours)</cell><cell>60.6</cell><cell cols="3">12.2</cell><cell></cell><cell></cell></row><row><cell>KeyTrack [32]</cell><cell></cell><cell>61.2</cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DetTrack (offline) [67]</cell><cell>64.1</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDIES OF SKELETON CHOICE AND DECODER CONFIGURATIONS FOR SINGLE-IMAGE POSE ESTIMATION. ALL RESULTS (EXCEPT WHERE EXPLICITLY STATED OTHERWISE) ARE PRODUCED WITH THE SAME SHUFFLENETV2K16 MODEL ON THE COCO VAL SET [2] ON A SINGLE GTX1080TI. FIRST, WE REVIEW DIFFERENT BACKBONE ARCHITECTURES (A RESNET50 [50] AND A LARGER SHUFFLENETV2 [51]). SECOND, WE SHOW THAT ONLY USING CONFIDENT KEYPOINTS LEADS TO A LARGE DROP IN PRECISION. THIRD, WE OBSERVE THAT THE FRONTIER DECODER IS MORE IMPORTANT FOR DENSER SKELETONS WHILE INCURRING ALMOST NO OVERHEAD ON SPARSE SKELETONS. FOURTH, WE CAN PRODUCE A MEMORY-EFFICIENT VERSION OF OUR DECODER AT A COST OF 1.4% IN AP. THE BIGGEST DROP IN ACCURACY COMES FROM NOT RESCORING THE CAF FIELD AND THE LARGEST CONTRIBUTOR TO INCREASING THE INFERENCE TIME IS NOT RESCORING THE SEEDS.</figDesc><table><row><cell></cell><cell></cell><cell>AP</cell><cell>AP 0.50</cell><cell>AP 0.75</cell><cell>AP M</cell><cell>AP L</cell><cell>t [ms]</cell><cell>t dec [ms]</cell></row><row><cell></cell><cell>original (ShuffleNetV2K16)</cell><cell>66.8</cell><cell>86.5</cell><cell>73.2</cell><cell>62.1</cell><cell>74.6</cell><cell>50</cell><cell>19</cell></row><row><cell>Backbone</cell><cell>ResNet50</cell><cell>68.2</cell><cell>87.9</cell><cell>74.6</cell><cell>65.8</cell><cell>72.7</cell><cell>64</cell><cell>22</cell></row><row><cell></cell><cell>ShuffleNetV2K30</cell><cell>71.0</cell><cell>88.8</cell><cell>77.7</cell><cell>66.6</cell><cell>78.5</cell><cell>92</cell><cell>16</cell></row><row><cell>Keypoints</cell><cell>independent-only</cell><cell>-8.1</cell><cell>-6.3</cell><cell>-9.5</cell><cell>-8.7</cell><cell>-7.3</cell><cell>±0</cell><cell>±0</cell></row><row><cell>Frontier decoder</cell><cell>no-frontier</cell><cell>±0.0</cell><cell>-0.1</cell><cell>+0.1</cell><cell>±0.0</cell><cell>-0.1</cell><cell>-1</cell><cell>-1</cell></row><row><cell></cell><cell>dense</cell><cell>+0.1</cell><cell>+0.2</cell><cell>+0.2</cell><cell>-0.3</cell><cell>+0.5</cell><cell>+15</cell><cell>+15</cell></row><row><cell></cell><cell>no-frontier and dense</cell><cell>-0.3</cell><cell>+0.1</cell><cell>-0.1</cell><cell>-0.5</cell><cell>±0.0</cell><cell>+14</cell><cell>+14</cell></row><row><cell cols="2">memory efficient no seed rescoring</cell><cell>-0.1</cell><cell>-0.4</cell><cell>-0.1</cell><cell>+0.2</cell><cell>+0.1</cell><cell>+71</cell><cell>+54</cell></row><row><cell></cell><cell>no seed rescoring (with NMS)</cell><cell>+0.1</cell><cell>+0.1</cell><cell>±0.0</cell><cell>+0.2</cell><cell>+0.0</cell><cell>+19</cell><cell>+15</cell></row><row><cell></cell><cell>no CAF rescoring</cell><cell>-1.0</cell><cell>-0.3</cell><cell>-1.0</cell><cell>-1.0</cell><cell>-1.7</cell><cell>-1</cell><cell>-1</cell></row><row><cell></cell><cell>no rescoring (with NMS), without HR</cell><cell>-1.4</cell><cell>-0.4</cell><cell>-1.4</cell><cell>-1.0</cell><cell>-2.3</cell><cell>+9</cell><cell>+7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE V BASELINES</head><label>V</label><figDesc>AND ABLATION STUDIES ON THE POSETRACK 2018 VALIDATION SET [13] ON A SINGLE V100 GPU. WE OUTPERFORM HUNGARIAN TRACKERS WITH EUCLIDEAN AND OKS DISTANCE FUNCTIONS IN ACCURACY FOR A SMALL OVERHEAD IN FPS. WE ALSO STUDY OUR SENSITIVITY TO THE INPUT IMAGE SIZE. FOR IMAGE SIZES OF 513PX, WE OBSERVE A DROP OF 2.9 IN MOTA BUT RUN 82% FASTER AT 22.2 FPS.</figDesc><table><row><cell></cell><cell></cell><cell>MOTA</cell><cell>FPS</cell></row><row><cell></cell><cell>original (801px)</cell><cell>66.4</cell><cell>12.2</cell></row><row><cell>Hungarian</cell><cell>euclidean</cell><cell>-1.5</cell><cell>+4%</cell></row><row><cell></cell><cell>OKS</cell><cell>-2.0</cell><cell>+1%</cell></row><row><cell cols="2">Image size 513px</cell><cell>-2.9</cell><cell>+82%</cell></row><row><cell></cell><cell>641px</cell><cell>-0.9</cell><cell>+37%</cell></row><row><cell></cell><cell>1201px</cell><cell>-1.7</cell><cell>-49%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/vita-epfl/openpifpaf posetrack</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>VII. ACKNOWLEDGEMENTS This work was supported by the Swiss National Science Foundation under the Grant 2OOO21-L92326 and the SNSF Spark fund (190677). We also thank our lab members and reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">How safe are self-driving cars?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Crow</surname></persName>
		</author>
		<ptr target="https://rmi.org/safe-self-driving-cars/1" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Rocky Mountain Institute</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mot16: A benchmark for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2017 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1949" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<idno>1981. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6457" to="6465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Posetrack: Joint multi-person pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04596</idno>
		<title level="m">Joint flow: Temporal flow fields for multi person tracking</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>10 863-10 872. 2, 6, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>2, 6, 7, 9</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>2, 3, 4, 9</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Associative embedding: End-toend learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pose estimator and tracker using temporal flow maps for limbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient online multiperson 2d pose tracking with recurrent spatio-temporal affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Raaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A top-down approach to articulated human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lighttrack: A generic framework for online top-down human pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation for pose tracking with enhanced cascaded pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-person articulated tracking with spatial and temporal embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5664" to="5673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">15 keypoints is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Snower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02323</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Crossdomain adaptation for animal pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Occlusion-net: 2d/3d occluded keypoint localization using graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7326" to="7335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast animal pose estimation using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Aldarondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Willmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kislin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Shaevitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Who left the dogs out? 3d animal reconstruction with expectation maximization in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Boyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Atrw: A benchmark for amur tiger re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th</title>
		<meeting>the 28th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<title level="m">ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2590" to="2598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pretraining boosts out-of-domain robustness for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yüksekgönül</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mathis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>eeding of the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeplabcut: markerless pose estimation of userdefined body parts with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mamidanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Cury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
			<publisher>Nature Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning from synthetic animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">395</biblScope>
			<biblScope unit="page" from="12" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lions and tigers and bears: Capturing non-rigid, 3d, articulated shape from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3955" to="3963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Creatures great and smal: Recovering the shape and motion of animals from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Three-d safari: Learning to estimate zebra pose, shape, and texture from images &quot;in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berger-Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5358" to="5367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Carfusion: Combining point tracking and part detection for dynamic 3d reconstruction of vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1906" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gsnet: Joint vehicle pose and shape reconstruction with geometrical and scene-aware supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="515" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple baseline for vehicle pose estimation: Experimental validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">P</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fernandez-Llorca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132" to="539" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>eeding of the IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="6" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Blazeface: Sub-millisecond neural face detection on mobile gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bazarevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kartynnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vakunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Raveendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05047</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Incorporating second-order functional knowledge for better option pricing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bélisle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="472" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate o(1/k2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet Mathematics Doklady</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="372" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Efficient estimations from a slowly convergent robbinsmonro process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruppert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
		<respStmt>
			<orgName>Cornell University Operations Research and Industrial Engineering, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multiperson pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Combining detection and tracking for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Modolo</surname></persName>
		</author>
		<idno>pp. 11 088-11 096. 10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Sven Kreiss is a postdoc at the Visual Intelligence for Transportation (VITA) lab at EPFL in Switzerland focusing on perception with composite fields. Before returning to academia, he was the Senior Data Scientist at Sidewalk Labs (Alphabet, Google sister) and worked on geospatial machine learning for urban environments. Prior to his industry experience</title>
	</analytic>
	<monogr>
		<title level="m">Sven developed statistical tools and methods used in particle physics research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Lorenzo Bertoni is a doctoral student at the Visual Intelligence for Transportation (VITA) lab at EPFL in Switzerland focusing on 3D vision for vulnerable road users. Before joining EPFL, Lorenzo was a management consultant at Oliver Wyman and a visiting researcher at the University of California, Berkeley, working on predictive control of autonomous vehicles</title>
		<imprint/>
		<respStmt>
			<orgName>Lorenzo received Bachelors and Masters Degrees in Engineering from the Polytechnic University of Turin and the University of Illinois at Chicago</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
