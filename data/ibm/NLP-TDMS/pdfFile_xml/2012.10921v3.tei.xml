<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Geometry-Disentangled Representation for Complementary Understanding of 3D Object Point Cloud</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutian</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
							<email>xjqi@eee.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Geometry-Disentangled Representation for Complementary Understanding of 3D Object Point Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In 2D image processing, some attempts decompose images into high and low frequency components for describing edge and smooth parts respectively. Similarly, the contour and flat area of 3D objects, such as the boundary and seat area of a chair, describe different but also complementary geometries. However, such investigation is lost in previous deep networks that understand point clouds by directly treating all points or local patches equally. To solve this problem, we propose Geometry-Disentangled Attention Network (GDANet). GDANet introduces Geometry-Disentangle Module to dynamically disentangle point clouds into the contour and flat part of 3D objects, respectively denoted by sharp and gentle variation components. Then GDANet exploits Sharp-Gentle Complementary Attention Module that regards the features from sharp and gentle variation components as two holistic representations, and pays different attentions to them while fusing them respectively with original point cloud features. In this way, our method captures and refines the holistic and complementary 3D geometric semantics from two distinct disentangled components to supplement the local information. Extensive experiments on 3D object classification and segmentation benchmarks demonstrate that GDANet achieves the state-of-the-arts with fewer parameters. Code is released on https://github.com/mutianxu/GDANet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The capacity to analyze and comprehend 3D point clouds receives interests in computer vision community due to its wide applications in autonomous driving and robotics <ref type="bibr">(Rusu et al. 2008;</ref><ref type="bibr" target="#b17">Qi et al. 2018)</ref>. Recent studies explore deep learning methods to understand 3D point clouds inspired by their great success in computer vision applications <ref type="bibr" target="#b3">(He et al. 2015</ref><ref type="bibr" target="#b4">(He et al. , 2016</ref>. Deep networks <ref type="bibr" target="#b2">(Guo et al. 2016)</ref> can extract effective semantics of 3D point clouds with layered operations, in contrast to low-level handcrafted shape descriptors. The pioneer work PointNet <ref type="bibr" target="#b18">(Qi et al. 2017a</ref>) directly processes 3D points by Multi-layer Perceptrons(MLPs) (Hornik <ref type="figure">Figure 1</ref>: Examples of 3D objects, where sharp-variation component describes the contour areas, and gentle-variation component denotes the flat areas. Our method regards the features from these two disentangled components as two holistic representations, i.e., each point of the original point cloud is linked with all points of sharp and gentle variation components. This operation integrates complementary geometric information from two disentangled components. 1991), whose main idea is to learn a spatial encoding of each point and then aggregate all point features to a global point cloud signature. PointNet++ <ref type="bibr" target="#b19">(Qi et al. 2017b</ref>) adopts a hierarchical encoder-decoder structure to consider local regions, which downsamples point clouds in layers and gradually interpolates them to the original resolution. From another perspective, some recent efforts extend regular grid convolution <ref type="bibr" target="#b36">(Xu et al. 2018;</ref><ref type="bibr" target="#b9">Li et al. 2018</ref>; <ref type="bibr" target="#b24">Thomas et al. 2019</ref>) on irregular 3D point cloud configuration.</p><p>To fully utilize geometric information, some attempts <ref type="bibr" target="#b30">(Wang et al. 2019b;</ref><ref type="bibr" target="#b8">Lan et al. 2019)</ref> capture local geometric relations among center points and its neighbors. However, these works treat all points or local patches equally, which are entangled together with large redundancy, making it hard to capture the most related and key geometric interest to the network. Moreover, previous operations only capture the geometric information in local areas.</p><p>To remedy these defects, we need to disentangle point clouds into distinct components and learn the few-redundant information represented by these holistic components. In image processing, some attempts collect and combine the high-frequency (edge) and low-frequency (smooth) components with distinct characteristics filtered through digital signal processing. Similarly, the contour areas of 3D objects delineating skeletons provide basic structural information, while the flat areas depicting surfaces supply the geometric manifold context. When operating them separately, the network performs better by learning distinct but also complementary geometric representations of point clouds. This inspires us to extract and utilize the geometric information of the contour and flat areas disentangled from 3D objects.</p><p>Here comes a challenge about how to disentangle 3D objects into such holistic representations (contour and flat area) and utilize them for better understanding point clouds. Thanks to the graph signal processing <ref type="bibr" target="#b16">(Ortega et al. 2018;</ref><ref type="bibr" target="#b22">Sandryhaila and Moura 2014)</ref> who analyzes the frequency on graphs, we firstly extend this to our Geometry-Disentangle Module (GDM) that dynamically analyzes graph signals on 3D point clouds in different semantic levels and factorizes the original point cloud into the contour and flat parts of objects, respectively denoted by sharp and gentle variation components (explained at the end of Sec. 4.1). Further, we design Sharp-Gentle Complementary Attention Module (SGCAM) that pays different attentions to features from sharp and gentle variation components according to geometric correlation, then respectively fuses them with each original point features instead of only operating local patches. As shown in <ref type="figure">Fig. 1</ref>, the contour and flat area have distinct but also complementary effects on reflecting the geometry of objects.</p><p>Equipped with GDM and SGCAM, we propose GDANet who captures and refines the holistic and complementary geometries of 3D objects to supplement local neighboring information. Experimental results on challenging benchmarks demonstrate that our GDANet achieves the state of the arts, and is more lightweight and robust to density, rotation and noise. Thorough visualizations and analysis verify that our model effectively captures the complementary geometric information from two disentangled components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Point Cloud Models Based on Geometry. Recently the exploration on point cloud geometry has drawn large focus. Geo-CNN <ref type="bibr" target="#b8">(Lan et al. 2019</ref>) defines a convolution operation that aggregates edge features to capture the local geometric relations. In <ref type="bibr" target="#b35">(Xu, Zhou, and Qiao 2020)</ref>, they aggregate points from local neighbors with similar semantics in both Euclidean and Eigenvalue space. RS-CNN <ref type="bibr" target="#b14">(Liu et al. 2019c</ref>) extends regular CNN to encode geometric relation in a local point set by learning the topology. DensePoint ) recognizes the implicit geometry by extracting contextual shape semantics. However, the contour and flat area of 3D objects play different but complementary roles in modeling 3D objects. All the operations mentioned above neglect this property and treat all points or local patches equally. By contrast, we present Geometry-Disentangle Module to disentangle point clouds into sharp (contour) and gentle (flat area) variation components, which are two distinct representations of 3D objects. Attention Networks. The applications of attention mechanism in sequence-based tasks become popular <ref type="bibr" target="#b27">(Vaswani et al. 2017)</ref>, which helps to concentrate on the most relevant and significant parts. Some recent point cloud methods utilize attention to aggregate the neighboring features of each point <ref type="bibr" target="#b12">(Liu et al. 2019a)</ref>. GAC <ref type="bibr" target="#b28">(Wang et al. 2019a</ref>) proposes a graph attention convolution that can be carved into specific shapes by assigning attention weights to different neighbor points. Different from them, our network learns to assign different attention weights to the disentangled contour and flat areas of 3D objects based on geometric correlations. In Sec. 4.3, we also illustrate that not only the attention mechanism helps the network refine the disentangled feature but also our disentanglement strategy assists the attention module easily concentrate on the key geometric interests. Disentangled Representation. Recent attempts use disentangled representations in different applications. In general, the concept of disentanglement <ref type="bibr" target="#b0">(Bengio, Courville, and Vincent 2012)</ref> dominates representation learning, closely linking with human reasoning. In <ref type="bibr" target="#b33">(Xiao, Hong, and Ma 2018)</ref>, they separate a facial image into individual and shared factors encoding single attribute. <ref type="bibr" target="#b6">(Huang et al. 2018</ref>) processes images by decomposing latent space into content and style space. Yet, the disentangled representation on point cloud understanding remains untouched. Our method explicitly disentangles point clouds into two components denoting contour and flat area of objects, which are fused to provide distinct and complementary geometric information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Revisit Graph Signal Processing</head><p>Graph signal processing <ref type="bibr" target="#b16">(Ortega et al. 2018;</ref><ref type="bibr" target="#b22">Sandryhaila and Moura 2014)</ref> is based on a graph G = (V, A) where V = {v 1 , · · · , v N } denotes a set of N nodes and A ∈ R N ×N denotes a weight adjacency matrix encoding the dependencies between nodes. Using this graph, we refer to the one-channel features of the data on all nodes in vertex domain as s ∈ R N . Let A be a graph shift operator which takes a graph signal s ∈ R N as input and produces a new graph signal y = As. We also have the eigen decomposition A = V ΛV −1 , where the columns of matrix V are the eigenvectors of A and the diagonal eigenvalue matrix Λ ∈ R N ×N corresponds to ordered eigenvalues λ 1 , · · · , λ N .</p><p>Theorem 1 <ref type="bibr" target="#b16">(Ortega et al. 2018;</ref><ref type="bibr" target="#b22">Sandryhaila and Moura 2014)</ref>. T he ordered eigenvalues (λ 1 ≥ λ 2 ≥ · · · , ≥ λ N ) represent f requencies on the graph f rom low to high.</p><p>Accordingly, we obtain V −1 y = ΛV −1 s, and the graph Fourier transform of graph signal s and y are s = V −1 s, y = V −1 y, respectively. V −1 is the graph Fourier transform matrix. The components of s and y are considered as frequency contents of signal s and y.</p><p>As stated in (Sandryhaila and Moura 2014), a graph filter is a polynomial in the graph shift:</p><formula xml:id="formula_0">h(A) = L−1 =0 h A ,<label>(1)</label></formula><p>where h are filter coefficients and L is the length of the filter. It takes a graph signal s ∈ R N as the input and generates a filtered signal y</p><formula xml:id="formula_1">= h(A)s ∈ R N . Then y = V h(Λ)V −1 s, making V −1 y = h(Λ)V −1 s and y = h(Λ) s.</formula><p>Theorem 2 <ref type="bibr" target="#b16">(Ortega et al. 2018;</ref><ref type="bibr" target="#b22">Sandryhaila and Moura 2014)</ref>. T he diagonal matrix h(Λ) is the graph f requency response of the f ilter h(A), which is denoted as h(A). T he f requency response of λ i is</p><formula xml:id="formula_2">L−1 =0 h λ i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>We firstly design Geometry-Disentangle Module based on graph signal processing to decompose point clouds into sharp and gentle variation components (respectively denotes the contour and flat area). Further, we propose Sharp-Gentle Complementary Attention Module to fuse the point features from sharp and gentle variation components. Last, we introduce Geometry-Disentangled Attention Network equipped with these two modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Geometry-Disentangle Module</head><p>Graph Construction. We consider a point cloud con-</p><formula xml:id="formula_3">sisted of N points with C−dimensional features, which is denoted by a matrix X = [x 1 , x 2 , · · · , x N ] T = [s 1 , s 2 , · · · , s C ] ∈ R N ×C , where x i ∈ R C represents the i-th point and s c ∈ R N represents the c-th channel feature.</formula><p>Features can be 3D coordinates, normals and semantic features. We construct a graph G = (V, A) through an adjacency matrix A that encodes point similarity in the feature space. Each point x i ∈ R C is associated with a corresponding graph vertex i ∈ V and s c ∈ R N is a graph signal. The edge weight between two points x i and x j is</p><formula xml:id="formula_4">A i,j = f (||x i − x j || 2 ), ||x i − x j || 2 ≤ τ 0, otherwise ,<label>(2)</label></formula><p>where f (·) is an non-negative decreasing function (e.g., Gaussian function) which must ensure that A ∈ R N ×N is a diagonally dominant matrix and τ is a threshold. In addition, to handle the size-varying neighbors across different points and feature scales, we normalize all edge weights as follows:</p><formula xml:id="formula_5">Ã i,j = A i,j j A i,j ,<label>(3)</label></formula><p>whereÃ is still a diagonally dominant matrix. Now as illustrated in Theorem 1, we obtain a graph G = (V,Ã), where eigenvalues ofÃ λ 1 ≥λ 2 ≥ · · · , ≥λ N represent graph frequencies from low to high.</p><p>Disentangling Point Clouds into Sharp and Gentle Variation Components. In 2D image processing, high frequency component corresponding to intense pixel variation (edge) in spatial domain gets high response while low frequency component (smooth area) gets very low response after being processed by a high-pass filter. Here we aim to design a graph filter on our constructed G = (V,Ã) to select the points belongs to the contour and flat areas of 3D objects. Following Eq. (1), the key to design this graph filter is to construct the corresponding polynomial format of h(Ã).</p><p>Here we use the Laplacian operator as , where L = 2, h 0 = 1, h 1 = −1, making the polynomial format of the graph filter to be h(Ã) = I −Ã. This filter </p><formula xml:id="formula_6">h(Ã) =   1 −λ 1 0 · · · 0 0 1 −λ 2 · · · 0 . . . . . . . . . . . . 0 0 · · · 1 −λ N   .<label>(4)</label></formula><p>In this way, the eigenvaluesλ i are in a descending order, which represents that frequencies are ordered ascendingly according to Theorem 1. Due to the corresponding frequencies response 1 −λ i &lt; 1 −λ i+1 , the low frequency part is weakened after this filter. Hence, we call this filter h(Ã) = I −Ã a high-pass filter. Note: The eigenvalues representing frequencies is only for deducing the polynomial format of our high-pass filter h(Ã). The implementation of this filter only requires the calculation ofÃ.</p><p>Next, we apply h(Ã) to filter the point set X and get a filtered point set h(Ã)X. Due to h(Ã) = I −Ã, each point in h(Ã)X can be formulated as:</p><formula xml:id="formula_7">h(Ã)X i = x i − N jÃ i,j x j .<label>(5)</label></formula><p>When the distance between two point x i , x j is less than the threshold τ , A i,j remains non-zero value. Here h(Ã)X i actually equals to the difference between a point feature and the linear convex combination of its neighbors' features, which reflects the degree of each point's variation to its neighbors. Finally, we calculate the l 2 -norm of Eq. (5) at every point, and the larger l 2 -norm at a given point reflects sharp variation and means this point belongs to the contour of a 3D object, which is consistent to the edge areas obtained by a high-pass filter in 2D images. We put all original points in descending order as X o = [x 1 ,x 2 , · · · ,x N ] T according to l 2 -norm of Eq. (5). Following this order, we select the first M points X s = [x 1 ,x 2 , · · · ,x M ] T ∈ R M ×C called as sharp-variation component and the last M points </p><formula xml:id="formula_8">X g = [x N −M +1 ,x N −M +2 , · · · ,x N ] T ∈ R M ×C denoted by gentle-variation component.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sharp-Gentle Complementary Attention Module</head><p>The sharp and gentle variation components play different but complementary roles in representing the 3D object geometries, which should not be treated equally. However, most previous methods operate all points or local point sets equally. To solve this issue and utilize different variation components gained from Geometry-Disentangle Module, we design Sharp-Gentle Complementary Attention Module inspired by <ref type="bibr" target="#b27">(Vaswani et al. 2017;</ref><ref type="bibr" target="#b29">Wang et al. 2018)</ref>. It regards the features from two variation components as two holistic geometric representations and generates two attention matrices separately according to the geometric correlation. Then our module assigns the corresponding attention weights to features from two different variation components while respectively integrating them with the original input point features. The details of Sharp-Gentle Complementary Attention Module are shown in <ref type="figure">Fig. 3</ref> and elaborated below.</p><p>Attention Matrix. According to Sec. 4.1, we have original point cloud features X o , features of sharp-variation component X s and features of gentle-variation component X g . These features are firstly encoded by different nonlinear functions, and then are utilized to calculate different attention matrices as the following equation:</p><formula xml:id="formula_9">W s = Θ o (X o )·Θ s (X s ) T , W g = Φ o (X o )·Φ g (X g ) T ,<label>(6)</label></formula><p>where different nonlinear functions Θ o , Θ s , Φ o and Φ g are implemented by different MLPs without sharing parameters. In this way, we get two learnable adjacency matrices W s ∈ R N ×M and W g ∈ R N ×M , where M is the number of points in either X s or X g . Each row of W s or W g corresponds to attention weights between each original point feature and features from sharp and gentle variation components, respectively. Because the adjacency matrices W s and W g are computed as feature dot product, they can explicitly measure the semantic correlation or discrepancy between points.</p><p>Geometric Complementary Understanding. Next we apply the attention matrices W s and W g respectively to the features from sharp and gentle variation components so that the network can pay different attentions to them while fusing them with the original point features. The whole fusion procedure can be formulated as the following:</p><formula xml:id="formula_10">Y s = X o + W s · Ψ s (X s ) , (7) Y g = X o + W g · Ψ g (X g ) ,<label>(8)</label></formula><formula xml:id="formula_11">in element-wise: (Y s ) i = (X o ) i + M j=1 (W s ) ij · Ψ s ((X s ) j ),<label>(9)</label></formula><formula xml:id="formula_12">(Y g ) i = (X o ) i + M j=1 (W g ) ij · Ψ g ((X g ) j ),<label>(10)</label></formula><p>where two different nonlinear functions Ψ s and Ψ g are utilized to refine X s and X g . They are implemented by different MLPs without sharing parameters. Last we concatenate the features as the following equation:</p><formula xml:id="formula_13">Z = Y s ⊕ Y g .</formula><p>(11) Accordingly, our method regards features from sharp and gentle variation components as two holistic representations, i.e., all the point features with different attention weights from these two components are respectively linked with each original input point cloud feature. Our module explicitly conveys the complementary geometric information and the most relevant and key geometric interest to the network in a holistic way for better understanding 3D point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Self-Attention or Sharp-Gentle Attention</head><p>Self-Attention is an alternative holistic fusion strategy that pays different attentions to each point feature while linking them with the original point cloud itself, instead of integrating the feature from sharp and gentle variation components.</p><p>However, self-attention unavoidably brings large redundancy due to it operates all points together, making it hard to capture the most related geometric interest to the network. Yet this issue is alleviated after disentangling point clouds, where our model easily pays different attentions to two variation components carried with few redundancy and distinct geometric information.</p><p>To verify this, we compare our module with applying self-attention fusion on original point features and visualize the attention weights distribution in <ref type="figure">Fig. 5</ref>. The attention weights in self-attention are assigned irregularly, which indicates that self-attention method is limited to capture the most relevant geometric information. By contrast, through our module, different point features from sharp and gentle variation components are assigned different weights based on the geometric correlation with the anchor point in the original input. As shown in <ref type="figure">Fig. 5</ref>, a point in one leg of desk pays more attention to the points belongs to that leg among sharpvariation component and the relatively geometry-correlated points among gentle-variation component.</p><p>Therefore, compared with self-attention, our attention module drives the network to easily capture the distinct and complementary geometric information with less redundancy from two disentangled components. The quantitative comparison is listed in <ref type="table" target="#tab_6">Table 4</ref>. <ref type="figure">Figure 4</ref>: GDANet architecture for classification and segmentation. Our network disentangles the original point cloud into sharp and gentle variation components in different semantic levels, then fuses features from these two components with the input point features to supplement the KNN local context. <ref type="figure">Figure 5</ref>: Visualization of the attention weights distribution on Sharp-Gentle Complementary Attention and Selfattention. We select some points in original point clouds as the anchor points (in circle) to investigate their attention distributions, where the points drawn in red are assigned higher attention weights and the points drawn in blue are assigned lower attention weights with respect to anchor points. We observe that anchor points pay different attentions to points based on the geometric correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Geometry-Disentangled Attention Network</head><p>As illustrated in <ref type="figure">Fig. 4</ref>, we combine Geometry-Disentangle Module (GDM) and Sharp-Gentle Complementary Attention Module (SGCAM) as basic blocks to design Geometry-Disentangled Attention Network for 3D point cloud analysis. Before each block, features of each point and its Knearest neighbors (KNN) are concatenated through the local operator. In each block, an adjacency matrix is constructed in the feature space through GDM to disentangle points into sharp and gentle variation components. Then we fuse features from these two components with the original input of the block through SGCAM. A residual connection <ref type="bibr" target="#b4">(He et al. 2016</ref>) is applied at the end of each block. Two basic blocks are followed by another local operator. After the last MLP layer, the final global representation from max-pooling followed by fully connected and softmax layers is configured to the classification task. For the segmentation task, the outputs of the last MLP layer are directly passed through the fully connected as well as softmax layers to generate perpoint scores for semantic labels. Note that each original input point is not only integrated with its nearest neighbors to capture local structure, but also linked with all of the disentangled sharp and gentle variation components that beyond the local area to describe distinct and complementary 3D geometries. <ref type="table" target="#tab_6">Table 4</ref> shows the quantitative comparison of applying our modules with only using KNN.</p><p>Dynamic Adjacency Matrix Calculation. Inspired by <ref type="bibr" target="#b30">(Wang et al. 2019b)</ref>, the adjacency matrix at the beginning of the GDM in each block is calculated in a dynamic way depending on the features learned in different semantic levels. <ref type="figure" target="#fig_0">Fig. 2 (b)</ref> shows that our module successfully disentangles points into sharp and gentle components in various objects. This disentanglement module is jointly optimized during learning to help the network better model the geometric structure of objects. <ref type="table" target="#tab_9">Table 6</ref> in Sec 5.2 suggests the quantitative comparison of this dynamic operation with the operation of pre-selecting points before training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our network on shape classification task and part segmentation task on various datasets. Furthermore, we provide other experiments to analyze our network in depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">3D Point Cloud Processing</head><p>Object Classification. We firstly evaluate GDANet on ModelNet40 <ref type="bibr" target="#b32">(Wu et al. 2015)</ref>. It contains 9843 training models and 2468 test models in 40 categories and the data is uniformly sampled from the mesh models by <ref type="bibr" target="#b18">(Qi et al. 2017a</ref>). Same with <ref type="bibr" target="#b30">(Wang et al. 2019b)</ref>, the training data is augmented by randomly translating objects and shuffling the position of points. Similar to <ref type="bibr">(Qi et al. 2017a,b)</ref>, we perform several voting tests with random scaling and average the predictions during testing. <ref type="table" target="#tab_2">Table 2</ref> lists the quantitative comparisons with the state-of-the-art methods. GDANet outperforms other methods using only 1024 points as the input.</p><p>Our model is also tested on ScanObjectNN by <ref type="bibr">(Uy et al. 2019)</ref>, which is used to investigate the robustness to noisy objects with deformed geometric shape and non-uniform surface density in the real world. We adopt our model on the OBJ ONLY (simplest variant of the dataset) and OBJ BG (more noisy background). Sample objects of these variants are shown in <ref type="figure" target="#fig_2">Fig. 6</ref>. We retrain the methods listed in <ref type="bibr">(Uy et al. 2019)</ref>   <ref type="bibr" target="#b15">(Mao, Wang, and Li 2019)</ref> 84.0 <ref type="bibr" target="#b11">(Lin, Huang, and Wang 2020)</ref> 82.  <ref type="bibr" target="#b18">(Qi et al. 2017a)</ref> 1K points 89.2 PointNet++ <ref type="bibr" target="#b19">(Qi et al. 2017b)</ref> 1K points 90.7 SCN <ref type="bibr" target="#b34">(Xie et al. 2018)</ref> 1K points 90.0 KCNet <ref type="bibr" target="#b23">(Shen et al. 2018)</ref> 1K points 91.0 PointCNN <ref type="bibr" target="#b9">(Li et al. 2018)</ref> 1K points 92.2 PointWeb <ref type="bibr" target="#b39">(Zhao et al. 2019)</ref> 1K points 92.3 Point2Sequence <ref type="bibr" target="#b12">(Liu et al. 2019a)</ref> 1K points 92.6 DGCNN <ref type="bibr" target="#b30">(Wang et al. 2019b)</ref> 1K points 92.9 KPConv <ref type="bibr" target="#b24">(Thomas et al. 2019)</ref> 1K points 92.9 InterpCNN <ref type="bibr" target="#b15">(Mao, Wang, and Li 2019)</ref> 1K points 93.0 DensePoint  1K points 93.2 Geo-CNN <ref type="bibr" target="#b8">(Lan et al. 2019)</ref> 1K points 93.4 RS-CNN <ref type="bibr" target="#b14">(Liu et al. 2019c)</ref> 1K points 93.6 3D-GCN <ref type="bibr" target="#b11">(Lin, Huang, and Wang 2020)</ref> 1K points 92.1 FPConv  1K points 92.5 GSNet <ref type="bibr" target="#b35">(Xu, Zhou, and Qiao 2020)</ref> 1K points 92.9 GDANet(ours) 1K points 93.8 PointNet++ <ref type="bibr" target="#b19">(Qi et al. 2017b)</ref> 5K points+normal 91.9 PointConv <ref type="bibr" target="#b31">(Wu, Qi, and Fuxin 2019)</ref> 1K points+normal 92.5  are summarized in <ref type="table" target="#tab_4">Table 3</ref>, where our model gets the highest accuracy and the lowest performance drop from OBJ ONLY to OBJ BG. This proves the practicality of our method in the real world.</p><formula xml:id="formula_14">86.3 - - - - - - - - - - - - - - - - 3D-GCN</formula><p>Shape Part Segmentation. Shape Part segmentation is a more challenging task for fine-grained shape recognition. We employ ShapeNet Part <ref type="bibr" target="#b37">(Yi et al. 2016</ref>   GDANet performs better on objects with obvious geometric structure such as bag, mug and table. <ref type="figure" target="#fig_3">Fig. 7</ref> visualizes some segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Network Analysis</head><p>Ablation Study. The results are summarized in <ref type="table" target="#tab_6">Table 4</ref>. When the input point cloud is fused with features from both sharp and gentle variation components, the network achieves the best accuracy with 93.4%. GDANet also surpasses the architecture of only using KNN with 1.2%↑. Eventually, our method obtains an accuracy of 93.8% with voting tests. This experiment supports our claim that the disentangled sharp and gentle variation components cooperatively provide different and complementary geometric information to supplement KNN local semantics. Furthermore, we replace the selection of sharp and gentle variation components with random and Furthest Point Selection (FPS) in GDM. The results are listed in <ref type="table" target="#tab_8">Table 5</ref>, where disentangling point clouds into sharp and gentle variation components gets the highest accuracy and is noticeably robust on the noisy dataset ScanObjectNN <ref type="bibr">(Uy et al. 2019)</ref>. Empirically, our disentanglement strategy selects points carried with informative geometries instead of noisy points, which improves the noise robustness.</p><p>Moreover, the result of dynamic adjacency matrix calculation (Sec 4.4) is shown in <ref type="table" target="#tab_9">Table 6</ref>. We pre-compute the adjacency matrix on 3D coordinates to pre-disentangle different variation components, and fuse the features from them,   <ref type="figure" target="#fig_0">Fig. 2</ref> shows that our disentanglement strategy successfully decompose points into sharp (contour) and gentle (flat area) variation components. Last, we investigate the impact of the number of selected points M in GDM, which is shown in <ref type="table" target="#tab_10">Table 7</ref>. GDANet performs best when selecting 25% of input points, which proves the benefit of our disentanglement strategy. When the number of selected points equals to the number of input, it indicates self-attention.</p><p>Robustness Analysis. First, the robustness of our network on sampling density is tested by using sparser points as the input to GDANet trained with 1024 points. <ref type="figure">Fig. 8</ref> shows the result. Although sparser points bring more difficulties, GDANet still performs consistently at all density settings.</p><p>Moreover, <ref type="table" target="#tab_5">Table 8</ref> summarizes the results of rotation robustness, where GDANet performs best especially with 2.7%↑ than the second best at (s/s).</p><p>Last, our model is tested on ScanObjectNN <ref type="bibr">(Uy et al. 2019</ref>) that consists of noisy objects with deformed geometric shape and non-uniform surface density. <ref type="table" target="#tab_4">Table 3</ref> shows GDANet gains the lowest accuracy drop from OBJ ONLY to OBJ BG, which proves the noise robustness of GDANet.      Model Complexity. <ref type="table" target="#tab_12">Table 9</ref> shows the complexity by comparing the number of parameters. GDANet reduces the number of parameters by 84.9% and increases the accuracy with 0.9%↑ compared with KPConv <ref type="bibr" target="#b24">(Thomas et al. 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This work proposes GDANet for point cloud processing. Equipped with Geometry-Disentangle Module, GDANet dynamically disentangles point clouds into sharp and gentle variation components in different semantic levels, which respectively denotes the contour and flat area of a point cloud. Another core component is Sharp-Gentle Complementary Attention Module, which applies the attention mechanism to explore the relations between original points and different variation components to provide geometric complementary information for understanding point clouds. Extensive experiments have shown that our method achieves state-ofthe-art performances and decent robustness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of the process in our Geometry-Disentangle Module and the obtained sharp and gentle variation components of some objects. takes s c ∈ R N in this graph as the input and generates a filtered graph signal y c = h(Ã)s c ∈ R N . Following Theorem 2, the frequency response of h(Ã) with corresponding λ i is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2 (a)shows this pro-Figure 3: Visualization of Sharp-Gentle Complementary Attention Module. Features from different variation components are respectively integrated with the features from original point cloud, so that to provide complementary geometric information.cess andFig. 2 (b)visualizes sharp and gentle variation components disentangled by our trained network. We employ our Geometry-Disentangle Module on point features in different semantic levels, which is elaborated in Sec. 4.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of objects in ScanObjectNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Part Semantic Segmentation examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy (%) on ModelNet40 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Classification results (%) on ScanObjectNN dataset</cell></row><row><cell>(noise robustness test).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Figure 8 :</head><label>8</label><figDesc>Density robustness test. (a). Point cloud with random point dropout. (b). Test results on ModelNet40 of using sparser points as the input to a model trained with 1024 points. For fair comparisons, all methods have no data enhancement of random point dropout during training.</figDesc><table><row><cell>knn self sharp gentle voting acc. (%)</cell></row><row><cell>91.5</cell></row><row><cell>92.2</cell></row><row><cell>91.7</cell></row><row><cell>92.6</cell></row><row><cell>92.7</cell></row><row><cell>92.7</cell></row><row><cell>92.4</cell></row><row><cell>93.4</cell></row><row><cell>93.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Geometry-Disentangled complementary effect to</cell></row><row><cell>supplement KNN information in GDANet on ModelNet40.</cell></row><row><cell>'knn' indicates KNN aggregation, 'self' means the input</cell></row><row><cell>point cloud is fused with itself by self-attention, 'sharp' and</cell></row><row><cell>'gentle' denote the input point cloud is fused with features</cell></row><row><cell>of sharp and gentle variation, 'voting' is the voting strategy</cell></row><row><cell>during testing, respectively.</cell></row><row><cell>which gets 93.0% accuracy. Yet by dynamically calculating</cell></row><row><cell>the adjacency matrix on semantic features in GDM at differ-</cell></row><row><cell>ent levels, our network gains 0.8%↑. Our disentanglement</cell></row><row><cell>module is jointly optimized during training for modeling dif-</cell></row><row><cell>ferent geometric structures.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Classification results (%) of using different point selection methods in our Geometry-Disentangle Module.</figDesc><table><row><cell>method</cell><cell>acc. (%)</cell></row><row><cell>Precomputed</cell><cell>93.0</cell></row><row><cell>Dynamic</cell><cell>93.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Impact of dynamic strategy.</figDesc><table><row><cell>number 1024 512 256 128</cell></row><row><cell>acc. (%) 92.6 93.2 93.8 92.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Selecting different number of points in GDM on ModelNet40.</figDesc><table><row><cell>Method</cell><cell>z/z</cell><cell>s/s</cell></row><row><cell>PointNet(Qi et al. 2017a)</cell><cell cols="2">81.6 66.3</cell></row><row><cell cols="3">PointNet++(Qi et al. 2017b) 90.1 87.8</cell></row><row><cell cols="3">SpiderCNN(Xu et al. 2018) 83.5 69.6</cell></row><row><cell cols="3">DGCNN(Wang et al. 2019b) 90.4 82.6</cell></row><row><cell>GDANet(ours)</cell><cell cols="2">91.2 90.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Accuracy (%) comparisons of rotation on Model-Net40. z/z: both training and test sets are augmented by random rotation for z axis; s/s: both training and test sets are augmented by random rotation for three axis (x,y,z).</figDesc><table><row><cell>Method</cell><cell cols="2">#params acc. (%)</cell></row><row><cell>PointNet(Qi et al. 2017a)</cell><cell>3.50 M</cell><cell>89.2</cell></row><row><cell>PointNet++(Qi et al. 2017b)</cell><cell>1.48 M</cell><cell>90.7</cell></row><row><cell>KPConv(Thomas et al. 2019)</cell><cell>6.15 M</cell><cell>92.9</cell></row><row><cell>DGCNN(Wang et al. 2019b)</cell><cell>1.81 M</cell><cell>92.9</cell></row><row><cell cols="2">GSNet(Xu, Zhou, and Qiao 2020) 1.51 M</cell><cell>92.9</cell></row><row><cell>GDANet(ours)</cell><cell>0.93 M</cell><cell>93.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Comparisons of model complexity on ModelNet40.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>This work was supported in part by Guangdong Special Support Program under Grant (2016TX03X276), in part by the National Natural Science Foundation of China under Grant (61876176, U1713208), and in part by the Shenzhen Basic Research Program (CXB201104220032A), the Joint Laboratory of CAS-HK. This work was done during Mutian Xu's internship at Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<title level="m">Representation Learning: A Review and New Perspectives</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast Resampling of Three-Dimensional Point Clouds via Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vetro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kovačević</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="666" to="681" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-View 3D Object Retrieval With Deep Embedding Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2016.2609814</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5526" to="5537" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximation Capabilities of Multilayer Feedforward Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<idno type="DOI">10.1016/0893-6080(91)90009-T</idno>
		<idno>1016/0893-6080(91)90009-T</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal Unsupervised Image-to-image Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Escape From Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling Local Geometric Structure of 3D Point Clouds Using Geo-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution On X-Transformed Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FPConv: Learning Local Flattening for Point Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolution in the Cloud: Learning Deformable Kernels in 3D Graph Convolution Networks for Point Cloud Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Point2Sequence: Learning the Shape Representation of 3D Point Clouds with an Attention-based Sequence to Sequence Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DensePoint: Learning Densely Contextual Representation for Efficient Point Cloud Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5239" to="5248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relation-Shape Convolutional Neural Network for Point Cloud Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interpolated Convolutional Networks for 3D Point Cloud Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph Signal Processing: Overview, Challenges, and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kovačević</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2018.2820126</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="808" to="828" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Point-Net++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dolha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards 3D point cloud based object maps for household environments</title>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="927" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discrete Signal Processing on Graphs: Frequency Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSP.2014</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3042" to="3054" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">KPConv: Flexible and Deformable Convolution for Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data</title>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph Attention Convolution for Point Cloud Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local Neural Networks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic Graph CNN for Learning on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/3326362</idno>
		<ptr target="http://doi.acm.org/10.1145/3326362" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PointConv: Deep Convolutional Networks on 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A Deep Representation for Volumetric Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attentional ShapeCon-textNet for Point Cloud Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geometry Sharing Network for 3D Point Cloud Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12500" to="12507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spider-CNN: Deep Learning on Point Sets with Parameterized Convolutional Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Scalable Active Framework for Region Annotation in 3D Shape Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2980179.2980238</idno>
		<ptr target="http://doi.acm.org/10.1145/2980179.2980238" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
