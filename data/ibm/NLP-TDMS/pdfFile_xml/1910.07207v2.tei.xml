<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SOFT ACTOR-CRITIC FOR DISCRETE ACTION SETTINGS A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-21">October 21, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Christodoulou</surname></persName>
							<email>petros.christodoulou18@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SOFT ACTOR-CRITIC FOR DISCRETE ACTION SETTINGS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-21">October 21, 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Soft Actor-Critic is a state-of-the-art reinforcement learning algorithm for continuous action settings that is not applicable to discrete action settings. Many important settings involve discrete actions, however, and so here we derive an alternative version of the Soft Actor-Critic algorithm that is applicable to discrete action settings. We then show that, even without any hyperparameter tuning, it is competitive with the tuned model-free state-of-the-art on a selection of games from the Atari suite.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reinforcement Learning (RL) has famously made great progress in recent years, successfully being applied to settings such as board games <ref type="bibr" target="#b9">[Silver et al., 2017]</ref>, video games <ref type="bibr" target="#b7">[Mnih et al., 2015]</ref> and robot tasks <ref type="bibr" target="#b8">[OpenAI et al., 2018]</ref>. However, widespread adoption of RL in real-world domains has remained slow primarily because of its poor sample efficiency which <ref type="bibr" target="#b10">Wu et al. (2017)</ref> see as a "dominant concern in RL". <ref type="bibr" target="#b3">Haarnoja et al. (2018)</ref> provide the Soft Actor-Critic (SAC) algorithm which helps deal with this concern in continuous action settings. It has achieved model-free state-of-the-art sample efficiency in multiple challenging continuous control domains. Many domains however involve discrete rather than continuous actions and in these environments SAC is not currently applicable. This paper derives a version of SAC that is applicable to discrete action domains and then shows that it is competitive with the model-free state-of-the-art for discrete action domains in terms of sample efficiency on a selection of games from the Atari <ref type="bibr" target="#b0">[Bellemare et al., 2013]</ref> suite.</p><p>We proceed as follows: first we explain the derivation of Soft Actor-Critic for continuous action settings found in <ref type="bibr" target="#b3">Haarnoja et al. (2018)</ref> and <ref type="bibr" target="#b4">Haarnoja et al. (2019)</ref>, then we derive and explain the changes required to create a discrete action version of the algorithm, and finally we test the discrete action algorithm on the Atari suite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Soft Actor-Critic</head><p>Soft Actor-Critic <ref type="bibr" target="#b3">[Haarnoja et al., 2018]</ref> attempts to find a policy that maximises the maximum entropy objective:</p><formula xml:id="formula_0">π * = argmax π T t=0 E (st,at)∼τπ [γ t (r(s t , a t ) + αH(π(.|s t ))]<label>(1)</label></formula><p>where π is a policy, π * is the optimal policy, T is the number of timesteps, r : S × A → R is the reward function, γ ∈ [0, 1] is the discount rate, s t ∈ S is the state at timestep t, a t ∈ A is the action at timestep t, τ π is the distribution of trajectories induced by policy π, α determines the relative importance of the entropy term versus the reward and is called the temperature parameter, and H(π(.|s t ) is the entropy of the policy π at state s t and is calculated as H(π(.|s t )) = − log π(.|s t ).</p><p>To maximise the objective the authors use soft policy iteration which is a method of alternating between policy evaluation and policy improvement within the maximum entropy framework.</p><p>arXiv:1910.07207v2 [cs.</p><p>LG] 18 Oct 2019</p><p>The policy evaluation step involves computing the value of policy π. To do this they first define the soft state value function as:</p><formula xml:id="formula_1">V (s t ) := E at∼π [Q(s t , a t ) − α log(π(a t |s t ))]<label>(2)</label></formula><p>They then prove that in a tabular setting (i.e. when the state space is discrete) we can obtain the soft q-function by starting from a randomly initialised function Q : S × A → R and repeatedly applying the modified Bellman backup operator T π given by:</p><formula xml:id="formula_2">T π Q(s t , a t ) := r(s t , a t ) + γE st+1∼p(st,at) [V (s t+1 )]<label>(3)</label></formula><p>where p : S × A → S gives the distribution over the next state given the current state and action.</p><p>In the continuous state (instead of tabular) setting they explain that we instead firstly parameterise the soft q-function Q θ (s t , a t ) using a neural network with parameters θ. Then we train the soft Q-function to minimise the soft Bellman residual:</p><formula xml:id="formula_3">J Q (θ) = E (st,at)∼D [ 1 2 (Q θ (s t , a t ) − (r(s t , a t ) + γE st+1∼p(st,at) [Vθ(s t+1 )])) 2 ]<label>(4)</label></formula><p>where D is a replay buffer of past experiences and Vθ(s t+1 ) is estimated using a target network for Q and a monte-carlo estimate of (2) after sampling experiences from the replay buffer .</p><p>The policy improvement step then involves updating the policy in a direction that maximises the rewards it will achieve. To do this they use the soft Q-function calculated in the policy evaluation step to guide changes to the policy. Specifically, they update the policy towards the exponential of the new soft Q-function. Because they also want the policy to be tractable however, they restrict the possible policies to a parameterised family of distributions (e.g. Gaussian). To account for this, after updating the policy towards the exponential of the soft Q-function they then project it back into the space of acceptable policies using the information projection defined in terms of Kullback-Leibler divergence. So overall the policy improvement step is given by:</p><formula xml:id="formula_4">π new = argmin π∈Π D KL π(.|s t ) exp( 1 α Q π old (s t , .)) Z πold (s t )<label>(5)</label></formula><p>They note that partition function Z πold (s t ) is intractable but does not contribute to the gradient with respect to the new policy and so it can be ignored.</p><p>In the continuous state setting they parameterise the policy π φ (a t |s t ) using a neural network with parameters φ that outputs a mean and covariance that is then used to define a Gaussian policy. The policy parameters are then learned by minimizing the expected KL-divergence (5) after multiplying by the temperature parameter α and ignoring the partition function Z πold (s t ) as it does not impact the gradient:</p><formula xml:id="formula_5">J π (φ) = E st∼D [E at∼π φ [α log(π φ (a t |s t )) − Q θ (s t , a t )]]<label>(6)</label></formula><p>This involves taking an expectation over the policy's output distribution which means errors cannot be backpropagated in the normal way. To deal with this they use the reparameterisation trick <ref type="bibr" target="#b6">[Kingma and Welling, 2013</ref>] -instead of using the output of the policy network to form a stochastic action distribution directly, they combine its output with an input noise vector sampled from a spherical Gaussian. For example, in the one-dimensional case our network outputs a mean m and standard deviation s. We could randomly sample our action directly a ∼ N (m, s) but then we could not backpropagate the errors through this operation. So instead we do a = m + s where ∼ N (0, 1) which allows us to backpropagate as normal. To signify that they are reparameterising the policy in this way they write:</p><formula xml:id="formula_6">a t = f φ ( t ; s t )<label>(7)</label></formula><p>where t ∼ N (0, I). The new policy objective then becomes:</p><formula xml:id="formula_7">J π (φ) = E st∼D, t∼N [α log(π φ (f φ ( t ; s t )|s t )) − Q θ (s t , f φ ( t ; s t ))]<label>(8)</label></formula><p>where π φ is now defined implicitly in terms of f φ . They then go on to prove that in the tabular setting, alternating between policy evaluation and policy improvement as above will converge to the optimal policy. <ref type="bibr" target="#b4">Haarnoja et al. (2019)</ref> also provide an optional way of learning the temperature parameter so that we do not need to set it as a hyperparameter. They provide a long derivation for the temperature objective value, however because the details are not strictly relevant for our derivation of the discrete action version of SAC we do not repeat it here. The final objective they get to for the temperature parameter is however relevant and given by:</p><formula xml:id="formula_8">J(α) = E at∼πt [−α(log π t (a t |s t ) +H)]<label>(9)</label></formula><p>whereH is a constant vector equal to the hyperparameter representing the target entropy. They are unable to minimise this expression directly because of the expectation operator and so instead they minimise a monte-carlo estimate of it after sampling experiences from the replay buffer.</p><p>Lastly, in practice the authors maintain two separately trained soft Q-networks and then use the minimum of their two outputs to be the soft Q-network output in the above objectives. They do this because <ref type="bibr" target="#b2">Fujimoto, Hoof, and Meger (2018)</ref> showed that it helps combat state-value overestimation.</p><p>3 Soft Actor-Critic for Discrete Action Settings (SAC-Discrete)</p><p>We now derive a discrete action version of the above SAC algorithm. The first thing to note is that all the critical steps involved in deriving the objectives above hold whether the actions are continuous or discrete. All that changes is that π φ (a t |s t ) now outputs a probability instead of a density. Therefore the three objective functions J Q (θ) (4), J π (φ) (6) and J(α) (9) still hold. We must however make five important changes to the process of optimising these objective functions:</p><p>i) It is now more efficient to have the soft Q-function output the Q-value of each possible action rather than simply the action provided as an input, i.e. our Q function moves from Q : S × A → R to Q : S → R |A| . This was not possible before when there were infinitely many possible actions we could take.</p><p>ii) There is now no need for our policy to output the mean and covariance of our action distribution, instead it can directly output our action distribution. The policy therefore changes from π : S → R 2|A| to π : S → [0, 1] |A| where now we are applying a softmax function in the final layer of the policy to ensure it outputs a valid probability distribution.</p><p>iii) Before, in order to minimise the soft Q-function cost J Q (θ) (4) we had to plug in our sampled actions from the replay buffer to form a monte-carlo estimate of the soft state-value function (2). This was because estimating the soft state-value function in (2) involved taking an expectation over the action distribution. However, now, because our action set is discrete we can fully recover the action distribution and so there is no need to form a monte-carlo estimate and instead we can calculate the expectation directly. This change should reduce the variance involved in our estimate of the objective J Q (θ) (4). This means that we change our soft state-value calculation equation from (2) to:</p><formula xml:id="formula_9">V (s t ) := π(s t ) T [Q(s t ) − α log(π(s t ))]<label>(10)</label></formula><p>iv) Similarly, we can make the same change to our calculation of the temperature loss to also reduce the variance of that estimate. The temperature objective changes from (9) to:</p><formula xml:id="formula_10">J(α) = π t (s t ) T [−α(log(π t (s t )) +H)]<label>(11)</label></formula><p>v) Before, to minimise J π (φ) (6) we had to use the reparameterisation trick to allow gradients to pass through the expectations operator. However, now our policy outputs the exact action distribution we are able to calculcate the expectation directly. Therefore there is no need for the reparameterisation trick and the new objective for the policy changes from (8) to:</p><formula xml:id="formula_11">J π (φ) = E st∼D [π t (s t ) T [α log(π φ (s t )) − Q θ (s t )]]<label>(12)</label></formula><p>Combining all these changes, our algorithm for SAC with discrete actions (SAC-Discrete) is given by Algorithm 1.</p><p>Algorithm 1 Soft Actor-Critic with Discrete Actions (SAC-Discrete)</p><formula xml:id="formula_12">Initialise Q θ1 : S → R |A| , Q θ2 : S → R |A| , π φ : S → [0, 1] |A| Initialise local networks InitialiseQ θ1 : S → R |A| ,Q θ2 : S → R |A| Initialise target networks θ 1 ← θ 1 ,θ 2 ← θ 2</formula><p>Equalise target and local network weights D ← ∅ Initialize an empty replay buffer for each iteration do for each environment step do a t ∼ π φ (a t |s t ) Sample action from the policy s t+1 ∼ p(s t+1 |s t , a t )</p><p>Sample transition from the environment D ← D ∪ {(s t , a t , r(s t , a t ), s t+1 )} Store the transition in the replay buffer for each gradient step do  </p><formula xml:id="formula_13">θ i ← θ i − λ Q∇θi J(θ i ) for i ∈ {1, 2} Update the Q-function parameters φ ← φ − λ π∇φ J π (φ) Update policy weights α ← α − λ∇ α J(α) Update temperaturē Q i ← τ Q i + (1 − τ )Q i for i ∈ {1,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>To test the effectiveness of SAC-Discrete we run it for 100,000 steps on 20 Atari games for 5 random seeds each and compare its results with Rainbow which is a state-of-the-art model-free algorithm in terms of sample efficiency. The games vary significantly and were chosen a piori and so we believe the results on these 20 games are a good estimate for relative performance on the whole Atari suite of 49 games. We chose to run the algorithm for 100,000 steps because we are most interested in sample efficiency and <ref type="bibr" target="#b5">Kaiser et al. (2019)</ref> demonstrated that Rainbow can make significant progress on Atari games within 100,000 steps.</p><p>For SAC-Discrete actions we did no hyperparameter tuning and instead used a mixture of the hyperparameters found in <ref type="bibr" target="#b4">Haarnoja et al. (2019)</ref> and <ref type="bibr" target="#b5">Kaiser et al. (2019)</ref>. The hyperparameters can be found in Appendix B. The Rainbow results we compare to come from <ref type="bibr" target="#b5">Kaiser et al. (2019)</ref> and as they explain were the result of a significant amount of hyperparameter tuning. <ref type="bibr" target="#b11">1</ref> Therefore we are comparing the tuned Rainbow algorithm to our untuned SAC algorithm and so it is highly likely the relative performance of SAC could be improved if we spent time tuning its hyperparameters.</p><p>We find that SAC-Discrete achieves a better score than Rainbow in 10 out of 20 games with a median performance of -1%, maximum performance of +4330% and minimum of -99% - <ref type="figure" target="#fig_0">Figure 1</ref> summarises the results and Appendix A provides them in a table. Overall, we therefore consider the SAC-Discrete algorithm as roughly competitive with the model-free state-of-the-art on the Atari suite in terms of sample efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The original Soft Actor-Critic algorithm achieved state-of-the-art results on numerous continuous action settings but was not applicable to discrete action settings. To correct this we have derived a version of the algorithm called SAC-Discrete that is applicable to discrete action settings and have shown that it performs competitively with the model-free state-of-the-art on the Atari suite even without any hyperparameter tuning. We provide a Python implementation of the algorithm at the project's GitHub repository. 2 Appendix A SAC and Rainbow Atari Results <ref type="table">Table 1</ref>: SAC and Rainbow results on 20 Atari games. The mean SAC result of 5 random seeds is shown with the standard deviation in brackets. As a benchmark we also provide a column indicating the score an agent would get if it acted purely randomly. The Rainbow results come from <ref type="bibr" target="#b5">Kaiser et al., 2019</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Game</head><p>Random  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparing SAC-Discrete to Rainbow for 20 Atari games. The graph shows the average relative performance of SAC-Discrete over Rainbow over 5 random seeds where evaluation scores are calculated at the end of 100,000 steps of training. Note that no hyperparameter tuning was done to support the SAC scores compared to the Rainbow scores which benefited from substantial hyperparameter tuning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Hyperparameters used for SAC-Discrete results</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Layers</cell><cell>3 convolutional layers and 2 fully connected layers</cell></row><row><cell>Convolutional channels per layer</cell><cell>[32, 64, 64]</cell></row><row><cell>Convolutional kernel sizes per layer</cell><cell>[8, 4, 3]</cell></row><row><cell>Convolutional strides per layer</cell><cell>[4, 2, 1]</cell></row><row><cell>Convolutional padding per layer</cell><cell>[0, 0, 0]</cell></row><row><cell>Fully connected layer hidden units</cell><cell>[512, number of moves in game]</cell></row><row><cell>Batch size</cell><cell>64</cell></row><row><cell>Replay buffer size</cell><cell>1,000,000</cell></row><row><cell>Discount rate</cell><cell>0.99</cell></row><row><cell>Steps per learning update</cell><cell>4</cell></row><row><cell>Learning iterations per round</cell><cell>1</cell></row><row><cell>Learning rate</cell><cell>0.0003</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>Weight initialiser</cell><cell>He</cell></row><row><cell>Fixed network update frequency</cell><cell>8000</cell></row><row><cell>Loss</cell><cell>Mean squared error</cell></row><row><cell>Clip rewards</cell><cell>Clip to [-1, +1]</cell></row><row><cell>Initial random steps</cell><cell>20,000</cell></row><row><cell>Entropy target</cell><cell>0.98 * (-log (1 / |A|))</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Arcade Learning Environment: An Evaluation Platform for General Agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dopamine: A Research Framework for Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gelanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Addressing Function Approximation Error in Actor-Critic Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
		<title level="m">Soft Actor-Critic Algorithms and Applications</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Model Based Reinforcement Learning for Atari</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Osinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czechowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kozakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>In: arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human-Level Control Through Deep Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In: Nature</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning Dexterous In-Hand Manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciek</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Chociej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><forename type="middle">W</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Petron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno>ArXiv abs/1808.00177</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Mastering the Game of Go Without Human Knowledge&quot;. In: Nature</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalable Trust-Region Method for Deep Reinforcement Learning Using Kronecker-Factored Approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">provide no results for Rainbow and so for these two games only we ran the Rainbow algorithm ourselves. We used the Dopamine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">)</forename><surname>Spaceinvaders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<ptr target="https://colab.research.google.com/drive/11prfRfM5qrMsfUXV6cGY868HtwDphxaF2https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch" />
		<editor>Kaiser et al.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>We share the code used to do this in the colaboratory notebook</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
