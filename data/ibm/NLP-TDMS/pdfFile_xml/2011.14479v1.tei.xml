<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-scale Adaptive Task Attention Network for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxing</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxiong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Li</surname></persName>
							<email>yaohuili@smail.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlin</forename><surname>Chen</surname></persName>
							<email>clchen@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-scale Adaptive Task Attention Network for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of few-shot learning is to classify unseen categories with few labeled samples. Recently, the low-level information metric-learning based methods have achieved satisfying performance, since local representations (LRs) are more consistent between seen and unseen classes. However, most of these methods deal with each category in the support set independently, which is not sufficient to measure the relation between features, especially in a certain task. Moreover, the low-level information-based metric learning method suffers when dominant objects of different scales exist in a complex background. To address these issues, this paper proposes a novel Multi-scale Adaptive Task Attention Network (MATANet) for few-shot learning. Specifically, we first use a multi-scale feature generator to generate multiple features at different scales. Then, an adaptive task attention module is proposed to select the most important LRs among the entire task. Afterwards, a similarity-to-class module and a fusion layer are utilized to calculate a joint multi-scale similarity between the query image and the support set. Extensive experiments on popular benchmarks clearly show the effectiveness of the proposed MATANet compared with state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans can learn new concepts and objects with only one or a few samples easily. In order to imitate this ability of humans, many few-shot learning methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> have been proposed. However, most of these methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28]</ref> adopt image-level features for classification. Due to the scarcity of samples in few-shot image recognition tasks, classifying at such a level may not be effective enough. Instead, many methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> based on low-level information were proposed, i.e., local representations (LRs) of feature embeddings. These methods use low-level information to measure the distance between query images and support images, and they can achieve better recognition results. However, these methods do not measure the similarity between query images and support  <ref type="figure">Figure 1</ref>. The two main problems of the previous local representation based methods. (a) In different tasks, the most discriminative features are different. In task 1, the beak is the key distinguishing feature, while the most critical feature is the wing in task 2. (b) The scales of the dominant objects varies from image to image.</p><p>images in the context of the whole task, which does not make full use of the representation ability of local feature descriptors. When humans classify an image into one of several unseen classes, it is natural to look for semantic features that are shared only between certain classes and the query image. In other words, humans do not pay much attention to the features shared between classes when recognizing a category they have not seen before. For example, consider two 5-way 1-shot tasks in <ref type="figure">Figure 1(a)</ref>. In task1, we need to recognize a 'Black-footed Albatross' among 'Laysan Albatross', 'Heermann Gull', 'California Gull' and 'Rhinoceros Auklet'. While in task 2, we need to recognize a 'Black-footed Albatross' among 'European Goldfinch', 'Blue Grosbeak', 'Pine Grosbeak' and 'Yellow Warbler'. For task 1, the beak is a very distinguishing feature, but for task 2 it is not the most critical feature. Similarly, the wing is more important for task 2 than task 1. In summary, the importance of each LRs varies from task to task. As previously mentioned, although these existing methods can extract the relation between the query image and each support set independently, they do not consider the importance of each LRs under the whole task, and all the LRs are weighted equally, rather than the task-relevant LRs enjoy the higher weights. Moreover, these methods can only calculate their similarity at a single scale. As shown in <ref type="figure">Figure 1(b)</ref>, due to the scales of dominant objects in different images are dissimilar, we argue that it is more reasonable to calculate the similarity between query image and support set at multiple scales simultaneously.</p><p>To this end, we propose a novel Multi-scale Adaptive Task Attention Network for metric-learning based few-shot learning, which can be trained in an end-to-end manner. First, we represent all images as a collection of LRs at different scales by a multi-scale feature generator, rather than a global feature representation at the image level. Second, we measure the semantic similarity between the query image and the support set by calculating the semantic relation matrix. Afterwards, we employ an adaptive task attention module to select the most distinguishing feature in the current task. Third, to further make full use of LRs, we employ a similarity-to-class mechanism to determine which support class the query image belongs to at each scale. Finally, we adaptively fuse the similarities calculated from the features of different scales together.</p><p>To sum up, the main contributions are summarized as follows:</p><p>• To generate different scales features, we propose a multi-scale feature generator in few-shot learning tasks, which can provide multi-scale information for more comprehensive measurements. • We further propose a novel adaptive task attention mechanism by finding and weighing most discriminative local representations in the entire task, aiming to learn task-relevant feature representations for few-shot learning. • We conduct sufficient experiments on four benchmark datasets to verify the advancement of our model, and the performance of our model achieves the state-ofthe-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the recent few-shot learning literature, there are roughly two types of methods: meta-learning based method and metric-learning based method.</p><p>Meta-learning based methods. The main idea of metalearning based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13]</ref> is how to use existing experience or knowledge reasonably to achieve fast learning when faced with a new task, rather than starting from scratch. Santoro et al. <ref type="bibr" target="#b24">[25]</ref> adopted an LSTM to control the interaction between the network and the external memory module. Santoro et al. <ref type="bibr" target="#b22">[23]</ref> describe a new metalearning method by interpreting SGD update rules as a recursive gated model with trainable parameters. The purpose of MAML <ref type="bibr" target="#b5">[6]</ref> is to learn a good parameter initialization so that the model can quickly adapt to new tasks. Sun et al. <ref type="bibr" target="#b27">[28]</ref> proposed meta-transfer learning, which learns the scaling and shifting functions of DNN weight for each task. Jamel et al. <ref type="bibr" target="#b12">[13]</ref> proposed a task unbiased method, by introducing regularized loss terms to constrain the model to have as little preference for all tasks as possible when parameters are updated. Although these meta-learning based methods have achieved outstanding results on few-shot learning tasks, their complex memory addressing architecture is difficult to train. Compared with these methods, our proposed MATANet can be trained in an end-to-end manner.</p><p>Metric-learning based methods. The metric-learning based methods aim to learn an informative distance metric, as demonstrated by <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref>. Koch et al. <ref type="bibr" target="#b15">[16]</ref> used a Siamese Neural Network to tackle the one-shot learning problem. Snell et al. <ref type="bibr" target="#b26">[27]</ref> proposed Prototypical Networks, which first assumes that each type can be represented by a prototype, and the prototype can be obtained by calculating the mean value of the embedding representation of each class and then using a distance function to classify. In fact, we don't know which distance function is the best. Therefore, Sung et al. <ref type="bibr" target="#b28">[29]</ref> proposed a Relation Network to obtain the most suitable distance metric function through learning. The above methods are all based on the feature representation at the image level. Due to the scarcity of the number of samples, we can not well represent the distribution of each category on the image level features. In contrast, some recent work, such as DN4 <ref type="bibr" target="#b17">[18]</ref> and CovaMNet <ref type="bibr" target="#b18">[19]</ref> shows that the rich low-level features (i.e., LRs) have better representation capabilities. However, these methods measure the similarity between the query image and each support class independently, without considering the entire task together. Moreover, the methods based on low-level features only measure the similarity between query image and support set at a single scale, which may lead to a lower classification accuracy when the scales of dominant objects are different.</p><p>Unlike the above method, our MATANet calculates the similarity between the query image and the support set at multi-scale. And then we can obtain the final result through integrate multiple similarities from different scales. In addition, our MATANet can adaptively select task-relevant local features with discriminative semantics, as the process of hu-  the feature extractor F θ to learn local representations, the multi-scale feature generator g φ to generate multiple features at different scales, the adaptive task attention module Fϕ to generate adaptive task attention mask for selecting more important elements of semantic relation matrix, and the similarity-to-class module Fω to get a similarity score to determine which support class the query image belongs to. The black square indicates the predicted label. (Best view in color.) man recognition.</p><formula xml:id="formula_0">ℛ ! " Multi-scale Feature Generator ℒ ! ℒ " ℒ # ℒ $ ℒ %# ℒ %" ℒ %! ℒ %$ ℱ ' Fusion Layer ℒ ! ℒ "!</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Description on Few-shot Learning</head><p>In few-shot learning, there are usually three sets of data: a query set Q, a support set S, and an auxiliary set A. Note that Q and S share the same label space, while they have no intersection with the label space of A.</p><p>In this paper, we follow the definition of the common few-shot learning task. Given a support set that contains N previously unseen classes, with K samples for each class. We need to determine which class the query sample belongs to, which is called N-way K-shot tasks (e.g., 5-way 1-shot or 5-way 5-shot). To achieve this goal, we use an auxiliary set to train a model to learn transferable knowledge. The model is trained by the episodic training mechanism <ref type="bibr" target="#b29">[30]</ref>. In each episode, a new task is constructed randomly in A, and each task consists of two subsets: auxiliary support set A S and auxiliary query set A Q . Generally, in the training stage, hundreds of tasks are adopted to train the model.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, our MATANet is mainly composed of four modules: a feature extractor F θ , a multi-scale feature generator g φ , an adaptive task attention module F ϕ , and a similarity-to-class module F ω . All image samples are first fed into the F θ to get feature embeddings and rich LRs. In practice the feature extractor module could be 4layer CNN, ResNet12 <ref type="bibr" target="#b10">[11]</ref> or WRN <ref type="bibr" target="#b31">[32]</ref>. Then the multi-scale feature generator generates multiple features at different scales. Afterwards, semantic relation matrixes are calculated to measure the semantic relevance between query image and support set at each scale. The adaptive task attention module learns a task attention mask which can adaptively calculate the importance of each LR in the current task. And we use task attention masks to weighting semantic relation matrix to prominently display task-relevant elements. After that, the weighted semantic relation matrix is processed by similarity-to-class module F ω to determine which support class the query image belongs to. Finally, we adaptively fuse the similarities calculated from the features of different scales together by a learned vector. All the modules can be trained jointly in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-scale Feature Generator</head><p>As some recent studies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> on few-shot learning have proved, LRs show richer representation ability and can alleviate the problem of sample scarcity in few-shot learning. Therefore we use LRs to represent the features of each image. Given a query image A q Q through the feature extractor we can get a three-dimensional (3D) vector</p><formula xml:id="formula_1">F θ (A q Q ) ∈ R C×H×W .</formula><p>Under the N-way K-shot few-shot learning setting, there are K images for each support class in a certain task. Through feature extractor we can get a four-dimensional (4D) vector of support set S, which can be denoted as F θ (S) ∈ R N K×C×H×W .</p><p>The multi-scale feature generator aims to generate multi- ple features at different scales. As illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>, the multi-scale feature generator consists of five components: the first part has no processing unit, the input and output are the same; the second part has a 2 × 2 max-pooling layer; the third part has a 3 × 3 convolutional layer; the fourth part has a 5 × 5 convolutional layer; the fifth part has a 1 × 7 convolutional layer and a 7 × 1 convolutional layer.</p><formula xml:id="formula_2">Maxpool 2 × 2 leaky-ReLU BN Conv2d 1 × 7 leaky-ReLU BN Conv2d 5 × 5 leaky-ReLU BN Conv2d 7 × 1 ! (" # $ )/ ! (%) &amp; $' /&amp; %' &amp; $* /&amp; %* &amp; $+ /&amp; %+ &amp; $, /&amp; %, leaky-ReLU BN Conv2d 3 × 3 !" / #"</formula><p>Through the multi-scale feature generator, we can get the</p><formula xml:id="formula_3">3D features L qz ∈ R Cz×Hz×Wz , z ∈ {1, 2, 3, 4, 5}, which can be regarded as a set of H z × W z C z -dimensional LRs L qz = [x 1 , ..., x HzWz ] ∈ R Cz×HzWz<label>(1)</label></formula><p>where x i is the i-th LRs. Through multi-scale feature generator we can also get the LRs of support set S</p><formula xml:id="formula_4">L Sz = [x 1 , ..., x N KHzWz ] ∈ R Cz×N KHzWz (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adaptive Task Attention Module</head><p>Under the N-way K-shot few-shot learning setting, we calculate the semantic relation matrix R z between a query image A q Q and support set S to measure semantic relevance by LRs at the z-th scale. Then the R z can be calculated as below</p><formula xml:id="formula_5">R z i,j = cos(L qz i , L Sz j ) (3) cos(L qz i , L Sz j ) = (L qz i ) T L Sz j L qz i · L Sz j (4) where i ∈ {1, ..., H z W z } , j ∈ {1, ..., N KH z W z } , z ∈ {1, 2, 3, 4, 5}, R z i,j is (i, j) element of R z</formula><p>reflecting the distance between the i-th LR of the query image and the j-th LR of support set at the z-th scale and cos(·, ·) is Cosine distence function.</p><p>Each row in R z represents the semantic relation of each LR in the query image to all LRs of all images in the support set, i.e., semantic relation vector R z i represent the relation between i-th LR of query image A q Q to all N KH z W z LRs of support set at z-th scale. R z can be decomposed into N submatrices R zn , n ∈ {1, ..., N } according to columns, representing the semantic relation between the query image and each support class.</p><p>Then we can calculate the task attention score of each element of R z for the current task by</p><formula xml:id="formula_6">α z i = N KHzWz j=1 R z i,j HzWz i=1 N KHzWz j=1 R z i,j<label>(5)</label></formula><p>The task attention mask α z is consist of all task attention scores</p><formula xml:id="formula_7">α z i , i ∈ {1, ..., H z W z }. Afterwards, we use dot- product to weight R z i by α z i M z i = α z i · R z i<label>(6)</label></formula><p>where M z i is the i-th row of weighted semantic relation matrix. By this way, we can get the weighted semantic relation matrix M z , which can be decomposed into N submatrices M zn , n ∈ {1, ..., N } according to columns. While the semantic relations of task-irrelevant regions are suppressed; meanwhile, the semantic relations of task-relevant regions are enhanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Similarity-to-Class Module</head><p>Similarity-to-Class Module aims to determine which support class the query image belongs to. In this module, for each LR of the query image, we find the k most similar LR of all support LRs for class n. Then, we sum kH z W z selected LRs as the similarity score between the query image and the n-th support class at the z-th scale</p><formula xml:id="formula_8">P zn = kHzWz i=1 T opk(M zn i )<label>(7)</label></formula><p>where P zn means the similarity between the query image and support class n at the z-th scale and T opk(·) means selecting the k lagest elements in each row of the weighted semantic relation matrix M zn . Typically, we set k to 3 on the miniImagenet dataset. Since the small diversity within the different classes, we set k to 1 on three fine-grained datasets for our MATANet to capture the most discriminative features and avoid introducing noise. Under the N-way K-shot few-shot learning setting, we can get semantic similarity vectors P z ∈ R N . these five parts. Specifically, the final fusion similarity between a query A q Q and support set S can be defined as follows</p><formula xml:id="formula_9">P q = w 1 · P 1 + w 2 · P 2 + w 3 · P 3 + w 4 · P 4 + w 5 · P 5 (8)</formula><p>Under the 5-way 1-shot few-shot learning setting, input a query image, we will get five similarity vectors P z , z ∈ {1, 2, 3, 4, 5}. We first balance the size of these five vectors by a Batch Normalization layer. Then, we concatenate these five vectors and use a 1D convolution layer with the kernel size of 1 × 1 and the dilation value of 5. Then we can get a weighted 5-dimensional similarity vector P q , we use it for final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we perform extensive experiments to verify the advance and effectiveness of MATANet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>miniImageNet. As a small subset of ImageNet <ref type="bibr" target="#b3">[4]</ref>, The dataset consists of 100 categories, each containing 600 images. We use common splits as in <ref type="bibr" target="#b5">[6]</ref>, which devides the dataset into training, validation and test dataset with 64/16/20 classes respectively.</p><p>We also conduct experiments on three fine-grained image recognition datasets.</p><p>CUB Birds <ref type="bibr" target="#b30">[31]</ref> is composed of 11, 788 images of 200 birds species. Stanford Dogs <ref type="bibr" target="#b13">[14]</ref> is contains 120 categories of dogs and 20, 480 images. Stanford Cars <ref type="bibr" target="#b16">[17]</ref> consists of 196 categories of cars with 16, 185 images.</p><p>For fair comparisons, we strictly follow the splits used in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> on Stanford Dogs and Stanford Cars, and follow the splits used in <ref type="bibr" target="#b2">[3]</ref> on CUB Birds as <ref type="table" target="#tab_0">Table 1</ref> shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network architecture</head><p>It is a well-known fact that using deeper networks to extract features or using pre-trained models can achieve higher accuracy. We follow the basic feature extractor module which is adopted in previous works, to make a fair comparison with other works. The feature extractor module F θ consists of 4 convolutional blocks. Specifically, each convolutional block consists of a convolutional layer (with 3×3 convolution and 64 filters), a batch normalization layer, and a leaky ReLU non-linearity. Besides, we add a 2 × 2 maxpooling layer to the first two convolution blocks. The reason for using only two max-pooling layers is we can get more LRs to capture the semantic relation between them. For example, in a 5-way 1-shot few-shot learning task, if we use four maxpooling layers, we can only get 25 LRs for an 84 × 84 input image. In contrast, if we only use two maxpooling layers, we will get 441 LRs, which will be helpful for us to find local semantic relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>Our experiments are conducted under the N-way Kshot setting on four benchmarks. All the images in four benchmarks are resized to 84 × 84. During the training stage, we randomly construct 250, 000 episodes to train our MATANet for the miniImageNet and Stanford Cars, and 300,000 for the other datasets by episodic training mechanism. In each episode, we select 15 or 10 query images from each class for the 1-shot or 5-shot setting, respectively, i.e., in a 5-way 1-shot task, we have 75 query images and 5 support images. We adopt the Adam algorithm <ref type="bibr" target="#b14">[15]</ref> with a cross-entropy (CE) loss to train the network. Also, the initial learning rate is set to 0.001 and reduce it by half of every 50,000 episodes. During the test stage, 600 episodes are constructed from the test set, and this test process will be repeated five times. Then the mean accuracy and 95% confidence intervals will be reported simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Baselines</head><p>To evaluate the effectiveness of our MATANet on the miniImageNet dataset, we make comparisons with stateof-the art methods. Since our method is a metric-learning based method, we mainly compare our MATANet with methods in this branch, including Matching Nets <ref type="bibr" target="#b29">[30]</ref>, Prototypical Nets <ref type="bibr" target="#b26">[27]</ref>, Relation Nets <ref type="bibr" target="#b28">[29]</ref>, GNN <ref type="bibr" target="#b7">[8]</ref>, IMP <ref type="bibr" target="#b0">[1]</ref>, CovaMNet <ref type="bibr" target="#b18">[19]</ref>, DN4 <ref type="bibr" target="#b17">[18]</ref>, SAML <ref type="bibr" target="#b9">[10]</ref> and DSN <ref type="bibr" target="#b25">[26]</ref> . We also pick five meta-learning models Meta LSTM <ref type="bibr" target="#b22">[23]</ref>, MAML <ref type="bibr" target="#b5">[6]</ref>, TAML-Entropy <ref type="bibr" target="#b12">[13]</ref>, MAML+L2F <ref type="bibr" target="#b1">[2]</ref> and WarpGrad <ref type="bibr" target="#b6">[7]</ref> for reference.</p><p>We compare seven few-shot leraning methods on finegrained datasets, Matching Nets <ref type="bibr" target="#b29">[30]</ref>, Prototypical Nets <ref type="bibr" target="#b26">[27]</ref>, MAML <ref type="bibr" target="#b5">[6]</ref>, Relation Nets <ref type="bibr" target="#b28">[29]</ref>, CovaMNet <ref type="bibr" target="#b18">[19]</ref>, DN4 <ref type="bibr" target="#b17">[18]</ref>, and PABN +cpt /LRPABN +cpt <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparisons with the SOTA Methods</head><p>Our method is compared with several state-of-the-art methods under 5-way 1-shot and 5-way 5-shot few-shot learning settings.</p><p>Results on miniImageNet. The experimental results on miniImageNet are reported in <ref type="table">Table 2</ref>. It can be observed that our method significantly outperforms other methods under both 5-way 1-shot and 5-shot settings. Especially,  <ref type="bibr" target="#b17">[18]</ref>, ‡ results re-implemented in the same setting for a fair comparison. For CUB Birds, we adopt the results for first four methods from <ref type="bibr" target="#b2">[3]</ref>, re-implement CovaMNet <ref type="bibr" target="#b18">[19]</ref> and DN4 <ref type="bibr" target="#b17">[18]</ref>, and adopt the results reported by the original work for PABN+cpt/LRPABN+cpt <ref type="bibr" target="#b11">[12]</ref>. (Top two performances are in bold)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>we are 2.5% better than the second best method <ref type="bibr" target="#b6">[7]</ref> under the 5-way 1-shot setting, with an accuracy rate of 53.63%. Similarly, we achieve 72.67% under the 5-way 5-shot setting, with an improvement of 2.3% from the second best method <ref type="bibr" target="#b17">[18]</ref>. Note that, our model gains 4.7% and 2.3% improvements over the most relevant work [18] on 1-shot and 5-shot, respectively, which proposes an image-to-class mechanism to find the relation at class-level. This improvement verifies the effectiveness of our model, which can adaptively select the most discriminative local features at multiple scales in a certain task.</p><p>Results on fine-grained datasets. From <ref type="table">Table 3</ref>, it can be observed that the proposed MATANet outperforms all other state-of-the-art methods under both 5-way 1-shot and 5-way 5-shot few-shot learning settings. Especially for the 5-way 1-shot task, our method achieves 13.3%, 21.6%, and 5.8% gains over the second best on Stanford Dogs, Stanford Cars, and CUB Birds, respectively. For the 5-way 5-shot task, our method achieves 10.7%, 3.7%, and 3.0% gains over the second best on three datasets. The reason why we can achieve these state-of-the-art performances is that MATANet can adaptively select the task-relevant LRs at multiple scales for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Venue Backbone Type 5-Way Accuracy(%)  <ref type="table">Table 4</ref>. Comparison with other state-of-the-art methods that use deeper backbones with 95% confidence intervals on mini-ImageNet. The third column shows which kind of embedding is employed. The fourth column shows which type of method belongs to. * Results reported by the original work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Discussion</head><p>Influence of superparameter k. The results of the empirical study about using different k in MATANet. In the similarity-to-class module, for each LR of the query image, we need to search out the k most similar LRs of all support LRs in each class. Next, we adaptively integrate five relation scores that be calculated at different scales for final prediction. How to choose a suitable k is particularly significant. For this purpose, we conduct a contrast experiment on miniImageNet dataset under both 5-way 1-shot and 5-way 5-shot settings by varying the value of k ∈ {1, 3, 5, 7, 9}. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, the value of k has a moderate influence on classification performance, so we should choose a specific k for each task.</p><p>Influence of backbone networks. Besides the simple Conv-64F, we also use other deeper feature extractors to evaluate our model, i.e., ResNet12 and WRN-28-10. We compared other state-of-the-art methods that using  these deeper feature extractors, including Dynamic-Net <ref type="bibr" target="#b8">[9]</ref>, SNAIL <ref type="bibr" target="#b20">[21]</ref>, TPN <ref type="bibr" target="#b19">[20]</ref>, MAML+L2F <ref type="bibr" target="#b1">[2]</ref>, Qiao <ref type="bibr" target="#b21">[22]</ref>, LEO <ref type="bibr" target="#b23">[24]</ref>, Fine-tuning <ref type="bibr" target="#b4">[5]</ref> and LEO+L2F <ref type="bibr" target="#b1">[2]</ref>. When using deeper feature extractors, the accacy of MATANet reaches 60.13% and 62.43% for the 5-way 1-shot task, 75.42% and 79.13% for the 5-shot task, by using ResNet12 and WRN-28-10, repectively. Moreover, when using same deeper feature exactor, our MATANet outperforms all other methods under both 5-way 1-shot and 5-way 5-shot few-shot learning settings (see <ref type="table">Table 4</ref>).</p><p>Influence of metric functions. The results on the different metric functions using in Adaptive Task Attention Module are reported in <ref type="table" target="#tab_3">Table 5</ref>, and the Tanimito Index is represented as T (a, b) = a·b a · b −a·b . To measure the semantic relations between feature descriptors, a suitable metric functions is a key factors. It can be seen in <ref type="table" target="#tab_3">Table 5</ref>, the best metric function is Cosine Similarity.</p><p>Ablation study. To further verify the effectiveness of the multi-scale feature generator, adaptive task-attention module, and similarity-to-class module, we perform an ablation study on miniImageNet. We remove g φ , F ϕ and F ω from the MATANet respectively to confirm that each part of the model is indispensable. We remove g φ , F ϕ and F ω simul- <ref type="figure" target="#fig_5">Figure 5</ref>. Visualization of the selected LRs. In a 5-way 1-shot task, for each LR of the query image, our method can find the k (i.e., k=3) most similar LRs of all support LRs for a certain class, and weight them according to the importance of the query LR in the current task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>5-Way Accuracy(%) taneously as the baseline method. As seen in <ref type="table" target="#tab_4">Table 6</ref>, the main improvement comes from the adaptive task-attention module F ϕ . If we remove F ϕ , the performance will be reduced by 3.7%, 4.8% on 1-shot, 5-shot tasks, respectively. This empirical study proves that the discriminative F ϕ gives a performance boost and results in more discriminative features for classification. Similarly, if we remove g φ , the performance will be reduced by 1.3%, 0.9% on 1-shot, 5-shot tasks, respectively. Moreover, if we remove F ω , the performance will be reduced by 1.1%, 0.8% on 1-shot, 5-shot tasks, respectively. Visualization of the selected LRs. As shown in <ref type="figure" target="#fig_5">Figure  5</ref>, for the LRs in red, yellow, green and orange boxes in the query image, we visualized the k (i.e., k=3) most discriminative LRs selected by MATANet. In <ref type="bibr" target="#b17">[18]</ref>, they will equally use the selected LRs for the final classification. However, in our method, the LRs corresponding to these boxes are treated differently. In task 1, the task attention score corresponding to the red box and yellow box is 0.069 and 0.041, respectively, so the LRs corresponding to the red box will play a more important role in the final classification. Similarly, in task 2, the LRs corresponding to the orange box will play a more important role in the final classification. This is because the beak is obviously more discriminative than the wing in task 1. While in Task 2 the wing is significantly more discriminative than the beak, which verifies that our method can automatically select the most discriminative LRs in the current task. Moreover, it can be seen from <ref type="figure" target="#fig_5">Figure 5</ref> that the scales of the dominant objects in different images are different, which may affect the performance of the model. This once again verified the necessity of our multi-scale feature generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we revisit the local representation based metric-learning and propose a novel Multi-scale Adaptive Task Attention Network (MATANet) for few-shot learning, aiming to learn more discriminative task-relevant local representations at different scales by generating multiple features at different scale and looking at the context of the entire task. By taking a view of the entire task, our method is able to adaptively select the most discriminative local representations in the current task at different scales. Extensive experiments on four benchmark datasets demonstrate the effectiveness and advantages of the proposed MATANet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Visualization Results</head><p>In our article, we visualized the k (i.e., k=3) most discriminative LRs selected by MATANet. This is achieved by outputting their index and corresponding task attention values.</p><p>We provide some classification examples in <ref type="figure">Figure 6</ref>, compare with the most relevant work DN4. By sending a fixed testing batch through the model, which consists of one support sample and five query samples for each of five classes, the prediction of MATANet only contains 7 mislabels in the entire 25 queries, while the prediction of DN4 has 10 wrong labels. That validates the effectiveness of the MATANet. Our model is able to adaptively select the most discriminative local representations in the current task at different scales. We also find that in some classes like Black-footed Akbatross and Rhinoceros Auklet, the high intra-variance and low inter-variance confuse all the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Number of Trainable Parameters</head><p>We compare the number of trainable parameters to verify the efficiency of the proposed MATANet, as <ref type="table">Table 7</ref> shows. Since no other trainable parameters are introduced except for the embedding module F θ Prototypical Nets and DN4 become the most light-weight models. GNN adopts a larger embedding module (i.e., the filter number is 256), which draws a great contribution from the number of parameters. Relation Network and CovaMNet adopt additional architectures to boost the result, which also introduces a huge number of trainable parameters. However, the proposed MATANet only introduces a small number of the trainable parameters, while achieves a better result than the methods above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Params Accuracy(%)  <ref type="table">Table 7</ref>. The number of trainable parameters in different models and the corresponding classification accuracies on miniImageNet under 5-way 1-shot setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tranining procedure</head><p>The training procedure of the proposed MATANet is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation of MATANet</head><p>We provide a PyTorch implementation of MATANet for few-shot learning.</p><p>Our code is avaliable at https://github.com/chenhaoxing/MATANet.  <ref type="figure">Figure 6</ref>. Some visual classification results of comparing methods over CUB Birds dataset. Both DN4 and MATANet use the same data batch under the 5-way 1-shot setting, and for each class, we randomly select five query images as the testing data. We adopt five colors to label the support classes separately. As to the query images, we label the images with the color corresponding to the class label predicted by different models.</p><p>Algorithm 1 Tranining procedure Input:</p><formula xml:id="formula_10">Eposidic task T = {A S , A Q }, superparameter k. while no converge do for A q Q in A Q do L qz ← G φ (F θ (A Q )) L Sz ← G φ (F θ (S))</formula><p>Obtain semantic relation matrix R z by Eq. (3) Calculate adaptive task score α z by Eq. (5) Reweighting R z by Eq. (6) Calculate P q by Eq. (7) and Eq. (8) end for L ← − Ylog(P) mini-Batch Adam to minimize L, update θ, φ, ϕ and ω end while</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The framework of MATANet under the 5-way 1-shot image classification setting. The model mainly consists of four modules:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The architecture of the multi-scale feature generator g φ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Experimental results of MATANet using different superparameter k on miniImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Metric functions 5 -</head><label>5</label><figDesc>Way Accuracy(%)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dataset Stanford Dogs Stanford Cars CUB BirdsThe splits of three fine-grained datasets. N all is the number of all classes. Ntrain, N val and Ntest indicate the number of classes in training set, validation set and test set.</figDesc><table><row><cell>N all</cell><cell>120</cell><cell>196</cell><cell>200</cell></row><row><cell>N train</cell><cell>70</cell><cell>130</cell><cell>100</cell></row><row><cell>N val</cell><cell>20</cell><cell>17</cell><cell>50</cell></row><row><cell>N test</cell><cell>30</cell><cell>49</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Experimental results of MATANet using different metric functions on miniImageNet, d(a, b):the euclidean distance between vector a and b.</figDesc><table><row><cell></cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>e −d(a,b)</cell><cell cols="2">52.82±0.83 71.85±0.74</cell></row><row><cell>1 1+d(a,b)</cell><cell cols="2">52.11±0.85 71.25±0.76</cell></row><row><cell>Tanimito Index</cell><cell cols="2">53.25±0.84 72.11±0.73</cell></row><row><cell cols="3">Cosine Similarity 53.63±0.83 72.67±0.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>The ablation study on miniImageNet for the proposed MATANet.</figDesc><table><row><cell></cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>baseline</cell><cell cols="2">51.12±0.73 67.35±0.63</cell></row><row><cell>w/o g φ</cell><cell cols="2">52.95±0.77 72.02±0.69</cell></row><row><cell>w/o F ϕ</cell><cell cols="2">51.66±0.77 69.12±0.77</cell></row><row><cell>w/o F ω</cell><cell cols="2">53.02±0.79 72.07±0.73</cell></row><row><cell cols="3">MATANet(ours) 53.63±0.83 72.67±0.76</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">.5. Classification with an adaptive fusion strategySince the five relation scores have been calculated, we need to design a fusion module to integrate them. In order to solve this problem, we adopt a learnable five-dimensional w = [w 1 , w 2 , w 3 , w 4 , w 5 ] vector to adaptively integrate</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Table 2. Comparison with other state-of-the-art methods with 95% confidence intervals on mini-ImageNet. The third column shows which kind of embedding is employed. The fourth column shows which type of the method belongs to, i.e, meta-learning based, metric-learning based, and other kinds of methods. * Results reported by the original work. (Top two performances are in bold)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Infinite mixture prototypes for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Kelsey R Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04552</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to forget for meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokil</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04232</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02729</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Metalearning with warped gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Flennerhag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00025</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collect and select: Semantic alignment metric learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fusheng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Low-rank pairwise alignment bilinear network for few-shot fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Task agnostic meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nityananda</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</title>
		<meeting>CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Revisiting local descriptor based image-to-class measure for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7260" to="7268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distribution consistency based covariance metric networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8642" to="8649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10002</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fewshot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sygnowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive subspaces for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
