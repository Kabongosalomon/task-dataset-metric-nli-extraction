<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEDNET: A LIGHTWEIGHT ENCODER-DECODER NETWORK FOR REAL-TIME SEMANTIC SEGMENTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center of Communications and Networking</orgName>
								<orgName type="institution">Nanjing University of Posts &amp; Telecommunications</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center of Communications and Networking</orgName>
								<orgName type="institution">Nanjing University of Posts &amp; Telecommunications</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Lab. of Broadband Wireless Communications and Sensor Network Technology</orgName>
								<orgName type="institution">Nanjing University of Posts &amp; Telecommunications</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center of Communications and Networking</orgName>
								<orgName type="institution">Nanjing University of Posts &amp; Telecommunications</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center of Communications and Networking</orgName>
								<orgName type="institution">Nanjing University of Posts &amp; Telecommunications</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Gao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="institution">Nanjing University of Posts &amp; Telecommunications</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center of Communications and Networking</orgName>
								<orgName type="institution">Nanjing University of Posts &amp; Telecommunications</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longin</forename><forename type="middle">Jan</forename><surname>Latecki</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">Temple University</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEDNET: A LIGHTWEIGHT ENCODER-DECODER NETWORK FOR REAL-TIME SEMANTIC SEGMENTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-CNN</term>
					<term>Lightweight network</term>
					<term>Encoder- decoder network</term>
					<term>ResNet</term>
					<term>Real-time semantic segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The extensive computational burden limits the usage of CNNs in mobile devices for dense estimation tasks. In this paper, we present a lightweight network to address this problem, namely LEDNet, which employs an asymmetric encoderdecoder architecture for the task of real-time semantic segmentation. More specifically, the encoder adopts a ResNet as backbone network, where two new operations, channel split and shuffle, are utilized in each residual block to greatly reduce computation cost while maintaining higher segmentation accuracy. On the other hand, an attention pyramid network (APN) is employed in the decoder to further lighten the entire network complexity. Our model has less than 1M parameters, and is able to run at over 71 FPS in a single GTX 1080Ti GPU. The comprehensive experiments demonstrate that our approach achieves state-of-the-art results in terms of speed and accuracy trade-off on CityScapes dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recently, building deeper and larger convolutional neural networks (CNNs) is a primary trend for solving scene understanding tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. The most accurate CNNs usually have hundreds of convolutional layers and thousands of feature channels. In spite of achieving higher performance, these advances are at the sacrifice of running time and speed. Especially in the context of many real-world scenarios, such as augmented reality, robotics, and self-driving car to name of few, the computationally cheap networks with smaller size are often required to carry out online estimation in a timely fashion. Therefore, those accurate networks requiring enormous resources are not suitable for computationally limited mobile platforms (e.g., drones, robots, and smartphones), which have limited energy overhead, restrictive memory constraints, and reduced computational capabilities. Such kind of limitation is particularly prominent on the computationally heavy task of semantic segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, where the goal here is to assign a semantic category label for each image pixel.</p><p>In order to overcome this problem, many lightweight style networks have been designed to balance the segmentation accuracy and implementing efficiency, which are roughly divided into two categories: network compression <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> and convolution factorization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. The first category prefers to reduce inference computation by compressing pre-trained networks, including hashing <ref type="bibr" target="#b8">[9]</ref>, pruning <ref type="bibr" target="#b9">[10]</ref>, and quantization <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. To further remove the redundancy, an alternative approach to lighten CNNs depends on sparse coding theory <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. On the contrary, motivated from the convolution factorization principle (CFP) that decomposes a standard convolution into group convolution and depthwise separable convolution <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref>, the second category focuses on directly training network with smaller size. For example, ENet <ref type="bibr" target="#b12">[13]</ref> employs ResNet <ref type="bibr" target="#b1">[2]</ref> as backbone to perform efficient inference. Zhao et al. <ref type="bibr" target="#b18">[19]</ref> propose a cascade network that incorporates high-level label guidance to improve performance. In <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref>, a symmetrical encoder-decoder architecture is adopted, which greatly reduce the number of parameters while maintaining the accuracy. Although some work have conducted preliminary research on lightweight architecture networks, pursuing the best accuracy in very limited computational budgets is still an open research question for the task of real-time semantic segmentation.</p><p>In this paper, we aim at solving this trade-off as a whole, without sitting on only one of its sides. We introduce a novel lightweight network called LEDNet, adopting an asymmetric encoder-decoder architecture for real-time semantic segmentation. As shown in <ref type="figure">Figure 1</ref>, our LEDNet is composed of two parts: encoder and decoder network. Following CFP, the core unit of encoder is a novel residual module that leverages skip connections and convolutions with channel split and shuffle. While the skip connections allow the convolutions to learn residual functions that facilitate training, the split and shuffle operations enhance the information exchange within the feature channels while maintaining similar computational costs compared to 1D factorized convolutions. In the decoder, instead of complicated dilated convolution <ref type="bibr" target="#b19">[20]</ref>, we design an attention pyramid network (APN) to extract dense features, where the attention mechanism is utilized to estimate semantic label for each pixel. Our contributions are three-folds: <ref type="bibr" target="#b0">(1)</ref> The asymmetrical architecture of our LEDNet leads to the great reduction of network parameters, which accelerates the inference process; (2) The channel split and shuffle operations in our residual layer leverage network size and powerful feature representation. In addition, channel shuffle is also differentiable, which means it can be embedded into network structures for end-to-end training. (3) Attention mechanism of feature pyramid is employed to design APN in our decoder-end, further lightening the complexity of the whole network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OUR APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Residual Module with Split and Shuffle Operations</head><p>We focus on solving the efficiency limitation that is essentially present in the residual block, which is used in recent accurate CNNs for image classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref> and semantic segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. The recent years have witnessed multiple successful instances of lightweight residual layer <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>, such as bottleneck <ref type="figure">(Figure 2</ref>  where the pointwise convolution is widely used. However, the contrary opinion of <ref type="bibr" target="#b20">[21]</ref> claims that pointwise convolution accounts for most of the computational complexity, which is especially disadvantageous for lightweight models.  <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>, (b) non-bottleneck-1D <ref type="bibr" target="#b13">[14]</ref>, (c) ShuffleNet <ref type="bibr" target="#b20">[21]</ref>, and (d) our SS-nbt module.</p><p>To balance performance and efficiency given limited computational budgets, we introduce two simple operators, called channel split and shuffle, in residual layer. We refer to this proposed module as split-shuffle-non-bottleneck(SS-nbt), as depicted in <ref type="figure">Figure 2</ref> (d). Motivated from <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>, a splittransform-merge strategy is employed in the designment of our SS-nbt, approaching the representational power of large and dense layers, but at a considerably lower computational complexity. At the beginning of each SS-nbt, the input is split into two lower-dimensional branches, where each one has half channels of the input. To avoid pointwise convolution, the transformation is performed using a set of specialized 1D filters (e.g., 1 × 3, 3 × 1), and the convolutional outputs of two branches are merged using concatenation so that the number of channels keeps the same. To facilitate training, the stacked output is added with input through the branch of identity mapping. The same channel shuffle operation <ref type="bibr" target="#b20">[21]</ref> is finally used to enable information communication between two split branches. After the shuffle, the next SS-nbt unit begins. It is clear that our residual module is not only efficient, but also accurate. Firstly, the high efficiency in each SS-nbt allows us to use more feature channels. Secondly, in each SSnbt unit, the merged feature channels are randomly shuffled, and then join into next unit. This can be regarded as a kind of feature reuse, which to some extent enlarges network capacity without significantly increasing complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">LEDNet Architecture Designment</head><p>As shown in <ref type="table" target="#tab_2">Table 1</ref>, our LEDNet follows an encoder-decoder architecture. Unlike <ref type="bibr" target="#b6">[7]</ref>, our approach employs an asymmetric sequential architecture, where a encoder produces downsampled feature maps, and a subsequent decoder adopts APN that upsamples the feature maps to match input resolution. Besides SS-nbt unit, the encoder also includes downsampling unit, which is performed by stacking two parallel outputs of a single 3 × 3 convolution with stride 2 and a Max-pooling. Downsampling enables more deeper network to gather context, while at the same time helps to reduce computation. Note we postpone downsampling in encoder, in the similar spirit of <ref type="bibr" target="#b17">[18]</ref>. Moreover, the usage of dilated convolutions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref> allows our architecture to have large receptive field, leading to an improvement in accuracy. Compared to the use of larger kernel sizes, this technique has been proven more effective in terms of computational cost and parameters.</p><p>Inspired by attention mechanism <ref type="bibr" target="#b23">[24]</ref>, our decoder designs a APN to perform dense estimation using spatial-wise attention. To increase receptive field, the APN adopts a pyramid attention module, which integrates features from three different pyramid scales. As shown in <ref type="figure">Figure 1</ref>, we first utilize 3 × 3, 5 × 5, and 7 × 7 convolution with stride 2 to form multi-scale feature pyramid. Then the pyramid structure fuses information of different scales step-by-step, which can incorporate neighbor scales of context more precisely. Since high-level feature maps has small resolution, using large kernel size does not bring too much computation burden. Thereafter, a 1 × 1 convolution is applied to the output of encoder, then the convolutional feature maps are pixel-wisely multiplied by the pyramid attention features. To further enhance performance, a global average pooling branch is introduced to integrate global context prior attention. Finally, an upsampling unit is employed to match the resolution of input image. Benefiting from pyramid architecture, APN can capture multi-scale context cues, and produce pixel-level attention for convolutional features. Unlike DeepLab <ref type="bibr" target="#b4">[5]</ref> and PSPNet <ref type="bibr" target="#b7">[8]</ref> that stack multi-scale feature maps, our context information is pixel-wisely multiplied with original convolutional features, without introducing too much computational budgets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation Details</head><p>We select widely-used CityScapes dataset <ref type="bibr" target="#b25">[26]</ref> to evaluate our LEDNet, which includes 19 object categories and one additional background. Beside the images with fine pixel-level annotations that contain 2,975 training, 500 validation and 1,525 testing images, we also use the 20K coarsely annotated images for training. We adopt mean intersection-over-union (mIoU) averaged across all classes and categories to evaluate segmentation accuracy, while running time, speed (FPS), and model size (number of parameters) to measure implementing efficiency. To show the advantages of LEDNet, we selected 6 state-of-the-art lightweight networks as baselines, including SegNet <ref type="bibr" target="#b6">[7]</ref>, ENet <ref type="bibr" target="#b12">[13]</ref>, ERFNet <ref type="bibr" target="#b13">[14]</ref>, ICNet <ref type="bibr" target="#b18">[19]</ref>, CGNet <ref type="bibr" target="#b24">[25]</ref>, and ESPNet <ref type="bibr" target="#b19">[20]</ref>. For fair comparison, all the methods are conducted on the same hardware platform of Dell workstation with a single GTX 1080Ti GPU. We favor a large minibatch size (set as 5) to make full use of the GPU memory, where the initial learning rate is 5 × 10 −4 and the 'poly' learning rate policy is adopted with power 0.9, together with momentum and weight decay are set to 0.9 and 10 −4 , respectively.  <ref type="bibr" target="#b6">[7]</ref>, ENet <ref type="bibr" target="#b12">[13]</ref>, ERFNet <ref type="bibr" target="#b13">[14]</ref>, ESPNet <ref type="bibr" target="#b19">[20]</ref>, ICNet <ref type="bibr" target="#b18">[19]</ref>, CGNet <ref type="bibr" target="#b24">[25]</ref>, and our LEDNet. (Best viewed in color)  <ref type="bibr" target="#b6">[7]</ref>. Although ENet <ref type="bibr" target="#b12">[13]</ref>, an another efficient network, is 1.5× efficient, and has 3× less parameters, but delivers poor segmentation accuracy of 10% drop than our LEDNet. <ref type="figure">Figure 3</ref> shows some visual examples of segmentation outputs on the CityScapes validation set. It is demonstrated that, compared with baselines, our LEDNet not only correctly classifies object with different scales, but also produces consistent qualitative results for all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION AND FUTURE WORK</head><p>This paper has described a LEDNet model, which designs an asymmetric encoder-decoder architecture for real-time semantic segmentation. The encoder adopts channel split and shuffle operations in residual layer, enhancing information communication in the manner of feature reuse. On the other hand, the decoder employs a APN, where the spatial pyramid structure is beneficial to enlarge receptive fields without introducing significant computational budgets. The entire network is trained end-to-end. The experimental results show our LEDNet achieves best trade-off on CityScapes dataset in terms of segmentation accuracy and implementing efficiency. The future work includes decomposing standard convolution in APN into 1D convolution, resulting in further lightweight network while still remaining segmentation accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a)), non-bottleneck-1D(Figure 2 (b)), and ShuffleNet module(Figure 2 (c)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>The architecture of LEDNet. "Size" denotes the dimension of output feature maps, C is the number of classes.</figDesc><table><row><cell cols="2">Stage Type</cell><cell>Size</cell></row><row><cell></cell><cell>Downsampling Unit</cell><cell>512 × 256 × 32</cell></row><row><cell></cell><cell>3× SS-nbt Unit</cell><cell>512 × 256 × 32</cell></row><row><cell></cell><cell>Downsampling Unit</cell><cell>256 × 128 × 64</cell></row><row><cell></cell><cell>2× SS-nbt Unit</cell><cell>256 × 128 × 64</cell></row><row><cell></cell><cell>Downsampling Unit</cell><cell>128 × 64 × 128</cell></row><row><cell>Encoder</cell><cell>SS-nbt Unit (dilated r = 1) SS-nbt Unit (dilated r = 2) SS-nbt Unit (dilated r = 5)</cell><cell>128 × 64 × 128 128 × 64 × 128 128 × 64 × 128</cell></row><row><cell></cell><cell>SS-nbt Unit (dilated r = 9)</cell><cell>128 × 64 × 128</cell></row><row><cell></cell><cell>SS-nbt Unit (dilated r = 2)</cell><cell>128 × 64 × 128</cell></row><row><cell></cell><cell>SS-nbt Unit (dilated r = 5)</cell><cell>128 × 64 × 128</cell></row><row><cell></cell><cell>SS-nbt Unit (dilated r = 9)</cell><cell>128 × 64 × 128</cell></row><row><cell></cell><cell cols="2">SS-nbt Unit (dilated r = 17) 128 × 64 × 128</cell></row><row><cell>Decoder</cell><cell>APN Module Upsampling Unit ( ×8)</cell><cell>128 × 64 × C 1024 × 512 × C</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison with the state-of-the-art approaches in terms of segmentation accuracy and implementing efficiency.</figDesc><table><row><cell>Method</cell><cell cols="4">Cla Cat Time(ms) Speed(Fps) Para(M)</cell></row><row><cell cols="2">SegNet[7] 57.0 79.1</cell><cell>67</cell><cell>15</cell><cell>29.5</cell></row><row><cell cols="2">ENet[13] 58.3 80.4</cell><cell>34</cell><cell>31</cell><cell>0.36</cell></row><row><cell cols="2">ESPNet[20] 60.3 82.2</cell><cell>9</cell><cell>112</cell><cell>0.40</cell></row><row><cell cols="2">CGNet[25] 64.8 85.7</cell><cell>20</cell><cell>50</cell><cell>0.50</cell></row><row><cell cols="2">ICNet [19] 69.5 86.4</cell><cell>33</cell><cell>30</cell><cell>7.80</cell></row><row><cell>Ours</cell><cell>70.6 87.1</cell><cell>14</cell><cell>71</cell><cell>0.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Individual category results on the CityScapes test set in terms of class and category mIoU scores. Methods trained using both fine and coarse data are marked with superscript ' †'. Method Roa Sid Bui Wal Fen Pol TLi TSi Veg Ter Sky Ped Rid Car Tru Bus Tra Mot Bic Cla Cat SegNet [7] 96.4 73.2 84.0 28.4 29.0 35.7 39.8 45.1 87.0 63.8 91.8 62.8 42.8 89.3 38.1 43.1 44.1 35.8 51.9 57.0 79.1 ENet [13] 96.3 74.2 75.0 32.2 33.2 43.4 34.1 44.0 88.6 61.4 90.6 65.5 38.4 90.6 36.9 50.5 48.1 38.8 55.4 58.3 80.4 ESPNet [20] 97.0 77.5 76.2 35.0 36.1 45.0 35.6 46.3 90.8 63.2 92.6 67.0 40.9 92.3 38.1 52.5 50.1 41.8 57.2 60.3 82.2 CGNet [25] 95.5 78.7 88.1 40.0 43.0 54.1 59.8 63.9 89.6 67.6 92.9 74.9 54.9 90.2 44.1 59.5 25.2 47.3 60.2 64.8 85.7 ERFNet [14] 97.2 80.0 89.5 41.6 45.3 56.4 60.5 64.6 91.4 68.7 94.2 76.1 56.4 92.4 45.7 60.6 27.0 48.7 61.8 66.3 85.2 ICNet [19] 97.1 79.2 89.7 43.2 48.9 61.5 60.4 63.4 91.5 68.3 93.5 74.6 56.1 92.6 51.3 72.7 51.3 53.6 70.5 69.5 86.79.5 91.6 47.7 49.9 62.8 61.3 72.8 92.6 61.2 94.9 76.2 53.7 90.9 64.4 64.0 52.7 44.4 71.6 70.6 87.1Fig. 3. The visual comparison on CityScapes val dataset. From left to right are input images, ground truth, segmentation outputs from SegNet</figDesc><table><row><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 and</head><label>2</label><figDesc>Table 3report comparison results, demonstrating that LEDNet achieves the best available trade-off in terms of accuracy and efficiency. Among all the approaches, our LEDNet yields 70.6% class mIoU and 87.1% category mIoU, respectively, where 13 out of the 19 categories obtains best scores. Regarding to the efficiency, LEDNet is nearly 5× faster and 30× smaller than SegNet</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Evan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Trevor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Refinenet: multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guosheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chunhua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quantized convolutional neural networks for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TITS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mobilenets: efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pensky</surname></persName>
		</author>
		<title level="m">Sparse convolutional neural networks,&quot; in CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08545v2</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06815v3</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cgnet: A light-weight context guided network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08201v1</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
