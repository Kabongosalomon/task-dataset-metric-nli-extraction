<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Variable Model for Multi-modal Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
							<email>iacer.calixto@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">ILLC The University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Rios</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ILLC The University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
							<email>w.aziz@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">ILLC The University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Variable Model for Multi-modal Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation (MMT) through a latent variable model. This latent variable can be seen as a multi-modal stochastic embedding of an image and its description in a foreign language. It is used in a target-language decoder and also to predict image features. Importantly, our model formulation utilises visual and textual inputs during training but does not require that images be available at test time. We show that our latent variable MMT formulation improves considerably over strong baselines, including a multi-task learning approach (Elliott and Kádár, 2017) and a conditional variational auto-encoder approach <ref type="bibr" target="#b43">(Toyama et al., 2016)</ref>. Finally, we show improvements due to (i) predicting image features in addition to only conditioning on them, (ii) imposing a constraint on the minimum amount of information encoded in the latent variable, and (iii) by training on additional target-language image descriptions (i.e. synthetic data).</p><p>1 Code and pre-trained models will be released soon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-modal machine translation (MMT) is an exciting novel take on machine translation (MT) where we are interested in learning to translate sentences in the presence of visual input (mostly images). In the last three years there have been shared tasks <ref type="bibr" target="#b1">Barrault et al., 2018)</ref> where many research groups proposed different techniques to integrate images into MT, e.g. <ref type="bibr" target="#b3">Caglayan et al. (2017)</ref>; <ref type="bibr" target="#b22">Libovický and Helcl (2017)</ref>.</p><p>Most MMT models expand neural machine translation (NMT) architectures <ref type="bibr" target="#b40">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> to additionally condition on an image in order to compute the likelihood of a translation in context. This gives the model a chance to exploit correlations in visual and language data, but also means that images must be available at test time. An exception to this rule is the work of <ref type="bibr" target="#b43">Toyama et al. (2016)</ref> who exploit the framework of conditional variational auto-encoders (CVAEs) <ref type="bibr" target="#b37">(Sohn et al., 2015)</ref> to decouple the encoder used for posterior inference at training time from the encoder used for generation at test time. Rather than conditioning on image features, the model of <ref type="bibr" target="#b11">Elliott and Kádár (2017)</ref> learns to rank image features using language data in a multi-task learning (MTL) framework, therefore sharing parameters between a translation (generative) and a sentence-image ranking model (discriminative). This similarly exploits correlations between the two modalities and has the advantage that images are also not necessary at test time.</p><p>In this work, we also aim at translating without images at test time, yet learning a visually grounded translation model. To that end, we resort to probabilistic modelling instead of multi-task learning and estimate a joint distribution over translations and images. In a nutshell, we propose to model the interaction between visual and textual features through a latent variable. This latent variable can be seen as a stochastic embedding which is used in the target-language decoder, as well as to predict image features. Our experiments show that this joint formulation improves over an MTL approach <ref type="bibr" target="#b11">(Elliott and Kádár, 2017)</ref>, which does model both modalities but not jointly, and over the CVAE of <ref type="bibr" target="#b43">Toyama et al. (2016)</ref>, which uses image features to condition an inference network but crucially does not model the images.</p><p>The main contributions of this paper are: 1</p><p>• we propose a novel multi-modal NMT model that incorporates image features through latent variables in a deep generative model.</p><p>• our latent variable MMT formulation improves considerably over strong baselines, and compares favourably to the state-of-the-art.</p><p>• we exploit correlations between both modalities at training time through a joint generative approach and do not require images at prediction time.</p><p>The remainder of this paper is organised as follows. In §2, we describe our variational MMT models. In §3, we introduce the data sets we used and report experiments and assess how our models compare to prior work. In §4, we position our approach with respect to the literature. Finally, in §5 we draw conclusions and provide avenues for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Variational Multi-modal NMT</head><p>Similarly to standard NMT, in MMT we wish to translate a source sequence x m 1 x 1 , · · · , x m into a target sequence y n 1 y 1 , · · · , y n . The main difference is the presence of an image v which illustrates the sentence pair x m 1 , y n 1 . We do not model images directly, but instead an 2048dimensional vector of pre-activations of a ResNet-50's pool5 layer <ref type="bibr" target="#b13">(He et al., 2015)</ref>.</p><p>In our variational MMT models, image features are assumed to be generated by transforming a stochastic latent embedding z, which is also used to inform the RNN decoder in translating source sentences into a target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative model</head><p>We propose a generative model of translation and image generation where both the image v and the target sentence y n 1 are independently generated given a common stochastic embedding z. The generative story is as follows. We observe a source sentence x m 1 and draw an embedding z from a latent Gaussian model,</p><formula xml:id="formula_0">Z|x m 1 ∼ N (µ, diag(σ 2 )) µ = f µ (x m 1 ; θ) σ = f σ (x m 1 ; θ) ,<label>(1)</label></formula><p>where f µ (·) and f σ (·) map from a source sentence to a vector of locations µ ∈ R c and a vector of scales σ ∈ R c &gt;0 , respectively. We then proceed to draw the image features from a Gaussian observation model,</p><formula xml:id="formula_1">V |z ∼ N (ν, ς 2 I) ν = f ν (z; θ) ,<label>(2)</label></formula><p>where f ν (·) maps from z to a vector of locations ν ∈ R o , and ς ∈ R &gt;0 is a hyperparameter of the model (we use 1). Conditioned on z and on the source sentence x m 1 , and independently of v, we generate a translation by drawing each target word in context from a Categorical observation model,</p><formula xml:id="formula_2">Y j |x m 1 , z, y &lt;j ∼ Cat(π j ) π j = f π (x m 1 , y &lt;j , z; θ) ,<label>(3)</label></formula><p>where f π (·) maps z, x m 1 , and a prefix translation y &lt;j to the parameters π j of a categorical distribution over the target vocabulary. Functions f µ (·), f σ (·), f ν (·), and f π (·) are implemented as neural networks whose parameters are collectively denoted by θ. In particular, implementing f π (·) is as simple as augmenting a standard NMT architecture <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b25">Luong et al., 2015)</ref>, i.e. encoder-decoder with attention, with an additional input z available at every time-step. All other functions are single-layer MLPs that transform the average encoder hidden state to the dimensionality of the corresponding Gaussian variable followed by an appropriate activation. <ref type="bibr">2</ref> Note that in effect we model a joint distribution</p><formula xml:id="formula_3">p θ (y n 1 , v, z|x m 1 ) = p θ (z|x m 1 )p θ (v|z)P θ (y n 1 |x m 1 , z)<label>(4)</label></formula><p>consisting of three components which we parameterise directly. As there are no observations for z, we cannot estimate these components directly. We must instead marginalise z out, which yields the marginal</p><formula xml:id="formula_4">P θ (y n 1 , v|x m 1 ) = p θ (z|x m 1 )p θ (v|z)P θ (y n 1 |x m 1 , z)dz .<label>(5)</label></formula><p>An important statistical consideration about this model is that even though y n 1 and v are conditionally independent given z, they are marginally dependent. This means that we have designed a data generating process where our observations y n 1 , v|x m 1 are not assumed to have been independently produced. 3 This is in direct contrast with multi-task learning or joint modelling without latent variables-for an extended discussion see <ref type="bibr">(Eikema and Aziz, 2018, § 3)</ref>. (a) VMMTC: given the source text x m 1 , we model the joint likelihood of the translation y n 1 , the image (features) v, and a stochastic embedding z sampled from a conditional latent Gaussian model. Note that the stochastic embedding is the sole responsible for assigning a probability to the observation v, and it helps assign a probability to the translation.  Finally, <ref type="figure" target="#fig_2">Figure 1</ref> (left) is a graphical depiction of the generative model: shaded circles denote observed random variables, unshaded circles indicate latent random variables, deterministic quantities are not circled; the internal plate indicates iteration over time-steps, the external plate indicates iteration over the training data. Note that deterministic parameters θ are global to all training instances, while stochastic embeddings z are local to each tuple x m 1 , y n 1 , v .</p><p>Inference Parameter estimation for our model is challenging due to the intractability of the marginal likelihood function (5). We can however employ variational inference (VI) <ref type="bibr" target="#b17">(Jordan et al., 1999)</ref>, in particular amortised VI <ref type="bibr" target="#b20">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b29">Rezende et al., 2014)</ref>, and estimate parameters to maximise a lowerbound</p><formula xml:id="formula_5">E q λ (z|x m 1 ,y n 1 ,v) [log p θ (v|z) + log P θ (y n 1 |x m 1 , z)] − KL(q λ (z|x m 1 , y n 1 , v)||p θ (z|x m 1 ))<label>(6)</label></formula><p>on the log-likelihood function. This evidence lowerbound (ELBO) is expressed in terms of an inference model q λ (z|x m 1 , y n 1 , v) which we design having tractability in mind. In particular, our approximate posterior is a Gaussian distribution</p><formula xml:id="formula_6">q λ (z|x m 1 , y n 1 , v) = N (z|u, diag(s 2 )) u = g u (x m 1 , y n 1 , v; λ) s = g s (x m 1 , y n 1 , v; λ)<label>(7)</label></formula><p>parametrised by an inference network, that is, an independently parameterised neural network (whose parameters we denote collectively by λ) which maps from observations, in our case a sentence pair and an image, to a variational location u ∈ R c and a variational scale s ∈ R c &gt;0 . <ref type="figure" target="#fig_2">Figure 1</ref> (right) is a graphical depiction of the inference model.</p><p>Location-scale variables (e.g. Gaussians) can be reparametrised, i.e. we can obtain a latent sample via a deterministic transformation of the variational parameters and a sample from the standard Gaussian distribution:</p><formula xml:id="formula_7">z = u + s where ∼ N (0, I) . (8)</formula><p>This reparametrisation enables backpropagation through stochastic units <ref type="bibr" target="#b20">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b42">Titsias and Lázaro-Gredilla, 2014)</ref>. In addition, for two Gaussians the KL term in the ELBO (6) can be computed in closed form (Kingma and Welling, 2014, Appendix B). Altogether, we can obtain a reparameterised gradient estimate of the ELBO, we use a single sample estimate of the first term, and count on stochastic gradient descent to attain a local optimum of (6).</p><p>Architecture All of our parametric functions are neural network architectures. In particular, f π is a standard sequence-to-sequence architecture with attention and a softmax output. We build upon OpenNMT <ref type="bibr" target="#b21">(Klein et al., 2017)</ref>, which we modify slightly by providing z as additional input to the target-language decoder at each time step. Location layers f µ , f ν and g u , and scale layers f σ and g s , are feed-forward networks with a single ReLU hidden layer. Furthermore, location layers have a linear output while scale layers have a softplus output. For the generative model, f µ and f σ transform the average source-language encoder hidden state.</p><p>We let the inference model condition on sourcelanguage encodings without updating them, and we use a target-language bidirectional LSTM encoder in order to also condition on the complete target sentence. Then g u and g s transform a concatenation of the average source-language encoder hidden state, the average target-language bidirectional encoder hidden state, and the image features.</p><p>Fixed Gaussian prior We have just presented our variational MMT model in its full generalitywe refer to that model as VMMT C . However, keeping in mind that MMT datasets are rather small, it is desirable to simplify some of our model's components. In particular, the estimated latent Gaussian model <ref type="formula" target="#formula_0">(1)</ref> can be replaced by a fixed standard Gaussian prior, i.e., Z ∼ N (0, I)-we refer to this model as VMMT F . Along with this change it is convenient to modify the inference model to condition on x m 1 alone, which allow us to use the inference model for both training and prediction. Importantly this also sidesteps the need for a target-language bidirectional LSTM encoder, which leaves us a smaller set of inference parameters λ to estimate. Interestingly, this model does not rely on features from v, instead only using it as learning signal through the objective in <ref type="formula" target="#formula_5">(6)</ref>, which is in direct contrast with the model of <ref type="bibr" target="#b43">Toyama et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Our encoder is a 2-layer 500D bidirectional RNN with GRU, the latent embedding z, source, and target word embeddings are also 500D each, and trained jointly with the model. We use OpenNMT to implement all our models <ref type="bibr" target="#b21">(Klein et al., 2017)</ref>. All model parameters are initialised sampling from a uniform distribution U(−0.1, +0.1) and bias vectors are initialised to 0. Visual features are obtained by feeding images to the pre-trained ResNet-50 and using the activations of the pool5 layer <ref type="bibr" target="#b13">(He et al., 2015)</ref>. We apply dropout with a probability of 0.5 in the encoder bidirectional RNN, the image features, the decoder RNN, and before emitting a target word.</p><p>All models are trained using the Adam optimiser (Kingma and Ba, 2014) with an initial learning rate of 0.002 and minibatches of size 40, where each training instance consists of one English sentence, one German sentence and one image (MMT). Models are trained for up to 40 epochs and we perform model selection based on BLEU4, and use the best performing model on the validation set to translate test data. Moreover, we halt training if the model does not improve BLEU4 scores on the vali-dation set for 10 epochs or more. We report mean and standard deviation over 4 independent runs for all models we trained ourselves (NMT, VMMT F , VMMT C ), and other baseline results are the ones reported in the authors' publications <ref type="bibr" target="#b43">(Toyama et al., 2016;</ref><ref type="bibr" target="#b11">Elliott and Kádár, 2017)</ref>.</p><p>We preprocess our data by tokenizing, lowercasing, and converting words to subword tokens using a bilingual BPE model with 10k merge operations <ref type="bibr" target="#b33">(Sennrich et al., 2016b)</ref>. We quantitatively evaluate translation quality using case-insensitive and tokenized outputs in terms of BLEU4 <ref type="bibr" target="#b26">(Papineni et al., 2002)</ref>, METEOR <ref type="bibr" target="#b7">(Denkowski and Lavie, 2014)</ref>, chrF3 <ref type="bibr" target="#b27">(Popović, 2015)</ref>, and BEER (Stanojević and Sima'an, 2014). By using these, we hope to include word-level metrics which are traditionally used by the MT community (i.e. BLEU and METEOR), as well as more recent metrics which operate at the character level and that better correlate with human judgements of translation quality (i.e. chrF3 and BEER) <ref type="bibr" target="#b2">(Bojar et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>The Flickr30k dataset <ref type="bibr" target="#b45">(Young et al., 2014)</ref> consists of images from Flickr and their English descriptions. We use the translated Multi30k (M30k T ) dataset , i.e. an extension of Flickr30k where for each image one of its English descriptions was translated into German by a professional translator. Training, validation and test sets contain 29k, 1014 and 1k images respectively, each accompanied by the original English sentence and its translation into German.</p><p>Since this dataset is very small, we also investigate the effect of including more in-domain data to train our models. To that purpose, we use additional 145K monolingual German descriptions released as part of the Multi30k dataset to the task of image description generation . We refer to this dataset as comparable Multi30k (M30k C ). Descriptions in the comparable Multi30k were collected independently of existing English descriptions and describe the same 29K images as in the M30k T dataset.</p><p>In order to obtain features for images, we use ResNet-50 <ref type="bibr" target="#b13">(He et al., 2015)</ref> pre-trained on Ima-geNet <ref type="bibr" target="#b30">(Russakovsky et al., 2015)</ref>. We report experiments using pool5 features as our image features, i.e. 2048-dimensional pre-activations of the last layer of the network.  For each model, we report the mean and standard deviation over 4 independent runs where models were selected using validation BLEU4 scores. Best mean baseline scores per metric are underlined and best overall results (i.e. means) are in bold. We highlight in green/red the improvement brought by our models compared to the best baseline mean score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We compare our work against three different baselines. The first one is a standard text-only sequenceto-sequence NMT model with attention <ref type="bibr" target="#b25">(Luong et al., 2015)</ref>, trained from scratch using hyperparameters described above. The second baseline is the variational multi-modal MT model Model G proposed by <ref type="bibr" target="#b43">Toyama et al. (2016)</ref>, where global image features are used as additional input to condition an inference network. Finally, a third baseline is the Imagination model of Elliott and Kádár (2017), a multi-task MMT model which uses a shared source-language encoder RNN and is trained in two tasks: to translate from English into German and on image-sentence ranking (English↔image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Translated Multi30k</head><p>We now report on experiments conducted with models trained to translate from English into German using the translated Multi30k data set (M30k T ).</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we compare our variational MMT models-VMMT C for the general case with a conditional Gaussian latent model, and VMMT F for the simpler case of a fixed Gaussian prior-to the three baselines described above. The general trend is that both formulations of our VMMT improve with respect to all three baselines. We note an improvement in BLEU and METEOR mean scores compared to the Imagination model <ref type="bibr" target="#b11">(Elliott and Kádár, 2017)</ref>, as well as reduced variance (though note this is based on only 4 independent runs in our case, and 3 independent runs of Imagination). Both models VMMT F and VMMT C outperform Model G according to BLEU and perform comparably according to METEOR, especially since results reported by <ref type="bibr" target="#b43">(Toyama et al., 2016)</ref> are based on a single run.</p><p>Moreover, we also note that both our models outperform the text-only NMT baseline according to all four metrics, and by 1%-1.4% according chrF3 and BEER, both being metrics well-suited to measure the quality of translations into German and generated with subwords units.</p><p>Finally, one interesting finding is that the fixedprior model VMMT F performs slightly better than the conditional model VMMT C according to all four metrics studied. We speculate this is due to VMMT F 's simpler parameterisation, after all, we have just about 29k training instances to estimate two sets of parameters (θ and λ) and the more complex VMMT C requires an additional bidirectional LSTM encoder for the target text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Back-translated Comparable Multi30k</head><p>Since the translated Multi30k dataset is very small, we also investigate the effect of including more in-domain data to train our models. For that purpose, we use additional 145K monolingual German descriptions released as part of the comparable Multi30k dataset (M30k C ). We train a text-only NMT model to translate from German into English using the original 29K parallel sentences in the translated Multi30k (without images), and apply this model to back-translate the 145K German descriptions into English <ref type="bibr" target="#b32">(Sennrich et al., 2016a)</ref>.</p><p>In this set of experiments, we explore how pretraining models NMT, VMMT F and VMMT C using both the translated and back-translated comparable Multi30k affects results. Models are pre-trained on mini-batches with a one-to-one ratio of translated and back-translated data. 4 All three models NMT, VMMT F and VMMT C , are further finetuned on the translated Multi30k until convergence, and model selection using BLEU is only applied during fine-tuning and not at the pre-training stage.</p><p>In <ref type="figure" target="#fig_4">Figure 2</ref>, we inspect for how many epochs should a model be pre-trained using the additional noisy back-translated descriptions, and note that both VMMT F and VMMT C reach best BLEU scores on the validation set when pre-trained for about 3 epochs. As shown in <ref type="figure" target="#fig_4">Figure 2</ref>, we note that when using additional noisy data VMMT C , which uses a conditional prior, performs considerably better than its counterpart VMMT F , which has a fixed prior. These results indicate that VMMT C makes better use of additional synthetic data than VMMT F . Some of the reasons that explain these results are (i) the conditional prior p(z|x) can learn to be sensitive to whether x is gold-standard or synthetic, whereas p(z) cannot; (ii) in the conditional case the posterior approximation q(z|x, y, v) can directly exploit different patterns arising from a gold-standard versus a synthetic x, y pair; and finally (iii) our synthetic data is made of targetlanguage gold-standard image descriptions, which help train the inference network's target-language BiLSTM encoder.</p><p>In <ref type="table">Table 2</ref>, we show results when applying VMMT F and VMMT C to translate the Multi30k test set. Both models and the NMT baseline are pretrained on the translated and the back-translated comparable Multi30k data sets, and are selected according to validation set BLEU scores. For comparison, we also include results for Imagina-  <ref type="table">Table 2</ref>: Results for models pre-trained using the translated and comparable Multi30k to translate the Multi30k test set. We report the mean and standard deviation over 4 independent runs. Our best overall results are highlighted in bold, and we highlight in green/red the improvement/decrease brought by our models compared to the baseline mean score. We additionally show results for the Imagination model trained on 4× more data (as reported in the authors' paper).</p><p>tion <ref type="bibr" target="#b11">(Elliott and Kádár, 2017)</ref> when trained on the translated Multi30k, the WMT News Commentary English-German dataset (240K parallel sentence pairs) and the MSCOCO image description dataset (414K German descriptions of 83K images, i.e. 5 descriptions for each image). In contrast, our models observe 29K images (i.e. the same as the models evaluated in Section 3.3) plus 145K German descriptions only. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablative experiments</head><p>In our ablation we are interested in finding out to what extent the model makes use of the latent space, i.e. how important is the latent variable.</p><p>KL free bits A common issue when training latent variable models with a strong decoder is having the KL term in the loss collapse to the prior. In practice, that would mean the model have virtually not used the latent variable z to predict image features v, but mostly as a source of stochasticity in the decoder. This can happen because the model has access to informative features from the source bi-LSTM encoder and need not learn a difficult mapping from observations to latent representations predictive of image features. For that reason, we wish to measure how well can we train latent variable MMT models while ensuring that the KL term in the loss (Equation <ref type="formula" target="#formula_5">(6)</ref>) does not collapse to the prior. We use the free bits heuristic <ref type="bibr" target="#b19">(Kingma et al., 2016)</ref> to impose a constraint on the minimum amount of information the latent variable must encode therefore preventing it  from collapsing.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, we see the results of different models trained using different number of free bits in the KL component. We note that including free bits improves translations slightly, but note that finding the number of free bits is not straightforward (i.e. it is a hyperparameter that needs searching).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Discussion</head><p>In <ref type="table" target="#tab_5">Table 4</ref> we show how our different models translate two examples of the M30k test set. In the first example (id#801), training on additional backtranslated data improves variational models but not the NMT baseline, whereas in the second example (id#873) differences between baseline and variational models still persist even when training on additional back-translated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Even though there has been growing interest in variational approaches to machine translation <ref type="bibr" target="#b46">(Zhang et al., 2016;</ref><ref type="bibr" target="#b31">Schulz et al., 2018;</ref><ref type="bibr" target="#b34">Shah and Barber, 2018)</ref> and to tasks that integrate vision and language, e.g. image description generation <ref type="bibr" target="#b28">(Pu et al., 2016;</ref><ref type="bibr" target="#b44">Wang et al., 2017)</ref>, relatively little attention has been dedicated to variational models for multi-modal translation. This is partly due to the fact that multi-modal machine translation was only recently addressed by the MT community by means of a shared task <ref type="bibr" target="#b1">Barrault et al., 2018)</ref>. Nevertheless, we now discuss relevant variational and deterministic multi-modal MT models in the literature.</p><p>Fully supervised MMT models. All submissions to the three runs of the multi-modal MT shared tasks <ref type="bibr" target="#b1">Barrault et al., 2018)</ref> model conditional probabilities directly without latent variables.</p><p>Perhaps the first MMT model proposed prior to these shared tasks is that of <ref type="bibr" target="#b14">Hitschler et al. (2016)</ref>, who used image features to re-rank translations of image descriptions generated by a phrase-based statistical MT model (PBSMT) and reported significant improvements. <ref type="bibr" target="#b35">Shah et al. (2016)</ref> propose a similar model where image logits are used to rerank the output of PBSMT. Global image features, i.e. features computed over an entire image (such as pool5 ResNet-50 features used in this work), have been directly used as "tokens" in the source sentence, to initialise encoder RNN hidden states, or as additional information used to initialise the decoder RNN states <ref type="bibr" target="#b16">(Huang et al., 2016;</ref><ref type="bibr" target="#b23">Libovický et al., 2016;</ref>. On the other hand, spatial visual features, i.e. local features that encode different parts of the image separately in different vectors, have been used in doubly-attentive models where there is one attention mechanism over the source RNN hidden states and another one over the image features <ref type="bibr" target="#b4">(Caglayan et al., 2016;</ref>.</p><p>Finally, <ref type="bibr" target="#b3">Caglayan et al. (2017)</ref> proposed to interact image features with target word embeddings, more specifically to perform an element-wise multiplication of the (projected) global image features and the target word embeddings before feeding the target word embeddings into their decoder GRU. They reported significant improvements by using image features to gate target word embeddings and won the 2017 Multi-modal MT shared task .</p><p>Multi-task MMT models. Multi-task learning MMT models are easily applicable to translate sentences without images (at test time), which is an advantage over the above-mentioned models. <ref type="bibr" target="#b24">Luong et al. (2016)</ref> proposed a multi-task approach where a model is trained using two tasks and a shared decoder: the main task is to translate from German into English and the secondary task is to generate English descriptions given an image. They show improvements in the main translation task when also training for the secondary image description task. Their model is large, i.e. a 4-layer encoder LSTM and a 4-layer decoder LSTM, and their best set up uses a ratio of 0.05 image descrip-  the NMT baseline translates it as "scheibe" (disk) and "bogen" (bow), and VMMT C also incorrectly translates it as "bogen" (bow). However, VMMT C translates without errors when trained on additional back-translated data, i.e. "torbogen" (archway). In the second example, the NMT baseline translates bay as "luft" (air) or "meer" (sea), whereas VMMT F translates it as "bucht" (bay) or "wellen" (waves) and VMMT C always as "bucht" (bay).</p><p>tion generation training data samples in comparison to translation training data samples. <ref type="bibr" target="#b11">Elliott and Kádár (2017)</ref> propose an MTL model trained to do translation (English→German) and sentenceimage ranking (English↔image), using a standard word cross-entropy and margin-based losses as its task objectives, respectively. Their model uses the pre-trained GoogleNet v3 CNN <ref type="bibr" target="#b41">(Szegedy et al., 2016)</ref> to extract pool5 features, and has a 1-layer source-language bidirectional GRU encoder and a 1-layer GRU decoder.</p><p>Variational MMT models. <ref type="bibr" target="#b43">Toyama et al. (2016)</ref> proposed a variational MMT model that is likely the most similar model to the one we put forward in this work. They build on the variational neural MT (VNMT) model of <ref type="bibr" target="#b46">Zhang et al. (2016)</ref>, which is a conditional latent model where a Gaussiandistributed prior of z is parameterised as a function of the the source sentence x m 1 , i.e. p(z|x m 1 ), and both x m 1 and z are used at each time step in an attentive decoder RNN, P (y j |x m 1 , z, y &lt;j ). In <ref type="bibr" target="#b43">Toyama et al. (2016)</ref>, image features are used as input to the inference model q λ (z|x m 1 , y n 1 , v) that approximates the posterior over the latent variable, but otherwise are not modelled and not used in the generative network. Differently from their work, we use image features in all our generative models, and propose modelling them as random observed outcomes while still being able to use our model to translate without images at test time. In the conditional case, we further use image features for posterior inference. Additionally, we also investigate both conditional and fixed priors, i.e. p(z|x m 1 ) and p(z), whereas their model is always conditional. Interestingly, we found in our experiments that fixed-prior models perform slightly better than conditional ones under limited training data. <ref type="bibr" target="#b43">Toyama et al. (2016)</ref> uses the pre-trained VGG19 CNN <ref type="bibr" target="#b36">(Simonyan and Zisserman, 2015)</ref> to extract FC7 features, and additionally experiment with using additional features from object detections obtained with the Fast RCNN network <ref type="bibr" target="#b12">(Girshick, 2015)</ref>. One more difference between their work and ours is that we only use the ResNet-50 network to extract pool5 features, and no additional pretrained CNN nor object detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future work</head><p>We have proposed a latent variable model for multimodal neural machine translation and have shown benefits from both modelling images and promoting use of latent space. We also show that in the absence of enough data to train a more complex inference network a simple fixed prior suffices, whereas when more training data is available (even noisy data) a conditional prior is preferable. Importantly, our models compare favourably to the state-of-theart.</p><p>In future work we will explore other generative models for multi-modal MT, as well as different ways to directly incorporate images into these models. We are also interested in modelling different views of the image, such as global vs. local image features, and also on modelling pixels directly (and using larger image collections).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Inference model for VMMTC: to approximate the true posterior we have access to both modalities (text x m 1 , y n 1 and image v).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Generative model of target text and image features (left), and inference model (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>↑ 0.8 56.0 (0.1) ↑ 0.0 62.1 (0.1) ↑ 1.1 66.6 (0.1) ↑ 1.4 VMMT C 37.4 (0.3) ↑ 0.6 55.8 (0.1) ↓ 0.2 62.0 (0.1) ↑ 1.0 66.5 (0.1) ↑ 1.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Validation set BLEU scores per number of pre-trained epochs for models VMMT C and VMMT F pre-trained using the comparable Multi30k and translated Multi30k data sets. The height of a bar represents the mean and the black vertical lines indicate ±1 std over 4 independent runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of applying variational MMT models to translate the Multi30k test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Results of applying VMMT models trained</cell></row><row><cell>with different numbers of free bits in the KL (Kingma</cell></row><row><cell>et al., 2016) to translate the Multi30k validation set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>a bycicle pedals through an archway . a man throws a fishing net into the bay . reference ein mann fährt auf einem fahrrad durch einen torbogen . ein mann wirft ein fischernetz in die bucht . NMT ein mann auf einem fahrrad fährt durch eine scheibe . ein mann wirft ein fischernetz in die luft . VMMTF ein mann auf einem fahrrad fährt durch einen torbogen . ein mann wirft ein fischernetz in die bucht . VMMTC ein mann auf einem fahrrad fährt durch einen bogen . ein mann wirft ein fischernetz in die bucht . NMT ein mann auf einem fahrrad fährt durch einen bogen . ein mann wirft ein fischernetz ins meer . VMMTF ein mann auf einem fahrrad fährt durch einen torbogen . ein mann wirft ein fischernetz in den wellen . VMMTC ein mann auf einem fahrrad fährt durch einen torbogen . ein mann wirft ein fischernetz in die bucht .</figDesc><table><row><cell>Model</cell><cell>Example #801</cell><cell>Example #873</cell></row><row><cell>source</cell><cell>a man on M30kT</cell><cell>M30kT</cell></row><row><cell></cell><cell>M30kT + back-translated M30kC</cell><cell>M30kT + back-translated M30kC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Translations for examples 801 and 873 of the M30k test set. In the first example, neither the NMT baseline (with or without back-translated data) nor model VMMT C (trained on limited data) could translate archway correctly;</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Locations have support on the entire real space, thus we use linear activations, scales must be strictly positive, thus we use a softplus activation.3 This is an aspect of the model we aim to explore more explicitly in the near future.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">One pre-training epoch corresponds to about 290K examples, i.e. we up-sample the smaller translated Multi30k data set to achieve the one-to-one ratio.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">There are no additional images because the comparable Multi30k consists of additional German descriptions for the same 29K images already in the translated Multi30k.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Model Architecture</head><p>Once again, we wish to translate a source sequence x m 1 x 1 , · · · , x m into a target sequence y n 1 y 1 , · · · , y n , and also predict image features v. In <ref type="figure">Figure 3</ref>, we illustrate generative and inference networks for models VMMT C and VMMT F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Generative model</head><p>Source-language encoder The source-language encoder is deterministic and implemented using a 2-layer bidirectional Long Short-Term Memory (LSTM) network <ref type="bibr" target="#b15">(Hochreiter and Schmidhuber, 1997)</ref>:</p><p>where emb is the source look-up matrix, trained jointly with the model, and h m 1 are the final source hidden states.</p><p>Target-language decoder Now we assume that z is given, and will discuss how to compute it later on. The translation model consists of a sequence of draws from a Categorical distribution over the target-language vocabulary (independently from image features v):</p><p>where f θ parameterises the distribution with an attentive encoder-decoder architecture:</p><p>where the attention mechanism is a bilinear attention <ref type="bibr" target="#b25">(Luong et al., 2015)</ref>, and the generative parameters are</p><p>Image decoder We do not model images directly, but instead as a 2048-dimensional feature vector v of pre-activations of a ResNet-50's pool5 layer. We simply draw image features from a Gaussian observation model:</p><p>where a multi-layer perceptron (MLP) maps from z to a vector of locations ν ∈ R o , and ς ∈ R &gt;0 is a hyper-parameter of the model (we use 1).</p><p>Conditional prior VMMT C Given a source sentence x m 1 , we draw an embedding z from a latent Gaussian model:</p><p>where Equations <ref type="formula">(11)</ref> and <ref type="formula">(12)</ref> employ two multilayer perceptrons (MLPs) to map from a source sentence (i.e. source hidden states) to a vector of locations µ ∈ R c and a vector of scales σ ∈ R c &gt;0 , respectively.</p><p>Fixed prior VMMT F In the MMT model VMMT F , we simply have a draw from a standard Normal prior:</p><p>Z ∼ N (0, I).</p><p>All MLPs have one hidden layer and are implemented as below (eqs. (10) to (12)):</p><p>MLP(·) = affine(ReLU(affine( · ; θ)); θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Inference model</head><p>The inference network shares the source-language encoder with the generative model and differs depending on the model (VMMT C or VMMT F ).</p><p>Conditional prior VMMT C Model VMMT C 's approximate posterior q λ (z|x m 1 , y n 1 , v) is a Gaussian distribution:</p><p>Z|x m 1 , y n 1 , v ∼ N (u, diag(s 2 ); λ).</p><p>We use two bidirectional LSTMs, one over sourceand the other over target-language words, respectively. To reduce the number of model parameters, we re-use the entire source-language BiLSTM and the target-language embeddings in the generative model but prevent updates to the generative model's parameters by blocking gradients from being back-propagated <ref type="formula">(Equation 9</ref>). Concretely, the inference model is parameterised as below:</p><p>where the set of the inference network parameters are λ = {λ x , λ y , λ v , λ mu , λ sigma }.</p><p>Fixed prior VMMT F Model VMMT F 's approximate posterior q λ (z|x m 1 ) is also a Gaussian:</p><p>where we re-use the source-language BiLSTM from the generative model but prevent updates to its parameters by blocking gradients from being backpropagated <ref type="formula">(Equation 9</ref>). Concretely, the inference model is parameterised as below:</p><p>h m 1 = detach(BiLSTM(x m 1 ; θ emb-x,lstmf-x,lstmb-x )), h x = avg(affine(h m 1 ; λ x )), u = MLP(h x ; λ mu ), s = softplus(MLP(h x ; λ sigma )),</p><p>where the set of the inference network parameters are λ = {λ x , λ mu , λ sigma }.</p><p>Finally, all MLPs are implemented as below:</p><p>MLP(·) = affine(ReLU(affine( · ; λ)); λ).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Findings of the third shared task on multimodal machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiraag</forename><surname>Lala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="304" to="323" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Results of the wmt17 metrics shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Kamran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="489" to="513" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LIUM-CVC Submissions for WMT17 Multimodal Translation Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bardet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multimodal attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<idno>abs/1609.03976</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incorporating Global Visual Features into Attention-based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Doubly-Attentive Decoder for Multi-modal Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1913" to="1924" />
		</imprint>
	</monogr>
	<note>Vancouver, Canada</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Autoencoding variational neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Eikema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10564</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Findings of the second shared task on multimodal machine translation and multilingual image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="215" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi30K: Multilingual English-German Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Vision and Language</title>
		<meeting>the 5th Workshop on Vision and Language<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagination improves multimodal translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kádár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="130" to="141" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.169</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep Residual Learning for Image Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal Pivots for Image Caption Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigehiko</forename><surname>Schamoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2399" to="2409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attentionbased Multimodal Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sz-Rung</forename><surname>Shiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="639" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrencek</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">OpenNMT: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-4012</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention Strategies for Multi-Source Sequenceto-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CUNI System for WMT16 Automatic Post-Editing and Multimodal Translation Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Tlustý</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-Task Sequence to Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>Philadelphia, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">chrf: character n-gram f-score for automatic mt evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="392" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Variational autoencoder for deep learning of images, labels and captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2352" to="2360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning, ICML 2014</title>
		<meeting>the 31th International Conference on Machine Learning, ICML 2014<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A stochastic decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generative neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1346" to="1355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SHEF-Multimodal: Grounding Machine Translation on Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Shared Task on Multimodal Machine Translation and Crosslingual Image Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation, WMT 2016, colocated with ACL 2016</title>
		<meeting>the First Conference on Machine Translation, WMT 2016, colocated with ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fitting sentence level translation evaluation with many dense features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Stanojević</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="202" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Doubly stochastic variational bayes for nonconjugate inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1971" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Neural machine translation with latent semantic of image and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joji</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Misono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotaro</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<idno>abs/1611.08459</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5756" to="5766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Variational neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
