<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRAINING KEYWORD SPOTTERS WITH LIMITED AND SYNTHESIZED SPEECH DATA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Kilgour</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Roblek</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Sharifi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">TRAINING KEYWORD SPOTTERS WITH LIMITED AND SYNTHESIZED SPEECH DATA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-keyword spotting</term>
					<term>spoken term detection</term>
					<term>limited data</term>
					<term>speech synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the rise of low power speech-enabled devices, there is a growing demand to quickly produce models for recognizing arbitrary sets of keywords. As with many machine learning tasks, one of the most challenging parts in the model creation process is obtaining a sufficient amount of training data. In this paper, we explore the effectiveness of synthesized speech data in training small, spoken term detection models of around 400k parameters. Instead of training such models directly on the audio or low level features such as MFCCs, we use a pre-trained speech embedding model trained to extract useful features for keyword spotting models. Using this speech embedding, we show that a model which detects 10 keywords when trained on only synthetic speech is equivalent to a model trained on over 500 real examples. We also show that a model without our speech embeddings would need to be trained on over 4000 real examples to reach the same accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the proliferation of personal smart devices, there has been a corresponding increase in the use of voice control. Although it is possible to use full speech recognition systems that either run on the cloud or are optimized for on device usage, both of these approaches have disadvantages. Offloading the computation to the cloud implies sending the continuous stream of audio recorded by the device to an external server, which raises many privacy concerns, increases the overall cost of the device and reduces its reliability. And while on device speech recognition systems would alleviate these problems, they require an order of magnitude more power than keyword spotters designed to detect only the specific set of keywords relevant to the device's use. In cases where the commands issued to the smart device are more complex, the keyword spotter can be used as trigger for a full speech recognition system that is either run on the device or in the cloud.</p><p>A setup that allows for quick generation of models to recognize custom sets of keywords can allow device manufacturers to rapidly prototype and iterate during product development. It may also provide a mechanism for end users to customize the speech interfaces of their devices, giving them more control when interacting with their devices.</p><p>In this paper, we investigate how many training examples we actually need to build a working keyword detector and examine two approaches of reducing the amount to a level that would more easily facilitate rapid prototyping. First we show how a speech embedding model trained to produce embeddings for keyword spotting can be used to reduce the number of examples required to train a keyword spotter on a new set of keywords from 35k down to under 2k. We further demonstrate that replacing these 2k training examples with speech examples generated from a speech synthesizer only reduces the accuracy by about 2% to 92.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>There has been much related prior work in the areas of keyword spotting systems, augmentation with synthesized speech, transfer learning, and multi-task learning. Some of the work within the past few years in the area of keyword spotting include that of Chen et al <ref type="bibr" target="#b1">[2]</ref> on using a DNN for recognizing the phrase "okay google", the hybrid systems of TDNN-HMM <ref type="bibr" target="#b15">[16]</ref> and DNN-HMM <ref type="bibr" target="#b10">[11]</ref> used in Amazon's Alexa, and Sigitia et al's work for Apple's Siri <ref type="bibr" target="#b14">[15]</ref> (also of DNN-HMM architecture). There are also examples of using convolutional networks for keyword spotting. Couke et al <ref type="bibr" target="#b4">[5]</ref> employed the dilated convolutions of the WaveNet <ref type="bibr" target="#b8">[9]</ref> architecture and showed results that were more robust to noise than both LSTM or regular CNN based models. The temporal convolution on the ResNet <ref type="bibr" target="#b5">[6]</ref> architecture of Choi et al <ref type="bibr" target="#b2">[3]</ref> is notable for achieving an accuracy of 96.6% on the Speech Commands dataset <ref type="bibr" target="#b16">[17]</ref> with only 350k parameters.</p><p>On the subject of transfer learning and multi-task learning, Sun et al <ref type="bibr" target="#b15">[16]</ref> use a full speech recognition system to initialize the weights of their keyword spotter as well as a parallel task where they share the bottom layers for the keyword spotter with the speech recognition system's acoustic model. Raju et al <ref type="bibr" target="#b10">[11]</ref> use a similar multi-target training setup for their keyword spotter.</p><p>The use of synthesized speech for data augmentation is also not new <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref>. In particular, the work described in this paper utilizes a Tacotron 2 <ref type="bibr" target="#b13">[14]</ref> based synthesizer to generate the synthetic speech examples, as is the case with the work by Li et al <ref type="bibr" target="#b6">[7]</ref> and Rosenberg et al <ref type="bibr" target="#b11">[12]</ref>. arXiv:2002.01322v1 [eess.AS] 31 Jan 2020</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODEL ARCHITECTURE</head><p>Our model is designed for deployment in an environment where both memory and compute power are very limited, such as on a digital signal processor (DSP). It runs on top of a low footprint feature extractor that provides a 32 dimensional log mel feature vector covering the frequency range from 60 Hz to 3800 Hz, quantized to 8 bits every 10 ms. Under the assumption that much of the work required to classify a small set of keywords is independent of the actual keywords, we split the model into two parts: an embedding model that contains 5 convolutional blocks (∼330k weights) and a head model that contains a single convolutional block along with a classification block (∼55k weights).</p><p>Each detection requires an input context of 198 feature vectors (approximately 2s), which is provided by a low memory footprint feature extractor. This is followed by 6 convolutional blocks and a classification block. Each convolutional block consists of 5 layers: a 1x3 convolution, a 3x1 convolution, a maxpool layer, a 1x3 convolution, and a 3x1 convolution. The final block only uses 3x1 convolutions as the frequency dimension is 1 after the previous layers. Starting with 24 channels in the first block, we increase the number of channels by 24 in each new block until a maximum of 96 is reached. The classification block can be either a maxpool layer followed by a 1x1 convolution if a continuous stream of predictions is required, or a maxpool layer followed by a fully connected layer. The results reported Section 5 are based on evaluations on single words where the fully connected layer variant is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Embedding Model</head><p>The embedding model converts a stream of audio into a stream of 96-dimensional feature vectors, one every 80 ms. To ensure that the resulting embedding is useful for arbitrary sets of keywords, we took 5000 keywords (most of them are actually 2-3 words long) and split them into random groups of 40 keywords. The resulting 125 groups of keywords are used to train 125 keyword spotting models with shared weights for the embedding model part (see <ref type="figure" target="#fig_0">Figure 1</ref>). We used roughly 200 million 2-second audio clips from YouTube for training, of which 100 million contained the target keywords and the other 100 million were used as non-target examples. This embedding model is available on Tensorflow Hub for public use. The embedding model is trained using TensorFlow [1] on 20 GPUs for 2 days and is available on TensorFlow Hub (https: //tfhub.dev/google/speech_embedding/1) for reuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Head Model</head><p>We include the embedding model in our graph as a Tensor-Flow Hub module with fine-tuning switched off. A head </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL SETUP</head><p>The data used in our experiments can be categorized into two types: real speech data and synthesized speech data. In addition to the YouTube data used to train the embedding model, we also use the Speech Commands dataset <ref type="bibr" target="#b16">[17]</ref> as a source of our real speech data. The speech commands dataset contains 80k short utterances of 35 different words, 10 of which are considered target words. It is not used at all in the training of the embedding model; the embedding model is trained only on the YouTube data. All reported accuracy numbers are on the Speech Commands evaluation set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baseline and the Effects of Limited Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Synthesized Speech Data</head><p>We synthesize each of the 35 speech command words using 92 different voices 1 with a text-to-speech system based on Tacotron 2 <ref type="bibr" target="#b13">[14]</ref>, with considerations for fast synthesis that could be reasonably run on device. The quality of these examples is similar to what can be generated using the Google Cloud Text-to-Speech API <ref type="bibr" target="#b3">[4]</ref>. We examine two use cases for synthetic data, either as a replacement for real data or as additional data that could be used to supplement existing real data. To test the first use case, we begin by training models on only synthetic speech and gradually replace random subsets of various sizes with real examples. For the second case, we start with the synthesized speech data and add sets of real speech data randomly selected from the Speech Commands training set. For both cases, again we perform each data selection and model training 20 times and compute their mean and standard deviation.</p><p>In addition, we also experimented with creating additional synthetic speech examples from the directly synthesized speech examples via traditional data augmentation techniques. We tried both pitch-and tempo-shifts, as well as adding room reverberation effects and white noise. Unfortunately, the preliminary results were disappointing, as we saw no significant improvements in accuracy when compared to using only the directly synthesized speech data. This parallels the findings of <ref type="bibr" target="#b9">[10]</ref> where their augmentation of the small amount of data available for a new class failed to improve its classifier performance. We also suspect this may be due to the embedding model already having learned to deal with these distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>For use as baselines, we trained both a full model and a head model using the full speech commands data. In <ref type="table" target="#tab_0">Table 1</ref> we Using our speech embeddings improves the performance of the head model trained on the full speech commands dataset only slightly, to 97.7%. We can, however, see significant gains on the head models trained on small numbers of examples. Using 3k real examples, we see the accuracy increase from 88.7% to 95.3%, which is equivalent to a 58% relative decrease in error. Performance of the model trained on only synthetic speech jumps to 92.6% with a relative error rate reduction of 83%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Replacing Real Data</head><p>These last two results can be seen as the two data points at the extremes of the possibilities between using only real speech data and using only synthetic data to train the head model. We explore in <ref type="figure" target="#fig_2">Figure 2</ref> some intermediate points on this continuum, where we use a certain percentage of real examples with the remaining being synthetic. The plot shows the mean and variance of the results based on 20 random selections per percentage-combination. Overall the error bars are quite large and indicate a lot of variability between different random selections of training data. Despite this variability, we see a clear general tendency that by simply replacing a small number of synthetic examples with real examples, we already bridge a large part of the gap between the two extremes. After replacing about 50% of the data with real data, the curve begins to saturate as the accuracy differences of the models have already been reduced by almost 90%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Augmenting Real Data</head><p>We explored the use of synthetic data to augment the real data that already exists, or to reduce the amount of real data that needs to be collected. In real applications, this approach is . This is also when we see a significant difference between the models trained on only real examples and models trained on those augmented with synthetic data.</p><p>When the training data has 10 real examples per word, the difference from having the synthetic data has grown to 2.5% (absolute) and reaches 3% when training data has only 5 real examples per word. This is also the point where the accuracy of the head model trained without synthetic data drops below the accuracy of a full model trained on 125 real examples per word. The accuracy of head models augmented with synthetic data is consistently higher than the accuracy of a full model trained on 125 examples per word. This is true even when the number real examples being augmented is 0, so that the head model is in fact trained on only synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, we demonstrate the effectiveness of using synthesized speech data for training keyword spotting systems in the context of a speech embedding model. While synthesized speech does not provide enough useful information to train a full keyword spotter, it can be used to train a head model on top of a well trained speech embedding model. The accuracy of such a head model is equivalent to the accuracy of a head model using 50 real examples per word, or a full model trained on over 4000 real examples per word. Our small head model can be trained in a few minutes on a modern GPU. In future work we plan to investigate how to efficiently train the model on a small device with limited compute resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>We would like to thank Marco Tagliasacchi, Dick Lyon, Andrew Rosenberg and Bhuvana Ramabhadran for their help and feedback.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Training setup of the embedding model (light orange block). Multiple head models with different groups of target keywords are trained in parallel on top of the shared embedding model which learns an embedding that should be useful for any arbitrary group of keywords. After training, the head models are discarded. model can be trained on single GPU in under 30 minutes. On small data sets, this can be much faster; with models trained on 100k examples converging in 180 seconds, and models trained on 1000 examples converging in around 30-40 seconds. This head model architecture scales well with the number of keywords, requiring only an extra 96 weights per new target keyword. A fully connected model on the same input would require 1536 weights per new target keyword.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><label></label><figDesc>baseline is established by training and testing on the whole speech commands dataset. For both the full model and the head model on top of a pre-trained embedding model, we evaluate the effects of training on a limited amount of data by creating training sets of various sizes. These training sets are created by selecting between 1 and 1000 random examples from the Speech Commands training set for each of the 35 speech command words. Since the Speech Commands</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Performance with different proportions of real vs synthetic speech examples. compare the models trained using only our generated set of 3220 synthetic examples and models trained on an equivalent number of real examples. Training the full model on only the synthetic examples results in an accuracy of 56.7%. Using the same number of real examples, the accuracy increases to 88.7%. With the full speech commands dataset, we are able to obtain an accuracy of 97.4%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Performance of different amounts of real speech examples, with and without augmentation using synthetic speech examples, optionally based on the speech embedding model. The standard deviation over 20 runs is shown for each data point. more realistic, as one would almost certainly use all the synthetic speech examples that one has, and go about collecting increasing amounts of real speech examples. Figure 3 shows these results. At the right of the plot we can see that when we have 1000 real examples per word (35k total), using the embedding model improves the accuracy noticeably from 94.8% to 96.8%. The inclusion of the synthetic speech data does not result in any further improvements. Decreasing the number of real examples to 125 per word reduces the accuracy of the full model by 4.5% (absolute) to 90.3%, while the head model, at an accuracy of 95.8%, only loses 1% (absolute). This is also the first point where adding the synthetic data produces a measurable improvement of 0.1%. Reducing the number of real examples further causes the performance of the full model to rapidly decrease. As we decrease the number of real examples, before dropping below 50 examples per word, the performance of the head model trained on only real examples is better than or on par with the full model trained on 1000 examples per word. Head models that are also trained on synthetic data remain at least as accurate as the full model trained on 1000 examples per word until the number of real examples drops below about 25</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of the full and head models trained on synthetic data, an equivalent amount of real data, and the full speech commands training data set.</figDesc><table><row><cell>Training data</cell><cell cols="3">Size Full model Head model</cell></row><row><cell>speech commands</cell><cell>80k</cell><cell>97.4%</cell><cell>97.7%</cell></row><row><cell>synthetic data</cell><cell>3220</cell><cell>56.7%</cell><cell>92.6%</cell></row><row><cell cols="2">equivalent real data 3220</cell><cell>88.7%</cell><cell>95.3%</cell></row><row><cell cols="4">dataset contains 35 words, this results in models trained on</cell></row><row><cell cols="4">35 to 35000 examples. To take into consideration the vari-</cell></row><row><cell cols="4">ability in the selections, for each choice of training set size,</cell></row><row><cell cols="4">we perform the selection and training 20 times and compute</cell></row><row><cell cols="2">the mean and standard deviation.</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We originally synthesized 100 examples per word, but later realized that 8 voices were duplicates and removed them.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martn</forename><surname>Abadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs.DC</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Small-footprint keyword spotting using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4087" to="4091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Temporal Convolution for Realtime Keyword Spotting on Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03814</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.SD</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Google Cloud Text-to-Speech API</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Cloud</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/text-to-speech/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>visited on 10/15/2019)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient Keyword Spotting Using Dilated Convolutions and Gating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2019.8683474</idno>
		<idno>DOI: 10 . 1109/ icassp . 2019 . 8683474</idno>
		<ptr target="http://dx.doi.org/10.1109/icassp.2019.8683474" />
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Training Neural Speech Recognition Systems with Synthetic Speech Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00707</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leveraging Sequence-to-Sequence Speech Synthesis for Enhancing Acoustic-to-Word Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Mimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<biblScope unit="page" from="477" to="484" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A Generative Model for Raw Audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs.SD</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lowshot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5822" to="5830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Data Augmentation for Robust Keyword Spotting under Playback Interference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Raju</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00563</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Speech Recognition with Augmented Synthesized Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11699</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Using Synthesized Speech to Improve Speech Recognition for Low-Resource Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luise</forename><surname>Valentin Rygaard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2018.8461368</idno>
		<idno>DOI: 10 . 1109 /icassp . 2018 . 8461368</idno>
		<ptr target="http://dx.doi.org/10.1109/ICASSP.2018.8461368" />
	</analytic>
	<monogr>
		<title level="m">Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on Acoustics, Speech and Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient Voice Trigger Detection for Low Resource Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Sigtia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Compressed Time Delay Neural Network for Small-Footprint Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limitedvocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
