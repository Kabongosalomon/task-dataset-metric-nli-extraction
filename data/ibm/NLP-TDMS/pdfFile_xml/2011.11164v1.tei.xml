<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learnable Boundary Guided Adversarial Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SmartMore 3 Tencent</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SmartMore 3 Tencent</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learnable Boundary Guided Adversarial Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous adversarial training raises model robustness under the compromise of accuracy on natural data. In this paper, our target is to reduce natural accuracy degradation. We use the model logits from one clean model M natural to guide learning of the robust model M robust , taking into consideration that logits from the well trained clean model M natural embed the most discriminative features of natural data, e.g., generalizable classifier boundary. Our solution is to constrain logits from the robust model M robust that takes adversarial examples as input and make it similar to those from a clean model M natural fed with corresponding natural data. It lets M robust inherit the classifier boundary of M natural . Thus, we name our method Boundary Guided Adversarial Training (BGAT). Moreover, we generalize BGAT to Learnable Boundary Guided Adversarial Training (LBGAT) by training M natural and M robust simultaneously and collaboratively to learn one most robustnessfriendly classifier boundary for the strongest robustness. Extensive experiments are conducted on CIFAR-10, CIFAR-100 and challenging Tiny ImageNet datasets. Along with other state-of-the-art adversarial training approaches, e.g., Adversarial Logit Pairing (ALP) and TRADES, the performance is further enhanced.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have achieved great success in many tasks. With the concern of security of deep models, several methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr">33]</ref> have shown that deep models could be vulnerable to adversarial attack. Data that is intentionally optimized may easily fool strong classifiers.</p><p>In response to the vulnerability of deep neural networks, adversarial defense has become an essential topic in computer vision. There are now a sizable body of work exploring different ways to get adversarial settings, including defensive distillation <ref type="bibr" target="#b26">[26]</ref>, feature squeezing <ref type="bibr" target="#b48">[48]</ref>, randomization based methods <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b9">10]</ref> and augmenting the training Vanilla AT <ref type="bibr" target="#b23">[23]</ref> ALP <ref type="bibr" target="#b17">[18]</ref> TRADES <ref type="bibr" target="#b51">[51]</ref> Ours Acc (%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Natural Acc (%) Robust Acc (%) <ref type="figure">Figure 1</ref>: Model robustness on CIFAR-100 evaluated with 20 iterations PGD under white-box attack. "Natural Acc" represents classification accuracy on natural (clean) data. "Robust Acc" represents classification accuracy on adversarial data. Our method (LBGAT+TRADES with α = 0) improves robustness with the least natural accuracy degradation.</p><p>with adversarial examples <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b38">38]</ref>, i.e., adversarial training. However, training a robust model is still challenging. Recently, adversarial training with PGD attack <ref type="bibr" target="#b23">[23]</ref> becomes an effective defense strategy. However, when plotting results of recent work <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">23]</ref> in <ref type="figure">Fig. 1</ref>, it is still noticeable that higher robustness is often accompanied with more accuracy degradation on natural data classification. Different from previous work that mainly pursues various ways to improve robustness, we meanwhile pursue accuracy preservation on natural data. In this paper, we propose a novel adversarial training scheme, which significantly improves classification accuracy on natural data. It also achieves high robustness under black-and white-box attack. We take advantage of logits from a clean model, which is trained only on natural data, to guide the learning of a robust model.</p><p>A conceptual illustration is shown in <ref type="figure" target="#fig_1">Fig. 2</ref> to explain our motivation. As shown in (a), when only trained on natural (clean) data, the learned model M natural separates natural data (plotted in yellow) well. But it easily fails to classify perturbed data and misclassifies the dark circle into the rectangle category. Previous standard adversarial train- , adversarial examples (plotted in black) can be mostly correctly classified with this strategy. However, some clean data is wrong. Thus, our motivation is to leverage the clean model M natural to improve the natural data accuracy of M robust . In order to seek guidance from clean model M natural , we expect the logit output of adversarial example x adv from M robust to be similar to logits output of corresponding natural data x that goes through M natural . As plotted in <ref type="figure" target="#fig_1">Fig.  2(b)</ref>, the classifier boundary of our M robust is constrained by that of the clean model, which helps classify the clean data into correct categories. At the same time, adversarial examples are also correctly labeled, benefiting from the adversarial training scheme.</p><p>Instead of constraining M robust with the classifier boundary from one well trained static M natural , we further generalize our BGAT method to Learnable Boundary Guided Adversarial Training (LBGAT) by training M natural and our required model M robust at the same time to dynamically adjust classifier boundary of M natural and learn the robustness-friendly one to further help M robust enhance robustness. To show the flexibility of our method, we incorporate our model into state-of-the-art methods Adversarial Logit Pairing (ALP) <ref type="bibr" target="#b17">[18]</ref> and TRADES <ref type="bibr" target="#b51">[51]</ref> respectively and accomplish remarkable improvement over the baselines.</p><p>We conduct experiments on CIFAR-10, CIFAR-100, and the more challenging Tiny ImageNet to evaluate the performance of our models under both white-and black-box attack. Our models achieve impressive performance on these datasets. For example, our "LBGAT+TRADES (α = 0)" improves TRADES (α = 6) by 13.53% and 3.3% on nat-ural data of CIFAR-100 and CIFAR-10 respectively. It is noteworthy that the robust accuracy is even slightly better than TRADES (α = 6) under the strongest auto-attack <ref type="bibr" target="#b7">[8]</ref>. On Tiny-ImageNet, our "LBGAT+TRADES (α = 0)" outperforms TRADES (α = 6) by 9.29% on natural data and surpasses it in aspect of model robustness under 20 iterations of PGD attack. Moreover, under the black-box attack setting, our best model is 12.83% higher than TRADES (α = 6) with the natural trained model as the source model on CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Adversarial Attack</head><p>White-box Attack Szegedy et al. <ref type="bibr" target="#b35">[35]</ref> observed that CNNs are vulnerable to adversarial examples computed by the proposed box-constrained L-BFGS attack method. Goodfellow et al. <ref type="bibr" target="#b12">[13]</ref> attributed the existence of adversarial examples to the linear nature of networks, which yields the fast gradient sign method (FGSM) for efficiently generating adversarial examples.</p><p>FGSM was further extended to different versions of iterative attack methods. Kurakin et al. <ref type="bibr" target="#b19">[20]</ref> showed that adversarial examples could exist in the physical world with an I-FGSM attack and iteratively applied FGSM multiple times with a small step size. Madry et al. <ref type="bibr" target="#b23">[23]</ref> proposed Projected Gradient Descent (PGD) method as a universal "first-order adversary", i.e., the most active attack utilizing the local first-order information about the network.</p><p>Dong et al. <ref type="bibr" target="#b10">[11]</ref> integrated the momentum term into an iterative process for attack, called MI-FGSM, to stabilize update of directions and escape from poor local maxima during iterations. This method obtains more transferable adversarial examples. Moreover, boundary-based methods like DeepFool <ref type="bibr" target="#b25">[25]</ref> and optimization-based methods like C&amp;W <ref type="bibr" target="#b2">[3]</ref> were also developed, making adversarial defense more challenging. Recently, the ensemble of diverse attack methods -auto-attack <ref type="bibr" target="#b7">[8]</ref> by Croce et al., consisting of APGD-CE <ref type="bibr" target="#b7">[8]</ref>, APGD-DLR <ref type="bibr" target="#b7">[8]</ref>, FAB <ref type="bibr" target="#b6">[7]</ref>, and Square Attack <ref type="bibr" target="#b0">[1]</ref>, became popular benchmark for testing model robustness.</p><p>Black-box Attack There are also many ways to explore the transferability of adversarial examples for the blackbox attack. Liu et al. <ref type="bibr" target="#b20">[21]</ref> was the first to study the transferability of targeted adversarial examples. They observed that a large proportion of target adversarial examples were able to transfer with their target labels using the proposed ensemble-based attack method. Dong et al. <ref type="bibr" target="#b10">[11]</ref> showed that iterative attack methods incorporating the momentum term achieved better transferability. Further, Xie et al. <ref type="bibr" target="#b47">[47]</ref> boosted the transferability of adversarial examples by creating diverse input patterns with random resize and random padding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Adversarial Defense</head><p>Recent work focuses generally on developing defense methods to improve model robustness, including input transformation-based methods, randomization based methods <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b9">10]</ref>, and adversarial training <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b38">38]</ref>. Athalye et al. <ref type="bibr" target="#b1">[2]</ref> showed that adversarial training with PGD had withstood active attacks. Tramèr et al. <ref type="bibr" target="#b38">[38]</ref>   <ref type="bibr" target="#b45">[45]</ref> studied the effect of normalization in adversarial training and proposed the Mixture BN mechanism that uses separate batch normalization layers for natural data and adversarial examples in one model. It still requires the strong assumption of knowing whether an image is natural or adversarial, at inference time, which may not be that practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Knowledge Distillation</head><p>Knowledge distillation was first used in <ref type="bibr" target="#b15">[16]</ref> by Hinton et al., which was then widely applied to distill knowledge from a teacher model to a student model. The typical application of knowledge distillation is model compression, transferring from a large network or ensembles to a small network that better suits low-cost computing. Since this work, several methods <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">37]</ref> were proposed to further improve performance on model compressing and other tasks.</p><p>Goldblum et al. <ref type="bibr" target="#b11">[12]</ref> analyzed the application of knowledge distillation in adversarial training and proposed Adversarial Robust Distillation (ARD) to transfer robustness from a large adversarially trained model to a smaller one. In this paper, we propose to use one robustness-friendly boundary learned by one natural model, not necessarily large, to guide the adversarial training without cross-entropy loss. By this way, the robust model can sufficiently inherit the classifier boundary and thus preserves high accuracy on natural data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Boundary Guided Adversarial Training</head><p>As suggested by Madry et al. <ref type="bibr" target="#b23">[23]</ref>, projected gradient descent (PGD) is a universal first-order adversary. Robust methods to defense PGD might be able to resist attack stemming from other first-order methods as well. Similarly, we use adversarial training with PGD as</p><formula xml:id="formula_0">min θ E (x,y)∈p data arg max δL (θ, x + δ, y)<label>(1)</label></formula><p>wherep data is the training data distribution,L(θ, x, y) is the standard cross-entropy loss function with data point x and its corresponding true label y. θ represents parameters of the model, and the maximization with respect to δ is approximated using noisy BIM <ref type="bibr" target="#b19">[20]</ref>. We denote the adversarial example x + δ across the paper as x adv . Following previous work <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b23">23]</ref>, δ is bounded by l ∞ . Our expectation of the robust model is to achieve decent robustness and at the same time keep high accuracy on natural images. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, we make use of logits from a clean model to help shape the classifier boundary of the robust model. The logits of our required robust model M robust with x adv taken as input should be similar to those of M natural taking x as input. This relation is expressed as</p><formula xml:id="formula_1">min θ E (x,y)∈p data L M robust (x adv ), M natural (x) (2)</formula><p>where L is Mean Square Error (MSE) loss function in our experiments and M(x) denotes the logits of model M taking x as input. θ is the parameter of M robust . We randomly initialize M robust and off-line train M natural on natural data in our experiments.</p><p>Our method can be understood from the perspective of classifier boundary guidance. Here we give analysis of why our method can yield high performance on natural data. Classifier Boundary Guidance Since we assume that M natural is well trained on natural data, logits from M natural embed more discriminative features for classification, especially the classifier boundary. According to Eq. <ref type="formula">(2)</ref>, when we impose the logits constraints, the system penalizes more on those pairs (x and x adv ) that have more substantial discrepancy in classification. Therefore, this logit guidance makes M robust inherit decent classifier boundary for adversarial data. Actually, the inherited classifier boundary is still applicable to natural data in following explanation. It is noteworthy that the adversarial example x adv is located in the l ∞ ball of x. According to the min-max mechanism of PGD <ref type="bibr" target="#b23">[23]</ref>, when the adversarial training converges, the loss value corresponding to x adv is always larger than the loss value corresponding to x when passing x adv and x into the same model M robust . Therefore, when we pull x adv into the correct class with our proposed logits constraints, x is also squeezed into the correct class. Thus the inherited classifier boundary from M natural separates natural data well and preserves high natural accuracy. Read mini-batch X = {x 1 , ..., x m }, Y = {y 1 , ..., y m } from training set; <ref type="bibr">6:</ref> Get adversarial examples X adv = {x adv 1 , ..., x adv m } by PGD attack with input X, Y ; <ref type="bibr">7:</ref> output n = M natural (X); <ref type="bibr">8:</ref> output r = M robust (X adv ); <ref type="bibr">9:</ref> loss ce = cross − entropy(σ(output n ), Y ); <ref type="bibr">10:</ref> loss reg = L(output n , output r ); 11: <ref type="bibr">13:</ref> until training converges oratively. The loss function is therefore changed from Eq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learnable</head><formula xml:id="formula_2">θ * = θ * − η 2 m i=1 ∇ θ * (βloss ce + loss reg )/m; 12: θ = θ − η 2 m i=1 ∇ θ (βloss ce + loss reg )/m;</formula><formula xml:id="formula_3">(2) to min θ,θ * E (x,y)∈p data L M robust (x adv ), M natural (x) + β CE σ(M natural (x)), y<label>(3)</label></formula><p>where x adv is the adversarial example corresponding to its natural data x, and y is the true label. σ(·) is a softmax function. CE represents cross-entropy loss, M natural and M robust are parameterized by θ * and θ respectively. We use Mean Square Error (MSE) loss as L function. β is the trade-off parameter. In this paper, we choose β = 1. We randomly initialize M robust and M natural in our experiments. As shown in <ref type="figure" target="#fig_3">Fig 3,</ref> under the regularization of the proposed logits constraints, i.e., the L(·) loss item in Eq. (3), M natural adaptively learns one most robustness-friendly classifier boundary during the collaborative training. At the same time, it guarantees least performance degradation on natural data with CE(·) loss item in Eq. (3). Note there is no additional cross-entropy loss for optimizing M robust , which makes the classifier boundary be sufficiently inherited from M natural . More details are listed in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural Classifier Boundary Improving Robustness</head><p>Zhang et al. <ref type="bibr" target="#b51">[51]</ref> identified trade-off between performance on natural data and robust accuracy. Xie et al. <ref type="bibr" target="#b43">[43]</ref> observed that adversarial examples were helpful to model generalization ability on natural images. However, using mod-els trained only with natural data to enhance model robustness remains unexplored. We instead notice that proper classifier boundary learned by the natural trained model not only helps preserve high natural accuracy but also enhances model robustness (2.44% improvement on CIFAR-100 dataset under the strongest auto-attack <ref type="bibr" target="#b7">[8]</ref> shown in Table 4). We attribute the improvement to guidance of the natural classifier boundary in the course of adversarial training.</p><p>Comparison with ALP Adversarial logit pairing (ALP) <ref type="bibr" target="#b17">[18]</ref> pairs logits from x and x adv output from the same robust model, encouraging similar representation between x and x adv . Ours is totally different by nature. Eqs. (2) (3) reveals that logits of x adv from robust model M robust are guided by logits output of x from model M natural , leading to more generalizable classifier boundary for natural data. The logits come from two different models. This is the most fundamental difference to ALP. We list ALP as one baseline and give extensive comparisons in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Flexibility</head><p>Our method provides a new training scheme for adversarial training. It does not conflict or overlap with other adversarial training methods. We show the flexibility of our approach by using it in other state-of-the-art methods, e.g., Adversarial Logit Pairing (ALP) <ref type="bibr" target="#b17">[18]</ref> and TRADES method <ref type="bibr" target="#b51">[51]</ref>. We validate the improvement over these baselines.</p><p>Combined with Adversarial Logit Paring Adversarial logit pairing (ALP) requires the logits of natural data x and the corresponding adversarial example x adv to be the same in one model, which is achieved by adding an extra mean square loss item between two logits output. We combine our BGAT with ALP as the loss of</p><formula xml:id="formula_4">min θ E (x,y)∈p data L M robust (x adv ), M natural (x) + α M SE M robust (x adv ), M robust (x)<label>(4)</label></formula><p>where α is a trade-off parameter. σ(·) is a softmax function and y is the true label. θ is the parameter of M robust . We replace the cross-entropy loss item CE(σ(M robust (x adv )), y) in the original ALP loss function with our Eq. (2).</p><p>Combined with TRADES The proposed TRADES algorithm <ref type="bibr" target="#b51">[51]</ref> explores the trade-off between model robustness and accuracy on natural data by optimizing one regularized surrogate loss. We use our BGAT in the TRADES algorithm as</p><formula xml:id="formula_5">min θ E (x,y)∈p data L M robust (x adv ), M natural (x) + α DKL σ(M robust (x adv ))||σ(M robust (x))<label>(5)</label></formula><p>where α is still a trade-off parameter. θ is the parameter of M robust . σ(·) is softmax function and y is the true label. D KL (·) is the boundary error term, pushing classifier boundary away from data point x, originally defined in TRADES <ref type="bibr" target="#b51">[51]</ref>. We replace the cross-entropy loss item of CE(σ(M robust (x)), y) in original TRADES loss with our Eq. (2).</p><p>It is noted that our LBGAT method can also be combined with both ALP and TRADES methods by simply replacing the first loss item in Eqs. <ref type="bibr" target="#b3">(4)</ref> and <ref type="formula" target="#formula_5">(5)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we verify the effectiveness of our methods by conducting both white-and black-box attack following the same experimental settings in <ref type="bibr" target="#b51">[51]</ref>, i.e., applying F GSM k (white-box or black-box) attack with 20 iterations, perturbation size = 0.031 with step size 0.003.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets.</head><p>To evaluate the robustness of our models, we conduct extensive experiments on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. CIFAR-100 has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. Tiny Imagenet <ref type="bibr" target="#b8">[9]</ref>, which is with more complex data, is a miniature of ImageNet dataset. It has 200 classes. Each class has 500 training images, 50 validation images.</p><p>In our experiments, we resize the image to 32x32 and normalize pixel values to [0,1]. Following <ref type="bibr" target="#b51">[51]</ref>, we perform standard data augmentation including random crops with 4 pixels of padding and random horizontal flip during training.</p><p>Training Details. We use the same neural network architecture as <ref type="bibr" target="#b51">[51]</ref>, i.e., the wide residual network WRN-34-10. Following <ref type="bibr" target="#b51">[51]</ref>, We set perturbation = 0.031, perturbation step size η 1 = 0.007, number of iterations K = 10, learning rate η 2 = 0.1, batch size m = 128, and number of training epochs 100 with transition epochs {75, 90} on the training dataset. Similarly, SGD optimizer with momentum 0.9 and weight decay 2e-4 is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Effectiveness of Our Methods</head><p>We first verify the effectiveness of our methods compared with vanilla Adversarial Training (AT). Evaluation of model robustness is under the white-box attack using the same setting as described at the beginning of Sec. 4. Both our BGAT and LBGAT methods significantly outperform vanilla AT shown by results in <ref type="table" target="#tab_1">Table 1</ref>. As analyzed in Sec. 3.2, the BGAT method can achieve higher natural accuracy while the LBGAT method tends to have stronger ro- bustness. Since we aim to achieve the strongest robustness while preserving natural accuracy as high as possible, we treat LBGAT as our core method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Combing with ALP and TRADES</head><p>To verify the flexibility of our method, we show that, combined with our BGAT and LBGAT methods, ALP and TRADES further improve performance. For ALP, BGAT+ALP and LBGAT+ALP methods, we adopt α = 1 following the setting in <ref type="bibr" target="#b17">[18]</ref>. For the TRADES method, we adopt α = 6, with which TRADES achieves the best robustness, as demonstrated in <ref type="bibr" target="#b51">[51]</ref>.</p><p>The evaluation is under the white-box attack following the same setting as described at the beginning of Sec. 4. We summarize the results in <ref type="table" target="#tab_2">Table 2</ref>. Equipped with regularization items of ALP and TRADES, our method can further enhance model robustness. For CIFAR-100, LBGAT+ALP outperforms ALP by 2.92% and 6.31% respectively on natural accuracy and robust accuracy under the whitebox attack respectively. Meanwhile, the BGAT+TRADES method also outperforms TRADES in terms of both natural accuracy and robustness under the white-box attack for CIFAR-10, which manifests the great flexibility of our method. To show the advantages of our methods more intuitively, we plot the comparison results on the more challenging CIFAR-100 dataset in <ref type="figure">Fig. 4</ref>, which indicates that models trained with our method can have stronger robustness while preserving high natural accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Robustness on CIFAR-10 and CIFAR-100</head><p>White-box Regular Attacks. We evaluate the robustness of our models under the white-box attack using the same setting as described at the beginning of Sec. 4. For CIFAR-10, our LBGAT+TRADES (α = 0) achieves 88.22% accuracy on natural images, which outperforms TRADES (α = 6) by 3.3% at the same time remaining 57.55% robust accuracy, 0.94% higher than that of TRADES (α = 6). For CIFAR-100, our LBGAT+TRADES (α = 0) achieves 70.03% accuracy on natural images and 33.01% robust accuracy, improving TRADES (α = 6) by 13.53% and 2.08% respectively. Moreover, our LBGAT+TRADES (α = 6) further boosts robustness to 57.78% and 35.50% on CIFAR-10 and CIFAR-100 respectively.</p><p>We also apply several other regular attack methods, like FGSM and CW, to evaluate our models. Compared with TRADES, our proposed methods consistently achieve better accuracy on natural images and stronger robustness on both CIFAR-10 and CIFAR-100 datasets. The details of our results are presented in <ref type="table">Table 3</ref>. Note that the CW attack denotes using CW-loss within the PGD framework here. The evaluation under CW attack is also with 20 iterations, step size 0.003 and perturbation = 0.031.</p><p>White-box Auto-Attack (AA). Auto-Attack <ref type="bibr" target="#b7">[8]</ref> is to reliably evaluate model robustness with an ensemble of diverse strong attack methods, including APGD-CE, APGD-DLR, FAB, and Square Attack. We use the open-source code from <ref type="bibr" target="#b7">[8]</ref> to test our models with perturbation size 0.031. The results are listed in <ref type="table">Table 3</ref>. Compared with TRADES(α = 6), our LBGAT+TRADES(α = 0) model improves natural accuracy by 13.53% and 3.3% on CIFAR-100 and CIFAR-10 separately, while achieving comparable robustness. Our LBGAT+TRADES(α = 6) model further boosts robust accuracy, obtaining 29.34% and 53.14% on CIFAR-100 and CIFAR-10, outperforming TRADES(α = 6) by 2.44% and 0.5% respectively. <ref type="table">Table 3</ref>: Comparison of our method with previous defense models under white-box attack on CIFAR-10 and CIFAR-100. We use ResNet18 as M natural for LBGAT method. Acc n represents accuracy on natural images while Acc r represents robustness of models. AA is the strongest attack, i.e., auto-attack <ref type="bibr" target="#b7">[8]</ref>. * denotes the model is WRN-34-20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Defense</head><p>Attack  Black-box Attacks. We verify the robustness of our models under the black-box attack. We first train models without using adversarial training on the CIFAR-10 and CIFAR-100 datasets. The same network architectures that are specified at the beginning of this section, i.e., the WRN-34-10 architecture <ref type="bibr" target="#b49">[49]</ref>, are adopted. We denote these models by  <ref type="table" target="#tab_4">Table 4</ref> naturally trained models as (Natural). The accuracy of teh naturally trained WRN-34-10 model is 95.80% on the CIFAR-10 dataset and 78.76% on the CIFAR-100 dataset. We also implement the method proposed in <ref type="bibr" target="#b51">[51]</ref> on both datasets with their open-source codebase. For both datasets, the F GSM k (black-box) method is applied to attack various defense models. We set = 0.031 and apply F GSM k (black-box) attack with 20 iterations with step size set to 0.003. Note that the setup is the same as that specified in the white-box attack.</p><p>The results on CIFAR-100 are summarized in <ref type="table" target="#tab_4">Table 4</ref>. We use source models to generate adversarial perturbations where the perturbation directions are according to the gradi-ents of the source models on the input images. Our models are more robust against black-box attack transferred from naturally trained models and TRADES <ref type="bibr" target="#b51">[51]</ref>, while yielding stronger robustness under white-box attack and higher performance on natural images. Specifically, our best model is 12.83% and 8.60% higher than TRADES(α = 6) with the natural trained model and robust model as the source model separately on CIFAR-100. For robustness under black-box attack with one robust source model, our model is tested under TRADES(α = 6) while TRADES is tested under our LBGAT trained model. A more clear comparison between our method and TRADES method can be seen in <ref type="figure">Fig. 5</ref>, which exhibits the results on the more challenging dataset CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Robustness on Tiny-ImageNet.</head><p>To further demonstrate the effectiveness of our method on more complex data, we conduct experiments on Tiny ImageNet. <ref type="table">Table 5</ref> shows the experimental results. Our method is better than ALP, and better than TRADES, surpassing baselines with a large margin. Specifically, our LBGAT+TRADES(α = 0) outperforms the most robust baseline -TRADES(α = 6) by 9.29% on natural data, meanwhile LBGAT+TRADES(α = 6) is 3% higher than it on adversarial data, which testifies the effectiveness of our approach again. <ref type="table">Table 5</ref>: Results on Tiny ImageNet <ref type="bibr" target="#b8">[9]</ref>. The same evaluation setting with CIFAR is applied under 20 iterations PGD white-box attack. We adopt ResNet18 as M natural for LB-GAT methods. Acc n represents accuracy on natural images while Acc r represents robustness of models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation study for L function</head><p>To show the importance of boundary guidance from M natural , we conduct ablation experiments with and without cross-entropy loss for M robust in Eq. (3). Experimental results are summarized in <ref type="table">Table 6</ref>. "w/o" additional crossentropy loss for M robust enjoys 2.05% higher robust accuracy than "w/", which further manifests vast importance of the natural classifier boundary guidance. <ref type="table">Table 6</ref>: Ablation study for L function on CIFAR-10. 20 iterations PGD white-box attack is applied. We adopt ResNet18 as M natural for LBGAT method. Acc n represents accuracy on natural images while Acc r represents robustness of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Acc n Acc r vanilla AT 86.82% 52.87% TRADES (α = 6) 84.92% 56.61% LBGAT (α = 0) w/ 88.35% 55.50% LBGAT (α = 0) w/o 88.22% 57.55%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have proposed the Boundary Guided Adversarial Training (BGAT) method, to improve model robustness without losing much accuracy on natural data. Our approach can be understood from the perspective of natural classifier boundary guidance. We also generalized BGAT to Learnable Boundary Guided Adversarial Training (LB-GAT) by collaboratively training M natural and M robust together to learn one most robustness-friendly classifier boundary and further enhance model robustness. Extensive experiments on CIFAR-10, CIFAR-100, and more challenging Tiny ImageNet datasets proved the effectiveness of our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learnable Boundary Guided Adversarial Training Supplementary File</head><p>A. Our Method Creates New SOTA Under the Strongest Auto-Attack on CIFAR-100</p><p>To further show the effectiveness of our method, we compare with more previous works. The experimental results are shown in <ref type="table">Table 7</ref>. On the more challlenging CIFAR-100 dataset, our method creates a new state-of-the-art (SOTA) on both robustness and natural accuracy. Specifically, our LBGAT (α = 0) model with WideResNet-34-10 architecture significantly outperforms previous SOAT method <ref type="bibr" target="#b4">[5]</ref> by 7.08% in the aspect of performance on natural data. Meanwhile, our method surpasses it with respect to model robustness. Further, our strongest model LBGAT (α = 6) with WideResNet-34-10 architecture enjoys 2.4% higher robustness than <ref type="bibr" target="#b4">[5]</ref>.</p><p>Moreover, It is worthy to note that our LBGAT (α = 6) model achieves even strong robustness than the model, by Hendrycks et al. <ref type="bibr" target="#b14">[15]</ref>, trained with a large amount of additional unlabeled data. At the same time, we also surpasses it in the aspect of natural accuracy. <ref type="table">Table 7</ref>: More comparisons under the strongest Auto-Attack on CIFAR-100 dataset. " †" denotes numbers are directly copied from <ref type="bibr" target="#b7">[8]</ref>. " " denotes that the method has used additional unlabeled data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Classifier boundary of clean model (b) Classifier boundary of our robust model (c) Classifier boundary of robust model trained with previous methods Conceptual illustration of our method vs. previous adversarial training approaches. Solid lines denote real classifier boundary of the trained model, while the dotted line is the classifier boundary of the clean model M natural . Different shapes represent logits of images in various classes. Black color marks adversarial examples. ing methods, e.g., Madry et al. [23] mainly improve the robustness towards adversarial examples. As shown in Fig. 2(c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>raised model robustness under black-box attack by the proposed ensemble adversarial training, i.e., producing adversarial examples by static ensemble models. Madry et al. [23] used the universal first-order adversary, i.e., PGD attack, to obtain adversarial examples in the course of adversarial training. Differently, Kannan et al. [18] enhanced model robustness with adversarial logit pairing, which encourages the logits from natural images and adversarial examples to be similar to each other in the same model. Moreover, Zhang et al. [51] regularized the output from natural images and adversarial examples with the KLdivergence function, meanwhile using a variant of PGD attack. Xie et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Learnable Boundary Guided Adversarial Training (LBGAT). M natural and M robust learn collaboratively in training. (x, y) is a batch of natural images and labels. x adv is the corresponding adversarial examples of x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>with Eq. (3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 : 3 TRADESFigure 5 :</head><label>435</label><figDesc>Comparisons with state-of-the-art defense methods on CIFAR-100 with 20 iterations PGD under whitebox attack. Black dots represent our methods, while green points stand for other recent methods. More details are included in Tables 1, 2 and "White-box Robust Acc" represents classification accuracy under white-box attack. "Black-box Robust Acc" represents classification accuracy under black-box attack. Models on the right of the red line are evaluated with the clean model as the source one, while models on the left of the red line models are evaluated with the robust model as the source. More details are included in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Boundary Guided Adv. Training For BGAT method, M robust is constrained by logits of the static M natural . The well trained M natural has the most desirable classifier boundary for natural data. Thus, inheriting such classifier boundary, M robust tends to achieve high performance on natural images. Learnable Boundary Guided Adversarial Training (LBGAT) 1: Input: step size η 1 and learning rate η 2 , batch size m, number of iterations K in inner optimization, model M robust parameterized by θ, M natural parameterized by θ * . β is one hyper-parameter. 2: Output: robust model M robust with θ. 3: Initialize M robust and M natural randomly or with pretrained configuration.</figDesc><table /><note>Nevertheless, the classifier boundary coming from static M natural might not be the most suitable choice for pursu- ing robustness. We generalize the BGAT method to Learn- able Boundary Guided Adversarial Training (LBGAT) by training M natural and M robust simultaneously and collab- Algorithm 14: repeat5:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with vanilla AT method. For BGAT, we use the ensemble of WideResNet and InceptionRes-NetV2 model as M natural . ResNet18 as M natural is for LBGAT on CIFAR-10 and CIFAR-100. To rule out randomness, we get numbers by running two independently trained models and taking the average. Acc n represents accuracy on natural images, while Acc r represents the robustness of models.</figDesc><table><row><cell>Methods</cell><cell>Accn</cell><cell>Accr</cell><cell>Datasets</cell></row><row><cell cols="2">vanilla AT 60.90%</cell><cell>27.46%</cell><cell>CIFAR-100</cell></row><row><cell>BGAT</cell><cell>67.72%</cell><cell>30.20%</cell><cell>CIFAR-100</cell></row><row><cell>LBGAT</cell><cell>66.29%</cell><cell>34.30%</cell><cell>CIFAR-100</cell></row><row><cell cols="2">vanilla AT 86.82%</cell><cell>52.87%</cell><cell>CIFAR-10</cell></row><row><cell>BGAT</cell><cell>89.00%</cell><cell>55.40%</cell><cell>CIFAR-10</cell></row><row><cell>LBGAT</cell><cell>87.08%</cell><cell>56.60%</cell><cell>CIFAR-10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Our method is supplementary to ALP and TRADES. For BGAT, we use the ensemble of WideRes-Net and InceptionResNetV2 model as M natural . ResNet18 is adopted as M natural for LBGAT+TRADES and LB-GAT+ALP. To rule out randomness, the numbers are averaged over 2 independently trained models. Acc n represents accuracy on natural images while Acc r represents robustness of models.</figDesc><table><row><cell>Methods</cell><cell>Accn</cell><cell>Accr</cell><cell>Datasets</cell></row><row><cell>ALP</cell><cell>59.75%</cell><cell>28.94%</cell><cell>CIFAR-100</cell></row><row><cell>BGAT+ALP</cell><cell>63.46%</cell><cell>31.27%</cell><cell>CIFAR-100</cell></row><row><cell>LBGAT+ALP</cell><cell>62.67%</cell><cell>35.25%</cell><cell>CIFAR-100</cell></row><row><cell>TRADES (α = 1)</cell><cell>62.37%</cell><cell>25.31%</cell><cell>CIFAR-100</cell></row><row><cell>TRADES (α = 6)</cell><cell>56.51%</cell><cell>30.94%</cell><cell>CIFAR-100</cell></row><row><cell>BGAT+TRADES (α = 0)</cell><cell>71.27%</cell><cell>28.70%</cell><cell>CIFAR-100</cell></row><row><cell cols="2">LBGAT+TRADES (α = 0) 70.03%</cell><cell>33.01%</cell><cell>CIFAR-100</cell></row><row><cell cols="2">LBGAT+TRADES (α = 6) 60.43%</cell><cell>35.50%</cell><cell>CIFAR-100</cell></row><row><cell>ALP</cell><cell>85.55%</cell><cell>54.59%</cell><cell>CIFAR-10</cell></row><row><cell>BGAT+ALP</cell><cell>86.58%</cell><cell>55.74%</cell><cell>CIFAR-10</cell></row><row><cell>LBGAT+ALP</cell><cell>85.05%</cell><cell>57.60%</cell><cell>CIFAR-10</cell></row><row><cell>TRADES (α = 1)</cell><cell>88.64%</cell><cell>49.14%</cell><cell>CIFAR-10</cell></row><row><cell>TRADES (α = 6)</cell><cell>84.92%</cell><cell>56.61%</cell><cell>CIFAR-10</cell></row><row><cell>BGAT+TRADES (α = 0)</cell><cell>89.06%</cell><cell>56.75%</cell><cell>CIFAR-10</cell></row><row><cell cols="2">LBGAT+TRADES (α = 0) 88.22%</cell><cell>57.55%</cell><cell>CIFAR-10</cell></row><row><cell cols="2">LBGAT+TRADES (α = 6) 81.98%</cell><cell>57.78%</cell><cell>CIFAR-10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of our method with previous defense models under black-box attack on CIFAR-100 and CIFAR-10. To rule out randomness, the numbers are averaged over 2 independently trained models. Acc n represents accuracy on natural images. BAcc r represents robustness under black-box attack. W Acc r represents robustness under white-box attack</figDesc><table><row><cell>Target Models</cell><cell>BAcc r</cell><cell>WAcc r</cell><cell>Acc n</cell><cell>Source Models</cell><cell>Dataset</cell></row><row><cell>TRADES (α = 1)</cell><cell cols="3">61.29% 25.31% 62.37%</cell><cell>Natural</cell><cell>CIFAR-100</cell></row><row><cell>TRADES (α = 6)</cell><cell cols="3">55.52% 30.93% 56.51%</cell><cell>Natural</cell><cell>CIFAR-100</cell></row><row><cell>LBGAT+ALP</cell><cell cols="3">61.38% 35.25% 62.67%</cell><cell>Natural</cell><cell>CIFAR100</cell></row><row><cell>LBGAT+TRADES(α=0)</cell><cell cols="3">68.35% 33.01% 70.03%</cell><cell>Natural</cell><cell>CIFAR-100</cell></row><row><cell>TRADES(α = 1)</cell><cell cols="5">42.32% 25.31% 62.37% LBGAT+TRADES(α=0) CIFAR-100</cell></row><row><cell>TRADES(α = 6)</cell><cell cols="5">41.67% 30.93% 56.51% LBGAT+TRADES(α=0) CIFAR-100</cell></row><row><cell>LBGAT+ALP</cell><cell cols="3">45.68% 35.25% 62.67%</cell><cell>TRADES (α = 6)</cell><cell>CIFAR-100</cell></row><row><cell>LBGAT+TRADES(α=0)</cell><cell cols="3">50.27% 33.01% 70.03%</cell><cell>TRADES (α = 6)</cell><cell>CIFAR-100</cell></row><row><cell>TRADES (α = 1)</cell><cell cols="3">87.00% 49.14% 88.64%</cell><cell>Natural</cell><cell>CIFAR-10</cell></row><row><cell>TRADES (α = 6)</cell><cell cols="3">83.30% 56.61% 84.92%</cell><cell>Natural</cell><cell>CIFAR-10</cell></row><row><cell cols="4">LBGAT+TRADES (α = 0) 87.20% 57.55% 88.22%</cell><cell>Natural</cell><cell>CIFAR-10</cell></row><row><cell>TRADES (α = 1)</cell><cell cols="5">66.18% 49.14% 88.64% LBGAT+TRADES(α=0) CIFAR-10</cell></row><row><cell>TRADES (α = 6)</cell><cell cols="5">67.18% 56.61% 84.92% LBGAT+TRADES(α=0) CIFAR-10</cell></row><row><cell cols="4">LBGAT+TRADES (α = 0) 68.45% 57.55% 88.22%</cell><cell>TRADES(α=6)</cell><cell>CIFAR-10</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Square attack: a query-efficient black-box adversarial attack via random search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Croce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>Nicolas Flammarion, and Matthias Hein</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE SP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Jacobian adversarially regularized networks for robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Yew Soon Ong, and Jie Fu</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01278,2020.11</idno>
		<title level="m">Efficient robust training via backward smoothing</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial robustness: From self-supervised pre-training to fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Minimally distorted adversarial examples with a fast adaptive boundary attack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adversarially robust distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Feizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adversarial logit pairing. CoRR, abs/1803.06373</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Harini Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Harnessing the vulnerability of latent layers in adversarially trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshitha</forename><surname>Machiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and blackbox attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Data-free knowledge distillation for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Raphael Gontijo Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thad</forename><surname>Fenu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Starner</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1710.07535</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Metric learning for adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepfool: A simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Seyed-Mohsen Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE SP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adversarial robustness through local linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongli</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnamurthy</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overfitting in adversarially robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep model compression: Distilling knowledge from noisy teachers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhusan</forename><surname>Bharat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineeth</forename><forename type="middle">N</forename><surname>Sau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
		<idno>abs/1610.09650</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adversarial training for free! In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Amin Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Curls &amp; whey: Boosting black-box adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improving adversarial robustness through progressive hardening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chawin</forename><surname>Sitawarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supriyo</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09347,2020.11</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bilateral adversarial training: Towards fast training of more robust models against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast is better than free: Revisiting adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Enhancing adversarial defense by k-winners-take-all</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxi</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Intriguing properties of adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03787</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving transferability of adversarial examples with input diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improving transferability of adversarial examples with input diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NDSS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">You only propagate once: Accelerating adversarial training via maximal principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Theoretically principled trade-off between robustness and accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Distributionally adversarial attack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<idno>WideResNet-34-20 71.00% 27.66%</idno>
		<title level="m">Methods Model Acc n Acc r LBGAT (α = 0) Ours</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">LBGAT (α = 0) Ours WideResNet</title>
		<idno>34-10 70.03% 27.05%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">LBGAT (α = 6) Ours WideResNet</title>
		<idno>34-10 60.43% 29.34%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sitawarin</surname></persName>
		</author>
		<idno>34] † WideResNet-34-10 62.82% 24.57%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<idno>5] † WideResNet-34-10 62.15% 26.94%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">More Comparisons Under the Strongest Auto-Attack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno>on CIFAR-10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">We also compare with more previous methods on CIFAR-10 dataset. The experimental results are summarized in Table 8</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Our LBGAT (α = 0) model with WideResNet-34-10 architecture can consistently enjoy higher natural performance while keeping the strongest robustness</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">More comparisons under the strongest Auto-Attack on CIFAR-10 dataset</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>denotes numbers are directly copied</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Wideresnet</surname></persName>
		</author>
		<idno>34-20 85.34% 53.42%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<idno>34-10 88.64% 48.11%</idno>
		<title level="m">TRADES(α = 1) WideResNet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumari</surname></persName>
		</author>
		<idno>19] † WideResNet-34-10 87.80% 49.12%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mao</surname></persName>
		</author>
		<idno>24] † WideResNet-34-10 86.21% 47.41%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">† *</forename><surname>Wideresnet</surname></persName>
		</author>
		<idno>34-10 87.20% 44.83%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Wideresnet</surname></persName>
		</author>
		<idno>34-10 93.79% 0.26%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno>40] † * WideResNet-28-10 92.80% 29.35%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qin</surname></persName>
		</author>
		<idno>28] † WideResNet-40-8 86.28% 52.81%</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
