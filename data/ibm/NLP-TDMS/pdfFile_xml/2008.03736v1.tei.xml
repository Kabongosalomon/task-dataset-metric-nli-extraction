<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast and Accurate Neural CRF Constituency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houquan</forename><surname>Zhou</surname></persName>
							<email>hqzhou@stu.suda.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast and Accurate Neural CRF Constituency Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating probability distribution is one of the core issues in the NLP field. However, in both deep learning (DL) and pre-DL eras, unlike the vast applications of linear-chain CRF in sequence labeling tasks, very few works have applied tree-structure CRF to constituency parsing, mainly due to the complexity and inefficiency of the inside-outside algorithm. This work presents a fast and accurate neural CRF constituency parser. The key idea is to batchify the inside algorithm for loss computation by direct large tensor operations on GPU, and meanwhile avoid the outside algorithm for gradient computation via efficient back-propagation. We also propose a simple two-stage bracketing-thenlabeling parsing approach to improve efficiency further. To improve the parsing performance, inspired by recent progress in dependency parsing, we introduce a new scoring architecture based on boundary representation and biaffine attention, and a beneficial dropout strategy. Experiments on PTB, CTB5.1, and CTB7 show that our two-stage CRF parser achieves new state-of-the-art performance on both settings of w/o and w/ BERT, and can parse over 1,000 sentences per second. We release our code at https://github.com/yzhangcs/crfpar.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given an input sentence, constituency parsing aims to build a hierarchical tree as depicted in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, where the leaf or terminal nodes correspond to input words and non-terminal nodes are constituents (e.g., VP 3,5 ). As a fundamental yet challenging task in the natural language processing (NLP) field, constituency parsing has attracted a lot of research attention since large-scale treebanks were annotated, such as Penn Treebank (PTB), Penn Chinese Treebank (CTB), etc. Parsing outputs are also proven to be extensively useful for a wide range of downstream applications <ref type="bibr" target="#b0">[Akoury et al., 2019;</ref>. * Yu Zhang and Houquan Zhou make equal contributions to this work. Zhenghua Li is the corresponding author. As one of the most influential works, <ref type="bibr" target="#b1">Collins [1997]</ref> extends methods from probabilistic context-free grammars (PCFGs) to lexicalized grammars. Since then, constituency parsing has been dominated by such generative models for a long time, among which the widely used Berkeley parser adopts an unlexicalized PCFG with latent non-terminal splitting annotations <ref type="bibr" target="#b9">[Matsuzaki et al., 2005;</ref><ref type="bibr" target="#b9">Petrov and Klein, 2007]</ref>. As for discriminative models, there exist two representative lines of research. The first line adopts the graphbased view based on dynamic programming decoding, using either local max-entropy estimation <ref type="bibr" target="#b6">[Kaplan et al., 2004]</ref> or global max-margin training <ref type="bibr">[Taskar et al., 2004]</ref>. The second group builds a tree via a sequence of shift-reduce actions based on greedy or beam decoding, known as the transitionbased view <ref type="bibr" target="#b10">[Sagae and Lavie, 2005;</ref><ref type="bibr" target="#b12">Zhu et al., 2013]</ref>.</p><p>Recently, constituency parsing has achieved significant progress thanks to the impressive capability of deep neural networks in context representation. Two typical and popular works are respectively the transition-based parser of Cross and Huang <ref type="bibr">[2016]</ref> and the graph-based parser of Stern et al. <ref type="bibr">[2017]</ref>. As discriminative models, the two parsers share several commonalities, both using 1) multi-layer BiL-STM as encoder; 2) minus features from BiLSTM outputs as span representations; 3) MLP for span scoring; 4) maxmargin training loss. Most works <ref type="bibr" target="#b4">[Gaddy et al., 2018;</ref><ref type="bibr" target="#b7">Kitaev and Klein, 2018]</ref> mainly follow the two parsers and achieve much higher parsing accuracy than traditional nonneural models, especially with contextualized word representations trained with language modeling loss on large-scale unlabeled data <ref type="bibr" target="#b9">[Peters et al., 2018;</ref><ref type="bibr" target="#b2">Devlin et al., 2019]</ref>.</p><p>Despite the rapid progress, existing constituency parsing research suffers from two closely related drawbacks. First, parsing (also for training) speed is slow and can hardly satisfy the requirement of real-life systems. Second, the lack of explicitly modeling tree/subtree probabilities may hinder the effectiveness of utilizing parsing outputs. On the one hand, estimating probability distribution is one of the core issues in the NLP field <ref type="bibr" target="#b8">[Le and Zuidema, 2014]</ref>. On the other hand, compared with unbounded tree scores, tree probabilities can better serve high-level tasks as soft features <ref type="bibr" target="#b5">[Jin et al., 2020]</ref>, and marginal probabilities of subtrees can support the more sophisticated Minimum Bayes Risk (MBR) decoding <ref type="bibr" target="#b10">[Smith and Smith, 2007]</ref>.</p><p>In <ref type="bibr">fact, Finkel et al. [2008]</ref> and <ref type="bibr" target="#b2">Durrett and Klein [2015]</ref> both propose CRF-based constituency parsing by directly modeling the conditional probability. However, both models are extremely inefficient due to the high time-complexity of the inside-outside algorithm for loss and gradient computation, especially the outside procedure. The issue becomes more severe in the DL era since all previous works perform the inside-outside computation on CPUs according to our knowledge and switching between GPU and CPU is expensive.</p><p>This work proposes a fast and accurate CRF constituency parser by substantially extending the graph-based parser of Stern et al. <ref type="bibr">[2017]</ref>. The key contribution is that we batchify the inside algorithm for direct loss and gradient computation on GPU. Meanwhile, we find that the outside algorithm can be efficiently fulfilled by automatic backpropagation, which is shown to be equally efficient with the inside (forward) procedure, naturally verifying the great theoretical work of <ref type="bibr" target="#b3">Eisner [2016]</ref>. Similarly, we batchify the Cocke-Kasami-Younger (CKY) algorithm for fast decoding.</p><p>In summary, we make the following contributions.</p><p>• We for the first time propose a fast and accurate CRF constituency parser for directly modeling (marginal) probabilities of trees and subtrees. The efficiency issue, which bothers the community for a long time, is well solved by elegantly batchifying the inside and CKY algorithms for direct computation on GPU.</p><p>• We propose a two-stage bracketing-then-labeling parsing approach that is more efficient and achieves slightly better performance than the one-stage method.</p><p>• We propose a new span scoring architecture based on span boundary representation and biaffine attention scoring, which performs better than the widely used minusfeature method. We also show that the parsing performance can be improved by a large margin via better parameter settings such as dropout configuration.</p><p>• Experiments on three English and Chinese benchmark datasets show that our proposed two-stage CRF parser achieves new state-of-the-art parsing performance under both settings of w/o and w/ BERT <ref type="bibr" target="#b2">[Devlin et al., 2019]</ref>). In terms of parsing speed, our parser can parse over 1,000 sentences per second.  2 Two-stage CRF Parsing Formally, given a sentence consisting of n words x = w 0 , . . . , w n−1 , a constituency parse tree, as depicted in <ref type="figure">Figure</ref> 1(a), is denoted as t, and (i, j, l) ∈ t is a constituent spanning w i ...w j with a syntactic label l ∈ L. Alternatively, a tree can be factored into two parts, i.e., t = (y, l), where y is an unlabeled (a.k.a. bracketed) tree and l is a label sequence for all constituents in a certain order. Interchangeably, (3, 5, VP) is also denoted as VP 3,5 . To accommodate the inside and CKY algorithms, we transform the original tree into those of Chomsky normal form (CNF) using the NLTK tool 1 , as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). Particularly, consecutive unary productions such as X i,j → Y i,j are collapsed into one X+Y i,j . We adopt left binarization since preliminary experiments show it is slightly superior to right binarization. After obtaining the 1-best tree via CKY decoding, the CNF tree is recovered into the n-ary form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Definition</head><p>In this work, we adopt a two-stage bracketing-then-labeling framework for constituency parsing, which we show not only simplifies the model architecture but also improves efficiency, compared with the traditional one-stage approach adopted in previous works <ref type="bibr" target="#b10">[Stern et al., 2017;</ref><ref type="bibr" target="#b4">Gaddy et al., 2018]</ref>.</p><p>First stage: bracketing. Given x, the goal of the first stage is to find an optimal unlabeled tree y. The score of a tree is decomposed into the scores of all contained constituents.</p><formula xml:id="formula_0">s(x, y) = (i,j)∈y s(i, j)<label>(1)</label></formula><p>Under CRF, the conditional probability is</p><formula xml:id="formula_1">p(y | x) = e s(x,y) Z(x) ≡ y ∈T (x) e s(x,y )<label>(2)</label></formula><p>where Z(x) is known as the normalization term, and T (x) is the set of legal trees. Given all constituent scores s(i, j), we use the CKY algorithm to find the optimal treeŷ. y = arg max y s(x, y) = arg max y p(y | x)</p><p>(3)</p><p>Second stage: labeling. Given a sentence x and a tree y, the second stage independently predicts a label for each con-</p><formula xml:id="formula_2">stituent (i, j) ∈ y.l = arg max l∈L s(i, j, l)<label>(4)</label></formula><p>Note that we use gold-standard unlabeled trees for loss computation during training. For a sentence of length n, all CNF trees contain the same 2n − 1 constituents. Therefore, this stage has a time complexity of O(n|L|).</p><p>Time complexity analysis. The CKY algorithm has a time complexity of O(n 3 ). Therefore, the time complexity of our two-stage parsing approach is O(n 3 + n|L|). In contrast, for the one-stage parsing approach, the CKY algorithm needs to determine the best label for all n 2 spans and thus needs O(n 3 + n 2 |L|), where |L| is usually very large (e.g., 138 for English in <ref type="table">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scoring Architecture</head><p>This subsection introduces the network architecture for scoring spans and labels, as shown in <ref type="figure">Figure 2</ref>, which mostly follows Stern et al.</p><p>[2017] with two important modifications: 1) boundary representation and biaffine attention for score computation; 2) better parameter settings following Dozat and Manning <ref type="bibr">[2017]</ref>.</p><p>Inputs. For the ith word, its input vector e i is the concatenation of the word embedding and character-level representation:</p><formula xml:id="formula_3">e i = e word i ⊕ CharLSTM(w i )<label>(5)</label></formula><p>where CharLSTM(w i ) is the output vectors after feeding the character sequence into a BiLSTM layer <ref type="bibr" target="#b8">[Lample et al., 2016]</ref>. Previous works show that replacing POS tag embeddings with CharLSTM(w i ) leads to consistent improvement <ref type="bibr" target="#b7">[Kitaev and Klein, 2018]</ref>. This can also simplify the model without the need of predicting POS tags (n-fold jack-knifing on training data).</p><p>BiLSTM encoder. We employ three BiLSTM layers over the input vectors for context encoding. We denote as f i and b i respectively the output vectors of the top-layer forward and backward LSTMs for w i . In this work, we borrow most parameter settings from the dependency parser of Dozat and Manning <ref type="bibr">[2017]</ref>. We find that the dropout strategy is very crucial for parsing performance, which differs from the vanilla implementation of Stern et al. <ref type="bibr">[2017]</ref> in two aspects.</p><p>First, for each word w i , e word i and CharLSTM(w i ) are dropped as a whole, either unchanged or becoming a 0 vector. If one vector is dropped into 0, the other is compensated with a ratio of 2. Second, the same LSTM layer shares the same dropout masks at different timesteps (words).</p><p>Boundary representation. For each word w i , we compose the context-aware word representation following Stern et al. <ref type="bibr">[2017]</ref>. <ref type="bibr">2</ref> h</p><formula xml:id="formula_4">i = f i ⊕ b i+1 (6)</formula><p>The dimensions of h i is 800. Instead of directly applying a single MLP to h i , we observe that a word must act as either left or right boundaries in all constituents in a given tree. Therefore, we employ two MLPs to make such distinction and obtain left and right boundary representation vectors.</p><formula xml:id="formula_5">r l i ; r r i = MLP l (h i ) ; MLP r (h i )<label>(7)</label></formula><p>The dimension d of r l/r i is 500. As pointed out by Dozat and Manning <ref type="bibr">[2017]</ref>, MLPs reduce the dimension of h i and, more importantly, detain only syntax-related information, thus alleviating the risk of over-fitting.</p><p>Biaffine scoring. Given the boundary representations, we score each candidate constituent (i, j) using biaffine operation over the left boundary representation of w i and the right boundary representation of w j .</p><formula xml:id="formula_6">s(i, j) = r l i 1 T Wr r j (8) where W ∈ R d×d .</formula><p>It is analogous to compute scores of constituent labels s(i, j, l). Two extra MLPs are applied to h i to obtain boundary representationsr l/r i (with dimensiond). We then use |L| biaffines (Rd ×d ) to obtain all label scores. Since |L| is large, we use a small dimensiond of 100 forr  <ref type="bibr" target="#b11">[Wang and Chang, 2016;</ref><ref type="bibr" target="#b1">Cross and Huang, 2016]</ref> and apply MLPs to compute span scores.</p><formula xml:id="formula_7">s(i, j) = MLP(h i − h j )<label>(9)</label></formula><p>We show that our new scoring method is clearly superior in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Loss</head><p>For a training instance (x, y, l), The training loss is composed of two parts.</p><formula xml:id="formula_8">L(x, y, l) = L bracket (x, y) + L label (x, y, l)<label>(10)</label></formula><p>The first term is the sentence-level global CRF loss, trying to maximize the conditional probability:</p><formula xml:id="formula_9">L bracket (x, y) = −s(x, y) + log Z(x)<label>(11)</label></formula><p>where log Z(x) can be computed using the inside algorithm in O(n 3 ) time complexity. The second term is the standard constituent-level crossentropy loss for the labeling stage.</p><p>Algorithm 1 Batchified Inside Algorithm. 1: define: S ∈ R n×n×B £ B is #sents in a batch 2: initialize: all S :,: = 0 3: for w = 1 to n do £ span width 4:</p><p>Parallel computation on 0 ≤ i,j &lt; n, r,0 ≤ b &lt; B 5:</p><formula xml:id="formula_10">S i,j=i+w = log i≤r&lt;j exp (S i,r + S r+1,j ) + s(i, j) 6: end for 7: return S 0,n−1 ≡ log Z</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Efficient Training and Decoding</head><p>This section describes how we perform efficient training and decoding via batchifying the inside and CKY algorithms for direct computation on GPU. We also show that the very complex outside algorithm can be avoided and fulfilled by the back-propagation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Batchified Inside Algorithm</head><p>To compute log Z in Equation 11 and feature gradients, all previous works on CRF parsing <ref type="bibr" target="#b3">[Finkel et al., 2008;</ref><ref type="bibr" target="#b2">Durrett and Klein, 2015]</ref> explicitly perform the inside-outside algorithm on CPUs. Unlike linear-chain CRF, it seems very difficult to batchify tree-structure algorithms.</p><p>In this work, we find that it is feasible to propose a batchified version of the inside algorithm, as shown in Algorithm 1.</p><p>The key idea is to pack the scores of same-width spans for all instances in the data batch into large tensors. This allows us to do computation and aggregation simultaneously via efficient large tensor operation. Since computation for all 0 ≤ i,j &lt; n, r,0 ≤ b &lt; B is performed in parallel on GPU, the algorithm only needs O(n) steps. Our code will give more technical details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Outside via Back-propagation</head><p>Traditionally, the outside algorithm is considered as indispensable for computing marginal probabilities of subtrees, which further compose feature gradients. In practice, the outside algorithm is more complex and at least twice slower than the inside algorithm. Though possible, it is more complicated to batchify the outside algorithm. Fortunately, this issue is erased in the deep learning era since the back-propagation procedure is designed to obtain gradients. In fact, Eisner <ref type="bibr">[2016]</ref> proposes a theoretical discussion on the equivalence between the back-propagation and outside procedures.</p><p>Since we use a batchified inside algorithm during the forward phase, the back-propagation is conducted based on large tensor computation, which is thus equally efficient.</p><p>It is also noteworthy that, by setting the loss to log Z and performing back-propagation, we can obtain the marginal probabilities of spans (i, j), which is exactly the correspond- <ref type="table" target="#tab_2">#Train  #Dev  #Test  #labels  original  CNF   PTB  39,832  1,700  2,416  26  138  CTB5.1  18,104  352  348  26  162  CTB7  46,572  2,079  2,796  28  265   Table 1</ref>: Data statistics, including the number of sentences and constituent labels. For "#labels", we list the number of labels in both original and converted CNF trees.</p><p>ing gradients.</p><formula xml:id="formula_11">p((i, j) | x) = y:(i,j)∈y p(y | x) = ∂ log Z(x) ∂s(i, j)<label>(12)</label></formula><p>Marginal probabilities are also useful in many subsequent NLP tasks as soft features. Please refer to <ref type="bibr" target="#b3">Eisner [2016]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding</head><p>As mentioned above, we employ the CKY algorithm to obtain the 1-best tree during the parsing phase, as shown in Equation 3. The CKY algorithm is almost identical to the inside algorithm except for replacing the sum-product with a max product (refer to Line 5 in Algorithm 1) and thus can also be efficiently batchified.</p><p>To perform MBR decoding, we simply replace the span scores s(i, j) with the marginal probabilities p((i, j) | x) in Equation 1 and 3. However, we find this has little influence on parsing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Data. We conduct experiments on three English and Chinese datasets. The first two datasets, i.e., PTB and CTB5.1, are widely used in the community. We follow the conventional train/dev/test data split. Considering that both CTB5.1dev/test only have about 350 sentences, we also use the larger CTB7 for more robust investigations, following the data split suggested in the official manual. <ref type="table">Table 1</ref> shows the data statistics. We can see that CNF introduces many new constituent label types, most (about 75%) of which are from the collapsing process of consecutive unary rules. Evaluation. As mentioned earlier, we convert 1-best CNF trees into n-ary trees after parsing for evaluation. Here, it may be useful to mention a small detail. The predicted 1best CNF tree may contain inconsistent productions since the decoding algorithm does not have such constraints. Taking <ref type="figure" target="#fig_0">Figure 1(b)</ref> as an example, the model may output VP 3,5 → PP * 3,3 NP 4,5 , where VP is incompatible with PP * . During the n-ary post-processing, we simply ignore the concrete label string PP before the " * " symbol. In view of this, performance may be slightly improved by adding such constraints during decoding.</p><p>We use the standard constituent-level labeled precision, recall, F-score (P/R/F) as the evaluation metrics with the EVALB tool 3 . Specifically, a predicted constituent such as  VP 3,5 is considered correct if it also appears in the goldstandard tree. 4</p><p>Parameter settings. We directly adopt the same hyperparameter settings of the dependency parser of Dozat and Manning [2017] without further tuning. The only difference is the use of CharLSTM word representations instead of POS tag embeddings. The dimensions of char embedding, word embedding, and CharLSTM outputs are 50, 100, 100, respectively. All dropout ratios are 0.33. The mini-batch size is 5,000 words. The training process continues at most 1,000 epochs and is stopped if the peak performance on dev data does not increase in 100 consecutive epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Comparison on Dev Data</head><p>We conduct the model study on dev data from two aspects: 1) CRF vs. max-margin training loss; 2) two-stage vs. one-stage parsing. The first three lines of <ref type="table" target="#tab_2">Table 2</ref> shows the results. The three models use the same scoring architecture and parameters. Following previous practice <ref type="bibr" target="#b10">[Stern et al., 2017]</ref>, onestage models use only scores of labeled constituents s(i, j, l).</p><p>In order to verify the effectiveness of the two-stage parsing, we also list the results of "CRF (one-stage)", which directly scores labeled constituents.</p><p>s(x, y, l) = (i,j,l)∈(y,l) s(i, j, l)</p><p>As discussed in the last paragraph of Section 2.1, the inside and CKY algorithms become a bit more complicated for the one-stage parser that two-stage. From the first two rows, we can see that under the onestage parsing framework, the CRF loss leads to similar performance on English but consistently outperforms the maxmargin loss by about 0.5 F-score on both Chinese datasets. The max-margin loss has one extra hyper-parameter, namely the margin value, which is set to 1 according to preliminary results on English and not tuned on Chinese for simplic-4 Since some researchers may implement their own evaluation scripts, some details about EVALB need to be clarified for fair comparison: 1) Empty constituents like {-NONE-} are removed during data pre-processing. 2) Root constituents ({TOP, S1} for English and an empty string for Chinese) are ignored for evaluation. 3) Constituents spanning a English punctuation mark like {:, ", ", ., ?, !} are also ignored. Please note that Chinese punctuation marks are evaluated as normal words. 4) Some label sets like {ADVP, PRT} are regarded as equivalent.  ity. We suspect that the performance on Chinese with maxmargin loss may be improved with more tuning. Overall, we can conclude that the two training loss settings achieve very close performance, and CRF has an extra advantage of probabilistic modeling.</p><p>Comparing the second and third rows, the two CRF parsers achieve nearly the same performance on CTB5.1 and the twostage parser achieves modest improvement over the one-stage parser by about 0.2 F-score on both PTB and CTB7. Therefore, we can conclude that our proposed two-stage parsing approach is superior in simplicity and efficiency (see <ref type="table" target="#tab_4">Table 3</ref>) without hurting performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study on Dev Data</head><p>To gain insights into the contributions of individual components in our proposed framework, we then conduct the ablation study by undoing one component at a time. Results are shown in the bottom four rows of <ref type="table" target="#tab_2">Table 2</ref>. Impact of MBR decoding. By default, we employ CKY decoding over marginal probabilities, a.k.a. MBR decoding. The "w/o MBR" row presents the results of performing decoding over span scores. Such comparison is very interesting since it is usually assumed that MBR decoding is theoretically superior to vanilla decoding. However, the results clearly show that the two decoding methods achieve nearly identical performance. Impact of scoring architectures. In order to measure the effectiveness of our new scoring architecture, we revert the biaffine scorers to the "minus features" method adopted by <ref type="bibr" target="#b10">Stern et al. [2017]</ref> (refer to Equation 9). It is clear that our proposed scoring method is superior to the widely used  minus-feature method, and achieves a consistent and substantial improvement of about 0.5 F-score on all three datasets.</p><p>Impact of dropout strategy. We keep other model settings unchanged and only replace the dropout strategy borrowed from Dozat and Manning <ref type="bibr">[2017]</ref> with the vanilla dropout strategy adopted by <ref type="bibr" target="#b10">Stern et al. [2017]</ref>. This leads to a very large and consistent performance drop of 0.96, 1.39 and 1.59 in F-score on the three datasets, respectively. <ref type="bibr" target="#b7">Kitaev and Klein [2018]</ref> replaced BiLSTMs with a self-attention encoder in Stern et al. <ref type="bibr">[2017]</ref> and achieved a large improvement of 1.0 F-score by separating content and position attention. Similarly, this work shows that the BiLSTM-based parser can be very competitive with proper parameter settings. We can see that our one-stage CRF parser is much more efficient than previous parsers by directly performing decoding on GPU. Our two-stage parser can parse 1,092 sentences per sentence, which is three times faster than <ref type="bibr" target="#b7">Kitaev and Klein [2018]</ref>. Of course, it is noteworthy that those parsers <ref type="bibr" target="#b10">[Stern et al., 2017;</ref><ref type="bibr" target="#b7">Kitaev and Klein, 2018</ref>] may be equally efficient by adopting our batchifying techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Speed Comparison</head><p>The parser of <ref type="bibr">Gómez-Rodríguez and Vilares [2018]</ref> is also very efficient by treating parsing as a sequence labeling task. However, the parsing performance is much lower, as shown in <ref type="table" target="#tab_6">Table 4</ref>.</p><p>The two-stage parser is only about 10% faster than the onestage counterpart. The gap seems small considering the significant difference in time complexity as discussed (see Section 2.1). The reason is that the two parsers share the same encoding and scoring components, which consume a large portion of the parsing time.</p><p>Using MBR decoding requires an extra run of the inside and back-propagation algorithms for computing marginal probabilities, and thus is less efficient. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the performance gap is very slight between w/ and w/o MBR. <ref type="table" target="#tab_6">Table 4</ref> shows the final results on the test datasets under two settings, i.e., w/o and w/ ELMo/BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Comparison on Test Data</head><p>Most previous works do not use pretrained word embedding but use randomly initialized ones instead, except for Zhou and Zhao <ref type="bibr">[2019]</ref>, who use Glove for English and structured skip-gram embeddings. For pretrained word embeddings, we use Glove (100d) for English PTB 5 , and adopt the embeddings of  trained on Gigaword 3rd Edition for Chinese. It is clear that our parser benefits substantially from the pretrained word embeddings. <ref type="bibr">6</ref> We also make comparisons with recent related works on constituency parsing, as discussed in Section 5. We can see that our BiLSTM-based parser outperforms the basic <ref type="bibr" target="#b10">Stern et al. [2017]</ref> by a very large margin, mostly owing to the new scoring architecture and better dropout settings. Compared with the previous state-of-the-art self-attentive parser <ref type="bibr" target="#b7">[Kitaev and Klein, 2018]</ref>, our parser achieves an absolute improvement of 0.16 on PTB and 1.67 on CTB5.1 without any language-specific settings.</p><p>The CTB5.1 results of Zhou and Zhao [2019] is obtained by rerunning their released code using predicted POS tags. We follow their descriptions 7 to produce the POS tags. It is noteworthy that their reported results accidentally use gold POS tags on CTB5.1, which is confirmed after several turns of email communication. We are grateful for their patience and help. We reran their released code using gold POS tags, and got 92.14 in F-score on CTB5-test, very close to the results reported in their paper. Our parser achieves 92.66 Fscore with gold POS tags. Another detail about their paper should be clarified: for dependency parsing on Chinese, they adopt two different data split settings, both using Stanford dependencies 3.3.0 and gold POS tags.</p><p>The bottom three rows list the results under the setting of using ELMo/BERT. We use bert-large-cased 8 (24 layers, 1024 dimensions, 16 heads) for PTB following <ref type="bibr" target="#b7">Kitaev et al. [2019]</ref>, and bert-base-chinese (12 layers, 768 dimensions, 12 heads) for CTB. It is clear that using BERT representations can help our parser by a very large margin on all datasets. Our parser also outperforms the multilingual parser of <ref type="bibr" target="#b7">Kitaev et al. [2019]</ref>, which uses extra multilingual resources. In summary, we can conclude that our parser achieves state-of-theart performance in both languages and both settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>Because of the inefficiency issue, there exist only a few previous works on CRF constituency parsing. <ref type="bibr" target="#b3">Finkel et al. [2008]</ref> propose the first non-neural feature-rich CRF constituency parser. <ref type="bibr" target="#b2">Durrett and Klein [2015]</ref> extend the work of Finkel et al. <ref type="bibr">[2008]</ref> and use a feedforward neural network with nonlinear activation for scoring anchored production rules. Both works perform explicit inside-outside computations on CPUs and suffer from a severe inefficiency issue.</p><p>This work is built on the high-performance modern neural parser based on a BiLSTM encoder <ref type="bibr" target="#b10">[Stern et al., 2017]</ref>, which first applies minus features <ref type="bibr" target="#b1">[Cross and Huang, 2016]</ref> for span scoring to graph-based constituency parsing. Several recent works follow Stern et al. <ref type="bibr">[2017]</ref>. <ref type="bibr" target="#b4">Gaddy et al. [2018]</ref> try to analyze what and how much context is implicitly encoded by BiLSTMs. <ref type="bibr" target="#b7">Kitaev and Klein [2018]</ref> replace twolayer BiLSTM with self-attention layers and find considerable improvement via separated content and position attending. In contrast, this work shows that the parser of Stern et al. <ref type="bibr">[2017]</ref> outperforms <ref type="bibr" target="#b7">Kitaev and Klein [2018]</ref> by properly configuring BiLSTMs such as the dropout strategy (see <ref type="table" target="#tab_2">Table 2</ref>). Please also kindly notice that <ref type="bibr" target="#b7">Kitaev and Klein [2018]</ref> use very large word embeddings.</p><p>Batchification is straightforward and well-solved for sequence labeling tasks, as shown in the implementation of NCRF++ 9 . However, very few works turned sight to treestructures. In a slightly earlier work, we for the first time propose to batchify tree-structured inside and Viterbi <ref type="bibr">(Eisner)</ref> computation for GPU acceleration for the dependency parsing . This work is an extension to the constituency parsing with different inside and Viterbi (CKY) algorithms.</p><p>As an independent and concurrent work to ours, Torch-Struct 10 , kindly brought up by a reviewer, has also implemented batchified TreeCRF algorithms for constituency parsing <ref type="bibr" target="#b10">[Rush, 2020]</ref>. However, Torch-Struct aims to provide 8 https://github.com/huggingface/transformers 9 https://github.com/jiesutd/NCRFpp 10 https://github.com/harvardnlp/pytorch-struct general-purpose basic implementations for structure prediction algorithms. In contrast, we work on sophisticated parsing models, and aim to advance the state-of-the-art CRF constituency parsing in both accuracy and efficiency.</p><p>Meanwhile, there is a recent trend of extremely simplifying the constituency parsing task without explicit structural consideration or the use of CKY decoding. <ref type="bibr">Gómez-Rodríguez and Vilares [2018]</ref> propose a sequence labeling approach for constituency parsing by designing a complex tag encoding tree information for each input word. Vilares et al. <ref type="bibr">[2019]</ref> further enhance the sequence labeling approach via several augmentation strategies such as multi-task learning and policy gradients. <ref type="bibr" target="#b10">Shen et al. [2018]</ref> propose to predict a scalar distance in the gold-standard parse tree for each neighboring word pairs and employ bottom-up greedy search to find an optimal tree. However, all the above works lag behind the mainstream approaches by a large margin in terms of parsing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we propose a fast and accurate neural CRF constituency parser. We show that the inside and CKY algorithms can be effectively batchified to accommodate direct large tensor computation on GPU, leading to dramatic efficiency improvement. The back-propagation procedure is equally efficient and erases the need for the outside algorithm for gradient computation. Experiments on three English and Chinese benchmark datasets lead to several promising findings. First, the simple two-stage bracketing-then-labeling approach is more efficient than one-stage parsing without hurting performance. Second, our new scoring architecture achieves higher performance than the previous method based on minus features. Third, the dropout strategy we introduce can improve parsing performance by a large margin. Finally, our proposed parser achieves new state-of-the-art performances with a parsing speed of over 1,000 sentences per second.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example constituency trees. Part-of-speech (POS) tags are not used as inputs in this work and thus excluded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>reduce memory and computation cost. Previous scoring method. Stern et al. [2017] use minus features of BiLSTM outputs as span representations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on dev data. All models use randomly initialized word embeddings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Speed comparison on PTB test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results on test data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>compares different parsing models in terms of pars-ing speed. Our models are both run on a machine with Intel Xeon E5-2650 v4 CPU and Nvidia GeForce GTX 1080 Ti GPU. Berkeley Parser and ZPar are two representative non-neural parsers without access to GPU. Stern et al. [2017] em-ploy max-margin training and perform CKY-like decoding on CPUs. Kitaev and Klein [2018] use a self-attention encoder and perform decoding using Cython for acceleration.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.nltk.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our preliminary experiments show that fi ⊕ bi+1 achieves consistent improvement over fi ⊕ bi. The possible reason may be that both fi and bi use ei as input and thus provide redundant information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://nlp.stanford.edu/projects/glove6  We have also tried the structured skip-gram embeddings kindly shared by Zhou and Zhao[2019]  for Chinese, and achieved similar performance by using our own embeddings. 7 https://github.com/DoodleJZ/HPSG-Neural-Parser</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for their helpful comments. This work was supported by National Natural Science Foundation of China (Grant No. 61525205, 61876116) and a Project Funded by the Priority Academic Program Development (PAPD) of Jiangsu Higher Education Institutions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nader Akoury, Kalpesh Krishna, and Mohit Iyyer. Syntactically supervised transformers for faster neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akoury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1269" to="1281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collins ; Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Timothy Dozat and Christopher D. Manning. Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="302" to="312" />
		</imprint>
	</monogr>
	<note>Proceedings of NAACL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. Efficient, feature-based, conditional random field parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="959" to="967" />
		</imprint>
	</monogr>
	<note>Proceedings of WS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What&apos;s going on in neural constituency parsers? an analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gaddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gómez-Rodríguez and Vilares</title>
		<editor>Carlos Gómez-Rodríguez and David Vilares</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relation extraction exploiting full dependency forests</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speed and accuracy in shallow and deep stochastic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multilingual constituency parsing with selfattention and pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klein ; Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kitaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3499" to="3505" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The inside-outside recursive neural network model for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="729" to="739" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI<address><addrLine>Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,</addrLine></address></meeting>
		<imprint>
			<publisher>Petrov and Klein</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
	<note>Proceedings of NAACL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zhiyang Teng and Yue Zhang. Two local models for neural constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush ; Kenji Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00876</idno>
	</analytic>
	<monogr>
		<title level="m">Torch-struct: Deep structured prediction library</title>
		<meeting><address><addrLine>Dan Klein, Mike Collins, Daphne Koller, and Christopher Manning</addrLine></address></meeting>
		<imprint>
			<publisher>Teng and Zhang</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="3372" to="3383" />
		</imprint>
	</monogr>
	<note>Proceedings of NAACL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hieu Pham, Pengcheng Yin, and Graham Neubig. A tree-based decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang ; Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Xinyi Wang</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4772" to="4777" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient second-order TreeCRF for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
