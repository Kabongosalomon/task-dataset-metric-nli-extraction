<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Group and Label Fine-Grained Shape Com-ponents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-11">2018. November 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyue</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinping</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">Xu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyue</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinping</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Beihang University BIN ZHOU</orgName>
								<orgName type="institution" key="instit2">Beihang University HAIYUE FANG</orgName>
								<orgName type="institution" key="instit3">Beihang University XIAOWU CHEN</orgName>
								<orgName type="institution" key="instit4">Beihang University QINPING ZHAO</orgName>
								<orgName type="institution" key="instit5">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Technology and Princeton University</orgName>
								<orgName type="institution">National University of Defense</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Beihang University</orgName>
								<orgName type="institution" key="instit2">Beihang University</orgName>
								<address>
									<addrLine>Beihang University; Kai Xu</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Technology and Princeton University</orgName>
								<orgName type="institution">National University of Defense</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<country># Components</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Group and Label Fine-Grained Shape Com-ponents</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Transactions on Graphics</title>
						<imprint>
							<biblScope unit="volume">37</biblScope>
							<biblScope unit="issue">1</biblScope>
							<date type="published" when="2018-11">2018. November 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3272127.3275009</idno>
					<note type="submission">Publication date: November 2018.</note>
					<note>Corresponding author: Authors&apos; addresses: Xiaogang Wang, Beihang University; Bin Zhou, Beihang University; 0730-0301/2018/11-ART1 $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: • Computing methodologies → Shape analysis; Additional Key Words and Phrases: Shape segmentation</term>
					<term>semantic labeling</term>
					<term>fine-grained components</term>
					<term>part hypotheses</term>
					<term>data-driven shape analysis ACM Reference Format:</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>solution to grouping and labeling of the fine-grained components. However, directly characterizing the shape of individual components for the purpose of labeling is unreliable, since they can be arbitrarily tiny and semantically meaningless. We propose to generate part hypotheses from the components based on a hierarchical grouping strategy, and perform labeling on those part groups instead of directly on the components. Part hypotheses are mid-level elements which are more probable to carry semantic information. A multiscale 3D convolutional neural network is trained to extract context-aware features for the hypotheses. To accomplish a labeled segmentation of the whole shape, we formulate higher-order conditional random fields (CRFs) to infer an optimal label assignment for all components. Extensive experiments demonstrate that our method achieves significantly robust labeling results on raw 3D models from public shape repositories. Our work also contributes the first benchmark for component-wise labeling. <ref type="figure">Fig. 2</ref>. Statistics on component count (left) and size (right) of the models in ShapeNetCore. The left histogram is measured only for car models. In the right one, size is measured by the ratio of bounding box volume between a component and the whole shape.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. We study a novel problem of semantic labeling of raw 3D models from online shape repositories, composed of highly fine-grained components. Given a 3D shape comprising numerous human-crafted components, our method produces high quality labeling result (indicated by distinct colors). This is achieved by aggregating the components into part hypotheses and characterizing these mid-level elements for robust grouping and labeling of fine-grained components.</p><p>A majority of stock 3D models in modern shape repositories are assembled with many fine-grained components. The main cause of such data form is the component-wise modeling process widely practiced by human modelers. These modeling components thus inherently reflect some function-based shape decomposition the artist had in mind during modeling. On the other hand, modeling components represent an over-segmentation since a functional part is usually modeled as a multi-component assembly. Based on these observations, we advocate that labeled segmentation of stock 3D models should not overlook the modeling components and propose a learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Semantic or labeled segmentation of 3D shapes has gained significant performance boost over recent years, benefiting from the advances of machine learning techniques <ref type="bibr" target="#b8">[Hu et al. 2012;</ref><ref type="bibr" target="#b13">Kalogerakis et al. 2010]</ref>, and more recently of deep neural networks <ref type="bibr" target="#b12">[Kalogerakis et al. 2017;</ref>. Existing methods have so far been dealing with manifold meshes, point clouds, or volumes. They are, however, not specifically designed to handle most stock 3D models, which typically assembles up to hundreds of highly fine-grained components <ref type="figure">(Figure 1</ref>). Multi-component assembly is the most commonly seen data form in modern 3D shape repositories (e.g., Trimble 3D Warehouse <ref type="bibr">[Tri 2017]</ref> and ShapeNet ). See <ref type="figure">Figure 2</ref>(left) for the statistics of component counts in ShapeNetCore.</p><p>Multi-view projective segmentation <ref type="bibr" target="#b12">[Kalogerakis et al. 2017;</ref>] is perhaps the most feasible approach for handling multicomponent shapes, among all existing techniques. View-based methods are representation independent, making them applicable to nonmanifold models. However, a major drawback of this approach is that it cannot handle shapes with severe self-occlusion. Components hidden from the surface are invisible to any view, thus cannot be labeled. <ref type="figure" target="#fig_0">Figure 3(a)</ref> shows such an example: The seats in the car are completely occluded by the car shell and thus cannot be segmented or labeled correctly by view-based methods.</p><p>Most off-the-shelf 3D models are created by human modelers in a component-by-component fashion. Generally, human modelers tend to have in mind a meaningful decomposition of the target object before starting. Such decomposition is inherently related to functionality, mimicking the actual production of the man-made objects, e.g., a car is decomposed into shell, hood, wheels, seats, etc. Therefore, we advocate that the segmentation of such models should not overlook the components coming with the models. Meanwhile, these components usually represent an over-segmentation -a functional part might be modeled as an assembly of multiple sub-parts. A natural solution to semantic segmentation thus seems to be a labeled grouping of the modeling components.</p><p>A few facts about the components of stock models, however, make their grouping and labeling especially difficult. First, the decomposition of these models is often highly fine-grained. See the tiny components the bicycle model in <ref type="figure">Figure 1</ref> contains. Taking the car models in ShapeNetCore for example, about 85% contains over 100 components. Second, the size of components varies significantly; see <ref type="figure">Figure 2</ref>(right). Third, different modelers may have different opinions about shape composition, making the components of the same functional part highly inconsistent across different shapes. The example in <ref type="figure" target="#fig_0">Figure 3</ref>(b) shows that the wheel parts from different vehicle models have very different composition. Due to these reasons, it is very unreliable to directly characterize the shape of individual components for the purpose of labeling.</p><p>These facts motivate us to consider larger and more meaningful elements, for achieving a robust semantic labeling of fine-grained components. In particular, we propose to generate part hypotheses from the components, representing potential functional or semantic parts of the object. This is achieved by a series of effective grouping strategies, which is proven robust with extensive evaluation. Our task then becomes labeling the true part hypotheses while pruning those false ones, instead of directly labeling the individual components. Working with part hypotheses enables us to learn more informative shape representation, based on which reliable labeling can be conducted. Part hypothesis is similar in spirit to mid-level patch for image understanding which admits more discriminative descriptors than feature points <ref type="bibr" target="#b27">[Singh et al. 2012]</ref>.</p><p>To achieve a powerful part hypotheses labeling, we adopt 3D Convolutional Neural Networks (CNN) to extract features from the volumetric representation of part hypotheses. In order to learn features that capture not only local part geometry but also global, contextual information, we design a network that takes two scales of 3D volume as input. The local scale encodes the part hypothesis of interest itself, through feature extraction over the voxelization of the part within its bounding box. The global volume takes the bounding box of the whole shape as input, and encodes the context with two channels contrasting the volume occupancy of the part hypothesis itself and that of the remaining parts. The network outputs the labeling probabilities of the part hypothesis over different part categories, which are used for final labeled segmentation.</p><p>To accomplish a labeled segmentation of the whole shape, we formulate higher-order Conditional Random Fields (CRFs) to infer an optimal label assignment for each component. Our CRF-based model achieves highly accurate labeling, while saving the effort on preparing large amount of high-order relational data for training a deep model. Consequently, our design choice, combining CNNbased part hypothesis feature and higher-order CRFs, achieves a good balance between model generality and complexity.</p><p>We validate our approach on our multi-component labeling (MCL) benchmark dataset. The multi-component 3D models are collected from both ShapeNet and 3D Warehouse, with all components manually labeled. Our method achieves significantly higher accuracy in grouping and labeling highly fine-grained components than alternative approaches. We also demonstrate how our method can be applied to fine-grained part correspondence for 3D shapes, achieving state-of-the-art results.</p><p>The main contributions of our paper include:</p><p>• We study a new problem of labeled segmentation of stock 3D models based on the pre-existing, highly fine-grained components, and approach the problem with a novel solution of part hypothesis generation and characterization. • We propose a multi-scale 3D CNN for encoding both local and contextual information for part hypothesis labeling, as well as a CRF-based formulation for component labeling. • We build the first benchmark for multi-component labeling with component-wise ground-truth labels and conduct extensive evaluation over the benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Shape segmentation and labeling is one of the most classical and long-standing problems in shape analysis, with numerous methods having been proposed. Early studies <ref type="bibr" target="#b2">[Au et al. 2012;</ref><ref type="bibr" target="#b10">Huang et al. 2009;</ref><ref type="bibr" target="#b14">Katz and Tal 2003;</ref><ref type="bibr" target="#b25">Shapira et al. 2010;</ref>] most utilize hand-crafted geometry features. One geometric feature usually captures very limited aspects about shape decomposition and a wider practiced approach is to combine multiple features <ref type="bibr" target="#b13">[Kalogerakis et al. 2010</ref>].</p><p>To tackle the limitation of hand-crafted features, data-driven feature learning methods are proposed <ref type="bibr" target="#b36">[Xu et al. 2016]</ref>. <ref type="bibr" target="#b7">Guo et al. [2015]</ref> learned a compact representation of triangle for 3D mesh labeling by non-linearly combining and hierarchically compressing various geometry features with the deep CNNs. <ref type="bibr" target="#b35">Xie et al. [2014]</ref> proposed a fast method for 3D mesh segmentation and labeling based on Extreme Learning Machine. <ref type="bibr" target="#b39">Yi et al. [2017b]</ref> proposed a method, named SyncSpecCNN, to label the semantic part of 3D mesh. SyncSpecCNN trains vertex functions using CNNS, and conducts spectral analysis to enable kernel weight sharing by using localized information of mesh graphs. These methods achieve promising performance, while largely focusing on manifold and/or watertight surface mesh, but not suited for raw 3D models from modern shape repositories.</p><p>Recently, Kalogerakis et al. <ref type="bibr" target="#b0">[2017]</ref> proposed a deep architecture for segmenting and labeling semantic parts of 3D shape by combining multi-view fully convolutional networks and surface-based CRFs. Projection-based methods <ref type="bibr" target="#b12">[Kalogerakis et al. 2017;</ref> are suitable for imperfect (e.g., incomplete, self-intersecting, and noisy) 3D shapes, but inherently have a hard time on shapes with severe self-occlusion.  designed a novel type of neural network, named PointNet, for directly segmenting and labeling 3D point clouds while respecting the permutation invariance, obtaining state-of-the-art performance on point data.</p><p>Several unsupervised or semi-supervised methods are proposed for the co-segmentation and/or co-labeling of a collection of 3D shapes belonging to the same category <ref type="bibr" target="#b8">[Hu et al. 2012;</ref><ref type="bibr" target="#b9">Huang et al. 2011;</ref><ref type="bibr" target="#b21">Lv et al. 2012;</ref><ref type="bibr" target="#b26">Sidi et al. 2011;</ref><ref type="bibr" target="#b31">van Kaick et al. 2013;</ref><ref type="bibr" target="#b32">Wang et al. 2012;</ref><ref type="bibr" target="#b37">Xu et al. 2010</ref>]. Most of these methods are based on an over-segmentations of the input shapes. A grouping process is then conducted to form semantic segmentation and labeling. Such initial over-segments (e.g., superfaces) are analogy to our 'part hypotheses'. However, they are still too low level to capture meaningful part information. Our method benefits from the pre-existing fine-grained components, which makes part hypothesis based analysis possible.</p><p>Few works studied on semantic segmentation of multi-component models.  proposed to label and organize 3D scenes obtained from the Trimble 3D Warehouse into consistent hierarchies capturing semantic and functional substructures. The labeling is based on over-segmentation of the 3D input, and guided by a learned probabilistic grammar. <ref type="bibr" target="#b38">Yi et al. [2017a]</ref> proposed a method of converting the scene-graph of a multi-component shape into segmented parts by learning a category-specific canonical part hierarchy. Their method achieves fine-grained component labeling, while scene graphs are not always available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>Please refer to <ref type="figure">Figure 4</ref> for an overview of our algorithm pipeline. In the next, we describe the three algorithmic components, including part hypothesis generation, part hypothesis classification and scoring, and part composition inference and component labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generating part hypothesis</head><p>Part hypothesis. In our work, a semantic part, or part for short, refers to a semantically independent or functionally complete group of components. A part hypothesis is a component group which potentially represents a semantic part. When searching for a part hypothesis, we follow two principles. Firstly, a part hypothesis should cover as many as possible components of the corresponding groundtruth part. Secondly, the component coverage of a part hypothesis should be conservative, meaning that a hypothesis with missing components is preferred over that encompassing components across different semantic parts.</p><p>Grouping strategy. It is a non-trivial task to generate part hypotheses meeting the above requirements exactly. It is very likely that there is not a single optimal criterion that can be applied to generate hypotheses for any semantic part from a set of components. For example, the many components of a car wheel can seemingly grouped based on a compactness criterion. For bicycle chain, however, the tiny chain links are not compactly stacked at all, for which size based grouping might be more appropriate. Therefore, we design a grouping strategy encompassing three heuristic criteria, which are intuitively interpretable and computationally efficient. The grouping for each criterion is performed in a bottom-up fashion, based on a nested hierarchy. After that, a hypotheses selection <ref type="bibr">Fig. 4</ref>. An overview of our approach. Given a 3D model comprising many components (a), our method first performs hierarchical sampling of candidate part hypotheses with a bottom-up grouping procedure (b, Section 3.1). Each candidate is then fed into a multi-scale Convolutional Neural Networks which predicts its label and regresses a confidence score against each part label (c, Section 3.2). Finally, the optimal label of each component is inferred with a higher-order Conditional Random Fields, based on the confidence scores (d, Section 3.3).</p><p>process is conducted to ensure a conservative hypothesis coverage. The quality of generated part hypotheses is evaluated in Section 4.4.</p><p>Through a statistical analysis ( <ref type="figure" target="#fig_1">Figure 5</ref>), we found that most semantic parts are spatially compact, such as a door or a wheel of a car. We thus define a criterion called Center Distance, denoted by C center (a, b), to measure the compactness between two components a and b. It measures the distance between the barycenters of convex hull of two components, and encourages grouping of components which are spatially close to each other.</p><p>The second criterion, sharing the similar intuition as center distance, imposes a stronger test on compactness. This is motivated by the fact that the components in a functional part are typically tightly assembled. The Geometric Contact criterion, denoted as C contact (a, b), prioritizes the grouping of components with large area of geometric contact. Let V a and V b be the volume of component a and b, respectively, and C ab be the contact volume. The criterion is defined as the maximum of the ratio between contact volume and component volume:</p><formula xml:id="formula_0">C contact (a, b) = max{C ab /V a , C ab /V b }.</formula><p>Here, the volume of a component can be computed by counting the voxels occupied by the component in a global voxelization of the entire shape. The contact volume between two components can be computed as the number of overlapping voxels of the two components in the global voxelization. The semantic parts of a 3D model can be of arbitrary size, ranging from a rear-view mirror to the entire cab for a car; see the supplementary material. Thus for each part category, we sample a set of candidate proposals with varying sizes, to avoid missing the best one. We design a third criterion Group Size, denoted by C size (a, b), as the occupancy rate of the joint volume of component a and b over the volume of the whole shape. This criterion is used to control the grouping, sampling groups first in small size and then to large.</p><p>Hierarchical hypothesis sampling. We employ a hierarchical aggregation algorithm to generate part hypotheses. This is motivated by the fact that most off-the-shelf 3D models are assembled with components in a hierarchical manner. Given a shape, the sampling process starts from the input set of components, and groups in a greedy, bottom-up manner. At each time, the pair of adjacent components with the smallest grouping criterion measure are grouped into a new node. The process is repeated until reaching the root of the hierarchy and performed for each criterion separately. Figure 6 illustrates the grouping process for each grouping criterion separately. Nodes shaded in grey color in the hierarchies represent a sampled part hypothesis. The top few groups in each hierarchy is then selected to form the candidate set for the given shape, which will be discussed in the next.</p><p>Selection of part hypotheses. We first simply sort the hypotheses, corresponding to nodes in a hierarchy, based on their grouping order. Higher level nodes imply larger coverage, while lower ones correspond to smaller regions. To prevent the selection from overly favoring hypotheses large coverage, we introduce random factors into the selection process, in a similar spirit to <ref type="bibr" target="#b3">[Carreira and Sminchisescu 2012;</ref><ref type="bibr" target="#b22">Manen et al. 2014;</ref><ref type="bibr" target="#b30">van de Sande et al. 2011</ref>]. In particular, the initial sorting is perturbed by multiplying the sorted indices with a random number in (0, 1), and then resorting based on the resulting numbers. Finally, the top H hypotheses are selected for each hierarchy, thus yielding 3H hypotheses in total. In <ref type="figure" target="#fig_3">Figure 7</ref>, we visualize a few part hypotheses corresponding to some semantic parts for a bicycle model.</p><p>The Intersection of Union (IoU) based recall (the recall rate for a given IoU threshold w.r.t. ground-truth) of the hypothesis selection is given in <ref type="figure" target="#fig_4">Figure 8</ref>. It shows that our hierarchical sampling and selection method is quite effective in capturing the potential semantic parts, even for complicated structures such as chairs, bicycles and helicopters. Note that although the recall rates drop significantly around the IoU threshold of 0.6 for several categories, it does not hurt the performance since the recall rates for IoU of 0.6 are already high enough for the following CRF-based labeling algorithm to perform well. Results in <ref type="figure" target="#fig_7">Figure 12</ref> show that the labeling accuracy is stable with the number of sampled part hypotheses and 1000 proposals (our default choice) are sufficient for all categories.</p><p>Remarks on design choice. For part hypothesis generation, we opted for combinatorial search with a hierarchical guidance rather than a learning based approach. This is because the significantly varying number and size of components within a semantic part make it extremely difficult for, e.g., a CNN model, to capture the shape geometry with a fixed input resolution. Taking the part hypotheses in a bicycle model ( <ref type="figure" target="#fig_3">Figure 7)</ref> for example, the component size and  count differ greatly from part to part. On the other hand, a proper resolution of data representation for CNN is unknown before the part hypothesis is extracted -a chicken-and-egg problem! We have implemented and compared with a CNN-based method as a baseline, demonstrating clear advantage of our approach (Section 4.2 and 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classifying and Scoring of Part hypotheses</head><p>We train a neural network to classify a part hypothesis and produce a confidence score for it representing the confidence of the hypothesis being an independent semantic part. To achieve that, we first build a training dataset of multi-component 3D models with componentwise labels. We then design a multi-scale Convolutional Neural Networks (CNNs), which learns feature representation capturing not only local part geometry but also global context.</p><p>Training data. The multi-component 3D models used for training are collected from both ShapeNet ] and 3D warehouse <ref type="bibr">[Tri 2017]</ref>. The data comes from the training part of our multi-component labeling benchmark (see Section 4.1). Each model in the training set has a component-wise labeling, based on the semantic labels defined with WordNet. An overview of the humanlabeled models from the dataset can be found in the supplementary material. The training set contains eight object categories and two scene categories on which the statistics are detailed in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Input data representation. To train a CNN model for part hypothesis labeling, we opt for volumetric representation of part hypotheses as input, similar to <ref type="bibr" target="#b34">[Wu et al. 2015]</ref>. To achieve a multi-scale feature learning, we represent each hypothesis in three scales including a local scale based on a voxelization of its bounding box, a global scale, which takes the volume of the bounding box of the entire shape and contributes two channels. One channel encodes the volume occupancy of the part hypothesis itself and the other accounts for the context based on the occupancy of the remaining parts. For each scale, the volume resolution is fixed to 30 × 30 × 30. To avoid the global alignment among all shapes, we opt for training with many possible orientations of each shape. In practice, we use the up-right orientation of each shape and enumerate its four canonical orientations (Manhattan frames).</p><p>Data augmentation for balanced training. The hierarchical grouping of part hypotheses could make the training data unbalanced: Insufficient data is sampled for semantic categories containing small number of components (e.g., rear-view mirrors of cars). This will make our CNN model inadequately trained for these categories. To cope with this issue, we opt to synthesize more training data, for the categories with insufficient instances, based on the ground-truth of semantic parts in the training data. Specifically, we pursue two ways for data augmentation.</p><p>(1) Component deletion. Given a ground-truth semantic part, we randomly delete a few components and use the incomplete part as a training example. We typically remove up to 30% components.</p><p>(2) Component insertion. Given a ground-truth semantic part, we randomly insert a few components from the neighboring parts to form a training example. We stipulate that the newly added components do not exceed 30% of original ones.</p><p>Ground-truth labels and scores. We next compute a groundtruth part label and confidence score for each training part hypothesis, used for training our network for both label prediction and score regression. For a given part hypothesis, if its components labeled with a certain category occupy over 70% of the global voxelization of the entire shape, it is treated as a positive example for that category, and negative otherwise. For each hypothesis, we first compute its 3D Intersection of Union (IoU) against each ground-truth semantic part of the shape, in a global voxelization of 200 × 200 × 200. The highest IoU is set as its confidence score. This score measures the confidence of a hypothesis being an independent semantic part, which will be utilized in the final label inference in Section 3.3.</p><p>Network architecture. We design a multi-scale Convolutional Neural Networks (CNNs). The architecture of our network is given in <ref type="figure" target="#fig_5">Figure 9</ref>. The network has three towers, taking the inputs corresponding to the local and the two global channels mentioned above. We refer to these towers as local, global and contextual, respectively; see <ref type="figure" target="#fig_5">Figure 9</ref>. The feature maps output by the three towers are concatenated into one feature vector, and then fed into a few fully connected layers, yielding a 2048 feature vector. The final fully connected layer predicts a label and regresses a score for the input part hypothesis. In particular, our network produces a probability distribution over K + 1 part labels, p = (p 0 , . . . , p K ), with label 0 being null label. An unrecognized part is assigned with a null label. The second output is confidence score r . We use a joint loss L for both hypothesis classification and score regression:</p><formula xml:id="formula_1">L(p, r, c, s) = L cls (p, c) + L reg (r , s),<label>(1)</label></formula><p>where L cls (p, c) = − log p c is cross-entropy loss for label c. and L reg is smooth L 1 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Composite Inference and Labeling</head><p>Given the confidence scores of a sampled part hypothesis, the final stage of our method is to infer an optimal label assignment for each component. Given a multi-component 3D model, denoted by M, which comprises a set of components C. Each component c ∈ C is associated with a random variable x c ∈ X which takes a value from the part label set L = {l 1 , . . . , l K }. Let H denote the set of all part hypotheses. A part hypothesis h ∈ H is denoted by a set of components, h {c h i } i ⊂ C, and its labeling is represented a vector of random variables x h = (x h i ) i , with x h i ∈ X being the label assignment for component c h i . A possible label assignment to all components, denoted by X , is called a labeling for model M.</p><p>We construct a higher-order Conditional Random Fields (CRFs), to find the optimal labeling for all components, based on the part hypothesis analysis from the previous steps:</p><formula xml:id="formula_2">E(L) = c ∈ C φ(x c ) + λ h ∈H ψ (x h ),<label>(2)</label></formula><p>where the first term is the unary potential for each component and the second term is the higher order consistency potential defined with each hypothesis. The parameter λ is used to tune the importance of the two terms. We set λ = 0.1 in all our experiments. The CRF-based labeling is illustrated in <ref type="figure">Figure 10</ref>,</p><formula xml:id="formula_3">Unary potential. Suppose H c = {h c i } H c i=1</formula><p>be the set of part hypotheses containing component c. The unary potential φ(x c ) is defined as:</p><formula xml:id="formula_4">φ(x c ) = − log P(x c = l k ),<label>(3)</label></formula><p>where P(x c = l k ) is the probability of x c taking the label l k , and is defined as: <ref type="figure">Fig. 10</ref>. Illustration of our higher-order CRF. φ(x i ) is unary potential. ψ (x h ) is higher order consistency potential, which favours all components belonging to a part hypothesis taking the same label.</p><formula xml:id="formula_5">P(x c = l k ) = K c i=1 e w c i s c i p(l k |h c i ) K k =1 K c j=1 e w c j s c j p(l k |h c j ) ,<label>(4)</label></formula><p>where p(l k |h c i ) is the classification probability of hypothesis h c i against label l k , output of our hypothesis classification network. K c ⩽ H c is the top number of part hypotheses selected for computing the probability, based on the regressed confidence score for c. s c i is the confidence score for h c i , regressed by our network. w c i is a weight computed as the ratio between the volume of component c and that of hypothesis h c i . Higher order consistency potential. The goal of our CRF-based labeling is to resolve the inconsistency between different part hypotheses and compute a consistent component-wise labeling, resulting in a non-overlapping partition of all components. To this end, we design a higher order consistency potential <ref type="bibr" target="#b17">[Kohli et al. 2008;</ref><ref type="bibr" target="#b23">Park and Gould 2003]</ref>, based on the label purity of part hypotheses:</p><formula xml:id="formula_6">ψ (x h ) = N (x h ) 1 η γ max , if N (x h ) ≤ η γ max , otherwise<label>(5)</label></formula><p>where N (x h ) = min k {C h − n k (x h )}. C h is the number of components constituting part hypothesis h. n k (x h ) counts the number of random variables corresponding to hypothesis h which takes label k. η is the truncation parameter which controls the rigidity of the higher order consistency potential, and is set to 0.2 * C h in all our experiments. This means that up to 20% of h's components can take an arbitrary label. γ max = e −G(h)/C h , with G(h) being the label purity of a part hypothesis. The purity can be computed as the entropy of the classification probability output of our network:</p><formula xml:id="formula_7">G(h) = − K k=1 p(l k |h) log p(l k |h).<label>(6)</label></formula><p>where p(l k |h) is the classification probability of hypothesis h against label l k output, again, by our network. The consistency potential encourages components belonging to one part hypothesis to take the same label. However, it does not impose a hard constraint on label consistency by allowing a portion of the components within a part hypothesis to be labeled freely. This is achieved by the linear truncated cost over the number of inconsistent labels. This mechanism enables the components within a part hypothesis to be assigned to different labels, so that an optimal label assignment to all components could be found through compromising among all part hypotheses. The objective in Equation (2) can be efficiently optimized with the alpha-beta moving algorithm <ref type="bibr" target="#b17">[Kohli et al. 2008</ref>].</p><p>An alternative approach to CRF-based labeling would be formulating it as a deep learning model. The combination of CRF and deep neural networks has shown promising results on semantic segmentation of 2D images <ref type="bibr" target="#b41">[Zheng et al. 2015]</ref>. In the context of 3D component grouping and labeling, however, training such a deep model requires a large amount of relational data between different components, which is highly laborious. We believe our solution achieves a good balance between model generality and complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS AND EVALUATIONS 4.1 Multi-Component Labeling benchmark</head><p>To facilitate quantitative evaluation, we construct the first benchmark dataset with human-annotated, component-wise labels, named multi-component labeling benchmark, or MCL benchmark for short. The multi-component 3D models are collected from ShapeNet ] and 3D warehouse <ref type="bibr">[Tri 2017]</ref>, in which most 3D models are in the form of multi-component assembly. We manually annotate each model in our dataset by assigning a semantic category to each component, using our interactive annotation tool. The annotation tool is elaborated in the supplementary material. The semantic part categories are defined based on WordNet, which are summarized with an overview of the benchmark dataset in the supplementary material. Some statistics of the dataset are also given therein. <ref type="table" target="#tab_0">Table 1</ref> provides a summary and detailed statistics about our MCL benchmark dataset. For each category, about 20% ∼ 30% models are used for training, and the remaining for testing. Such a training/testing split is fixed all subsequent experiments. A few metrics on segmentation accuracy are defined to support quantitative evaluation of component labeling; see the following subsections for details. In the supplementary material, we provide an overview of the benchmark. We believe this benchmark would benefit more future research on component-wise shape analysis and data-driven shape modelling <ref type="bibr" target="#b29">Sung et al. 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Row 1 and 2 of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Labeling performance</head><p>We evaluate our semantic labeling based on our MCL benchmark. The performance is measured by average Intersection of Union (avg IoU). The results, reported in the last row of <ref type="table" target="#tab_0">Table 1</ref>, show that our approach achieves the best performance. In <ref type="figure" target="#fig_2">Figure 16</ref>, we show visually the labeling results. Our approach is able to produce robust labeling for fine-grained components with complex structure and severe self-occlusions.</p><p>We also test our method on the INRIA GAMMA 3D Mesh Database <ref type="bibr">[GAM 2017]</ref>, which is a large collection of human created 3D models. Our method, trained on the our MCL benchmark dataset, is applied to INRIA GAMMA database. <ref type="figure" target="#fig_6">Figure 11</ref> presents some labeling results on a few sample models, produced by our method. More results can be found in the supplementary material.</p><p>Comparison with baseline (random forest). To verify the effectiveness of our part hypothesis based analysis and multi-scale CNN based labeling, we implement a baseline using conventional method, i.e., hand-crafted features plus random forest classification. Specifically, we extract features, including light field descriptor <ref type="bibr" target="#b5">[Chen et al. 2010]</ref>, spherical harmonic descriptor <ref type="bibr" target="#b15">[Kazhdan et al. 2003</ref>], volume ratio and bounding box diameter, for each component, and feed them into a random forest classifier for component classification. We used the default parameter settings of the standard MATLAB toolbox for random forest, with the number of trees being 500. The comparison is shown in Table 1 (row 6). Our mid-level, part hypothesis analysis (the last row) significantly outperforms this alternative.</p><p>Comparison with baseline (CNN-based classification). The second baseline we compare to is a direct CNN-based component classification, without part hypothesis based analysis. Taking labeled components as training samples, we learn the network with the same architecture in <ref type="figure" target="#fig_5">Figure 9</ref>, except that only a classification loss, L cls , in Equation (1) is employed. The performance is reported in <ref type="table" target="#tab_0">Table 1 (row 7)</ref>. Our hypothesis-level analysis (the last row) achieves much higher labeling accuracy than component-level analysis, due to the fact that part hypotheses capture richer semantic information than individual components.</p><p>Comparison with baseline (CNN-based hypothesis generation). To demonstrate the difficulty of part hypothesis generation from fine-grained components with drastically varying numbers and sizes, we implement a CNN-based hypothesis generation through extending Fast RCNN <ref type="bibr" target="#b6">[Girshick 2015</ref>] to 3D volumetric representation. The network architecture and its detailed explanation can be found in the supplemental material. Taking the volumetric representation of a shape as input, the network is trained to predict at each voxel a 3D box representing part hypothesis. This is followed by another network for joint classification and refinement of the hypothesis regions. The training data utilize the ground-truth parts in our MCL dataset after voxelization. The results shown in <ref type="table" target="#tab_0">Table 1</ref> (row 8) are inferior to those of our method. The main reason is that the significant scale variation of components makes it difficult for volumetric representation to characterize their shape and structure. This justifies our design choice of hierarchical search for part hypothesis generation.</p><p>Comparison to state-of-the-art methods. We compare our approach with the methods in <ref type="bibr" target="#b38">[Yi et al. 2017a</ref>] and <ref type="bibr" target="#b7">[Guo et al. 2015]</ref>, both of which adopt multiple traditional features as inputs to train neural networks. For the shapes in our dataset, we compute both face-level and component-level geometric features, based on the original implementation of the two works. Details on the features can be found in the two original papers respectively. Note, however, the work <ref type="bibr" target="#b38">[Yi et al. 2017a</ref>] is able to produce hierarchical labeling while our method is not designed for this goal. To make the two methods comparable, we compare our labels to those of only leaf nodes produced by <ref type="bibr" target="#b38">[Yi et al. 2017a</ref>].</p><p>Our method is also compared with PointNet  and PointNet++ , two state-of-the-art deep learning based methods for semantic labeling of point clouds. We apply these methods by sampling the surface of the test shapes, while keeping the semantic labeling, resulting in about 10K points for each shape. To ensure a good performance of the two methods on our dataset and a fair comparison, we used their models pre-trained on ShapeNet and fine-tuned them on our training dataset.</p><p>We report per-category IoU percentage of these four methods on our benchmark dataset, see <ref type="table" target="#tab_0">Table 1</ref>. The results demonstrate the significant advantage of our part hypothesis analysis approach, with consistently more accurate labeling. In particular, our method significantly outperforms <ref type="bibr" target="#b38">[Yi et al. 2017a</ref>] on all categories and is comparable on 'office'. The significance is high (p-value &gt; 0.98) for models with severe self-occlusions such as vehicles, cabinets, motors, etc., and moderately high (p-value &gt; 0.92) for category 'bicycle' and 'lamp'. Another notable observation is that, all the alternative methods, especially PointNet and PointNet++, find a hard time in dealing with scene models. Scenes typically have more complicated structures due to the loose spatial coupling between objects. Our method, on the other hand, is able handle structures in various scales and forms, ranging from individual objects to compound scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter analyses and ablation studies</head><p>Parameter K c . When performing component inference and labeling (Section 3.3), the number of top-ranked part hypotheses, denoted by K c , selected for each component c in defining the unary potential (Equation <ref type="formula" target="#formula_5">(4)</ref>) is an important parameter of our method. We experiment the parameter settings K c being set to 1, 3, 5, 10 and all respectively, while keeping all other parameters unchanged. all means to use all part hypotheses of c (i.e., K c = H c ). The results of per-category average IoU are shown in row 16-20 of <ref type="table" target="#tab_0">Table 1</ref>. For object categories, the best performance is obtained when using K c = all for each component. For scene categories (the last two columns), however, K c &lt; all leads to better performance. This is because, for scene categories, the top ranked hypotheses, corresponding to the early groupings emerged in the hierarchical sampling process, are usually the individual objects in the scene. Such groups occur more frequently and hence more reliable to capture. The subsequent groupings, however, imply larger scale, inter-object structures. Since the spatial relationships between objects are usually loose, as we have pointed out earlier, such structures are less reliable (hard to learn), especially when the grouping scale becomes very large.</p><p>Labeling performance over part hypothesis count. We also evaluate our method with the varying number of part hypotheses generated. <ref type="figure" target="#fig_7">Figure 12</ref> shows the plots of avg IoU over the number of part hypotheses. The test is performed on six object categories of our benchmark dataset and the results on more categories can be found in the supplementary material. The same goes for all the plots shown in this paper. Generally, the performance grows as the number of hypotheses increases, but stops growing at a specific number. For all categories, we choose the hypothesis count no greater than 1000, even for structurally complicated categories such as vehicle and bicycle. This shows that our approach is insensitive to the initial number of part hypotheses.</p><p>Labeling performance without confidence score. For each part hypothesis, a confidence score is regressed by our network, which measures how likely it represents an independent semantic part. This score is employed in defining the unary potential in the global labeling inference. To test its effect, we experiment an ablated version of our method without considering this confidence score (by setting s c i = 1 in Equation <ref type="formula" target="#formula_5">(4)</ref>), while keeping all other parameters unchanged. The experimental results are reported in row 13 of <ref type="table" target="#tab_0">Table 1</ref>. For all categories, our method works better when incorporating confidence score. In particular, the improvement of average IoU over 'w/o score' ranges from 0.7% to 5.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation on part hypothesis generation</head><p>Part hypothesis quality vs. hypothesis count. In <ref type="figure" target="#fig_0">Figure 13</ref>, we study part hypothesis quality over the change of the number of sampled hypotheses. Part hypothesis quality is measured as follows: For the sampled hypotheses whose IoU is greater than 50%, we compute their recall rate over the ground-truth semantic parts. We find that the hypothesis quality (recall rate) grows rapidly as the number of hypotheses increases, becomes stable fast at a moderate hypothesis count. For complex categories (e.g., bicycle, vehicle, motor, office), the count is lower smaller 600, For other categories (e.g., cabinets, chair, lamp, living room), on the other hand, the number is no greater 200. These numbers show that our sampling algorithm produces high quality hypotheses with a moderate sampling size, much smaller than that of exhaustive enumeration.</p><p>Comparison to alternatives. We assess the quality of part hypotheses by comparing our hierarchical grouping algorithm with two alternative methods. The first method is the CNN-based hypothesis generation we have mentioned above. The second one learns for each shape category a Gaussian Mixture Model (GMM) modeling the position and scale distribution of bounding boxes of semantic parts. Given an input shape, the method generates part hypotheses by sampling the GMM of the corresponding shape category. To make the comparison, we generate the same number of top hypotheses for all methods. While our method samples the top number of hypotheses according to the hierarchical sampling order (Section 3.1), GMM samples based on probability. For CNN-based method, we use all hypotheses generated. We plot in <ref type="figure" target="#fig_9">Figure 14</ref> the curves of recall rate over average IoU for the three methods. It can be observed that our method produces the highest quality part hypotheses. The GMM-based method is a probabilistic sampling based approach which is fuzzy and cannot produce hypotheses with accurate boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Network analysis</head><p>To evaluate our network design, we study the effect of each of the three towers, local, global, and contextual, of our multi-scale CNNs. Specifically, we train and test networks with two different combinations, 'local only' and 'local+global'. In <ref type="table" target="#tab_0">Table 1</ref> (row 14 and 15), we show the results when different combinations of the towers are employed. Our multi-scale CNNs architecture (with all three towers) results in the best performance for object categories, while the 'local+global' architecture leads to higher performance for scene categories. This can be explained, again, by the fact that indoor scenes, even from the same category, have loosely defined structure and possess much layout variation. This makes it difficult to learn its global structure reliably, even with the help of contextual tower, when training data is not sufficiently large. Since our method is not targeted to scene parsing, we leave this for future work. Nevertheless, our method still obtains acceptable accuracy for scenes, verifying the ability of our method in handling structures across a large range of scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Training and testing time</head><p>Our implementation is built on top of the Caffe <ref type="bibr" target="#b11">[Jia et al. 2014</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">An application to shape correspondence</head><p>We apply our approach to component-level shape correspondence, which has been studied in <ref type="bibr" target="#b1">[Alhashim et al. 2015]</ref> and <ref type="bibr">[Zhu et al.</ref>  2017]. Given two multi-component shapes, we first use our method to group and label the components for each shape. Based on the semantic labeling, we find a global alignment for the local canonical frames of the two shapes. This is achieved by minimizing the spatial distance between every two components with the same label, each from one of the two shapes. Given a pair of semantic parts with the same label, each from one of the two shapes being matched, we align their bounding boxes and find correspondence for their enclosed components. In particular, we find for each component from one shape the spatially closest counterpart from the other shape. After the bidirectional search and a post-processing of conflict resolving (always keep the closer one if there are multiple matches), we return for each component from one shape a unique matching component from the other shape. We compare our simple method with the two state-of-the-arts, <ref type="bibr" target="#b1">[Alhashim et al. 2015]</ref> and <ref type="bibr" target="#b42">[Zhu et al. 2017]</ref>, on the benchmark dataset GeoTopo <ref type="bibr" target="#b1">[Alhashim et al. 2015]</ref>. <ref type="table">Table 3</ref> reports the results of precision and recall for component matching. Our method achieves comparable results to theirs, and performs better on categories with higher number of semantic parts, such as airplane and velocipedes. Note that our method does not consider any high-level structural information such as symmetry. <ref type="table">Table 3</ref>. Comparisons on precision and recall rates against two state-of-theart shape correspondence methods <ref type="bibr" target="#b1">[Alhashim et al. 2015]</ref> and <ref type="bibr" target="#b42">[Zhu et al. 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>GeoTopo DDS Our P R P R P R Chair 0.69 0.67 0.83 0.83 0.71 0.75 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have studied a new problem of labeled segmentation of offthe-shelf 3D models based on the pre-existing, highly fine-grained components. We approach the problem with a novel solution of part hypothesis analysis. The core idea of our approach is exploiting part hypothesis as a mid-level representation for semantic composition analysis of 3D shapes. This leads a highly robust labeling algorithm which can handle highly complicated structures in various scales and forms. Our work contributes, to the best of our knowledge, the first component-wise labeling algorithm that simultaneously works for single objects and compound scenes.</p><p>The success of our method is due to three key features: First, part hypotheses are generated in a principled way, based on a bottom-up When the bounding boxes of two part hypotheses overlap significantly, due to, for example, shape concavity, the labeling of them can be misled by each other. For example, the concave mudguard can be mistakenly labeled as wheel, due to its bounding box overlap against the wheel.</p><p>hierarchical grouping process, guided by three intuitive criteria. Second, a deep neural network is trained to encode part hypothesis, rather than components, accounting for both local geometric and global contextual information. Third, the higher order potential in our CRF-based formulation adopts a soft consistency constraint, providing more degree of freedom in optimal labeling search.</p><p>Limitations, failure cases and future works. Our approach has several limitations, which point out directions for future investigation. First, our current solution only groups the components but not further segment them, it thus cannot handle the case where the components are under-segmented with respect to semantic parts. <ref type="figure" target="#fig_1">Figure 15(a)</ref> shows two examples of such failure case. For such examples, a correct labeling cannot be obtained without a further breaking of this component. According to our statistics, only about 6% shapes in ShapeNet have such issue, based on our own set of semantic labels. As a future work, we would consider incorporating component-level segmentation into our framework. <ref type="figure" target="#fig_1">Figure 15(b)</ref> shows another type of failure case. When the bounding boxes of two part hypotheses overlap significantly, due to, for example, shape concavity, their labeling can be misleading. Currently, our method does not produce hierarchical part grouping and labeling, as in <ref type="bibr" target="#b38">[Yi et al. 2017a</ref>]. It would be interesting to investigate extending our hypothesis analysis for the task of hierarchical segmentation. For example, how to determine the order of grouping, or the structure of the hierarchy, is a non-trivial task. Another worthy topic for future research is the integration of CRF in the deep neural networks to make the entire model end-to-end trainable while avoiding relying on strong supervision <ref type="bibr" target="#b12">[Kalogerakis et al. 2017]</ref>. <ref type="figure" target="#fig_2">Fig. 16</ref>. A gallery of semantic labeling results on raw 3D models with complicated structure. 'GT' denotes ground-truth labeling annotated manually.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Hidden components, e.g., car seats in (a), and various fine-grained decompositions of vehicle wheels (b) found in the ShapeNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>The occupancy ratio of the bounding box of varying number of semantic parts over the entire model. The statistics are performed with our benchmark dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Hierarchical sampling of part hypotheses based on the three grouping criteria, center distance (a), group size (b) and geometric contact (c), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Some samples of part hypothesis corresponding to different semantic labels of bicycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Performance (recall rate over IoU) of part hypothesis generation in all object/scene categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>The architecture of our multi-scale Convolutional Neural Networks (CNNs) for part hypothesis classification and confidence regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>Labeling results on five sample models from the INRIA GAMMA models produced by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Labeling accuracy (average IoU) vs. number of part hypotheses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Recall rate on semantic parts over varying number of part hypotheses, when IoU against ground-truth is fixed to 50%, over six categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .</head><label>14</label><figDesc>Performance (recall rate vs. average IoU) comparisons between our hierarchical grouping algorithm and the two alternative methods (GMMand CNN-based hypothesis generation) over six categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 15 .</head><label>15</label><figDesc>There are two typical kinds of failure cases. (a): Our method cannot handle the case where the components are under-segmented with respect to semantic parts. In the left chair, the component marked in the dashed box encompasses both leg and back parts. The seat and back part of the right chair are merged as a single component. For such examples, a correct labeling cannot be obtained without a further breaking of this component. (b):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Accuracy of grouping and labeling (average Intersection of Union, in percentage) on our benchmark dataset MCL. Row 1: The average number of components for each category. Row 2: The number of annotated semantic labels for each category. Row 3: The maximum numbers of part hypotheses for each model used for final labeling inference. Row 4: Training / testing split (number of models) of our dataset. Row 5: The number of training examples (hypotheses) for each category. Row 6-20: Average IoU of baseline methods, state-of-the-art methods and our method in different settings.</figDesc><table><row><cell></cell><cell>Rows</cell><cell cols="5">Vehicle Bicycle Chair Cabinet Plane</cell><cell cols="5">Lamp Motor Helicopter Living room Office</cell></row><row><cell>1</cell><cell># Avg. components</cell><cell>649</cell><cell>572</cell><cell>31</cell><cell>53</cell><cell>111</cell><cell>17</cell><cell>188</cell><cell>178</cell><cell>197</cell><cell>276</cell></row><row><cell>2</cell><cell># Semantic labels</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>3</cell><cell>10</cell><cell>3</cell><cell>9</cell><cell>3</cell><cell>8</cell><cell>7</cell></row><row><cell>3</cell><cell># Part hypotheses (⩽)</cell><cell>1000</cell><cell>1000</cell><cell>200</cell><cell>200</cell><cell>1000</cell><cell>200</cell><cell>1000</cell><cell>1000</cell><cell>200</cell><cell>200</cell></row><row><cell>4</cell><cell># Train / # Test</cell><cell cols="7">26 / 83 30 / 68 33 / 81 25 / 57 17 / 78 30 / 70 22 / 87</cell><cell>21 / 84</cell><cell>30 / 72</cell><cell>30 / 72</cell></row><row><cell>5</cell><cell># Training hypo.</cell><cell>23787</cell><cell>28947</cell><cell>2149</cell><cell>4666</cell><cell>4812</cell><cell>1662</cell><cell>12800</cell><cell>8263</cell><cell>7090</cell><cell>14475</cell></row><row><cell>6</cell><cell>Baseline (Random Forest)</cell><cell>54.7</cell><cell>58.9</cell><cell>62.4</cell><cell>65.9</cell><cell>53.5</cell><cell>63.3</cell><cell>65.9</cell><cell>52.8</cell><cell>47.7</cell><cell>68.5</cell></row><row><cell>7</cell><cell>Baseline (CNN Classifier)</cell><cell>48.9</cell><cell>63.8</cell><cell>70.75</cell><cell>63.3</cell><cell>68.9</cell><cell>81.2</cell><cell>67.4</cell><cell>78.5</cell><cell>51.2</cell><cell>63.9</cell></row><row><cell>8</cell><cell>Baseline (CNN Hypo. Gen.)</cell><cell>56.3</cell><cell>51.9</cell><cell>68.5</cell><cell>45.7</cell><cell>58.5</cell><cell>71.1</cell><cell>53.1</cell><cell>72.2</cell><cell>58.6</cell><cell>69.1</cell></row><row><cell>9</cell><cell>PointNet [Su et al. 2017]</cell><cell>24.3</cell><cell>30.6</cell><cell>68.6</cell><cell>21.0</cell><cell>47.2</cell><cell>46.3</cell><cell>35.8</cell><cell>32.6</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">10 PointNet++ [Qi et al. 2017]</cell><cell>51.7</cell><cell>53.8</cell><cell>69.3</cell><cell>62.0</cell><cell>53.9</cell><cell>79.8</cell><cell>62.2</cell><cell>79.3</cell><cell>-</cell><cell>-</cell></row><row><cell>11</cell><cell>Guo et al. [2015]</cell><cell>27.1</cell><cell>25.2</cell><cell>34.2</cell><cell>68.8</cell><cell>38.6</cell><cell>79.1</cell><cell>41.6</cell><cell>80.1</cell><cell>33.7</cell><cell>28.5</cell></row><row><cell>12</cell><cell>Yi et al. [2017a]</cell><cell>65.2</cell><cell>63.0</cell><cell>61.9</cell><cell>70.6</cell><cell>59.3</cell><cell>82.2</cell><cell>67.5</cell><cell>78.9</cell><cell>56.6</cell><cell>68.6</cell></row><row><cell>13</cell><cell>Ours (w/o score)</cell><cell>71.5</cell><cell>66.8</cell><cell>72.5</cell><cell>76.5</cell><cell>71.4</cell><cell>87.6</cell><cell>70.7</cell><cell>81.2</cell><cell>63.3</cell><cell>60.1</cell></row><row><cell>14</cell><cell>Ours (local only)</cell><cell>50.4</cell><cell>52.4</cell><cell>60.4</cell><cell>68.6</cell><cell>61.3</cell><cell>73.5</cell><cell>60.4</cell><cell>78.5</cell><cell>62.7</cell><cell>54.8</cell></row><row><cell>15</cell><cell>Ours (local+global)</cell><cell>69.2</cell><cell>67.3</cell><cell>68.6</cell><cell>75.4</cell><cell>69.1</cell><cell>79.2</cell><cell>67.2</cell><cell>82.6</cell><cell>68.3</cell><cell>76.4</cell></row><row><cell>16</cell><cell>Ours (n = 1)</cell><cell>52.0</cell><cell>43.2</cell><cell>63.5</cell><cell>62.0</cell><cell>47.6</cell><cell>76.5</cell><cell>41.7</cell><cell>42.4</cell><cell>54.6</cell><cell>70.7</cell></row><row><cell>17</cell><cell>Ours (n = 3)</cell><cell>56.5</cell><cell>49.9</cell><cell>67.0</cell><cell>66.6</cell><cell>55.4</cell><cell>84.0</cell><cell>51.7</cell><cell>43.4</cell><cell>63.1</cell><cell>70.1</cell></row><row><cell>18</cell><cell>Ours (n = 5)</cell><cell>59.3</cell><cell>54.9</cell><cell>70.5</cell><cell>69.6</cell><cell>59.8</cell><cell>86.3</cell><cell>55.3</cell><cell>50.7</cell><cell>64.7</cell><cell>68.9</cell></row><row><cell>19</cell><cell>Ours (n = 10)</cell><cell>62.0</cell><cell>61.9</cell><cell>72.6</cell><cell>74.1</cell><cell>68.6</cell><cell>86.9</cell><cell>62.4</cell><cell>75.6</cell><cell>66.6</cell><cell>66.1</cell></row><row><cell>20</cell><cell>Ours (all)</cell><cell>73.7</cell><cell>68.1</cell><cell>74.3</cell><cell>78.7</cell><cell>76.5</cell><cell>88.3</cell><cell>71.7</cell><cell>83.3</cell><cell>66.1</cell><cell>65.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>] framework based on the standard settings. We used Adam [Kingma and Ba 2014] stochastic optimization for training with a mini-batch of size 64. The initial learning rate is 0.001. The numbers of training samples are listed in Table 1 (Row 5). Training takes about 40 minutes per 1K iteration, and about 13.3 hours per shape category. Testing our CNN network consumes about 0.02 seconds per part hypothesis. The whole task takes about 20 seconds per shape.Table 2shows the timing for various algorithmic components. Runtime computations were performed using a Nvidia GTX 1080 GPU and a 4 core Intel i7-5820K CPU machine.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Timing (in second) of various algorithmic components.</figDesc><table><row><cell cols="3">Technical component Objects Scenes</cell></row><row><cell>Hypothesis generation</cell><cell>5.2</cell><cell>5.6</cell></row><row><cell>CNN testing</cell><cell>10.5</cell><cell>4.5</cell></row><row><cell>Higher-order CRF</cell><cell>7.4</cell><cell>3.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 0 .</head><label>0</label><figDesc>63 0.61 0.81 0.86 0.79 0.80 Bed 0.60 0.62 0.78 0.81 0.75 0.72 Airplane 0.60 0.68 0.80 0.85 0.85 0.91 Velocipedes 0.47 0.44 0.43 0.49 0.48 0.55</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">ACM Transactions on Graphics, Vol. 37, No. 6, Article 1. Publication date: November 2018.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank the anonymous reviewers for their valuable comments. The authors are grateful to Hao Su for fruitful discussion, and Zheyuan Cai and Yahao Shi for the help on data preparation. This work was supported in part by NSFC (61572507, 61532003, 61622212,  61502023 and U1736217). Kai Xu is also supported by a visiting research scholarship offered by China Scholarship Council (CSC) and Princeton University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">GAMMA mesh database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>3d Warehouse</surname></persName>
		</author>
		<ptr target="https://www.rocq.inria.fr/gamma/gamma/download/download.php" />
		<imprint>
			<date type="published" when="2017-05-18" />
			<biblScope unit="page" from="2017" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deformation-driven topology-varying 3D shape correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricio</forename><surname>Simari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">236</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar Kin-Chung</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiew-Lan</forename><surname>Tai</surname></persName>
		</author>
		<title level="m">Mesh Segmentation with Concavity-Aware Fields. IEEE Transactions on Visualization and Computer Graphics (TVCG)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CPMC: Automatic Object Segmentation Using Constrained Parametric Min-Cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1312" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On Visual Similarity Based 3D Model Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Pei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Te</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum (Proc. Eurographics)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="223" to="232" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08083</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Fast R-CNN. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D Mesh Labeling via Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Co-Segmentation of 3D Shapes via Subspace Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum (Proc. SGP)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1703" to="1713" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint Shape Segmentation with Linear Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shape Decomposition using Modal Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum (Proc. Eurographics)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="407" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Caffe:Convolutional Architecture for Fast Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D shape segmentation with projective convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melinos</forename><surname>Averkiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning 3D Mesh Segmentation and Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical Mesh Decomposition using Fuzzy Clustering and Cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagi</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayellet</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="954" to="961" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rotation Invariant Spherical Harmonic Representation of 3D Shape Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SGP</title>
		<meeting>SGP</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="156" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Graph Cuts for Minimizing Robust Higher Order Potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><forename type="middle">H S</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Oxford Brookes University</publisher>
			<pubPlace>Uk</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
	<note type="report_type">Publication date</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GRASS: Generative Recursive Autoencoders for Shape Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proc. of SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">52</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Creating Consistent Scene Graphs Using a Probabilistic Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Xing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised Mesh Segmentation and Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum (Proc. Pacific Graphics)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2241" to="2248" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prime Object Proposals with Randomized Prim&apos;s Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2536" to="2543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On Learning Higher-Order Consistency Potentials for Multi-class Pixel Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungup</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="202" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<title level="m">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contextual Part Analogies in 3D Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shy</forename><surname>Shalom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="309" to="326" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised Co-Segmentation of a Set of Shapes via Descriptor-Space Spectral Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Sidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanir</forename><surname>Oliver Van Kaick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="126" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised Discovery of Mid-level Discriminative Patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>arXiv:cs.CV/1205.3137</idno>
		<ptr target="http://arxiv.org/abs/1205.3137" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Ruizhongtai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ComplementMe: Weakly-Supervised Component Suggestions for 3D Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyuk</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proc. of SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Segmentation as Selective Search for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">W</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1879" to="1886" />
		</imprint>
	</monogr>
	<note>Smeulders</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Co-Hierarchical Analysis of Shape Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Oliver Van Kaick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="69" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Active Co-Analysis of a Set of Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmulik</forename><surname>Asafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Oliver Van Kaick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="165" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Projective Analysis for 3D Shape Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A Deep Representation for Volumetric Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3D Shape Segmentation and Labeling via Extreme Learning Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhige</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Liu Abd Yueshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum (Proc. SGP)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="85" to="95" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Data-driven shape analysis and processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalogerakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH ASIA 2016 Courses</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Style-content separation by anisotropic part scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueshan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Quan</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">184</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning Hierarchical Shape Segmentation and Labeling from Online Repositories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Variational Mesh Decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In Proceedings of the IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deformation-driven shape correspondence via shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">I</forename><surname>Renjiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lira</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhashim</forename><surname>Ibraheem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">U</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">51</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
