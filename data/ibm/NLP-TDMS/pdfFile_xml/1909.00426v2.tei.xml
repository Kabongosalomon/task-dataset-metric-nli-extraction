<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global Entity Disambiguation with Pretrained Contextualized Embeddings of Words and Entities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Studio Ousia</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">RIKEN AIP</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koki</forename><surname>Washio</surname></persName>
							<email>kwashio@g.ecc.u-tokyo.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">RIKEN AIP</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
							<email>shindo@is.naist.jp</email>
							<affiliation key="aff2">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<settlement>Nara</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">RIKEN AIP</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
							<email>yuji.matsumoto@riken.jp</email>
							<affiliation key="aff3">
								<orgName type="institution">RIKEN AIP</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Global Entity Disambiguation with Pretrained Contextualized Embeddings of Words and Entities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new global entity disambiguation (ED) model based on contextualized embeddings of words and entities. Our model is based on a bidirectional transformer encoder (i.e., BERT) and produces contextualized embeddings for words and entities in the input text. The model is trained using a new masked entity prediction task that aims to train the model by predicting randomly masked entities in entity-annotated texts obtained from Wikipedia. We further extend the model by solving ED as a sequential decision task to capture global contextual information. We evaluate our model using six standard ED datasets and achieve new state-of-the-art results on all but one dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entity disambiguation (ED) refers to the task of assigning entity mentions in a text to corresponding entries in a knowledge base (KB). This task is challenging because of the ambiguity between entity names (e.g., "World Cup") and the entities they refer to (e.g., FIFA World Cup or Rugby World Cup). Recent ED models typically rely on two types of contextual information: local information based on words that co-occur with the mention, and global information based on document-level coherence of the disambiguation decisions. A key to improve the performance of ED is to combine both local and global information as observed in most recent ED models.</p><p>In this study, we propose a novel ED model based on contextualized embeddings of words and entities. The proposed model is based on BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>. Our model takes words and entities in the input document, and produces a contextualized embedding for each word and entity. Inspired by the masked language model (MLM) adopted in BERT, we propose masked entity prediction (MEP), a novel task that aims to train the model by predicting randomly masked entities based on words and non-masked entities. We train the model using texts and their entity annotations retrieved from Wikipedia.</p><p>Furthermore, we introduce a simple extension to the inference step of the model to capture global contextual information. Specifically, similar to the approach used in past work <ref type="bibr" target="#b4">(Fang et al., 2019;</ref><ref type="bibr" target="#b20">Yang et al., 2019)</ref>, we address ED as a sequential decision task that disambiguates mentions one by one, and uses words and already disambiguated entities to disambiguate new mentions.</p><p>We evaluate the proposed model using six standard ED datasets and achieve new state-of-the-art results on all but one dataset. Furthermore, we will publicize our code and trained embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Neural network-based approaches have recently achieved strong results on ED <ref type="table">(Ganea and Hof</ref> <ref type="bibr">mann, 2017;</ref><ref type="bibr" target="#b19">Yamada et al., 2017;</ref><ref type="bibr" target="#b10">Le and Titov, 2018;</ref><ref type="bibr" target="#b1">Cao et al., 2018;</ref><ref type="bibr" target="#b11">Le and Titov, 2019;</ref><ref type="bibr" target="#b20">Yang et al., 2019)</ref>. These approaches are typically based on embeddings of words and entities trained using a large KB (e.g., Wikipedia). Such embeddings enable us to design ED models that capture the contextual information required to address ED. These embeddings are typically based on conventional word embedding models (e.g., skip-gram <ref type="bibr" target="#b13">(Mikolov et al., 2013)</ref>) that assign a fixed embedding to each word and entity <ref type="bibr" target="#b18">(Yamada et al., 2016;</ref><ref type="bibr" target="#b2">Cao et al., 2017;</ref><ref type="bibr" target="#b5">Ganea and Hofmann, 2017)</ref>. <ref type="bibr" target="#b15">Shahbazi et al. (2019)</ref> and <ref type="bibr" target="#b0">Broscheit (2019)</ref> proposed ED models based on contextualized word embeddings, namely, ELMo <ref type="bibr" target="#b14">(Peters et al., 2018)</ref> and BERT, respectively. These models predict the referent entity of a mention using the contextualized embeddings of the constituent or surrounding words of the mention. However, unlike our proposed model, these models address the task based only on local contextual information.  3 Contextualized Embeddings of Words and Entities for ED <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the architecture of our contextualized embeddings of words and entities. Our model adopts a multi-layer bidirectional transformer encoder <ref type="bibr">(Vaswani et al., 2017)</ref>. Given a document, the model first constructs a sequence of tokens consisting of words in the document and entities appearing in the document. Then, the model represents the sequence as a sequence of input embeddings, one for each token, and generates a contextualized output embedding for each token. Both the input and output embeddings have H dimensions. Hereafter, we denote the number of words and that of entities in the vocabulary of our model by V w and V e , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Representation</head><p>Similar to the approach adopted in BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>, the input representation of a given token (word or entity) is constructed by summing the following three embeddings of H dimensions:</p><p>• Token embedding is the embedding of the corresponding token. The matrices of the word and entity token embeddings are represented as A ∈ R Vw×H and B ∈ R Ve×H , respectively.</p><p>• Token type embedding represents the type of token, namely, word type (denoted by C word ) or entity type (denoted by C entity ).</p><p>• Position embedding represents the position of the token in a word sequence. A word and an entity appearing at the i-th position in the sequence are represented as D i and E i , respectively. If an entity name contains multiple words, its position embedding is computed by averaging the embeddings of the corresponding positions.</p><p>Following BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>, we insert special tokens [CLS] and [SEP] to the word sequence as the first and last words, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Masked Entity Prediction</head><p>To train the model, we propose masked entity prediction (MEP), a novel task based on MLM. In particular, some percentage of the input entities are masked at random; then, the model learns to predict masked entities based on words and nonmasked entities. We represent masked entities using the special [MASK] entity token.</p><p>We adopt a model equivalent to the one used to predict words in MLM. Specifically, we predict the original entity corresponding to a masked entity by applying the softmax function over all entities in our vocabulary:</p><formula xml:id="formula_0">y M EP = softmax(Bm + b o ),<label>(1)</label></formula><p>where b o ∈ R Ve is the output bias, and m ∈ R H is derived as</p><formula xml:id="formula_1">m = layer norm gelu(W f h + b f ) , (2) where h ∈ R H is the output embedding corre- sponding to the masked entity, W f ∈ R H×H is the weight matrix, b f ∈ R H is the bias, gelu(·)</formula><p>is the gelu activation function <ref type="bibr" target="#b7">(Hendrycks and Gimpel, 2016)</ref>, and layer norm(·) is the layer normalization function (Lei <ref type="bibr">Ba et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>We used the same transformer architecture adopted in the BERT LARGE model <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>. We initialized the parameters of our model that were common with BERT (i.e., parameters in the transformer encoder and the embeddings for words) using the uncased version of the pretrained BERT LARGE model. 1 Other parameters, namely, the parameters in the MEP and the embeddings for entities, were initialized randomly. The model was trained via iterations over Wikipedia pages in a random order for seven epochs. We treated the hyperlinks as entity annotations, and masked 30% of all entities at random. The input text was tokenized to words using the BERT's tokenizer with its vocabulary consisting of V w = 30, 000 words. Similar to Ganea and Hofmann (2017), we built an entity vocabulary consisting of V e = 128, 040 entities, which were contained in the entity candidates in the datasets used in our experiments. We optimized the model by maximizing the log likelihood of MEP's predictions using Adam <ref type="bibr" target="#b9">(Kingma and Ba, 2014)</ref>. Further details are provided in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our ED Model</head><p>We describe our ED model in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Local ED Model</head><p>Given an input document with N mentions and their K entity candidates, our local ED model first creates an input sequence consisting of words in the document, and N masked entity tokens corresponding to the mentions in the document. Then, the model computes the embedding m ∈ R H for each mention using Eq. (2), and predicts the entity for each mention using the softmax function over its K entity candidates:</p><formula xml:id="formula_2">y ED = softmax(B * m + b * o ),<label>(3)</label></formula><p>where B * ∈ R K×H and b * o ∈ R K consist of the entity token embeddings and the output bias values corresponding to the entity candidates, respectively. Note that B * and b * o are the subsets of B and b o , respectively. This model is denoted as local in the remainder of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Global ED Model</head><p>Our global model addresses ED by resolving mentions sequentially for N steps. The model is described in Algorithm 1. First, the model initializes the entity of each mention using the [MASK] token. Then, for each step, the model predicts an entity for each mention, selects a mention with the highest probability produced by the softmax function in Eq.(3) in all unresolved mentions, and resolves the selected mention by assigning the predicted entity to the mention. This model is denoted as confidence-order in the remainder of the paper. Furthermore, we test a baseline model that selects a mention by its order of appearance in the document and denote it by natural-order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Train Accuracy <ref type="bibr" target="#b18">Yamada et al. (2016)</ref> 91.5 Ganea and Hofmann <ref type="formula" target="#formula_0">(2017)</ref> 92.22±0.14 <ref type="bibr" target="#b21">Yang et al. (2018)</ref> 93.0 <ref type="bibr" target="#b10">Le and Titov (2018)</ref> 93.07±0.27 <ref type="bibr" target="#b1">Cao et al. (2018)</ref> 80 <ref type="bibr" target="#b4">Fang et al. (2019)</ref> 94.3 <ref type="bibr" target="#b15">Shahbazi et al. (2019)</ref> 93.46±0.14 <ref type="bibr" target="#b11">Le and Titov (2019)</ref> 89.66±0.16 <ref type="bibr" target="#b0">Broscheit (2019)</ref> 87.9 Yang et al. <ref type="formula" target="#formula_0">(2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We test the proposed ED models using six standard ED datasets: AIDA-CoNLL 2 (CoNLL) (Hoffart et al., 2011), MSNBC (MSB), AQUAINT (AQ), ACE2004 (ACE), WNED-CWEB (CW), and WNED-WIKI (WW) <ref type="bibr" target="#b6">(Guo and Barbosa, 2018)</ref>. We consider only the mentions that refer to valid entities in Wikipedia. For all datasets, we use the KB+YAGO entity candidates and their associatedp(e|m) (Ganea and Hofmann, 2017), and use the top 30 candidates based onp(e|m). We split a document if it is longer than 512 words, which is the maximum word length of the BERT model. We report the in-KB accuracy for the CoNLL dataset, and the micro F1 score (averaged per mention) for the other datasets. Furthermore, we optionally fine-tune the model by maximizing the log likelihood of the ED predictions (ŷ ED ) using the training set of the CoNLL dataset. We mask 90% of the mentions and fix the entity token embeddings (B and B * ) and the output bias (b o and b * o ). The model is trained for two epochs using Adam. Additional details are provided in Appendix B. <ref type="table" target="#tab_1">Table 1</ref> presents the results of the CoNLL dataset. Our global models successfully outperformed all the recent strong models, including models based on ELMo <ref type="bibr" target="#b15">(Shahbazi et al., 2019)</ref> and BERT <ref type="bibr" target="#b0">(Broscheit, 2019)</ref>. Furthermore, our confidence-Name Train MSB AQ ACE CW WW Avg. <ref type="bibr" target="#b5">Ganea and Hofmann (2017)</ref> 93.7 88.5 88.5 77.9 77.5 85.2 <ref type="bibr" target="#b21">Yang et al. (2018)</ref> 92.6 89.9 88.5 81.8 79.2 86.4 <ref type="bibr" target="#b10">Le and Titov (2018)</ref> 93.9 88.3 89.9 77.5 78.0 85.5 <ref type="bibr" target="#b1">Cao et al. (2018)</ref> - <ref type="formula">87</ref>    <ref type="bibr" target="#b18">Yamada et al. (2016)</ref> and <ref type="bibr" target="#b5">Ganea and Hofmann (2017)</ref>. <ref type="table" target="#tab_4">Table 2</ref> presents the results of the datasets other than the CoNLL dataset. Our models trained only on our Wikipedia-based annotations outperformed recent strong models on the MSB, AQ, ACE, and WW datasets. Additionally, we also tested the performance of our models fine-tuned on the CoNLL dataset, and found that fine-tuning generally degraded the performance on these five datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results and Analysis</head><p>Furthermore, our local model performed equally or worse in comparison with our global models on all datasets. This clearly demonstrates the effectiveness of using global contextual information even if the local contextual information is modeled based on expressive contextualized embeddings. Moreover, the natural-order model performed worse than the confidence-order model on most datasets.</p><p>Additionally, our models performed relatively worse on the CW dataset. We consider that our model failed to capture important contextual information because this dataset is significantly longer on average than other datasets, i.e., approximately 1,700 words per document on average, which is more than thrice longer than the maximum word length of our model (i.e., 512 words). We also consider that <ref type="bibr" target="#b21">Yang et al. (2018)</ref> achieved excellent performance on this specific dataset because their model is based on various hand-engineered features capturing document-level contextual information.  To investigate how the global contextual information helped our model to improve performance, we manually analyzed the difference between the predictions of the local, natural-order, and confidence-order models. The CoNLL dataset was used to fine-tune and test the models.</p><p>The local model often failed to resolve mentions of common names referring to specific entities (e.g., "New York" referring to the basketball team New York Knicks). Global models were generally better to resolve such mentions because of the presence of strong global contextual information (e.g., mentions referring to basketball teams).</p><p>Furthermore, we found that the CoNLL dataset contains mentions that require a highly detailed context to resolve. For example, a mention "Matthew Burke" can refer to two different former Australian rugby players. Although the local and natural-order models incorrectly resolved this mention to the player who has the larger number of occurrences in our Wikipedia-based annotations, the confidence-order model successfully resolved this mention by disambiguating its contextual mentions, including his colleague players, in advance. We provide detailed inference of the corresponding document in Appendix C.</p><p>Next, we examined if our model learned effective embeddings for rare entities using the CoNLL dataset. Following Ganea and Hofmann (2017), we used the mentions of which entity candidates contain their gold entities, and measured the performance by dividing the mentions based on the frequency of their entities in the Wikipedia annotations used to train the embeddings. As presented in <ref type="table" target="#tab_6">Table 3</ref>, our models achieved enhanced performance to predict rare entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed a global ED model based on contextualized embeddings trained using Wikipedia. Our experimental results demonstrate the effectiveness of our model across a wide range of ED datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details of Training of Contextualized Embeddings</head><p>As the input corpus for training our contextualized embeddings, we used the December 2018 version of Wikipedia, comprising approximately 3.5 billion words and 11 million entity annotations. We generated input sequences by splitting the content of each page into sequences comprising ≤ 512 words and their entity annotations (i.e., hyperlinks).</p><p>To stabilize the training, we updated only those parameters that were randomly initialized (i.e., fixed the parameters initialized using BERT) at the first epoch, and updated all the parameters in the remaining six epochs. We implemented the model using PyTorch, and the training took approximately ten days using eight Tesla V100 GPUs.</p><p>The hyper-parameters used in the training are detailed in <ref type="table" target="#tab_8">Table 4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Fine-tuning on CoNLL Dataset</head><p>The hyper-parameters used in the fine-tuning on the CoNLL dataset are detailed in <ref type="table" target="#tab_10">Table 5</ref>. We selected these hyper-parameters from the search space described in <ref type="bibr" target="#b3">Devlin et al. (2019)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Example of Inference by</head><p>Confidence-order Model <ref type="figure">Figure 2</ref> shows an example of the inference performed by our confidence-order model fine-tuned on the CoNLL dataset. The document was obtained from the test set of the CoNLL dataset. As shown in the figure, the model started with unambiguous player names to recognize the topic of the document, and subsequently resolved the mentions that were challenging to resolve. Notably, the model correctly resolved the mention "Nigel Walker" to the corresponding former rugby player instead of a football player, and the mention "Matthew Burke" to the correct former Australian rugby player born in 1973 instead of the former Australian rugby player born in 1964, by resolving other contextual mentions, including their colleague players in advance. These two mentions are denoted in red in the figure. Note that our local model failed to resolve both mentions, and our natural-order model failed to resolve "Mattew Burke."</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of the proposed contextualized embeddings of words and entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Algorithm of our global ED model. Input: Words and mentions m1, . . . mN in the input document Initialize: ei ← [MASK], i = 1 . . . N repeat N times For all mentions, obtain entity predictionŝ e1 . . .êN using Eq.(2) and Eq.(3) using words and entities e1, ..., eN as inputs Select a mention mj that has the most confident prediction in all unresolved mentions ej ←êj end return {e1, . . . , eN }</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Micro F1 scores on the five ED datasets.</figDesc><table><row><cell>Train: whether the model is trained using the training</cell></row><row><cell>set of the CoNLL dataset.</cell></row><row><cell>order model trained only on our Wikipedia-</cell></row><row><cell>based annotations outperformed two recent mod-</cell></row><row><cell>els trained on the in-domain training set of the</cell></row><row><cell>CoNLL dataset, namely,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>In-KB accuracy on the CoNLL dataset split by the frequency in Wikipedia entity annotations. Our models were fine-tuned using the CoNLL dataset. G&amp;H2017: The results of<ref type="bibr" target="#b5">Ganea and Hofmann (2017)</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Hyper-parameters used for training our contextualized embeddings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>based on the accuracy on the development set of the CoNLL dataset.</figDesc><table><row><cell>Name</cell><cell>Value</cell></row><row><cell>maximum word length</cell><cell>512</cell></row><row><cell>number of epochs</cell><cell>2</cell></row><row><cell>batch size</cell><cell>16</cell></row><row><cell>learning rate</cell><cell>2e-5</cell></row><row><cell>learning rate decay</cell><cell>linear</cell></row><row><cell>warmup proportion</cell><cell>0.1</cell></row><row><cell>dropout</cell><cell>0.1</cell></row><row><cell>weight decay</cell><cell>0.01</cell></row><row><cell>gradient clipping</cell><cell>1.0</cell></row><row><cell>adam β 1</cell><cell>0.9</cell></row><row><cell>adam β 2</cell><cell>0.999</cell></row><row><cell>adam</cell><cell>1e-6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Hyper-parameters during fine-tuning on the CoNLL dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We initialized C word using BERT's segment embedding for sentence A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We used the test b set of the CoNLL dataset.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="677" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Collective Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="675" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1623" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint Entity Linking with Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference, WWW &apos;19</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="438" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Joint Entity Disambiguation with Local Neural Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Octavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2619" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Robust Named Entity Disambiguation with Random Walks. Semantic Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="459" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415v3</idno>
		<title level="m">Gaussian Error Linear Units (GELUs)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust Disambiguation of Named Entities in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980v9</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving Entity Linking by Modeling Latent Relations between Mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1595" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Boosting Entity Linking Performance by Leveraging Unlabeled Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1935" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450v1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer Normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Conference on Learning Representations</title>
		<meeting>the 2013 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Shahbazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasha</forename><surname>Obeidat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05762v2</idno>
		<title level="m">Entity-aware ELMo: Learning Contextual Entity Representation for Entity Disambiguation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<imprint>
			<pubPlace>Llion Jones, Aidan N Gomez, ukasz</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Distributed Representations of Texts and Entities from Knowledge Base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="397" to="411" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Dynamic Context Augmentation for Global Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Australia have won all four tests against 46: Italy, 47: Scotland, 48: Ireland and 45: Wales, and scored 414 points at an average of almost 35 points a game. League duties restricted the 28: Barbarians&apos; selectorial options but they still boast 13 internationals including 44: England full-back 16: Tim Stimpson and recalled wing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
			<affiliation>
				<orgName type="collaboration">captain</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
			<affiliation>
				<orgName type="collaboration">captain</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi Shefaet</forename><surname>Rahman</surname></persName>
			<affiliation>
				<orgName type="collaboration">captain</orgName>
			</affiliation>
		</author>
		<idno>Australia -15 -53</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Wales; Scotland; England; Pontypridd; New Zealand; Leicester; New Zealand; Matthew Burke; David Campese; Pat Howard, 9 -Sam Payne</addrLine></address></meeting>
		<imprint>
			<publisher>Owen Finegan</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5" to="21" />
		</imprint>
	</monogr>
	<note>: Nick Popplewell (49: Ireland). David Giffin, 4 -Tim Gavin, 3 -Andrew Blades, 2 -Marco Caputo, 1 -6: Dan Crowley. Order of Inference by Confidence-order Model</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">2: An illustrative example showing the inference performed by our fine-tuned confidence-order model on a document in the CoNLL dataset. Mentions are shown as underlined</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>England</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Wales ! England</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>England</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Wales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Wales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Wales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>England</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Scotland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Wales ! Pontypridd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Leicester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Australia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>England</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Wales ! Italy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Scotland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">!</forename><surname>Ireland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">! Nigel</forename><surname>Ireland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walker</surname></persName>
		</author>
		<editor>Allan Bateman ! Rob Howley ! Nick Popplewell ! Tony Underwood ! Darren Garforth ! Dan Crowley ! Tim Stimpson ! Neil Back ! Joe Roff ! Gregor Townsend ! Craig Quinnell ! All Black ! Owen Finegan ! Norm Hewitt ! Scott Quinnell ! Tim Stimpson ! Australia ! Norm Hewitt ! Dale McIntosh ! Tim Horan ! David Giffin ! Tony Underwood ! David Campese ! Ian Jones ! Ian Jones ! Daniel Herbert ! Barbarians ! Barbarians ! Pat Howard ! David Wilson !</editor>
		<imprint>
			<pubPlace>New Zealand ! New Zealand ! Matthew Burke Figure</pubPlace>
		</imprint>
	</monogr>
	<note>Numbers in bold face represent the selection order of the confidence-order model</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
