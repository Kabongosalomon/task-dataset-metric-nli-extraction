<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DER: Dynamically Expandable Representation for Class Incremental Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangwei</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DER: Dynamically Expandable Representation for Class Incremental Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of class incremental learning, which is a core step towards achieving adaptive vision intelligence. In particular, we consider the task setting of incremental learning with limited memory and aim to achieve better stability-plasticity trade-off. To this end, we propose a novel two-stage learning approach that utilizes a dynamically expandable representation for more effective incremental concept modeling. Specifically, at each incremental step, we freeze the previously learned representation and augment it with additional feature dimensions from a new learnable feature extractor. This enables us to integrate new visual concepts with retaining learned knowledge. We dynamically expand the representation according to the complexity of novel concepts by introducing a channel-level mask-based pruning strategy. Moreover, we introduce an auxiliary loss to encourage the model to learn diverse and discriminate features for novel concepts. We conduct extensive experiments on the three class incremental learning benchmarks and our method consistently outperforms other methods with a large margin. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human can easily accumulate visual knowledge from past experiences and incrementally learn novel concepts. Inspired by this, the problem of class incremental learning aims to design algorithms that can learn novel concepts in a sequential manner and eventually perform well on all observed classes. Such capability is indispensable for many real-world applications such as the intelligent robot <ref type="bibr" target="#b30">[31]</ref>, face recognition <ref type="bibr" target="#b18">[19]</ref> and autonomous driving <ref type="bibr" target="#b24">[25]</ref>. However, achieving human-level incremental learning remains * Both authors contributed equally. This work was supported by Shanghai NSF Grant (No. 18ZR1425100) <ref type="bibr" target="#b0">1</ref> Code is available at https://github.com/Rhyssiyan/DER-ClassIL.pytorch.  <ref type="figure">Figure 1</ref>: The average incremental accuracy for different model size. We compare our model with prior methods (WA <ref type="bibr" target="#b38">[39]</ref>, BiC <ref type="bibr" target="#b11">[12]</ref>, RPSNet <ref type="bibr" target="#b25">[26]</ref>, iCaRL <ref type="bibr" target="#b26">[27]</ref>, UCIR <ref type="bibr" target="#b11">[12]</ref>, PODNet <ref type="bibr" target="#b5">[6]</ref>) and the model trained on all the data (Joint) on the experiment CIFAR100-B0 of 10 steps. challenging for modern visual recognition systems.</p><p>There has been much effort attempting to address the incremental learning in literature <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref>. Among them, perhaps the most effective strategy is to keep a memory buffer that stores part of observed data for the rehearsal <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> in future. However, due to the limited size of data memory, such the incremental learning method still faces several typical challenges in the general continual learning task. In particular, it requires a model to effectively incorporate novel concepts without forgetting the existing knowledge, which is also known as stability-plasticity dilemma <ref type="bibr" target="#b8">[9]</ref>. In detail, excessive plasticity often causes large performance degradation of the old categories, referred to as catastrophic forgetting <ref type="bibr" target="#b7">[8]</ref>. On the contrary, excessive stability impedes the adaptation of novel concepts.</p><p>Most existing works attempt to achieve a trade-off between stability and plasticity by gradually updating data representation and class decision boundary for increasingly larger label spaces. For instance, regularization methods <ref type="bibr" target="#b3">[4]</ref> penalize the change of important weights of previously learned models, while knowledge distillation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34]</ref> preserves the network output with available data, and structure-based methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b0">1]</ref> keep old parameters fixed when allocating more for new categories. Nevertheless, all those methods either sacrifice the model plasticity for the stability, or are susceptible to forgetting due to feature degradation of old concepts. As shown in <ref type="figure">Figure 1</ref>, a large performance gap still exists between the model (Joint) trained on all data and previous state-of-the-art models.</p><p>In this work, we aim to address the above weaknesses and achieve a better stability-plasticity trade-off in the class incremental learning. To this end, we adopt a two-stage learning strategy, decoupling the adaptation of feature representation and final classifier head (or classifier for short) of a deep network <ref type="bibr" target="#b14">[15]</ref>. Within this framework, we propose a novel data representation, referred to as super-feature, capable of increasing its dimensionality to accommodate new classes. Our main idea is to freeze the previously learned representation and augment it with additional feature dimensions from a new learnable extractor in each incremental step. This enables us to retain the existing knowledge and provides enough flexibility to learn novel concepts. Moreover, our super-feature is expanded dynamically based on the complexity of novel concepts to maintain a compact representation.</p><p>To achieve this, we develop a modular deep classification network composed of a super-feature extractor network and a linear classifier. Our super-feature extractor network consists of multiple feature extractors with varying sizes, one for each incremental step. Specifically, at a new step, we expand the super-feature extractor network with a new feature extractor while keeping the parameters of previous extractors frozen. The features generated by all the extractors are concatenated together and fed into the classifier for the class prediction.</p><p>We train the new feature extractor and the classifier on the memory and the newly incoming data. To encourage the new extractor to learn diverse and discriminative features for new classes, we design an auxiliary loss on distinguishing new and old classes. Additionally, to remove the model redundancy and learn the compact features for novel classes, we apply a differentiable channel-level mask-based pruning method that dynamically prunes the network according to the difficulty of novel concepts. Finally, given the updated representation, we freeze the super-feature extractor and finetune the classifier on a balanced training subset to solve the class imbalance problem <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>We validate our approach on three commonly used benchmarks, including CIFAR-100, ImageNet-100, and ImageNet-1000 datasets. The empirical results and the ablation study demonstrate the superiority of our method over prior state-of-the-art approaches. Interestingly, we also find that our method could achieve positive backward and forward transfer between steps. The main contributions of our work are three-fold:</p><p>• To achieve better stability-plasticity trade-off, we develop a dynamically expandable representation and a two-stage strategy for the class incremental learning. • We propose an auxiliary loss to promote the newly added feature module to learn novel classes effectively and a model pruning step to learn compact features. • Our approach achieves the new state of the art performance on all three benchmarks under a wide range of model complexity, as shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Class incremental learning aims to learn new classes continuously. Some works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b22">23]</ref> try to solve the problem with no access to previously seen data. However, prevalent approaches are based on the rehearsal strategy with limited data memory, which can be mainly analyzed from representation learning and classifier learning. Representation Learning Current works can be mainly divided into the following three categories. Regularizationbased methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2]</ref> adopt Maximum a Posterior estimation to expect small changes in the important parameters and update the posterior of model parameters sequentially. However, its intractable computation typically requires approximations with a strong model assumption. For example, EWC <ref type="bibr" target="#b15">[16]</ref> uses Laplace approximation, which assumes weights falling into a local region of the optimal weights of last step. This severely restricts the model capacity to adapt to novel concepts.</p><p>Distillation-based methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34</ref>] use knowledge distillation <ref type="bibr" target="#b10">[11]</ref> to maintain the representation. iCaRL <ref type="bibr" target="#b26">[27]</ref> and EE2L <ref type="bibr" target="#b2">[3]</ref> compute the distillation loss on the network outputs. UCIR <ref type="bibr" target="#b11">[12]</ref> uses normalized feature vectors to apply the distillation loss instead of the prediction of the network. PODNet <ref type="bibr" target="#b5">[6]</ref> uses a spatial-based distillation loss to restrict the change of model. TPCIL <ref type="bibr" target="#b33">[34]</ref> makes the model preserve the topology of CNN's feature space. The performance of knowledge distillation depends on the quality and quantity of saved data.</p><p>Structure-based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20]</ref> keep the learned parameters related to previous classes fixed and allocate new parameters in different forms such as unused parameters, additional networks to learn novel knowledge. CPG <ref type="bibr" target="#b12">[13]</ref> proposes a compaction and selection/expansion mechanism that prunes the deep model and expands the architecture alternatively with selectively weight sharing. However, most structure-based <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35]</ref> methods are designed for task continual learning, which needs task identity during inference. For class incremental learning, RPSNet <ref type="bibr" target="#b25">[26]</ref> proposes a random path selection algorithm that progressively chooses optimal paths as sub-network for the new classes. CCGN <ref type="bibr" target="#b0">[1]</ref> equips each convolutional layer with task-specific gating modules to select filters to apply on the given input and uses a task predictor to choose the gating modules in inference. Classifier Learning Class-imbalance problem is the main challenge for classifier learning due to limited memory. Some works like LWF.MC <ref type="bibr" target="#b26">[27]</ref>, RWalk <ref type="bibr" target="#b3">[4]</ref> train the extractor and classifier jointly within one-stage training. By contrast, recently, there are many works to solve the class imbalance problem by introducing an independent classifier learning stage after representation learning. EEIL <ref type="bibr" target="#b2">[3]</ref> finetunes the classifier on a balanced training subset. BiC <ref type="bibr" target="#b32">[33]</ref> adds a bias correction layer to correct the model's outputs, where the layer is trained on a separate validation set. WA <ref type="bibr" target="#b38">[39]</ref> corrects the biased weights by aligning the norms of the weight vectors for new classes to those for old classes. Discussion Our work is a structure-based method and the most similar work to ours are RPSNet and CCGN. RPSNet cannot retain the intrinsic structure of each old concept and tends to gradually forget the learned concepts by summing the previously learned features and the newly learned features at each ConvNet stage. In CCGN, the learned representation may slowly degrade over steps as only the parameters of part of layers are frozen. By contrast, we keep the previously learned representation fixed and augment it with novel features parameterized by a new feature extractor. This enables us to preserve the intrinsic structure of old concepts in the subspace of previously learned representation, and re-use the structure via the final classifier to mitigate forgetting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we present our approach to the problem of class incremental learning, aiming to achieve a better tradeoff between stability and plasticity. To this end, we propose a dynamically expandable representation (DER) that incrementally augments previously learned representation with novel features and a two-stage learning strategy.</p><p>Below we first present the formulation of class incremental learning and an overview of our method in Sec. 3.1. Then we introduce the expandable representation learning and its loss function in Sec. 3.2. After this, we describe the dynamic expansion of our representation in Sec. 3.3 and the second stage of classifier learning in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Setup and Method Overview</head><p>Firstly, we introduce the problem setup of class incremental learning. In contrast to task incremental learning, class incremental learning does not require task id during inference. Specifically, during the class incremental learning, the model observes a stream of class groups {Y t } and their corresponding training data {D t }. Particularly, the incoming dataset D t at step t has a form of (x t i , y t i ) where x t i is the input image and y t i ∈ Y t is the label within the label set Y t . The label space of the model is all seen categories Y t = ∪ t i=1 Y i and the model is expected to predict well on all classes inỸ t .</p><p>Our method adopts the rehearsal strategy, which saves a part of data as the memory M t for future training. For the learning of step t, we decouple the learning process into two sequential stages as follows.</p><p>1) Representation Learning Stage. To achieve better tradeoff between stability and plasticity, we fix the previous feature representation and expand it with a new feature extractor trained on the incoming and memory data. We design an auxiliary loss on the novel extractor to promote it to learn diverse and discriminative features. To improve the model efficiency, we dynamically expand the representation according to the complexity of new classes via introducing a channel-level mask-based pruning method. The overview of our proposed representation is shown in <ref type="figure">Figure 2</ref>.</p><p>2) Classifier Learning Stage. After the learning of representation, we retrain the classifier with currently available datã D t = D t ∪ M t at step t to deal with the class imbalance problem via adopting the balanced finetuning method in <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Expandable Representation Learning</head><p>We first introduce our expandable representation. At step t, our model is composed of a super-feature extractor Φ t and the classifier H t . The super-feature extractor Φ t is built by expanding the feature extractor Φ t−1 with a newly created feature extractor F t . Specifically, given an image x ∈D t , the feature u extracted by Φ t is obtained by concatenation as follows</p><formula xml:id="formula_0">u = Φ t (x) = [Φ t−1 (x), F t (x)]<label>(1)</label></formula><p>Here we reuse the previous F 1 , . . . , F t−1 and encourage the new extractor F t to learn only novel aspect of new classes. The feature u is then fed into the classifier H t to make prediction as follows</p><formula xml:id="formula_1">p Ht (y|x) = Softmax(H t (u))<label>(2)</label></formula><p>Then the predictionŷ = arg max p Ht (y|x),ŷ ∈Ỹ t . The classifier is designed to match its new input and output dimensions for step t. The parameters of H t for the old features are inherited from H t−1 to retain old knowledge and its newly added parameters are randomly initialized.</p><p>To reduce catastrophic forgetting, we freeze the learned function Φ t−1 at step t, as it captures the intrinsic structure of previous data. In detail, the parameters of last step super-feature extractor θ Φt−1 and the statistics of Batch Normalization <ref type="bibr" target="#b13">[14]</ref> are not updated. Besides, we instantiate  <ref type="figure">Figure 2</ref>: Dynamically Expandable Representation Learning. At step t, the model is composed of super-feature extractor Φ t and classifier H t , where Φ t is built by expanding the existing super-feature extractor Φ P t−1 with new feature extractor F t . We also use an auxiliary classifier to regularize the model. Besides, the layer-wise channel-level mask is jointed learned with the representation, which is used to prune the network after the learning of model. F t with F t−1 as initialization to reuse previous knowledge for fast adaptation and forward transfer.</p><p>We can shed the light on the problem from the perspective of estimating the prior distribution p(θ Φt |D 1:t−1 ) given the previous data D 1:t−1 . Unlike previous regularization methods like EWC, we do not assume the prior distribution for t-th step is unimodal, which restricts the model flexibility and is typically not the case in practice. For our method, the model expands with new parameters by creating a separate feature extractor F t for incoming data and take a uniform distribution as the prior distribution p(θ Ft |D 1:t−1 ) which provides enough flexibility for the model to adapt to novel concepts. Meanwhile, for simplicity, we approximate the prior distribution p(θ Φt−1 |D 1:t−1 ) on the old parameters θ Φt−1 as the Dirac distribution, which maintains the information learned on D 1:t−1 . By integrating two prior distribution assumptions on p(θ Φt−1 |D 1:t−1 ) and p(θ Ft |D 1:t−1 ), we have more flexibility in achieving a better stabilityplasticity trade-off. Training Loss We learn the model with cross-entropy loss on memory and incoming data as follows</p><formula xml:id="formula_2">L Ht = − 1 |D t | |Dt| i=1 log(p Ht (y = y i |x i )))<label>(3)</label></formula><p>where x i is image and y i is the corresponding label.</p><p>To enforce the network to learn the diverse and discriminative features for novel concepts, we further develop an auxiliary loss operating on the novel feature F t (x). Specifically, we introduce an auxiliary classifier H a t , which predicts the probability p H a t (y|x) = Softmax(H a t (F t (x)). To encourage the network to learn features to discriminate between old and new concepts, the label space of H a t is |Y t |+1 including the new category set Y t and the other class by treating all old concepts as one category. Thusly, we intro-duce the auxiliary loss and obtain the expandable representation loss as follows</p><formula xml:id="formula_3">L ER = L Ht + λ a L H a t (4)</formula><p>where λ a is the hyper-parameter to control the effect of the auxiliary classifier. It is worth noting that λ a =0 for first step t = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamical Expansion</head><p>To remove the model redundancy and maintain a compact representation, we dynamically expand the superfeature according to the complexity of novel concepts. Specifically, we adopt a differentiable channel-level maskbased method to prune filters of the extractor F t , in which the masks are learned with the representation jointly. After the learning of the mask, we binarize the mask and prune the feature extractor F t to obtain the pruned network F P t . Channel-level Masks Our pruning method is based on differentiable channel-level masks, which is adapted from HAT <ref type="bibr" target="#b29">[30]</ref>. For the novel feature extractor F t , the input feature map of convolutional layer l for a given image x is denoted as f l . We introduce the channel mask m l ∈ R c l to control the size of layer l where m i l ∈ [0, 1] and c l is the number of channels of layer l. f l is modulated with the mask as follows</p><formula xml:id="formula_4">f l = f l m l (5)</formula><p>where f l is the masked feature map, means channel-level multiplication. To make the value of m l fall into the interval [0, 1], the gating function is adopted as follows</p><formula xml:id="formula_5">m l = σ(se l )<label>(6)</label></formula><p>where e l means learnable mask parameters, the gating function σ(·) uses the sigmoid function in this work and s is the scaling factor to control the sharpness of the function. With such a mask mechanism, the super-featureũ of step t can be rewritten as</p><formula xml:id="formula_6">u = Φ P t (x) = [F P 1 (x), F P 2 (x), ..., φ t (x)]<label>(7)</label></formula><p>During training, φ t (x) is F t (x) with the soft masks. For inference, we assign s a large value to binarize masks and obtain the pruned network F P t , and φ t (x) = F P t (x) Mask Learning During a epoch, a linear annealing schedule is applied for s as follows</p><formula xml:id="formula_7">s = 1 s max + (s max − 1 s max ) b − 1 B − 1<label>(8)</label></formula><p>where b is the batch index, s max 1 is the hyper-parameter to control the schedule, B is the number of batches in one epoch. The training epoch starts with all channels activated in a uniform way. Then the mask is progressively binarized with the increasing of batch index within a epoch.</p><p>One of the problems of the sigmoid function is that the gradient is unstable due to the s schedule. We compensate the gradient g el with respect to e l to remove the influence of s as follows</p><formula xml:id="formula_8">g el = σ(e l )[1 − σ(e l )] sσ(se l )[1 − σ(se l )] g el<label>(9)</label></formula><p>where g el is the compensated gradient.</p><p>Sparsity Loss At every step, we encourage the model to maximally reduce the number of parameters with a minimal performance drop. Motivated by this, we add a sparsity loss based on the ratio of used weights in all available weights.</p><formula xml:id="formula_9">L S = L l=1 K l m l−1 1 m l 1 L l=1 K l c l−1 c l<label>(10)</label></formula><p>where L is the number of layers, K l is the kernel size of convolution layer l, layer l=0 refers to the input image, and m 0 1 =3. After adding the sparsity loss, the final loss function is</p><formula xml:id="formula_10">L DER = L Ht + λ a L H a t + λ s L S<label>(11)</label></formula><p>where λ s is the hyper-parameter to control the model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Classifier Learning</head><p>At the representation learning stage, we re-train the classifier head in order to reduce the bias in the classifier weight introduced by the imbalanced training. Specifically, we first re-initialize the classifier with random weights and then sample a class-balanced subset from currently available dataD t . We train the classifier head only using the cross-entropy loss with a temperature δ in the Softmax <ref type="bibr" target="#b37">[38]</ref>. The temperature controls the smoothness of the Softmax function to improve the margins between classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive experiments to validate the effectiveness of our algorithm. Especially, we evaluate our method on CIFAR-100 <ref type="bibr" target="#b26">[27]</ref>, ImageNet-100 <ref type="bibr" target="#b26">[27]</ref> and ImageNet-1000 <ref type="bibr" target="#b26">[27]</ref> datasets with two widely used benchmark protocols. We also perform a series of ablation studies to evaluate the importance of each component and provides more insights into our method. Below we first start with the introduction of experiment setup and implementation details in Sec. 4.1, followed by the experimental results on the CIFAR100 dataset in Sec. 4.2. Then we present the evaluation results on both ImageNet-100 and ImageNet-1000 datasets in Sec. 4.3. Finally, we introduce the ablation study and analysis for our method in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup and Implementation Details</head><p>Datasets CIFAR-100 <ref type="bibr" target="#b16">[17]</ref> consists of 32x32 pixel color images with 100 classes. It contains 50,000 images for training with 500 images per class, and 10,000 images for evaluation with 100 images per class. ImageNet-1000 <ref type="bibr" target="#b4">[5]</ref> is a large-scale dataset from 1,000 classes which includes about 1.2 million RGB images for training and 50,000 images for validation. ImageNet-100 <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12]</ref> is built by selecting 100 classes from the ImageNet-1000 dataset.</p><p>Benchmark Protocols For the CIFAR-100 benchmark, we test our methods on two popular protocols including 1)CIFAR100-B0: we follow the protocol proposed in <ref type="bibr" target="#b26">[27]</ref>, which trains all 100 classes in several splits including 5, 10, 20, 50 incremental steps with fixed memory size of 2,000 exemplars over batches; 2)CIFAR100-B50: we follow the protocol introduced in <ref type="bibr" target="#b11">[12]</ref>, which starts from a model trained on 50 classes, and the remaining 50 classes are divided into splits of 2, 5, and 10 steps with 20 examples as memory per class. We compare the top-1 average incremental accuracy which takes the average of the accuracy for each step.</p><p>We also evaluate our method on ImageNet-100 with two protocols that are 1)ImageNet100-B0: the protocol <ref type="bibr" target="#b26">[27]</ref> trains the model in batches of 10 classes from scratch with fixed memory size 2,000 over batches; 2)ImageNet100-B50: the protocol <ref type="bibr" target="#b11">[12]</ref> starts from a model trained on 50 classes, and the remaining 50 classes come in 10 steps with 20 examples per class as memory. For the sake of fairness, we use the same ImageNet subset and class order following the protocols <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12]</ref>. For ImageNet-1000, we evaluate our method on the protocol <ref type="bibr" target="#b26">[27]</ref>, known as ImageNet1000-B0 benchmark, that trains the model in batches of 100 classes with 10 steps in total and set the fixed memory size as 20,000. Detailedly, we use the same class order as <ref type="bibr" target="#b26">[27]</ref> for ImageNet-1000. Moreover, we compare the top-1 and top-5 average incremental accuracy and the last step accuracy on ImageNet-100 and ImageNet-1000 datasets.   Implementation Details Our method is implemented with PyTorch <ref type="bibr" target="#b23">[24]</ref>. For CIFAR-100, we adopt ResNet-18 as feature extractor F t following RPSNet <ref type="bibr" target="#b25">[26]</ref>. We note that most previous works use a modified 32-layers ResNet <ref type="bibr" target="#b26">[27]</ref>, which has fewer channels and residual blocks compared to standard ResNet-32. We argue that such a small network is not suitable because it cannot achieve competitive results on CIFAR100 compared with standard 18-layers ResNet <ref type="bibr" target="#b9">[10]</ref> and may underestimate the performance of methods. We run these methods with standard ResNet-18 on the same class orders based on their code implementation. For those without releasing the codes, we report the results based on our implementation. For RPSNet, we use the results in their paper directly. For ImageNet-100 and ImageNet-1000 benchmarks, we use 18-layers ResNet as the basic network.</p><p>In these experiemnts, we select exemplars as memory based on the herding selection strategy <ref type="bibr" target="#b31">[32]</ref> following the previous works <ref type="bibr" target="#b26">[27]</ref>. Furthermore, we run experiments on three different class orders and report average±standard deviations in the results. We also provide the experimental results on CIFAR-100 based on modified 32-layers ResNet <ref type="bibr" target="#b26">[27]</ref> in the appendix, which proves the superiority of our method again. We follow the protocol in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref> and tune the hyper-parameters on a validation set created by holding out a part of original training data. The details of the hyperparameters are added to the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on CIFAR100</head><p>Quantitative Results <ref type="table" target="#tab_1">Table 1</ref> summarizes the results of CIFAR100-B0 benchmark. We can see that our method consistently outperforms other methods by a sizable margin at different incremental splits. As the number of steps increases in the split, it is observed that the margin between our method and other methods continuously increases which indicates that our method performs better on the difficult splits with longer steps. Particularly, under the incremental setting of 50 steps, we improve the average incremental accuracy from 64.32% to 72.05%(+7.73%) of ours with fewer parameters. It is worth noting that although huge model parameters are reduced, the performance degradation of our method caused by pruning can be ignored, which demonstrates that the success of our pruning method. As shown in the left panel of <ref type="figure" target="#fig_3">Figure 3</ref>, it is observed that our method consistently surpasses other methods at every step for different splits. Moreover, the gap between our method and other methods increases with the continuous adding of    novel classes. Specifically, under the incremental split of 50 steps, the last step accuracy is boosted from 42.75% to 58.66%(+15.91%), which further proves that the effectiveness of our method. We also compare the performance of our method with previous methods in <ref type="table" target="#tab_2">Table 2</ref> on the CIFAR100-B50 benchmark, which shows our method improves the performance with a significant gain in all splits. In particular, under the incremental setting of 10 steps, our method outperforms PODNet by 8.41% average incremental accuracy. As the right panel of <ref type="figure" target="#fig_3">Figure 3</ref> shows, our method performs better than other methods at each step for all splits. Especially, our method improves from 52.56% to 65.58%(+13.02%) for the last step accuracy in the split of 10 steps. Moreover, our method achieves similar performance with much fewer parameters compared to our method without pruning.</p><p>It is worth noting that previous methods often perform well only on one of the protocols where WA is the stateof-the-art on CIFAR100-B0 and PODNet is state-of-the-art on CIFAR100-B50. By contrast, our method consistently surpasses other methods on both protocols. The effects of model size We conduct extensive experiments to study the effect of model size on performance. As shown in <ref type="figure">Figure 1</ref>, we can see that our method consistently and significantly performs better than other methods at various model sizes.We also note that the improvement of our method compared to most other methods becomes more significant with the increasing of model size, which illustrates that our method can exploit the potential of a large model. <ref type="table" target="#tab_4">Table 3</ref> summarizes the experimental results for the ImageNet-100 and ImageNet-1000 datasets. We can see that our method consistently surpasses other methods with a considerable margin for all splits on ImageNet-100 and ImageNet-1000 datasets, especially the last step accuracy. Specifically, our method outperforms the state-ofthe-art with about 1.79% for the average top-5 accuracy on the ImageNet100-B0 benchmark. For ImageNet100-B50 benchmark, the last step top-1 accuracy is improved from 66.91% to 72.06%(+5.15%). Furthermore, our method improves the final step top-1 accuracy from 55.6% to 58.62%(+3.02%) on ImageNet1000-B0 benchmark. While the top-5 accuracy gap is smaller, we believe it is because top-5 accuracy is more tolerant to slightly inaccurate predictions and thus less sensitive to forgetting.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on ImageNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study and Analysis</head><p>We conduct exhaustive ablation study to evaluate the contribution of each component for our method. We also conduct sensitive study for hyper-parameters stated in the appendix. Moreover, we study the backward transfer and forward transfer of the representation for each method.</p><p>The effect of each component <ref type="table" target="#tab_6">Table 4</ref> summarizes the results of our ablative experiments on CIFAR100-B0 with 10 steps. We can see that the average accuracy is improved significantly from 61.84% to 73.26% by representation expansion. We also show that the performance of the model is further improved with 2.10% gain using auxiliary loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backward Transfer for Representation</head><p>To assess the quality of representation, we introduce an ideal decision boundary obtained by finetuning the classifier with all observed data, which allows us to exclude the influence of the classifier. We then define classification accuracy A t Y k at step t as the accuracy on the test images of class set Y k , where the prediction space of the model is restricted to Y k . By observing the A t Y k curve over t, we can see how the representation quality evolves along the increments. <ref type="figure" target="#fig_5">Figure 4</ref> shows the results of CIFAR100-B0 with 10 incremental steps. We also compute a backward transfer value for different methods as follows:</p><formula xml:id="formula_11">BWT = 1 T − 1 T i=2 1 i i j=1 A i Yj − A j Yj<label>(12)</label></formula><p>The results are shown in <ref type="table" target="#tab_8">Table 5</ref>. We can see that other methods suffer from severe forgetting. In contrast, our method even achieves positive backward transfer +1.36% and the accuracy increases with respect to steps, which further proves the superiority of our method.  Forward Transfer for Representation We also measure the influence of existing knowledge on the performance of subsequent concepts on CIFAR100-B0 with 10 incremental steps, known as forward transfer. Specifically, we define a forward transfer rate for representation as follows</p><formula xml:id="formula_12">FWT = 1 T − 1 T i=2 A i Yi −Ā i Yi<label>(13)</label></formula><p>whereĀ i Yi is the test accuracy obtained by model trained on available dataD t with only cross-entropy loss at random initialization. As shown in <ref type="table" target="#tab_8">Table 5</ref>, it is observed that most methods have negative forward transfer, which indicates that they sacrifice the flexibility of adaptation to novel concepts. By contrast, our method achieves +1.49% FWT which implies that our method not only makes the model highly flexible but also brings the positive forward transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose dynamically expandable representation to improve the representation for class incremental learning. At each step, we freeze previously learned representation and augment it with novel parameterized feature. We also introduce channel-level mask-based pruning to dynamically expand representation according to the difficulty of novel concepts and an auxiliary loss to learn the novel discriminative features better. We conduct exhaustive experiments on the three major incremental classification benchmarks. The experimental results show that our method consistently performs better than other methods with a sizable margin. Interestingly, we also find that our method can even achieve positive backward and forward transfer.</p><p>Representation learning stage For CIFAR-100, we use SGD to train the model with batch size 128, weight decay 0.0005. We adopt the warmup strategy with the ending learning rate 0.1 for 10 epochs. After the warmup, we run the SGD with 160 epochs and the learning rate decays at 100, 120 epochs with 0.1. For ImageNet100 and Im-ageNet1000, we adopt SGD with batch size 256, weight decay 0.0005. We also use the same warmup strategy as in CIFAR100. After the warmup, model is trained for 120 epochs. Learning rate starts from 0.1 and decays by 0.1 rates after 30, 60, 80, and 90 epochs.</p><p>For the coefficients in the loss function, λ a is set to 1 for all experiments in the paper. λ s is tuned to ensure our learned model has comparable number of parameters to other methods for fair comparison. Moreover, for simplicity, we set the same λ s for every steps in each experiment. For experiments of CIFAR100-B0, λ s is set as 0.75. For experiments of CIFAR100-B50, λ s is set as 0.25. For experiments on ImageNet100 and ImageNet, λ s is set as 0.75 for ImageNet100-B0, 0.5 for ImageNet100-B50 and 0.75 for ImageNet.</p><p>Classifier learning stage We adopt SGD optimizer with weight decay 0.0005 to update the classifier only for 30 epochs with SGD optimizer. Learning rate is 0.1 and decays with 0.1 rate at 15 epochs. The temperature of crossentropy loss are set as δ = 5 for CIFAR-100 and δ = 1 for ImageNet-100 and ImageNet-1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sensitive Study of Hyper-parameters</head><p>We conduct a sensitive study of our method on CVIFAR100-B0 10 steps with different λ a . The results are shown in <ref type="table" target="#tab_10">Table 6</ref>, which demonstrates our method is robust to λ a . We also conduct experiments for different λ s , which is shown in the <ref type="figure">Figure 1</ref> in the main body.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Quality of Decision Boundary</head><p>In this section, our goal is to verify that the high-quality linear classifier can be obtained even re-learning the old classes' decision boundary with memory M. Specifically, we compare our method with an 'ideal' strategy that uses all the previous data to train the classifier in the second stage. Such an upper bound achieves 76.14 ± 0.80% on the CIFAR100-B0 10 steps, which is only slightly higher than our method (74.64 ± 0.28%). We also observed similar results on the other benchmarks, which show the efficacy of our second-stage learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Latency</head><p>Regarding inference latency, we conduct an experimental comparison on the ImageNet with GTX 1080Ti. Our method achieves 1.07ms/image, which is comparable to other baseline methods, such as BiC and WA, which are 0.99ms/image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results for modified 32-layer ResNet</head><p>Most works use a modified 32-layer ResNet following iCaRL <ref type="bibr" target="#b26">[27]</ref>. We also compare the results of our method with other methods based on the modified 32-layer ResNet. The results on CIFAR100-B0 are shown in <ref type="table" target="#tab_12">Table 7</ref> and the results on CIFAR100-B50 are shown in <ref type="table" target="#tab_13">Table 8</ref>. The results of other methods are reported in their papers. It can be found that our method still outperforms other methods on both CIFAR100-B0 and CIFAR100-B50 even with a small network like modified ResNet-32.</p><p>F. More detailed results on CIFAR100 <ref type="figure" target="#fig_7">Figure 5</ref> shows the performance with respect to steps on CIFAR100-B0 with 5 incremental steps and 10 incremental steps and CIFAR100-B0 with 2 incremental steps. This also illustrates the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Detailed results on ImageNet</head><p>We also show the curves of performance with respect to steps on ImageNet100-B0, ImageNet100-B50 and ImageNet1000-B0 in <ref type="figure" target="#fig_9">Figure 6</ref>, which proves the effectiveness of our method on complex datasets.       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The performance for each step. Left is evaluated on CIFAR100-B0 of 20 and 50 steps and Right is evaluated on CIFAR100-B50 of 5 and 10 steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Analysis. The backward transfer of representation by observing the changes of A t Y1 for different splits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>The performance for each step. Left is evaluated on CIFAR100-B0 of 5 steps. Middle is evaluated on CIFAR100-B0 of 10 steps. Right is evaluated on CIFAR100-B50 of 2 steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>The performance for each step. Left is evaluated on ImageNet100-B0 of 10 steps. Middle is evaluated on ImageNet100-B50 of 10 steps. Right is evaluated on ImageNet1000-B0 of 10 steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on CIFAR100-B0 benchmark which is averaged over three runs. #Paras means the average number of parameters used during inference over steps, which is counted by million. Avg means the average accuracy (%) over steps. Ours(w/o P) means our method without pruning.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>2Steps</cell><cell></cell><cell>5Steps</cell><cell></cell><cell>10Steps</cell></row><row><cell></cell><cell>#Paras</cell><cell>Avg</cell><cell cols="2">#Paras Avg</cell><cell cols="2">#Paras Avg</cell></row><row><cell>Bound</cell><cell>11.2</cell><cell>77.22</cell><cell>11.2</cell><cell>79.89</cell><cell>11.2</cell><cell>79.91</cell></row><row><cell>iCaRL[27]</cell><cell>11.2</cell><cell>71.33 ±0.35</cell><cell>11.2</cell><cell>65.06 ±0.53</cell><cell>11.2</cell><cell>58.59 ±0.95</cell></row><row><cell>UCIR[12]</cell><cell>11.2</cell><cell>67.21 ±0.35</cell><cell>11.2</cell><cell>64.28 ±0.19</cell><cell>11.2</cell><cell>59.92 ±2.4</cell></row><row><cell>BiC[12]</cell><cell>11.2</cell><cell>72.47 ±0.99</cell><cell>11.2</cell><cell>66.62 ±0.45</cell><cell>11.2</cell><cell>60.25 ±0.34</cell></row><row><cell>WA[39]</cell><cell>11.2</cell><cell>71.43 ±0.65</cell><cell>11.2</cell><cell>64.01 ±1.62</cell><cell>11.2</cell><cell>57.86 ±0.81</cell></row><row><cell>PODNet[6]</cell><cell>11.2</cell><cell>71.30 ±0.46</cell><cell>11.2</cell><cell>67.25 ±0.27</cell><cell>11.2</cell><cell>64.04 ±0.43</cell></row><row><cell cols="2">Ours(w/o P) 22.4</cell><cell>74.61 ±0.52 (+2.14)</cell><cell>39.2</cell><cell>73.21 ±0.78 (+5.96)</cell><cell>67.2</cell><cell>72.81 ±0.88 (+8.77)</cell></row><row><cell>Ours</cell><cell>3.90</cell><cell>74.57 ±0.42 (+2.10)</cell><cell>6.13</cell><cell>72.60 ±0.78 (+5.35)</cell><cell>8.79</cell><cell>72.45 ±0.76 (+8.41)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on CIFAR100-B50 (average over 3 runs). #Paras means the average number of parameters used during inference over steps, which is counted by million. Avg means the average accuracy (%) over steps. Ours(w/o P) means our method without pruning.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on ImageNet-100 and ImageNet-1000 datasets.Left: The results on ImageNet100-B0 and ImageNet1000-B0 benchmark. Right: The results on ImageNet100-B50 benchmark. #Paras means the average number of parameters during inference over steps, which is counted by million. Avg means the average accuracy (%) over steps. Last is the accuracy (%) of the last step. Ours(w/o P) means our method without pruning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The contribution of each component. E.R. means expandable representation. Aux. means using auxiliary loss.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Backward transfer and Forward transfer (FWT) for representation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>±0.06 74.41 ±0.16 74.64 ±0.28 74.52 ±0.33 73.54 ±0.22</figDesc><table><row><cell>λ a</cell><cell>0.1</cell><cell>0.5</cell><cell>1</cell><cell>5</cell><cell>10</cell></row><row><cell cols="2">Avg 74.12</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Sensitive study on effects of λ a</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Results on CIFAR100-B0 benchmark using modified 32-layer ResNet. #Paras means the average number of parameters used during inference over steps, which is counted by million. Avg means the average accuracy (%) over steps. Ours(w/o P) means our method without pruning.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>2Steps</cell><cell></cell><cell>5Steps</cell><cell></cell><cell>10Steps</cell></row><row><cell></cell><cell>#Paras</cell><cell>Avg</cell><cell cols="2">#Paras Avg</cell><cell cols="2">#Paras Avg</cell></row><row><cell>UCIR [12]</cell><cell>0.46</cell><cell>66.76</cell><cell>0.46</cell><cell>63.42</cell><cell>0.46</cell><cell>60.18</cell></row><row><cell>PODNet [6]</cell><cell>-</cell><cell>-</cell><cell>0.46</cell><cell>64.83</cell><cell>0.46</cell><cell>64.03</cell></row><row><cell>TPCIL [34]</cell><cell>-</cell><cell>-</cell><cell>0.46</cell><cell>65.34</cell><cell>0.46</cell><cell>63.58</cell></row><row><cell cols="2">Ours(w/o P) 0.92</cell><cell cols="2">70.18(+3.42) 1.61</cell><cell cols="2">68.52(+3.18) 2.76</cell><cell>67.09(+3.06)</cell></row><row><cell>Ours</cell><cell>0.32</cell><cell cols="2">69.52(+2.76) 0.59</cell><cell cols="2">67.60(+2.26) 0.61</cell><cell>66.36(+2.33)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Results on CIFAR100-B50 using modified 32-layer ResNet. #Paras means the average number of parameters used during inference over steps, which is counted by million. Avg means the average accuracy (%) over steps. Ours(w/o P) means our method without pruning.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>A. Hyperparameters</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conditional channel gated networks for task-aware continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><forename type="middle">Ehteshami</forename><surname>Bejnordi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory aware synapses: Learning what (not) to forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolás</forename><surname>Marín-Jiménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Riemannian walk for incremental learning: Understanding forgetting and intransigence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Podnet: Pooled outputs distillation for small-tasks incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Douillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Ollion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pathnet: Evolution channels gradient descent in super neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using noise to compute error surfaces in connectionist networks: A novel means of reducing catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive resonance theory: How a brain learns to consciously attend, learn, and recognize a changing world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compacting, picking and growing for unforgetting continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Hao</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-En</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Hung</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Song</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<publisher>PNAS</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting by incremental moment matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An incremental face recognition system based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lufan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Vision Applications (MVA)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning(ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a single network to multiple tasks by learning to mask weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Packnet: Adding multiple tasks to a single network by iterative pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to remember: A synaptic plasticity driven framework for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksiy</forename><surname>Ostapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tassilo</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Jahnichen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Incremental lifelong deep learning for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Random path selection for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jathushan</forename><surname>Rajasegaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in neural networks: the role of rehearsal mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">V</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Two-Stream Conference on Artificial Neural Networks and Expert Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting, rehearsal and pseudorehearsal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">V</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connect. Sci</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with hard attention to the task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didac</forename><surname>Suris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Lifelong robot learning. Robotics and autonomous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Herding dynamical weights to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large scale incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Topology-preserving class-incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiaoyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xinyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Xiaopeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Yihong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV), 2020</title>
		<meeting>the European Conference on Computer Vision (ECCV), 2020</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lifelong learning with dynamically expandable networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongtae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic drift compensation for class-incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartlomiej</forename><surname>Twardowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongmei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangling</forename><surname>Jui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">Xinnan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svebor</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04157</idno>
		<title level="m">Heated-up softmax embedding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Maintaining discrimination and fairness in class incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
