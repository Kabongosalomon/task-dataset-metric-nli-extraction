<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PYTORCH-BIGGRAPH: A LARGE-SCALE GRAPH EMBEDDING SYSTEM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothee</forename><surname>Lacroix</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Wehrstedt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Bose</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Peysakhovich</surname></persName>
						</author>
						<title level="a" type="main">PYTORCH-BIGGRAPH: A LARGE-SCALE GRAPH EMBEDDING SYSTEM</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph embedding methods produce unsupervised node features from graphs that can then be used for a variety of machine learning tasks. Modern graphs, particularly in industrial applications, contain billions of nodes and trillions of edges, which exceeds the capability of existing embedding systems. We present PyTorch-BigGraph (PBG), an embedding system that incorporates several modifications to traditional multi-relation embedding systems that allow it to scale to graphs with billions of nodes and trillions of edges. PBG uses graph partitioning to train arbitrarily large embeddings on either a single machine or in a distributed environment. We demonstrate comparable performance with existing embedding systems on common benchmarks, while allowing for scaling to arbitrarily large graphs and parallelization on multiple machines. We train and evaluate embeddings on several large social network graphs as well as the full Freebase dataset, which contains over 100 million nodes and 2 billion edges.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph structured data is a common input to a variety of machine learning tasks <ref type="bibr" target="#b36">(Wu et al., 2018;</ref><ref type="bibr" target="#b6">Cook &amp; Holder, 2006;</ref><ref type="bibr" target="#b25">Nickel et al., 2016a;</ref><ref type="bibr" target="#b13">Hamilton et al., 2017b)</ref>. Working with graph data directly is difficult, so a common technique is to use graph embedding methods to create vector representations for each node so that distances between these vectors predict the occurrence of edges in the graph. Graph embeddings have been have been shown to serve as useful features for downstream tasks such as recommender systems in e-commerce <ref type="bibr" target="#b35">(Wang et al., 2018)</ref>, link prediction in social media <ref type="bibr" target="#b29">(Perozzi et al., 2014)</ref>, predicting drug interactions and characterizing protein-protein networks <ref type="bibr" target="#b39">(Zitnik &amp; Leskovec, 2017)</ref>.</p><p>Graph data is common at modern web companies and poses an extra challenge to standard embedding methods: scale. For example, the Facebook graph includes over two billion user nodes and over a trillion edges representing friendships, likes, posts and other connections <ref type="bibr" target="#b5">(Ching et al., 2015)</ref>. The graph of users and products at Alibaba also consists of more than one billion users and two billion items <ref type="bibr" target="#b35">(Wang et al., 2018)</ref>. At Pinterest, the user to item graph includes over 2 billion entities and over 17 billion edges <ref type="bibr" target="#b38">(Ying et al., 2018)</ref>.</p><p>There are two main challenges for embedding graphs of this size. First, an embedding system must be fast enough to embed graphs with 10 11 − 10 12 edges in a reasonable time. Second, a model with two billion nodes and 100 embedding parameters per node (expressed as floats) would require 800GB of memory just to store its parameters, thus many standard methods exceed the memory capacity of typical commodity servers.</p><p>We present PyTorch-BigGraph (PBG), an embedding system that incorporates several modifications to standard models. The contribution of PBG is to scale to graphs with billions of nodes and trillions of edges. Important components of PBG are:</p><p>• A block decomposition of the adjacency matrix into N buckets, training on the edges from one bucket at a time. PBG then either swaps embeddings from each partition to disk to reduce memory usage, or performs distributed execution across multiple machines.</p><p>• A distributed execution model that leverages the block decomposition for the large parameter matrices, as well as a parameter server architecture for global parameters and feature embeddings for featurized nodes.</p><p>• Efficient negative sampling for nodes that samples negative nodes both uniformly and from the data, and reuses negatives within a batch to reduce memory bandwidth.</p><p>• Support for multi-entity, multi-relation graphs with perrelation configuration options such as edge weight and choice of relation operator.</p><p>We evaluate PBG on the Freebase, LiveJournal and YouTube graphs and show that it matches the performance of existing embedding systems.</p><p>We also report results on larger graphs. We construct an embedding of the full Freebase knowledge graph (121 million entities, 2.4 billion edges), which we release publicly with this paper. Partitioning of the Freebase graph reduces memory consumption by 88% with only a small degradation in the embedding quality, and distributed execution on 8 machines decreases training time by a factor of 4. We also perform experiments on a large Twitter graph showing similar results with near-linear scaling.</p><p>PBG has been released as an open source project at https://github.com/facebookresearch/ PyTorch-BigGraph. It is written entirely in Pytorch <ref type="bibr" target="#b28">(Paszke et al., 2017)</ref> with no external dependencies or custom operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Many types of models have been developed for multirelation graphs <ref type="bibr" target="#b3">(Bordes et al., 2011;</ref><ref type="bibr" target="#b24">Nickel et al., 2011;</ref><ref type="bibr" target="#b34">Trouillon et al., 2016)</ref>. Typically these models have been used in the context of entity representations in knowledge bases (e.g. Freebase or WordNet). Entities are given a base vector, these vectors are transformed by a learned function for each transformation, and existence of edges is predicted by some distance measure in the new space. More recent work by Wu et al. proposes modeling some entities as bags of other entities (rather than giving them explicit embeddings). PBG borrows many insights on loss functions and transformations from this literature.</p><p>There are significant engineering challenges to scaling graph embedding models. Proposed approaches in the literature include multi-level methods <ref type="bibr" target="#b22">(Liang et al., 2018)</ref>, distributed embedding systems <ref type="bibr" target="#b27">(Ordentlich et al., 2016;</ref><ref type="bibr" target="#b32">Shazeer et al., 2016)</ref>, as well as specialized methods for standard algorithms such as SVD and k-means on large graphs <ref type="bibr" target="#b5">(Ching et al., 2015)</ref>. Gains from large embedding systems have been documented in e-commerce <ref type="bibr" target="#b35">(Wang et al., 2018</ref>) and other applications.</p><p>There is an extensive literature on distributional semantics in natural language processing. A key breakthrough in this literature are algorithms such as word2vec which allowed word embedding methods to scale to larger corpora <ref type="bibr" target="#b23">(Mikolov et al., 2013)</ref>. Recent work has shown that there is economic value from ingesting even larger data sets using distributed word2vec systems <ref type="bibr" target="#b27">(Ordentlich et al., 2016)</ref>.</p><p>There is substantial prior work on scalable parallel algorithms for training machine learning models <ref type="bibr" target="#b7">(Dean et al., 2012)</ref>. Highly related to PBG is work on scaling various forms of matrix factorization <ref type="bibr" target="#b11">(Gupta et al., 1997;</ref><ref type="bibr" target="#b9">Gemulla et al., 2011)</ref>. Matrix factorization is closely related to embeddings, and has had widespread success in recommender systems <ref type="bibr" target="#b15">(Koren et al., 2009)</ref>.</p><p>Recent work proposes to construct embeddings by using graph convolutional neural networks (GCNs, <ref type="bibr" target="#b14">Kipf &amp; Welling 2016)</ref>. These methods have shown success when applied to problems at large-scale web companies <ref type="bibr" target="#b12">(Hamilton et al., 2017a;</ref><ref type="bibr" target="#b38">Ying et al., 2018)</ref>. The problem studied by the GCN is different than the one solved by PBG (mostly in that GCNs are typically applied to graphs where the nodes are already featurized). Combining ideas from graph embedding and GCN models is an interesting future direction both for theory and applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTI-RELATION EMBEDDINGS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>A multi-relation graph is a directed graph G = (V, R, E) where V are the nodes (aka entities), R is a set of relations, and E is a set of edges where a generic element e = (s, r, d) (source, relation, destination) where s, d ∈ V and r ∈ R.</p><p>We also discuss graphs that have multiple entity types. Such graphs have a set of entity types and a mapping from nodes to entity types, and each relation specifies a single entity type for source and destination nodes for all edges of that relation.</p><p>We will represent each entity and relation type with a vector of parameters. We will denote this vector as θ. A multirelation graph embedding uses a score function f (θ s , θ r , θ d ) that produces a score for each edge that attempts to maximize the score of f (θ s , θ r , θ d ) for any (s, r, d) ∈ E and minimizes it for (s, r, d) ∈ E.</p><p>PBG considers scoring functions between a transformed version of an edge's source and destination entities' vectors (θ s , θ d ):</p><formula xml:id="formula_0">f (θ s , θ r , θ d ) = sim g (s) (θ s , θ r ), g (d) (θ d , θ r )</formula><p>where θ r corresponds to parameters of the relation-specific transformation operator. Using a factorized scoring function produces a embeddings where the (transformed) similarity between node embeddings has semantic meaning.</p><p>PBG uses dot product or cosine similarity scoring functions, and a choice of relation operator g which include linear transformation, translation, and complex multiplication. This combination of scoring functions and relation operators allows PBG to train RESCAL, DistMult, TransE, and ComplEx models <ref type="bibr" target="#b24">(Nickel et al., 2011;</ref><ref type="bibr" target="#b37">Yang et al., 2014;</ref><ref type="bibr" target="#b4">Bordes et al., 2013;</ref><ref type="bibr" target="#b34">Trouillon et al., 2016)</ref>. 1 A subset of relation types may use the identity relation, so that the <ref type="figure">Figure 1</ref>. The PBG partitioning scheme for large graphs. Left: nodes are divided into P partitions that are sized to fit in memory. Edges are divided into buckets based on the partition of their source and destination nodes. In distributed mode, multiple buckets with non-overlapping partitions can be executed in parallel (red squares). Center: Entity types with small cardinality do not have to be partitioned; if all entity types used for tail nodes are unpartitioned, then edges can be divided into P buckets based only on source node partitions. Right: the 'inside-out' bucket order guarantees that buckets have at least one previously-trained embedding partition. Empirically, this ordering produces better embeddings than other alternatives (or random) untransformed entity embeddings predict edges of this relation.</p><p>Model</p><formula xml:id="formula_1">g(x, θ r ) sim(a, b) RESCAL A r x &lt; a, b &gt; TransE x + θ r cos(a, b) DistMult x θ r &lt; a, b &gt; ComplEx x θ r Re{&lt; a, b &gt;}</formula><p>We consider sparse graphs, so the input to PBG is a list of positive-labeled (existing) edges. Negative edges are constructed by sampling. In PBG negative samples are generated by corrupting positive edges by sampling either a new source or a destination for each existing edge <ref type="bibr" target="#b4">(Bordes et al., 2013)</ref>.</p><p>Because edge distributions in real world graphs are heavy tailed, the choice of how to sample nodes to construct negative examples can affect model quality <ref type="bibr" target="#b23">(Mikolov et al., 2013)</ref>. On one hand, if we sample negatives strictly according to the data distribution, there is no penalty for the model predicting high scores for edges with rare nodes. On the other hand, if we sample negatives uniformly, the model can perform very well (especially in large graphs) by simply scoring edges proportional to their source and destination node frequency in the dataset. Both of these results are undesirable, so in PBG we sample a fraction α of negatives according to their prevalence in the training data and (1 − α) achieved with ComplEx embeddings, but this may not generalize to all graphs. On small knowledge graphs, a general linear transform (RESCAL) does not perform as well as transformations with fewer parameters such as translation (as well as transformations that can be represented in the RESCAL model) because the relation operators overfit <ref type="bibr" target="#b26">(Nickel et al., 2016b)</ref>. However, we are interested in web interaction graphs which have a very small number of relations relative to entities, so the relation parameters do not contribute substantially to model size, nor are they prone to overfitting. of them uniformly at random. By default PBG uses α = .5.</p><p>In multi-entity graphs, negatives are only sampled from the correct entity type for an edge's relation. Thus, in our model, the score for an 'invalid' edge (wrong entity types) is undefined. The approach of using entity types has been studied before in the context of knowledge graphs <ref type="bibr" target="#b16">(Krompaß et al., 2015)</ref>, but we found it to be particularly important in graphs that have entity types with highly unbalanced numbers of nodes, e.g. 1 billion users vs. 1 million products. With uniform negative sampling over all nodes, the loss would be dominated by user negative nodes and would not optimize for ranking between user-product edges.</p><p>PBG optimizes a margin-based ranking objective between each edge e in the training data and a set of edges e constructed by corrupting e with either a sampled source or destination node (but not both).</p><formula xml:id="formula_2">L = e∈G e ∈S e max(f (e) − f (e ) + λ, 0))</formula><p>where λ is a margin hyperparameter and</p><formula xml:id="formula_3">S e = {(s , r, d)|s ∈ V } ∪ {(s, r, d |d ∈ V } .</formula><p>Logistic and softmax loss functions may also be used instead of a ranking objective in order to reproduce certain graph embedding models (e.g. <ref type="bibr" target="#b34">Trouillon et al. 2016)</ref>.</p><p>Model updates to the embeddings and relation parameters are performed via minibatch stochastic gradient descent (SGD). We use the Adagrad optimizer, and sum the accumulated gradient G over each embedding vector to reduce memory usage on large graphs <ref type="bibr" target="#b8">(Duchi et al., 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TRAINING AT SCALE</head><p>PBG is designed to operate on arbitrarily large graphs running on either a single machine or can be distributed across multiple machines. In either case, training occurs on a number of CPU threads equal to the number of machine cores, with no explicit synchronization between cores as described in <ref type="bibr" target="#b30">(Recht et al., 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Partitioning of Entities and Edges</head><p>PBG uses a partitioning scheme to support models that are too large to fit in memory on a single machine. This partitioning also allows for distributed training of the model.</p><p>Each entity type in G can be either partitioned or remain unpartitioned. Partitioned entities are split into P parts. P is chosen such that each part fits into memory or to support the desired level of parallelism for execution.</p><p>After entities are partitioned, edges are divided into buckets based on their source and destination entities' partitions. For example, if an edge has a source in partition p 1 and destination in partition p 2 then it is placed into bucket (p 1 , p 2 ). This creates P 2 buckets when both source and destination entity types are partitioned and P buckets if only source (or destination) entities are partitioned.</p><p>Each epoch of training iterates through each of the edge buckets. For edge bucket (p i , p j ), source and destination partitions i and j respectively are swapped from disk, and then the edges (or a subset of edges) are loaded and subdivided among the threads for training.</p><p>This graph partitioning introduces two functional changes to the base algorithm described in the last section. First, each candidate edge (s, r, d) is only compared to negatives (s, r, d ) in the ranking loss where d is drawn from the same partition (same for source nodes) 2 .</p><p>Second, edges are no longer sampled i.i.d. but are grouped by partition. Convergence under SGD to a stationary or chain-recurrent point, is still guaranteed under this modification (see <ref type="bibr" target="#b9">(Gemulla et al., 2011)</ref>, Sec. 4.2), but may suffer from slower convergence 34 .</p><p>We observe that the order of iterating through edge buckets may affect the final model. Specifically, for each edge bucket (p 1 , p 2 ) except the first, it is important that an edge bucket (p 1 , * ) or ( * , p 2 ) was trained in a previous iteration.</p><p>This constraint ensures that embeddings in all partitions are aligned in the same space. For single-machine embeddings, we found that an 'inside-out' ordering, illustrated in <ref type="figure">Figure</ref> 1, achieved the best performance while minimizing the number of swaps to disk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Distributed Execution</head><p>Existing distributed embedding systems typically use a parameter server architecture. In this architecture, a (possibly sharded) parameter server contains a key-value store of embeddings. At each SGD iteration, the embedding parameters required by a minibatch of data are requested from the parameter server, and gradients are (asynchronously) sent to the server to update the parameters.</p><p>The parameter server paradigm has been effective for training large sparse models <ref type="bibr" target="#b21">(Li et al., 2014)</ref>, but it has a number of drawbacks. One issue is that parameter-server based embedding frameworks require too much network bandwidth to run efficiently, since all embeddings for each minibatch of edges and their associated negative samples must be transferred at each SGD step <ref type="bibr" target="#b27">(Ordentlich et al., 2016)</ref> 5 . Furthermore, we found it necessary for effective research use that the same models could be run in a single-machine or distributed context, but the parameter server architecture limits the size of models that can be run on a single machine. Finally, we would like to avoid the potential convergence problems from asynchronous model updates since our embeddings are already partitioned into independent sets.</p><p>Given partitioned entities and edges PBG employs a parallelization scheme that combines a locking scheme over the model partitions described in Section 4.1, with an asynchronous parameter server architecture for shared parameters i.e. the relation operators as well as unpartitioned or featurized entity types.</p><p>In this parallelization scheme, illustrated in <ref type="figure">Figure 2</ref>, partitioned embeddings are locked by machines for training. Multiple edge buckets can be trained in parallel as long as they operate on disjoint sets of partitions, as shown in <ref type="figure">Figure 1 (left)</ref>. Training can proceed in parallel on up to P/2 machines. The locking of partitions is handled by a centralized lock server on one machine, which parcels out buckets to the workers in order to minimize communication (i.e. favors re-using a partition) The lock server also maintains the invariant described in Section 4.1, that only the first bucket should operate on two uninitialized partitions.</p><p>The partitioned embeddings themselves are stored in a partition server sharded across the N training machines. A <ref type="figure">Figure 2</ref>. A block diagram of the modules used for PBG's distributed mode. Arrows illustrate the communications that the Rank 2 Trainer performs for the training of one bucket. First, the trainer requests a bucket from the lock server on Rank 1, which locks that bucket's partitions. The trainer then saves any partitions that it is no longer using and loads new partitions that it needs to and from the sharded partition servers, at which point it can release its old partitions on the lock server. Edges are then loaded from a shared filesystem, and training occurs on multiple threads without inter-thread synchronization <ref type="bibr" target="#b30">(Recht et al., 2011)</ref>. In a separate thread, a small number of shared parameters are continuously synchronized with a sharded parameter server. Model checkpoints are occasionally written to the shared filesystem from the trainers. machine fetches the source and destination partitions, which are often multiple GB in size, from the partition server, and trains on a bucket of edges loaded from shared disk. Checkpoints of the partitioned entities are intermittently saved to shared disk. Some model parameters are global and thus cannot be partitioned. This most importantly includes relation parameters, as well as entity types that have very small cardinality or use featurized embeddings. There are a relatively small number of such parameters (&lt; 10 6 ), and they are handled via asynchronous updates with a sharded parameter server. Specifically, each trainer maintains a background thread that has access to all unpartitioned model parameters. This thread asynchronously fetches the parameters from the server and updates the local model, and pushes accumulated gradients from the local model to the parameter server. This thread performs continuous synchronization with some throttling to avoid saturating network bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Batched Negative Sampling</head><p>The negative sampling approach used by most graph embedding methods is highly memory (or network) bound because it requires B · B n · d floats of memory access to perform O(B ·B n ·d) floating-point operations (B ·B n dot products). Indeed, Wu et al.. report that training speed "is close to an inverse linear function of [number of negatives]".</p><p>To increase memory efficiency on large graphs, we observe that a single batch of B n sampled source or destination nodes can be reused to construct multiple negative examples. In a typical setup, PBG takes a batch of B = 1000 positive edges from the training set, and breaks it into chunks of 50 edges. The destination (equivalently, source) embeddings from each chunk is concatenated with 50 embeddings sampled uniformly from the tail entity type. The outer product of the 50 positives with the 200 sampled nodes equates to 9900 negative examples (excluding the induced positives). The training computation is summarized in <ref type="figure" target="#fig_0">Figure 3</ref>. This approach is much cheaper than sampling negatives for each batch. For each batch of B positive edges, only 3B embeddings are fetched from memory and 3BB n edge scores (dot products) are computed. The edge scores for a batch can be computed as a batched B n × B n matrix multiply, which can be executed with high efficiency. <ref type="figure" target="#fig_1">Figure  4</ref> shows the performance of PBG with different numbers of negative samples, with and without batched negatives.</p><p>In multi-relation graphs with a small number of relations, we construct batches of edges that all share the same relation type r. This improves training speed specifically for the linear relation operator f r (t) = A r t, because it can be formulated as a matrix-multiply f r (T ) = A r T.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We evaluate PBG on two types of graphs common in both the academic literature and practical applications.</p><p>In one set of experiments we focus on embedding real online social networks. We evaluate PBG constructed embeddings of the user-user interaction graph from LiveJournal <ref type="bibr" target="#b0">(Backstrom et al., 2006)</ref>  <ref type="bibr" target="#b20">(Leskovec et al., 2009</ref>), a user-user follow graph from Twitter <ref type="bibr" target="#b17">(Kwak et al., 2010)</ref>  <ref type="bibr" target="#b1">(Boldi &amp; Vigna, 2004)</ref>  <ref type="bibr" target="#b2">(Boldi et al., 2011)</ref>, and a user-user interaction graph from YouTube <ref type="bibr" target="#b33">(Tang &amp; Liu, 2009</ref>). The LiveJournal and Twitter data set we used are from SNAP <ref type="bibr" target="#b19">(Leskovec &amp; Krevl, 2014)</ref>.</p><p>We consider two types of tasks: link prediction in the graph and the use of the graph embedding vectors to predict other attributes of the nodes. We find that PBG is much faster and more scalable than existing methods while achieving comparable performance. Second, the distributed partitioning does not impact the quality of the learned embeddings on large graphs. Third, PBG allows for parallel execution and thus can decrease wallclock training time proportional the number of partitions.</p><p>We also consider using PBG to embed the Freebase knowledge graph. Knowledge graphs have a very different structure from social networks and the presence of many relation types allows us to study the effect of using various relation operators from the literature.</p><p>Here we find that PBG can again match (or exceed) state of the art performance but that some types of relation operators (e.g. ComplEx) require care when using distributed training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>For each dataset, we report the best results from a grid search of learning rates from 0.001 − 0.1, margins from 0.05−0.2 and negative batch sizes of 100−500, and choose the parameter settings based on the validation split. Results for FB15k are reported on the separate test split.</p><p>All experiments are performed on machines with 24 Intel ® Xeon ® cores (two sockets) and two hyperthreads per core, for a total of 48 virtual cores, and 256 GB of RAM.  <ref type="bibr" target="#b22">(Liang et al., 2018)</ref>. <ref type="figure">Figure 5</ref>. Learning curve for PBG and competing embedding methods on the LiveJournal dataset. For each approach, we evaluate the MRR after each epoch. DeepWalk takes more than 20 hours to train for a single epoch so we limit the number of walks during training for some runs to further reduce the computation time.</p><p>We use 40 HOGWILD threads for training. For distributed execution, we use a cluster of machines connected via 50Gb/s ethernet. We use the TCP backend for torch.distributed which in practice achieves approximately 1 GB/s send/receive bandwidth. For memory usage measurements we report peak resident set size sampled at 0.1 second intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">LiveJournal</head><p>We evaluate PBG performance on the LiveJournal dataset <ref type="bibr" target="#b0">(Backstrom et al., 2006;</ref><ref type="bibr" target="#b20">Leskovec et al., 2009</ref>) collected from the blogging site LiveJournal 6 , where users can follow others to form a social network. The dataset contains 4,847,571 nodes and 68,993,773 edges. We construct train and test splits of the dataset that contains 75% and 25% of the total edges.</p><p>We compare the PBG embedding performance with MILE, which can also scale to large graphs. MILE repeatedly coarsens the graphs into smaller ones and applies traditional embedding methods on coarsened graph at each level as well as a final refinement step to get the embeddings of the original graph. We also show the performance of DeepWalk, which is used as the base embedding method for MILE.</p><p>We evaluate the embeddings using the same methodology described in Section 5.4.2. To compare the computation time across different approaches, we report the learning curve of test MRR obtained by different approaches during training with respect to time (see <ref type="figure">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">YouTube</head><p>To show that PBG embeddings are useful for downstream supervised tasks, we apply PBG to the Youtube dataset <ref type="bibr" target="#b33">(Tang &amp; Liu, 2009</ref>). The dataset contains a social network between users on YouTube 7 , as well as the labels of these users that represent categories of groups they subscribed. This social network dataset contains 1,138,499 nodes and 2,990,443 edges.</p><p>We compare the performance of PBG embeddings with MILE embeddings and DeepWalk embeddings by applying those embeddings as features to perform a multi-label classification of users. We follow the typical methods <ref type="bibr" target="#b29">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b22">Liang et al., 2018)</ref> to evaluate the embedding performance, where we run a 10-fold cross validation by randomly selecting 90% of the labeled data as training data and the rest as testing data. We use the learned embedding as features and train a one-vs-rest logistic regression model to solve the multi-label node classfication problem.</p><p>We find that PBG embeddings perform comparably (slightly better) than competing methods (see <ref type="table" target="#tab_0">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Freebase Knowledge Graph</head><p>Freebase (FB) is a large knowledge graph that contains general facts extracted from Wikipedia, etc. The FB15k dataset consists of a subset of Freebase consisting of 14,951 entities, 1345 relations and 592,213 edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MRR Method</head><p>Raw Filtered Hit@10 RESCAL <ref type="bibr" target="#b24">(Nickel et al., 2011)</ref> 0.189 0.354 0.587 TransE <ref type="bibr" target="#b4">(Bordes et al., 2013)</ref> 0.222 0.463 0.749 HolE <ref type="bibr" target="#b26">(Nickel et al., 2016b)</ref> 0.232 0.524 0.739 ComplEx <ref type="bibr" target="#b34">(Trouillon et al., 2016)</ref> 0.242 0.692 0.840 R-GCN+ <ref type="bibr" target="#b31">(Schlichtkrull et al., 2018)</ref> 0.262 0.696 0.842 StarSpace <ref type="bibr" target="#b36">(Wu et al., 2018)</ref> --0.838 Reciprocal ComplEx-N3 <ref type="bibr" target="#b18">(Lacroix et al., 2018</ref>  <ref type="bibr" target="#b18">(Lacroix et al., 2018)</ref> use extremely large embedding dimension, which we do not reproduce here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">FB15K</head><p>We compare the performance of PBG embeddings on a link prediction task with existing embedding methods for knowledge graphs. We compare mean reciprocal rank and Hits@10 with existing methods for knowledge graph embeddings reported in <ref type="bibr" target="#b34">(Trouillon et al., 2016)</ref>. <ref type="bibr">8</ref> Results are shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>We embed FB15k with a complex multiplication relation operator as in <ref type="bibr" target="#b34">(Trouillon et al., 2016)</ref>. We evaluate PBG using two different configurations: one that is similar to the TransE model, and one similar to the ComplEx model. As in that work, we also find it beneficial to use separate relation embeddings for source negatives and destination negatives (described as 'reciprocal predicates' in <ref type="bibr" target="#b18">Lacroix et al. 2018)</ref>. For ComplEx, we train a 400-dimensional embedding for 50 epochs with a softmax loss over negatives using dot product similarity.</p><p>PBG performs comparably to the reported results for TransE and ComplEx models. In addition, recent papers have reported even stronger results for FB15k (and other small knowledge graphs like WordNet) using ComplEx with very large embeddings of thousands of dimensions <ref type="bibr" target="#b18">(Lacroix et al., 2018)</ref>. We managed to reproduce these architectures and results in the PBG framework but do not report the details here due to space constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Full Freebase</head><p>Next, we compare different numbers of partitions and distributed training using the full Freebase dataset 9 <ref type="bibr" target="#b10">(Google, 2018)</ref>. We use all entities and relations that appeared at least 5 times in the full dataset, resulting in a total of 121,216,723 nodes, 25,291 relations and 2,725,070,599 edges. We construct train, validation and test splits of the dataset, which contain 90%, 5%, 5% of the total edges, respectively. The data format we use for the full freebase dataset is the same as in the freebase 15k dataset described in Section 5.4.1.</p><p>To investigate the effect of number of partitions, we partition Freebase nodes uniformly into different numbers of partitions and measure model performance, training time, and peak memory usage. We then consider parallel training on different numbers of machines. For each number of machines M , we use 2M partitions (which is the minimum number of partitions that allows this level of parallelism.</p><p>Note that the full model size (d = 100) is 48.5 GB.</p><p>We train each model for 10 epochs, using the same grid search over hyperparameters for each number of partitions chosen from the same set grid search as FB15k. For the multi-machine evaluation, we use a consistent hyperparameters that had the best performance on single-machine training.</p><p>We evaluate the models with a link prediction task similar to that described in Section 5.  of entities according to their prevalence in the training data to produce negative edges which we use to compute mean reciprocal rank and hits@10 10 . We report these results raw (unfiltered), following prior work on large graphs <ref type="bibr" target="#b4">(Bordes et al., 2013)</ref>.</p><p>Results are reported in <ref type="table">Table 3</ref>, along with training time and memory usage.</p><p>We observe that on a single machine, peak memory usage decreases almost linearly with number of partitions, but training time increases somewhat due to extra time spent on I/O 11 . On multiple machines, the full model is sharded across the machines rather than on disk, so memory usage is higher with 2 machines, but decreases linearly as the number of machines increases. Training time also decreases with increasing number of machines, although there is again some overhead for training on multiple machines. This consists of a combination of I/O overhead and incomplete occupancy. The occupancy issue arises because there may not always be an available bucket with non-locked partitions for a machine to work on. Increasing the number of partitions relative to the number of machines will thus increase occupancy, but we don't examine this tradeoff in detail.</p><p>Freebase embeddings have nearly identical link prediction accuracy after 10 epochs of training with and without node partitioning and parallelization up to four machines. For the highest parallelization condition <ref type="formula">(8 machines</ref>  results are consistent with <ref type="table">Table 3</ref>: we observe a decrease in training time with multiple machines, without a loss in link prediction accuracy up to 8 machines.</p><p>In <ref type="figure" target="#fig_3">Figure 7</ref> we report the learning curve of test MRR obtained by different number of machines used during training with respect to epoch and time. Compared to the Freebase knowledge base learning curves in <ref type="figure" target="#fig_2">Figure 6</ref>, the Twitter graph shows more linear scaling of training time as the graph is partitioned and trained in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we present PyTorch-BigGraph, an embedding system that scales to graphs with billions of nodes and trillions of edges. PBG supports multi-entity, multi-relation graphs with per-relation configuration such as edge weight and choice of relation operator. To save on memory usage and to allow parallelization PBG performs a block decomposition of the adjacency matrix into N buckets, training on the edges from one bucket at a time.</p><p>We show that the quality of embeddings trained with PBG are comparable with existing embedding systems, and require less time to train. We show that partitioning of the Freebase graph reduces memory consumption by 88% without degrading embedding quality, and distributed execution on 8 machines speeds up training by a factor of 4. Our experiments have shown that embedding quality is quite robust to partitioning and parallelization in social network datasets, but may be more sensitive to parallelization when the number of relations is large, the degree distribution is highly skewed, or relation operators such as ComplEx are used. Thus improving the scaling for these more complicated models is an important area for future research.</p><p>We have presented PBG's performance on the largest publicly available graph datasets that we are aware of. However, the largest benefits of the PBG architecture come from graphs that are 1 − 2 orders of magnitude larger than these, where more fine-grained partitioning is necessary and exposes more parallelism. We hope that this work and the open source release of PBG helps to motivate the release of larger graph datasets and an increase in research and reported results on larger graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Memory-efficient batched negative sampling. Embeddings are fetched for the B source and destination entities in a batch of edges, as well as B uniformly-sampled source and destination entities. Each chunk of Bn/2 edges is corrupted with all source or destination entities in its chunk, as well as the corresponding chunk of the uniform embeddings, resulting in Bn negative examples per positive edge. The negative scores are computed via a batch matrix multiply.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Effect of number of negative samples per edge on training speed (d = 100). With unbatched negatives, training speed is inversely proportional to number of negatives, but with batched negatives, speed is nearly constant for Bn ≤ 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Learning curve for PBG models on the Freebase dataset with different number of machines used in training. MRR of learned embeddings is plotted as a function of epoch (top) and wallclock time (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Learning curve for PBG models on the Twitter dataset with different number of machines used in training. MRR of learned embeddings is plotted as a function of epoch (top) and wallclock time (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of PBG, DeepWalk, and MILE on the LiveJournal dataset and the YouTube dataset. Left: Link prediction evaluation and peak memory usage for the trained embeddings of the LiveJournal dataset and YouTube dataset. The ranking metrics on the test set are obtained by ranking positive edges among randomly sampled corrupted edges. Right: Micro-f1 and Macro-f1 on the user categories prediction task of the YouTube dataset when using learned embeddings as features. * Results obtained running software provided by the original authors. † Results reported in</figDesc><table><row><cell></cell><cell cols="2">LiveJournal</cell><cell></cell><cell></cell><cell cols="2">YouTube</cell></row><row><cell>Metric</cell><cell>MRR</cell><cell cols="2">MR Hits@10</cell><cell>Memory</cell><cell>Metric</cell><cell>Micro-F1 Macro-F1</cell></row><row><cell>DeepWalk*</cell><cell cols="2">0.691 234.6</cell><cell cols="2">0.842 61.23 GB</cell><cell>DeepWalk †</cell><cell>45.2%</cell><cell>34.7%</cell></row><row><cell>MILE (1 level)*</cell><cell cols="2">0.629 174.4</cell><cell cols="2">0.785 60.88 GB</cell><cell>MILE (6 level) †</cell><cell>46.1%</cell><cell>38.5%</cell></row><row><cell cols="3">MILE (5 levels)* 0.505 462.8</cell><cell cols="2">0.632 22.78 GB</cell><cell>MILE (8 levels) †</cell><cell>44.3%</cell><cell>35.3%</cell></row><row><cell cols="3">PBG (1 partition) 0.749 245.9</cell><cell cols="2">0.857 20.88 GB</cell><cell>PBG (1 partition)</cell><cell>48.0%</cell><cell>40.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of PBG with other embedding methods on the FB15k dataset. PBG embeddings are trained with both a TransE and ComplEx model, and in both cases perform similarly to the reported results for that model. The best reported results on FB15k</figDesc><table><row><cell>)</cell><cell>-</cell><cell>0.860</cell><cell>0.910</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 3. Model evaluation, training time (10 epochs) and peak memory usage for embeddings of the full Freebase knowledge graph. MRR and Hits@10 are evaluated in the raw setting. Left: Training with different numbers of partitions on a single machine. Right: Distributed training on different numbers of machines.</figDesc><table><row><cell cols="5"># Parts MRR Hits@10 Time (h) Mem (GB)</cell><cell cols="6"># Machines # Parts MRR Hits@10 Time (h) Mem (GB)</cell></row><row><cell>1</cell><cell>0.170</cell><cell>0.285</cell><cell>30</cell><cell>59.6</cell><cell>1</cell><cell>1</cell><cell>0.170</cell><cell>0.285</cell><cell>30</cell><cell>59.6</cell></row><row><cell>4</cell><cell>0.174</cell><cell>0.286</cell><cell>31</cell><cell>30.4</cell><cell>2</cell><cell>4</cell><cell>0.170</cell><cell>0.280</cell><cell>23</cell><cell>64.4</cell></row><row><cell>8</cell><cell>0.172</cell><cell>0.288</cell><cell>33</cell><cell>15.5</cell><cell>4</cell><cell>8</cell><cell>0.171</cell><cell>0.285</cell><cell>13</cell><cell>30.5</cell></row><row><cell>16</cell><cell>0.174</cell><cell>0.290</cell><cell>40</cell><cell>6.8</cell><cell>8</cell><cell>16</cell><cell>0.163</cell><cell>0.276</cell><cell>7.7</cell><cell>15.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">4.1. However due to the large</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">number of candidate nodes, for each edge in the eval set we</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">select 10, 000 candidate negative nodes sampled from the set</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>), a small degradation in MRR from 0.171 to 0.163 is observed. PBG embeddings trained with the ComplEx model perform better than TransE on the link prediction task, achieving MRR of 0.24 and Hits@10 of 0.32 with d = 200 and a single partition. However, our experiments show that training ComplEx models with multiple partitions and machines is unstable, and MRR varies from 0.15 to 0.22 across replicates. Further investigation of the performance of ComplEx models via PBG partitioning is left for future work.Table 4. Model evaluation, training time (10 epochs) and peak memory usage for embeddings of the Twitter graph. Left: Training with different numbers of partitions on a single machine. Right: Distributed training on different numbers of machines.</figDesc><table><row><cell cols="5"># Parts MRR Hits@10 Time (h) Mem (GB)</cell><cell cols="6"># Machines # Parts MRR Hits@10 Time (h) Mem (GB)</cell></row><row><cell>1</cell><cell>0.136</cell><cell>0.233</cell><cell>18.0</cell><cell>95.1</cell><cell>1</cell><cell>1</cell><cell>0.136</cell><cell>0.233</cell><cell>18.0</cell><cell>95.1</cell></row><row><cell>4</cell><cell>0.137</cell><cell>0.235</cell><cell>16.8</cell><cell>43.4</cell><cell>2</cell><cell>4</cell><cell>0.137</cell><cell>0.235</cell><cell>9.8</cell><cell>79.4</cell></row><row><cell>8</cell><cell>0.137</cell><cell>0.237</cell><cell>19.1</cell><cell>20.7</cell><cell>4</cell><cell>8</cell><cell>0.137</cell><cell>0.235</cell><cell>6.5</cell><cell>40.5</cell></row><row><cell>16</cell><cell>0.136</cell><cell>0.235</cell><cell>23.8</cell><cell>10.2</cell><cell>8</cell><cell>16</cell><cell>0.137</cell><cell>0.235</cell><cell>3.4</cell><cell>20.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.5 Twitter</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Finally, we consider the scaling of PBG on a social net-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">work graph in comparison to the Freebase knowledge graph</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">studied in Section 5.4.2. We embed a publicly available</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Twitter 12 subgraph (Kwak et al., 2010) (Boldi &amp; Vigna,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">2004) (Boldi et al., 2011) containing a social network be-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">tween 41,652,230 nodes and 1,468,365,182 edges with a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">single relation called "follow". We construct train, valida-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">tion and test splits of the dataset, which contain 90%, 5%,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">5% of the total edges, respectively.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">In Table 4 we report MRR and Hits@10 after 10 training</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">epochs as well as training time and peak memory usage</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">for different partitioning and parallelization schemes. The</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Facebook AI Research, New York, NY, USA. Correspondence to: Adam Lerer &lt;alerer@fb.com&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">PyTorch-BigGraph: A Large-scale Graph Embedding System</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For knowledge base datasets, state-of-the-art performance is</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This would not matter if we were using an independent loss for positives and negatives, e.g. a binary cross-entropy loss3  The slower convergence may be ameliorated by switching between the buckets ('stratum losses'<ref type="bibr" target="#b9">(Gemulla et al., 2011)</ref>) more frequently, i.e. in each epoch divide the edges from each bucket into N parts and iterate over the buckets N times, operating on one edge part each time.4  In practice, we use Adagrad rather than SGD.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In fact, our approach to batched negative sampling, described in Section 4.3 reduces the number of negatives that must be retrieved so would require less bandwidth than<ref type="bibr" target="#b27">(Ordentlich et al., 2016)</ref> if a parameter server was used.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://www.livejournal.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">www.youtube.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We report both raw and filtered ranking metrics for FB15k as described in<ref type="bibr" target="#b4">(Bordes et al., 2013)</ref>. For the filtered metrics, all edges that exist in the training, validation or test sets are removed from the set of candidate corrupted edges for ranking. This avoids artificially poor results due to true edges from the data being ranked above a test edge.9  Google, Freebase Data Dumps, https://developers.google.com/freebase, Sept. 10, 2018.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">We sample candidate negative nodes according to their prevalence in the data because the full Freebase dataset has such a long-tailed degree distribution that we find that models can achieve &gt; 50% hit@1 against 10, 000 uniformly-sampled negatives, which suggests that it is just performing ranking based on the degree distribution. 11 This I/O overhead is higher on sparser graphs and lower on denser graphs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">www.twitter.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENTS</head><p>We would like to acknowledge Adam Fisch, Keith Adams, Jason Weston, Antoine Bordes and Serkan Piantino for helping to formulate the initial ideas that led to this work, as well as Maximilian Nickel who provided helpful feedback on the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Group formation in large social networks: membership, growth, and evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The webgraph framework i: compression techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on World Wide Web</title>
		<meeting>the 13th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="595" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Layered label propagation: A multiresolution coordinate-free ordering for compressing social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="587" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakhnenko</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">One trillion edges: Graph processing at facebook-scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kabiljo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1804" to="1815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mining graph data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Holder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2999134.2999271" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale matrix factorization with distributed stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sismanis</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2020408.2020426</idno>
		<ptr target="http://doi.acm.org/10.1145/2020408.2020426" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="69" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Freebase data dumps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://developers.google.com/freebase/data" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Highly scalable parallel algorithms for sparse matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="502" to="520" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Type-constrained representation learning in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krompaß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="640" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What is twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krevl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Datasets</surname></persName>
		</author>
		<ptr target="http://snap.stanford.edu/data" />
		<title level="m">Stanford large network dataset collection</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="123" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scaling distributed machine learning with the parameter server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Shekita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-Y</forename><surname>Su</surname></persName>
		</author>
		<idno>978-1-931971-16- 4</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=2685048.2685095" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;14</title>
		<meeting>the 11th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;14<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="583" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A multilevel framework for scalable graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gurukar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mile</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09612</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2015.2483592</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2016-01" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="11" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Networkefficient distributed word2vec training system for large vocabularies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ordentlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cnudde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 25th ACM International Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1139" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepwalk</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2623330.2623732</idno>
		<ptr target="http://doi.acm.org/10.1145/2623330.2623732" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hogwild: A lockfree approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Swivel: Improving embeddings by noticing what&apos;s missing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Waterson</surname></persName>
		</author>
		<idno>abs/1602.02215</idno>
		<ptr target="http://arxiv.org/abs/1602.02215" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scalable learning of collective behavior based on sparse social dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Billion-scale commodity embedding for ecommerce recommendation in alibaba</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02349</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Starspace</surname></persName>
		</author>
		<title level="m">Embed all the things! In Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1412.6575</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>abs/1707.04638</idno>
		<ptr target="http://arxiv.org/abs/1707.04638" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
