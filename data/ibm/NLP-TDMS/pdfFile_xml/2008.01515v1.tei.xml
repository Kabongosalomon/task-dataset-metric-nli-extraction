<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting Multiple ICD-10 Codes from Brazilian-Portuguese Clinical Notes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">D</forename><surname>Reys</surname></persName>
							<email>arthur.reys@3778.care</email>
							<affiliation key="aff0">
								<orgName type="institution">Federal University of Santa Catarina. Florianópolis</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<addrLine>3778 Healthcare. Belo Horizonte</addrLine>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Silva</surname></persName>
							<email>danilo.silva@ufsc.br</email>
							<affiliation key="aff0">
								<orgName type="institution">Federal University of Santa Catarina. Florianópolis</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Severo</surname></persName>
							<email>severo@3778.care</email>
							<affiliation key="aff1">
								<address>
									<addrLine>3778 Healthcare. Belo Horizonte</addrLine>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saulo</forename><surname>Pedro</surname></persName>
							<email>saulo.pedro@3778.care</email>
							<affiliation key="aff1">
								<address>
									<addrLine>3778 Healthcare. Belo Horizonte</addrLine>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcia</forename><forename type="middle">M</forename><surname>De Sousa E Sá</surname></persName>
							<email>marcia.sa@hsl.org.br</email>
							<affiliation key="aff2">
								<orgName type="department">Syrian-Lebanese Hospital</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilherme</forename><forename type="middle">A C</forename><surname>Salgado</surname></persName>
							<email>guilherme@3778.care</email>
							<affiliation key="aff1">
								<address>
									<addrLine>3778 Healthcare. Belo Horizonte</addrLine>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting Multiple ICD-10 Codes from Brazilian-Portuguese Clinical Notes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>ICD coding · Clinical notes · Natural language processing · Multi-label classification · Neural networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ICD coding from electronic clinical records is a manual, timeconsuming and expensive process. Code assignment is, however, an important task for billing purposes and database organization. While many works have studied the problem of automated ICD coding from free text using machine learning techniques, most use records in the English language, especially from the MIMIC-III public dataset. This work presents results for a dataset with Brazilian Portuguese clinical notes. We develop and optimize a Logistic Regression model, a Convolutional Neural Network (CNN), a Gated Recurrent Unit Neural Network and a CNN with Attention (CNN-Att) for prediction of diagnosis ICD codes. We also report our results for the MIMIC-III dataset, which outperform previous work among models of the same families, as well as the state of the art. Compared to MIMIC-III, the Brazilian Portuguese dataset contains far fewer words per document, when only discharge summaries are used. We experiment concatenating additional documents available in this dataset, achieving a great boost in performance. The CNN-Att model achieves the best results on both datasets, with micro-averaged F1 score of 0.537 on MIMIC-III and 0.485 on our dataset with additional documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Throughout the stay of a patient in a hospital, a series of documents are written about their situation, including symptoms, clinical evolution, diagnoses and medical history. After the release of a patient, medical coders analyze their documentation and assign to that stay a list of codes based on the International Classification of Diseases (ICD), a standard system maintained by the World Health Organization <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b26">25]</ref>. Those codes identify a variety of clinical information, which is useful for billing purposes, health plan communication and organizing databases for research and statistical analysis <ref type="bibr" target="#b13">[12]</ref>.</p><p>Currently the ICD coding process is manually performed by specifically trained coders. The granularity of the coding system makes differences between similar codes very subtle. Moreover, much of the information in clinical records comes in unstructured free text and the language used is specific to the medical field, containing abbreviations, ambiguous terms and typos. Together, those factors make manual coding an expensive, time consuming and error-prone task.</p><p>The development of machine learning models over free text from Electronic Health Records (EHR) for automated ICD coding has been discussed for over two decades <ref type="bibr" target="#b16">[15]</ref>. Recently, models based on natural language processing techniques using advanced neural networks have shown relevant performance improvements <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b23">22]</ref>. However, most of these works involve English-language data. To the best of our knowledge, only <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b34">32]</ref> have considered a Portugueselanguage dataset. Except for <ref type="bibr" target="#b8">[8]</ref>, which focuses on a different task of coding the causes of death from death certificates, and <ref type="bibr" target="#b24">[23]</ref>, which aims at predicting groups of oncology ICD codes from pathology reports, all others use small datasets to predict a limited set of ICD codes. Also, none provide comparisons with accessible datasets.</p><p>In this work, we consider the problem of automatically assigning multiple diagnostic ICD codes to a patient stay based on Brazilian Portuguese free-text clinical notes, considering all available codes. Specifically, we develop and compare Logistic Regression (LR), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) and CNN-based attention models with optimized hyperparameters. We present a case study based on data from Syrian-Lebanese Hospital, a Brazilian hospital in São Paulo, where we intend to deploy our best performing model in order to support the ICD tagging process. Additionally, we provide results for the publicly available English-language dataset MIMIC-III (Medical Information Mart for Intensive Care) <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b15">14]</ref>, where we outperform previous work among models of the same families and the current state of the art. <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Previous Work</head><p>In the ICD coding task, researchers often have to decide which codes will be the target of the study. While some works consider all types of ICD codes <ref type="bibr" target="#b38">[36]</ref>, others use a limited amount of ICD codes <ref type="bibr" target="#b34">[32]</ref> or limit the scope to Diagnoses ICD codes <ref type="bibr" target="#b20">[19]</ref>. This is done mainly because of differences in datasets and the large class imbalance observed in the majority of them. As free text inputs for this specific task, most works use discharge summaries, as they condense information about a patient stay in a single document <ref type="bibr" target="#b23">[22]</ref>. However, <ref type="bibr" target="#b8">[8]</ref> and <ref type="bibr" target="#b38">[36]</ref> have experimented using additional documents.</p><p>The structure of the ICD system is used to develop a hierarchical approach to assist predictions in <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b29">[28]</ref>. A method based on ICD co-occurrence is proposed in <ref type="bibr" target="#b35">[33]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, overlaps between ICD descriptions and words in documents compose a rule-based method. More prominently, works use machine learning models such as SVM (Support Vector Machine) <ref type="bibr" target="#b29">[28]</ref>, Naive Bayes <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b27">26]</ref> and kNN (k-Nearest Neighbors) <ref type="bibr" target="#b31">[30]</ref>.</p><p>Convolutional Neural Networks (CNN) have been widely used in the literature, achieving good results in the ICD coding task <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b23">22]</ref>. The advantage of this architecture over more traditional machine learning models (such as LR and SVM) is its capability of capturing local contextual features <ref type="bibr" target="#b20">[19]</ref>. Recurrent Neural Networks have also been used due to their ability to associate information in longer contexts than CNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">10]</ref>. In particular, LSTM (Long Shortterm Memory) and GRU (Gated Recurrent Unit) recurrent networks capture information within a large contextual window. These approaches have achieved improvements over older machine learning models, as free text usually have high complexity and their comprehension rely on local and global semantic relations between terms and sequences.</p><p>In addition to neural networks, innovative models include ensemble of different architectures <ref type="bibr" target="#b37">[35,</ref><ref type="bibr" target="#b38">36]</ref> and per-label attention mechanisms <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b23">22]</ref>. Per-label attention consists of weighing a base representation of documents differently for each ICD code. In the specific task of this work, including only Diagnoses ICD codes, <ref type="bibr" target="#b23">[22]</ref> appears to hold the current state of the art.</p><p>Due to the limited availability of public EHRs, most works focus on MIMIC <ref type="bibr" target="#b15">[14]</ref>, a freely accessible dataset in English language. Works aimed at EHRs in Portuguese are rare and use different private data sources <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b34">32]</ref>. Among these, <ref type="bibr" target="#b6">[6]</ref> shows how an hierarchical approach can improve results in the ICD coding task. An approach based solely on structured data is presented in <ref type="bibr" target="#b9">[9]</ref>. In <ref type="bibr" target="#b34">[32]</ref>, a CNN with self-taught GloVe embeddings is presented to predict a small set of possible codes from free text, while a cost-sensitive learning approach is implemented to overcome class imbalance. These works use relatively small datasets and focus on few codes. In turn, <ref type="bibr" target="#b24">[23]</ref> and <ref type="bibr" target="#b8">[8]</ref> use large collections of data. In <ref type="bibr" target="#b24">[23]</ref>, SVM is used to predict groups of topographical and morphological oncology ICD codes from pathology reports, in a one-versus-all approach. Finally, <ref type="bibr" target="#b8">[8]</ref> uses a recurrent neural network with attention to predict ICD codes corresponding to death causes from death certificates and related documents. However, oncology ICD groups and death causes ICD codes still comprise smaller sets than diagnostic codes, while pathology reports and death certificates have significant structural, semantic and lexical differences from clinical notes such as discharge summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Extraction</head><p>Training a computational model over free text requires some kind of feature extraction method. Among different methods, some encode whole documents into vectors, without regard to the order of the words. This is called a Bag-of-Words (BoW) representation, with the most popular approach being TF-IDF (Term Frequency -Inverse Document Frequency) <ref type="bibr" target="#b32">[31]</ref>. Others generate latent vector representations of words, as Word2Vec <ref type="bibr" target="#b22">[21]</ref>, GloVE <ref type="bibr" target="#b28">[27]</ref> and FastText <ref type="bibr" target="#b2">[3]</ref>, allowing documents to be represented as a sequence of word vectors. More enhanced methods at word level include ELMo <ref type="bibr" target="#b30">[29]</ref> and BERT <ref type="bibr" target="#b7">[7]</ref>. In these methods, the same word can be mapped to different vectors, depending on their surrounding context. Other methods include character-level and paragraph-level representations <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b39">37]</ref>.</p><p>In this work we use TF-IDF features for Logistic Regression and Word2Vec for the neural networks. Hence, a detailed description of these methods is given.</p><p>Term Frequency -Inverse Document Frequency TF-IDF <ref type="bibr" target="#b32">[31]</ref> aims to reflect the importance of a word in a document, given a corpus. Based on a BoW model, a document is converted into a multi-hot encoding of words contained in it, based on vocabulary constructed from the corpus. Then, words are given weights based on their importance for each document. The importance of a word increases proportionally to the number of times it appears in that document (term frequency) and inversely proportional to the total of documents that contain it (inverse document frequency).</p><p>Word2Vec Word2Vec <ref type="bibr" target="#b22">[21]</ref> is a representation model that takes into account order and context of words in documents. The inputs are tokenized texts and the model builds a vocabulary associating words to correspondent fixed-dimension vectors. Tokens in the corpus are projected into a multi-dimensional space, allowing identification of interdependent relations between different terms, through cosine similarity.</p><p>Word2Vec is composed of a single hidden layer neural network. Two methods can be applied in the embedding training: CBoW (Continuous Bag-of-Words) and Skip-gram. In CBoW, a word is predicted from a limited amount of words that precede and succeed it. The context words are converted into BoW features, losing local ordering information between them. In Skip-gram, the task is to predict, from a given word, a limited amount of words around it. In this case, the order of the context words influences the network projection, as nearby words receive higher weights. The latent word representations resulting from Word2Vec training can be loaded as an embedding layer in neural network based models. An embedding layer is a mapping of discrete input variables (e.g. tokens representing words) to corresponding vector representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets and Preprocessing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MIMIC-III Dataset</head><p>The MIMIC-III dataset-the third revision of MIMIC, v1.4-is a publicly accessible English-language dataset that includes numerous tables relative to patients in Beth Israel Deaconess Medical Center, in the United States <ref type="bibr" target="#b15">[14]</ref>. Each admission of a patient to the hospital is associated to several documents, as well as to an ordered list of ICD codes, using the Diagnoses ICD-9-CM (where CM stands for Clinical Modification) coding system, at the most specific level (i.e. subcategories). As the majority of related works, only free text discharge summaries were selected, totaling 52722 hospital admissions from 41127 unique patients. We found a total of 6918 unique ICD codes associated with these documents.</p><p>We perform light preprocessing on the input texts, removing date/hour patterns, special characters and applying lowercase. The same data split as in <ref type="bibr" target="#b23">[22]</ref> was used, consisting of 47719 samples in the training set, 1631 in the validation set, and 3372 in the test set. In this split, no patient is listed in more than one subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HSL Dataset</head><p>The HSL dataset contains de-identified documents linked to patients from the Syrian-Lebanese Hospital (HSL). Collected between 2016 and 2018, texts are written in Brazilian Portuguese. The dataset includes different types of documents in free text. Each document has a hospital admission ID from which different documents can be linked. We removed all admissions that did not have a linked discharge summary, totaling 77005 admissions from 51298 unique patients. Each admission has a list of ICD codes tagged by professional medical coders using Diagnoses ICD-10 codes at the most specific level (i.e. subcategories). We found 5360 unique codes in the dataset.</p><p>Initially, we selected only discharge summaries (S), with each admission containing a single document. This set is referenced as HSL-S. However, after further analysis, we decided to include additional free text documents which were numerously available, in particular: clinical developments (E) and anamnesis/physical exams (A). Unlike discharge summaries, a wide range of types E and A documents are attached to each admission, from none to several texts. <ref type="table" target="#tab_0">Table 1</ref> shows the total of documents per type and the unique admissions and patients linked to these documents. These additional documents were concatenated to type S documents with the same admission ID, to form the input text for each sample. This better reproduces the human coding process that takes place at Syrian-Lebanese Hospital, where coders observe all documents of an admission to determine the correspondent ICD codes. We refer to the dataset with concatenated types S, E and A documents as HSL-SEA. Text preprocessing is done in the same way as in MIMIC-III. We split data ensuring no patient was present in more than one subset, totaling 69309 samples in the training set; 2313 in the validation set; and 5383 in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison Between Datasets</head><p>Besides language, the presented datasets have some relevant differences. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the cumulative distribution of word count per sample in all datasets, after text preprocessing. <ref type="table" target="#tab_0">Table 1</ref> presents differences in the number of documents selected for each dataset. As also shown in <ref type="table" target="#tab_0">Table 1</ref>, MIMIC-III discharge summaries have a much larger average of words per sample than HSL-S. The concatenation of S, E and A documents to HSL-SEA result in an average closer to MIMIC-III. From these statistics, we can assert that MIMIC-III discharge summaries contain, objectively, far more data than HSL-S, while having a closer average and distribution to HSL-SEA. Also, by looking at random samples, we noticed more detailed and well written texts in MIMIC-III.</p><p>The ICD coding systems adopted by the datasets are also different. While MIMIC-III uses a Clinical Modification of ICD-9, HSL uses the newer ICD-10. <ref type="figure" target="#fig_1">Fig. 2</ref> shows histograms of the number of ICD codes per sample for both datasets. While maximum, minimum and standard deviation are similar, the average number of ICD codes per sample is lower in HSL. Note that the classes are extremely imbalanced in both datasets, as some examples show in <ref type="table" target="#tab_1">Table 2</ref>. We also note that 4.47% and 4.48% of the ICD codes contained in the test sets are not present in the training sets, respectively, in MIMIC-III and HSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>In this section we present the evaluation metrics and models used in this work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Metrics</head><p>We used popular metrics for multi-label tasks, namely, F1, precision and recall, all micro-averaged over different classes <ref type="bibr" target="#b20">[19]</ref>. Micro-averaging presents a more representative result considering the large and imbalanced sets of classes, and is indeed used in most works that do not limit the number of ICD codes. Micro-averaged precision and recall are defined, respectively, as</p><formula xml:id="formula_0">P micro = C c=1 N n=1 y c nŷ c n C c=1 N n=1ŷ c n , R micro = C c=1 N n=1 y c nŷ c n C c=1 N n=1 y c n ,<label>(1)</label></formula><p>where C is the number of classes, N is the number of samples and y n andŷ n are, respectively, true and predicted vectors with C binary entries, each indicative of a class c in a sample n. The F1 score (higher is better) is defined as the harmonic mean between precision and recall. Each model (with the exception of the Constant model) outputs, per sample, a vector with C real-valued entries between 0 and 1 corresponding to the confidence of prediction for each class. In particular, if the model was trained on only C &lt; C classes, we assignŷ c n = 0, ∀n, for the remaining classes c not seen by the model. In order to compute the above metrics, we analyze a range of thresholds to binarize outputs, selecting the best one for each model based on F1 in the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Models</head><p>The models developed in this work are described below. We used the Keras 5 framework with Tensorflow 6 backend in all implementations. Models were trained to a maximum of 10 epochs. Instead of applying Early Stopping, after each epoch we computed F1 in the validation set. When training was over, we restored weights corresponding to the epoch with the best result. For our study we used an AWS EC2 virtual machine with 8 vCPUs and a NVIDIA T4 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constant (Top-k)</head><p>The objective of this baseline model is to determine whether the performance of real models is greater than that of an implementation which does not use the input texts.</p><p>The Constant model predicts a constant list of k ICD codes for all samples. The ICD codes selected are the k most occurring in the training set. The parameter k was optimized to obtain the best F1 in the validation sets, resulting in k = 15 for MIMIC-III and k = 8 for HSL.</p><p>Logistic Regression In the LR model, we convert the multi-label problem into a set of binary classification problems, one for each class. The inputs of the LR model are TF-IDF features computed over each dataset.</p><p>TF-IDF was implemented using Scikit-learn 7 . Stopwords were removed from the preprocessed texts, using default Portuguese and English stopwords from Natural Language Toolkit 8 . Maximum vocabulary size was fixed to the 20000 most frequent words.</p><p>The hyperparameters of the LR were optimized via Grid Search considering different optimizers, learning rates from 0.0001 to 0.1 in multiples of 10 and L2 regularizer parameters from 0 (no regularization) to 10, also in multiples of 10. The final model uses Adam optimizer with learning rate 0.1 and all other optimizer parameters set to default values. No regularization is performed. Each training epoch took 50 seconds for MIMIC-III and 60 seconds for HSL-S and HSL-SEA.</p><p>Convolutional Neural Network The CNN implemented in this work consists of an embedding layer loaded with Word2Vec word vectors, followed by a single one-dimensional convolutional layer and Batch Normalization <ref type="bibr" target="#b12">[11]</ref>. On the output, a Global Average Pooling operation precedes a fully connected layer with as many units as the number of classes for each dataset. It is based on the implementation of <ref type="bibr" target="#b23">[22]</ref>, but with some modifications: our tests showed that removing Dropout, adding Batch Normalization and increasing kernel size from 4 to 10 improved results, as well as performing Global Average Pooling instead of Global Max Pooling. The layers and respective parameters are shown in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>We used Adam optimizer with learning rate 0.001 for MIMIC-III and 0.003 for HSL-SEA.</p><p>Given that the CNN model involves Batch Normalization, it is mandatory for the inputs to have fixed sizes <ref type="bibr" target="#b12">[11]</ref>. However, samples have a large variation in number of words, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. To ensure a fixed-length input, texts with fewer words than needed were padded with padding tokens by the end, while texts with more than the maximum of words had their end truncated. The padding token points to a null vector in the embedding layer. Observing the distribution of text sizes among the datasets, the fixed-length of the inputs was set to: 2000, for the MIMIC-III dataset; 300, for HSL-S; and 4000, for HSL-SEA.</p><p>Word2Vec vectors were trained using Gensim 9 . The embeddings were selftrained due to the specificity of the Brazilian Portuguese clinical language, containing medical terms, abbreviations and acronyms <ref type="bibr" target="#b34">[32]</ref>. Words appearing in less than 10 samples were not considered. We experimented vector lengths between 100 and 600, CBoW and Skip-gram implementations, and whether stopwords should be removed. These parameters were optimized for the HSL-S dataset, resulting in vectors with length 300, Skip-gram training algorithm and stopwords not being removed.</p><p>Each epoch took 310 seconds when training for MIMIC-III and 820 seconds for HSL-SEA.</p><p>Recurrent Neural Network The RNN model consists of an embedding layer loaded with Word2Vec word vectors, followed by a GRU layer. Then, Batch Normalization and Global Average Pooling are performed. In the output we define a fully connected layer with as many units as the number of classes for each dataset. As in the CNN model, the samples were processed to fit in a fixedlength input, in this case to allow faster training on the GPU. In this work, we used GRU layers for their better results over traditional RNNs, while keeping a simpler architecture (and being more quickly trainable) than LSTMs <ref type="bibr" target="#b3">[4]</ref>.</p><p>Three base architectures were tested: the first one is such as shown in <ref type="table" target="#tab_2">Table 3</ref>; the second has an extra GRU layer; the last has a bidirectional GRU instead of a common GRU. The first architecture yielded the best results, so each parameter was then individually optimized from this base architecture.</p><p>Among the optimized parameters are: optimizer; learning rates from 4e-4 to 1e-2 in steps of 1e-4; masking of padding tokens, to avoid their influence on model predictions; sample weighting inversely proportional to the number of true ICD codes; fine-tuning of the embedding layer; and Pooling methods. Adam optimizer with 8e-4 learning rate resulted in improvements in F1, so as fine-tuning the embedding layer. Average Pooling proved to be greatly superior than Max Pooling. The final architecture is shown in <ref type="table" target="#tab_2">Table 3</ref>. This model is referred simply as GRU in the next sections.</p><p>The GRU model uses the same Word2Vec vectors trained for the CNNs. Training times per epoch were 268 seconds for MIMIC-III and 785 seconds for HSL-SEA.</p><p>Convolutional Neural Network with Attention The CNN model with Attention (CNN-Att) is based on the current state of the art CAML (Convolutional Attention for Multi-Label Classification) <ref type="bibr" target="#b23">[22]</ref>, with some modifications. The model is also similar to our conventional CNN model, with the only difference that the Global Pooling is replaced by a per-label attention mechanism (which computes a separate context vector for each label as a weighted average of the input sequence) and each fully-connected sigmoid output unit takes as input only its corresponding context vector. The attention operation is a scaled dotproduct <ref type="bibr" target="#b36">[34]</ref> and uses a separate trainable target vector for each label (see <ref type="bibr" target="#b23">[22]</ref> for details).</p><p>Compared to the original CAML model, we removed Dropout from the embedding layer, which in our initial experiments did not seem to improve the performance, and added Batch Normalization after the convolutional layer, since it typically allows for a faster convergence of training. We increased the number of filters in the convolutional layer from 50 to 500. These modifications improved metrics in our tests. Also, to allow faster convergence, we scheduled the learning rate to start at 0.001 in the first two epochs, and only then decrease to 0.0001. <ref type="table" target="#tab_2">Table 3</ref> presents the architecture and parameters used in the CNN-Att.</p><p>Following our other neural network models, we used Word2Vec word embeddings, and the samples were processed to fit a fixed-length input (see the CNN model subsection). Training the CNN-Att took 1600 seconds per epoch for MIMIC-III and 3700 seconds per epoch for HSL-SEA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>We trained our models using MIMIC-III and HSL datasets. This section shows achieved results and comparisons, as well as experiments regarding additional documents in HSL.   <ref type="bibr" target="#b23">[22]</ref> and SVM <ref type="bibr" target="#b20">[19]</ref> linear models, presented as baselines in these works. This suggests that these models were underfitting due to lack of hyperparameter optimization; indeed, we noticed that the LR from <ref type="bibr" target="#b23">[22]</ref> used a default L2 regularization parameter of 1, while we adopted no L2 regularization. The F1 achieved by our LR is comparable to CNN implementations with Word2Vec features found in <ref type="bibr" target="#b20">[19]</ref> and <ref type="bibr" target="#b23">[22]</ref>, while our CNN shows an improvement over these models. The GRU returns significant improvements over all previous models, as well as over a similar model presented in <ref type="bibr" target="#b23">[22]</ref>. Finally, the CNN-Att outperforms all other models, including the original CAML <ref type="bibr" target="#b23">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">HSL Results</head><p>For the HSL dataset, we first selected only discharge summaries (HSL-S), to allow a more direct comparison with MIMIC-III, which uses only this type of document. As HSL-S and MIMIC-III are very different datasets, we did not expect identical results. Even so, when training the LR model, the results we obtained were much lower than expected, namely, an F1 of 0.316, which is 20% below that of MIMIC-III.</p><p>These results, as well as the fact that HSL-S has a considerably lower average of words per sample than MIMIC-III, lead our study to experiment with other documents available in HSL. We trained the LR model on different combinations of concatenated documents: types S and A; types S and E; and types S, A and E (refer to Section 3.2 for an explanation of each document type). <ref type="table">Table 5</ref> presents metrics computed over the validation set. Clearly, adding documents to discharge summaries-thus increasing average words per sample-shows improvements in metrics, with a large increase in F1 when using HSL-SEA.</p><p>Considering the outcomes of these experiments, we then trained all models on HSL-SEA. As CNN and RNN are sensitive to the order of concatenation of documents, we experimented orders S-A-E and S-E-A. We adopted the latter one, as it achieved slightly better results. Compared to HSL-S, we achieved consistently better results when using HSL-SEA, for all models. Metrics on the HSL-SEA test set are shown in <ref type="table">Table 6</ref>. Once more, the CNN is slightly superior than the LR, while the GRU model shows improvements over both of those models. The CNN-Att model presents again the best results, significantly ahead of all other models. Note that each model on HSL-SEA achieves a performance comparable to (up to about 10% below) that same model on MIMIC-III. This is evidence that HSL-SEA has comparable quality to MIMIC-III discharge summaries for ICD code prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This work presented a study on automated ICD coding from free text, using four learning models trained on two datasets. For MIMIC-III, we reproduced and improved results of similar models in the literature, outperforming the state of the art on the prediction of diagnosis codes from discharge summaries. Results show that using a CNN with per-label attention outperforms conventional CNN, GRU and LR models, attaining a Micro-F1 of 0.537.</p><p>For the HSL dataset, we observed that using only discharge summaries was insufficient to achieve results similar to MIMIC-III. Besides the different coding system, word count statistics and detail levels in documents may explain the loss in performance. After concatenating additional documents found in HSL, we observed a significant improvement. Again, the best performance was achieved by our optimized CNN-Att model, with a Micro-F1 of 0.485.</p><p>We believe our best model trained on HSL could be suited to assist medical coders using clinical records in Brazilian Portuguese, allowing for gains in efficiency and a decrease in errors in the manual ICD tagging process. We are working towards the deployment of a pilot trial to test the usefulness of the model and better understand its limitations in a practical setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Word count per sample cumulative distribution on all datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>ICD count per sample histograms on MIMIC-III and HSL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of document types in MIMIC-III and HSL datasets. Dataset Unique patients Admissions Total documents Avg. words per sample a Concatenation of all documents corresponding to the same admission.</figDesc><table><row><cell>MIMIC-III b</cell><cell>41127</cell><cell>52722</cell><cell>52722</cell><cell>1327.5</cell></row><row><cell>HSL-S</cell><cell>51298</cell><cell>77005</cell><cell>77005</cell><cell>94.6</cell></row><row><cell>HSL-E</cell><cell>50899</cell><cell>76159</cell><cell>919713</cell><cell>1483.0</cell></row><row><cell>HSL-A</cell><cell>42153</cell><cell>59249</cell><cell>63423</cell><cell>155.4</cell></row><row><cell>HSL-SEA</cell><cell>51298</cell><cell>77005</cell><cell>1060141</cell><cell>1730.4</cell></row></table><note>ab Only discharge summaries.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Percentage of samples tagged with the 1st, 10th, 100th and 1000th most frequent ICD codes on MIMIC-III and HSL.</figDesc><table><row><cell cols="3">Dataset MIMIC-III HSL</cell></row><row><cell>1st</cell><cell cols="2">38.02% 34.37%</cell></row><row><cell>10th</cell><cell cols="2">11.67% 10.71%</cell></row><row><cell>100th</cell><cell>2.23%</cell><cell>1.26%</cell></row><row><cell>1000th</cell><cell>0.15%</cell><cell>0.06%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Architectures and parameters for the neural network models.</figDesc><table><row><cell>CNN</cell><cell>GRU</cell><cell>CNN-Att</cell></row><row><cell>Input</cell><cell>Input</cell><cell>Input</cell></row><row><cell>Embedding (size 300)</cell><cell>Embedding (size 300)</cell><cell>Embedding (size 300)</cell></row><row><cell>Conv1D</cell><cell>GRU</cell><cell>Conv1D</cell></row><row><cell>(500 filters, kernel 10, tanh)</cell><cell>(500 units, tanh)</cell><cell>(500 filters, kernel 10, tanh)</cell></row><row><cell>Batch Normalization</cell><cell>Batch Normalization</cell><cell>Batch Normalization</cell></row><row><cell cols="2">GlobalAveragePooling1D GlobalAveragePooling1D</cell><cell>Attention</cell></row><row><cell>Output (sigmoid)</cell><cell>Output (sigmoid)</cell><cell>Output (sigmoid)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Performance of different models on MIMIC-III dataset. Entries with no citation brackets correspond to our models.</figDesc><table><row><cell>Model</cell><cell cols="4">Threshold F1 Precision Recall</cell></row><row><cell>Constant</cell><cell>-</cell><cell cols="3">0.192 0.188 0.196</cell></row><row><cell>LR [22]</cell><cell>-</cell><cell>0.242</cell><cell>-</cell><cell>-</cell></row><row><cell>flat-SVM [19]</cell><cell>-</cell><cell cols="3">0.253 0.635 0.158</cell></row><row><cell>LR</cell><cell>0.19</cell><cell cols="3">0.406 0.425 0.388</cell></row><row><cell>CNN [22]</cell><cell>-</cell><cell>0.402</cell><cell>-</cell><cell>-</cell></row><row><cell>CNN [19]</cell><cell>-</cell><cell cols="3">0.399 0.440 0.366</cell></row><row><cell>CNN</cell><cell>0.30</cell><cell cols="3">0.423 0.467 0.387</cell></row><row><cell>Bi-GRU [22]</cell><cell>-</cell><cell>0.393</cell><cell>-</cell><cell>-</cell></row><row><cell>GRU</cell><cell>0.32</cell><cell cols="3">0.468 0.543 0.412</cell></row><row><cell>CAML [22]</cell><cell>-</cell><cell>0.524</cell><cell>-</cell><cell>-</cell></row><row><cell>CNN-Att</cell><cell>0.28</cell><cell cols="3">0.537 0.590 0.492</cell></row><row><cell>5.1 MIMIC-III Results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>shows the results obtained for all models on the MIMIC-III test set. As baselines for comparison, we also present results from other works in the literature.The Constant model achieves very poor results, as expected. Our LR with optimized hyperparameters greatly outperforms similar LR</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Validation metrics of LR model trained over HSL considering different concatenated document types. Performance of different models for HSL-SEA dataset.</figDesc><table><row><cell cols="3">Documents Threshold F1 Precision Recall</cell></row><row><cell>S</cell><cell>0.26</cell><cell>0.316 0.320 0.312</cell></row><row><cell>S and A</cell><cell>0.25</cell><cell>0.347 0.359 0.336</cell></row><row><cell>S and E</cell><cell>0.27</cell><cell>0.357 0.382 0.336</cell></row><row><cell>S, E and A</cell><cell>0.25</cell><cell>0.367 0.390 0.346</cell></row><row><cell cols="3">Model Threshold F1 Precision Recall</cell></row><row><cell>Constant</cell><cell>-</cell><cell>0.203 0.183 0.228</cell></row><row><cell>LR</cell><cell>0.25</cell><cell>0.368 0.400 0.340</cell></row><row><cell>CNN</cell><cell>0.26</cell><cell>0.374 0.386 0.363</cell></row><row><cell>GRU</cell><cell>0.29</cell><cell>0.441 0.508 0.390</cell></row><row><cell>CNN-Att</cell><cell>0.29</cell><cell>0.485 0.543 0.438</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Code for MIMIC-III is available at https://github.com/3778/icd-prediction-mimic.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://keras.io/ 6 https://tensorflow.org/ 7 https://scikit-learn.org/ 8 https://www.nltk.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://radimrehurek.com/gensim/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Ricardo Giglio, Dr. Mauro Cardoso, Dr. Flávio Amaro, and Marcio Gregory for helpful discussions, as well as 3778 Healthcare and Syrian-Lebanese Hospital for their support of this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tagging Patient Notes With ICD-9 Codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ayyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Bear Don&amp;apos;t Walk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th NIPS</title>
		<meeting>the 29th NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-label classification of patient notes a case study on icd code assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baumel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS 2014 Workshop on Deep Learning</title>
		<meeting>the NIPS 2014 Workshop on Deep Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic code assignment to medical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on BioNLP</title>
		<meeting>the Workshop on BioNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">129</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="DOI">10.3115/1572392.1572416</idno>
		<ptr target="https://doi.org/10.3115/1572392.1572416" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A hierarchical approach to the automatic categorization of medical documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R S</forename><surname>De Lima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H F</forename><surname>Laender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<idno type="DOI">10.1145/288627.288649</idno>
		<ptr target="https://doi.org/10.1145/288627.288649" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th CIKM</title>
		<meeting>the 7th CIKM</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT 2019</title>
		<meeting>the NAACL-HLT 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep neural models for ICD-10 coding of death certificates and autopsy reports in free-text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Duarte</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2018.02.011</idno>
		<ptr target="https://doi.org/10.1016/j.jbi.2018.02.011" />
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using Structured EHR Data and SVM to Support ICD-9-CM Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferrão</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICHI.2013.79</idno>
		<ptr target="https://doi.org/10.1109/ICHI.2013.79" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE ICHI</title>
		<meeting>the 2013 IEEE ICHI</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="511" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Empirical Evaluation of Deep Learning for ICD-9 Code Assignment using MIMIC-III Clinical Notes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Osorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Sy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="141" to="153" />
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.cmpb.2019.05.024</idno>
		<ptr target="https://doi.org/10.1016/j.cmpb.2019.05.024" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd ICML</title>
		<meeting>the 32nd ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining Electronic Health Records: Towards Better Research Applications and Clinical Care</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Brunak</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrg3208</idno>
		<ptr target="https://doi.org/10.1038/nrg3208" />
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="395" to="405" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mark</surname></persName>
		</author>
		<idno type="DOI">10.13026/C2XW26</idno>
		<ptr target="https://doi.org/10.13026/C2XW26" />
		<title level="m">The MIMIC III Clinical Database</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2016.35</idno>
		<ptr target="https://doi.org/10.1038/sdata.2016.35" />
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160035</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatic Assignment of ICD9 Codes To Discharge Summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<pubPlace>Amherst, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ICML</title>
		<meeting>the 31st ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02768</idno>
		<title level="m">Convolutional Neural Networks for Medical Diagnosis from Admission Notes</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ICD Coding from Clinical Text Using Multi-Filter Residual Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of he 34th AAAI Conference on Artificial Intelligence</title>
		<meeting>he 34th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated ICD-9 Coding via A Deep Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCBB.2018.2817488</idno>
		<ptr target="https://doi.org/10.1109/TCBB.2018.2817488" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1193" to="1202" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Machine learning and features selection for semi-automatic ICD-9-CM encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Medori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fairon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents</title>
		<meeting>the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents<address><addrLine>Los Angeles, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="84" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICLR Workshop</title>
		<meeting>the ICLR Workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Explainable Prediction of Medical Codes from Clinical Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mullenbach</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1100</idno>
		<ptr target="https://doi.org/10.18653/v1/N18-1100" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 NAACL-HLT</title>
		<meeting>the 2018 NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1101" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated Classification of Semi-Structured Pathology Reports into ICD-O Using SVM in Portuguese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oleynik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F C</forename><surname>Patrão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Health Technology and Informatics</title>
		<imprint>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page" from="256" to="260" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">International Classification of Diseases : [9th] Ninth Revision, Basic Tabulation List with Alphabetic Index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Organization</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Health Organization</title>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ICD-10 : international statistical classification of diseases and related health problems : tenth revision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Organization</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Health Organization</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automating the assignment of diagnosis codes to patient encounters using example-based and machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V S</forename><surname>Pakhomov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Buntrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Chute</surname></persName>
		</author>
		<idno type="DOI">10.1197/jamia.M2077</idno>
		<ptr target="https://doi.org/10.1197/jamia.M2077" />
	</analytic>
	<monogr>
		<title level="j">JAMIA</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="516" to="525" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 EMNLP</title>
		<meeting>the 2014 EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Diagnosis Code Assignment: Models and Evaluation Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perotte</surname></persName>
		</author>
		<idno type="DOI">10.1136/amiajnl-2013-002159</idno>
		<ptr target="https://doi.org/10.1136/amiajnl-2013-002159" />
	</analytic>
	<monogr>
		<title level="j">JAMIA</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="231" to="237" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
		<ptr target="https://doi.org/10.18653/v1/N18-1202" />
		<title level="m">Deep Contextualized Word Representations</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the 2018 NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From Episodes of Care to Diagnosis Codes: Automatic Text Categorization for Medico-Economic Encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AMIA Annual Symposium</title>
		<meeting>the AMIA Annual Symposium</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="636" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Term-Weighting Approaches in Automatic Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/0306-4573(88)90021-0</idno>
		<ptr target="https://doi.org/10.1016/0306-4573(88)90021-0" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using Deep Convolutional Neural Networks with Self-Taught Word Embeddings to Perform Clinical Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B V</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Gumiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Iberoamerican Journal of Applied Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Method for Modeling Co-Occurrence Propensity of Clinical Codes with Application to ICD-10-PCS Auto-Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subotin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocv201</idno>
		<ptr target="https://doi.org/10.1093/jamia/ocv201" />
	</analytic>
	<monogr>
		<title level="j">JAMIA</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="866" to="871" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st NIPS</title>
		<meeting>the 31st NIPS<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Neural Architecture for Automated ICD Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1098</idno>
		<ptr target="https://doi.org/10.18653/v1/P18-1098" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th ACL</title>
		<meeting>the 56th ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1066" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multimodal Machine Learning for Automated ICD Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Machine Learning for Healthcare Conference</title>
		<meeting>the 4th Machine Learning for Healthcare Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Character-level Convolutional Networks for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th NIPS</title>
		<meeting>the 28th NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
