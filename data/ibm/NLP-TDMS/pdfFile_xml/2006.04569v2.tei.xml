<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parameter-Efficient Person Re-identification in the 3D Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
							<email>zhe-dong.zheng@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Artificial Intelligence Institute (AAII)</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Artificial Intelligence Institute (AAII)</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Parameter-Efficient Person Re-identification in the 3D Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Noname manuscript No. (will be inserted by the editor) the date of receipt and acceptance should be inserted later</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Person re-identification · 3D human representation · Image retrieval · Point cloud · Graph convolutional networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>People live in a 3D world. However, existing works on person re-identification (re-id) mostly consider the semantic representation learning in a 2D space, intrinsically limiting the understanding of people. In this work, we address this limitation by exploring the prior knowledge of the 3D body structure. Specifically, we project 2D images to a 3D space and introduce a novel parameter-efficient Omni-scale Graph Network (OG-Net) to learn the pedestrian representation directly from 3D point clouds. OG-Net effectively exploits the local information provided by sparse 3D points and takes advantage of the structure and appearance information in a coherent manner. With the help of 3D geometry information, we can learn a new type of deep re-id feature free from noisy variants, such as scale and viewpoint. To our knowledge, we are among the first attempts to conduct person re-identification in the 3D space. We demonstrate through extensive experiments that the proposed method (1) eases the matching difficulty in the traditional 2D space, (2) exploits the complementary information of 2D appearance and 3D structure, (3) achieves competitive results with limited parameters on four large-scale person re-id datasets, and (4) has good scalability to unseen datasets. Our code, models and generated 3D human data are publicly available at https://github.com/layumi/person-reid-3d.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Person re-identification is usually regarded as an image retrieval problem of spotting the person in nonoverlapping cameras <ref type="bibr" target="#b7">[Gong et al., 2014</ref><ref type="bibr" target="#b79">, Zheng et al., 2016</ref><ref type="bibr" target="#b82">, Zheng et al., 2018a</ref><ref type="bibr" target="#b69">, Ye et al., 2020</ref>. Due to the rising demand of public safety and the fast development of camera network, person re-id has received increasing interests. These studies aim to save the human resource and efficiently find the person of interest, e.g., lost child in the airport, from thousands of candidate images. In recent years, the advance of person re-id is mainly due to two factors: 1) the availability of large-scale datasets and 2) the deeply-learned person representation. On one hand, deeply-learned models are usually data-hungry. The large-scale datasets <ref type="bibr" target="#b78">[Zheng et al., 2015</ref><ref type="bibr" target="#b29">, Liu et al., 2016</ref><ref type="bibr" target="#b59">, Wei et al., 2018</ref> facilitate the data-driven approaches. On the other hand, the development of Convolutional Neural Network (CNN) also provides the technical breakthrough of the pedestrian representation learning. Many efforts have been paid to improve the CNN-based model capability <ref type="bibr" target="#b87">[Zhou et al., 2019</ref><ref type="bibr" target="#b36">,Qian et al., 2017</ref><ref type="bibr" target="#b37">,Qian et al., 2019</ref>. Recently, some researchers and companies also claim that the model can surpass the human performance .</p><p>However, one inherent problem still remains: does the model really understand the person? People live in a 3D world. In contrast, we notice that most prevailing person re-id methods ignore the prior knowledge that human is a 3D non-rigid object, and only focus on learning the representation in 2D space. Although some pioneering works <ref type="bibr">[Barbosa et al., 2018, Sun and</ref> consider the 3D human structure, the pedestrian representation is still learned from the projected 2D arXiv:2006.04569v2 [cs.CV] 30 Oct 2020 <ref type="figure">Fig. 1</ref> Our brain generally associates the 2D appearance with prior knowledge of the 3D body shape. In this work, we intend to simulate this process and explore robust pedestrian representation with a lightweight model. (Dash arrows are missing in prevailing re-id methods.)</p><p>images. For instance, one of the existing works, Per-sonX , has applied the game engine to build 3D person models. However, representation learning is conducted in the 2D space by projecting the 3D model back to 2D images. This line of works is effective in data augmentation but might be suboptimal in representation learning. It is because the 2D data space intrinsically limits the model to understand the 3D geometry information of the person.</p><p>Inspired by the human ability of associating the 2D appearance with the 3D geometry structure (see <ref type="figure">Figure 1)</ref>, we argue that the key to learning an effective and scalable person representation is to consider the complementary information of 2D human appearance and 3D geometry structure. With the prior knowledge of 3D human geometry information, we could learn a depthaware model, thus making the representation robust to real-world scenarios. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, we map the visible surface to the human mesh, and make the person free from the 2D space. The intuition is that after mapping to the 3D space, the appearance information is correlated/aligned with the human structure. Without the need to worry about the part matching from two different viewpoints, the 3D data structure eases the matching difficulty in nature. The model could concentrate on learning the identity-related features, and dealing with the other intra-class variants, such as illumination conditions.</p><p>To fully take advantage of the 3D structure and 2D appearance, we propose a novel Omni-scale Graph Network for person re-id in the 3D space, called OG-Net. OG-Net is a parameter-efficient model based on graph neural network (GNN) to communicate between the discrete cloud points of arbitrary locations. Given the 3D point cloud and the corresponding color information, OG-Net predicts the person identity and outputs the robust human representation for subsequent matching. Following the spirit of the conventional convolutional neural network (CNN), we utilize 3D points to build the location topology, and deploy the corresponding RGB color to extract appearance information.</p><p>In particular, we propose Omni-scale module to aggregate the feature from multiple 3D receptive fields, which leverages multi-scale information in 3D data. Even though the basic OG-Net only consists of four Omni-scale modules, it has achieved competitive performance on four person re-id datasets.</p><p>Contribution. Our contributions are as follows. <ref type="formula" target="#formula_0">(1)</ref> We study person re-identification in the 3D space -a realistic scenario which could better reflect the nature of the 3D non-rigid human. To our knowledge, this work is among the early attempts to address this problem. <ref type="formula" target="#formula_2">(2)</ref> We propose a novel Omni-scale Graph Network to learn the feature from both human appearance and 3D geometry structure in a coherent manner. OG-Net leverages discrete 3D points to capture the multi-scale identity information.</p><p>(3) Extensive experiments on four person re-id benchmarks show the proposed method could achieve competitive performance with limited parameters. A more realistic transfer learning setting is also studied in this paper. We observe that OG-Net has good scalability to the unseen person re-id dataset.</p><p>2 Related work 2.1 Semantic Space for Person Re-id Recent years, convolutional neural network (CNN) models have been explored to map the pedestrian inputs, e.g., images, into one shared semantic space, where the data of the same identity is close and the data of different identities is apart from each other . Different optimization objectives have been studied. For instance, the contrastive loss is widely-used to discriminate different identities <ref type="bibr" target="#b82">, Zheng et al., 2018a</ref><ref type="bibr" target="#b24">, Lin et al., 2018</ref>, while the identification loss deploys the identity classification as the pretext task <ref type="bibr" target="#b79">[Zheng et al., 2016</ref><ref type="bibr" target="#b86">, Zhong et al., 2018</ref><ref type="bibr" target="#b70">, Yu et al., 2017</ref>. To simultaneously minimize the intra-class difference and maximize the inter-class gap, the triplet loss with different hard sampling strategies are also widely-studied <ref type="bibr" target="#b12">[Hermans et al., 2017</ref><ref type="bibr" target="#b40">, Ristani and Tomasi, 2018</ref>. <ref type="bibr" target="#b63">Xiao et al. [Xiao et al., 2017]</ref> propose the online instance matching loss to view the unlabeled data as negative samples, while <ref type="bibr" target="#b81">Zheng et al. [Zheng et al., 2017]</ref> design one label smooth loss to take advantage of synthetic data. Besides, several works <ref type="bibr" target="#b26">[Lin et al., 2019</ref><ref type="bibr" target="#b54">, Wang et al., 2018c</ref> utilize person attributes, e.g., gender, to help the model learning intermediate features. This line of works is orthogonal to our work -any semantic spaces or optimization objectives can be used in our work and better ones can benefit our approach. In this work, we do not intend to pursue the best semantic space, but Person is a 3D non-rigid object. In this work, we conduct the person re-identification in the 3D space, and learn a new type of robust re-id feature. Given one 2D image (a), we first (b) estimate the 3D pose via the off-the-shelf model <ref type="bibr" target="#b18">[Kanazawa et al., 2018]</ref>, followed by (c) mapping the RGB color of visible surfaces to corresponding points. The invisible parts are made transparent for visualization. (d) The appearance information is aligned with the human structure. We make the person free from the 2D space, and thus ease the matching difficulty.</p><p>focus on verifying the effectiveness of the 3D space and the proposed OG-Net. We, therefore, deploy the basic identification loss for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Part Matching for Person Re-id</head><p>To obtain the discriminative pedestrian representation, one line of research works resorts to mining local patterns, such as bodies, legs and arms, on 2D image inputs. The part matching is usually conducted on two different levels, i.e., the pixel level <ref type="bibr" target="#b44">[Su et al., 2017</ref><ref type="bibr" target="#b76">, Zhang et al., 2019</ref><ref type="bibr" target="#b77">, Zheng et al., 2019a</ref> and the feature level <ref type="bibr" target="#b52">, Wang et al., 2018b</ref>. The pixellevel part matching directly transforms the input image to one unified form. For instance, <ref type="bibr" target="#b44">Su et al. [Su et al., 2017]</ref> and <ref type="bibr" target="#b77">Zheng et al. [Zheng et al., 2019a]</ref> deploy the off-the-shelf pose estimator <ref type="bibr" target="#b60">[Wei et al., 2016]</ref> to predict the human key points, followed by cropping and resizing body parts for representation learning. Similarly, Zhang et al. <ref type="bibr" target="#b76">[Zhang et al., 2019]</ref> utilize the semantic segmentation predictor to crop and align body parts densely. Instead of cropping body parts, <ref type="bibr" target="#b42">Saquib et al. [Saquib Sarfraz et al., 2018]</ref> concatenate the rgb input with key point heatmap as input, and let model to learn the part attention by itself. In contrast, another line of works align the parts coarsely on the feature level, given that pedestrians usually stand in the image and are horizontally aligned in nature. Based on this assumption, Sun et al.  propose to split feature maps horizontally and learn the part feature in a relatively large receptive field. Taking one more step, MGN <ref type="bibr" target="#b52">[Wang et al., 2018b]</ref> explores more partition strategies and fuses different loss functions, further improving the performance. To obtain more finegrained information, several works et al. <ref type="bibr" target="#b17">[Kalayeh et al., 2018</ref><ref type="bibr" target="#b45">, Suh et al., 2018</ref><ref type="bibr" target="#b6">, Gao et al., 2020</ref><ref type="bibr" target="#b32">, Miao et al., 2019</ref><ref type="bibr" target="#b20">,Lan et al., 2020</ref> introduce one extra human parsing branch to provide part matching information in the feature level. Besides, to address the misdetection of the input image, Zheng et al. <ref type="bibr" target="#b83">[Zheng et al., 2018b]</ref> apply the spatial transformer network <ref type="bibr" target="#b16">[Jaderberg et al., 2015]</ref> to re-align feature maps. Different from existing works on part alignment in 2D space, the proposed method explores the 3D body structure, which is more close to the prior knowledge of human -a 3D non-rigid object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning from Synthetic Data</head><p>Another active research line is to leverage the synthetic human data. Although most datasets <ref type="bibr" target="#b78">[Zheng et al., 2015</ref><ref type="bibr" target="#b39">, Ristani et al., 2016</ref> provide more training data in recent years, the number of images per person is still limited . Therefore, the intra-class variants of every training pedestrian are limited, which largely compromise the model learning and hurt the model scalability to the real-world scenario. To address the data limitation, one line of existing works leverages the generative adversarial network (GAN) <ref type="bibr" target="#b8">[Goodfellow et al., 2014]</ref> to synthesize more high-quality training images, and let the model "see" more appearance vari-ants to learn the robust representation <ref type="bibr" target="#b7">, Ge et al., 2018</ref><ref type="bibr" target="#b4">, Eom and Ham, 2019</ref><ref type="bibr" target="#b38">, Qian et al., 2018</ref><ref type="bibr" target="#b27">, Liu et al., 2018</ref><ref type="bibr" target="#b86">, Zhong et al., 2018</ref><ref type="bibr" target="#b89">, Zou et al., 2020</ref>. <ref type="bibr" target="#b81">Zheng et al. [Zheng et al., 2017]</ref> first propose a new label smooth regularization for outliers to leverage imperfect generated images. In a similar spirit, Huang et al. <ref type="bibr" target="#b15">[Huang et al., 2018]</ref> deploy the pseudo label learning to assign refined labels for synthetic data. <ref type="bibr" target="#b38">Qian et al. [Qian et al., 2018]</ref> modify the generation model and add pedestrian images with different poses into training set, yielding the poseinvariant features. Inspired by the conventional encoderdecoder manner, Ge et al. <ref type="bibr" target="#b7">[Ge et al., 2018]</ref> propose FD-GAN to learn one pose-invariant feature when encoding the input image. DG-Net  disentangles the pedestrian image to two embeddings, i.e., appearance code and structure code, to generate diverse and realistic synthetic images. With the highquality synthetic data, more discriminative feature can be learned, in turn, improving re-id performance. Furthermore, several works <ref type="bibr" target="#b3">[Deng et al., 2018</ref><ref type="bibr" target="#b86">,Zhong et al., 2018</ref><ref type="bibr" target="#b85">, Zhong et al., 2020</ref><ref type="bibr" target="#b59">, Wei et al., 2018</ref> also apply GAN, i.e., CycleGAN , to cross-domain person re-identification by training the model with the target-style synthetic data. In contrast, another line of works <ref type="bibr" target="#b49">,Tang et al., 2019a</ref><ref type="bibr" target="#b68">, Yao et al., 2019</ref> is close to our work, which applies the game engine to build 3D models. Sun et al.  build a large number of 3D person models, and map models to 2D plane for generating more 2D training data. <ref type="bibr">Yao et al. [Yang et al., 2018b]</ref> and <ref type="bibr" target="#b49">Tang et al. [Tang et al., 2019a]</ref> manipulate the generation setting and leverage attributes, e.g., color and pose, to enable multi-task learning on 2D synthetic data. <ref type="bibr" target="#b25">Lin et al. [Lin et al., 2020]</ref> also leverage the synthetic data to learn the common knowledge of human structure, improving the model scalability on real data. However, different from our work, the above-mentioned studies are mostly investigated in the 2D space, and neglect the 3D geometry information of human bodies. In this work, we argue that the 3D space with the geometry knowledge could help to learn a new type of feature free from several intra-class visual variants, such as viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Learning from Point Clouds</head><p>The point cloud is a flexible geometric representation of 3D data structure, which could be obtained by most 3D data acquisition devices, such as radar. The point cloud data is usually unordered, and thus the conventional convolutional neural network (CNN) could not directly work on this kind of data. One of the earliest works, i.e., PointNet <ref type="bibr" target="#b34">[Qi et al., 2017a]</ref>, proposes to leverage the multi-layer perceptron (MLP) networks and max-pooling layer to fuse the information from multiple points. PointNet++ <ref type="bibr" target="#b35">[Qi et al., 2017b]</ref> takes one more step by introducing the sampling layer to distill salient points. To address the limitation in decoding, FoldingNet  adds one constant 2D plane to simulate the surface of 3D objects. However, the communication between the 3D points is still limited, and each point is treated independently most of the time. Therefore, Wang et al.  propose to leverage Graph Neural Network (GNN) <ref type="bibr" target="#b43">[Scarselli et al., 2008]</ref> to enable the information spread between the k-nearest points. Li et al.  take one more step and propose to deploy a deeper graph neural network structure, further boosting the performance. Similarly, in this work, we regard every person as one individual graph, while every RGB pixel and the corresponding location are viewed as one node in the graph. More details are provided in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We show a schematic overview of our framework in <ref type="figure">Figure</ref> 3. We next introduce some notations and assumptions, followed by the details of how to learn from 3D points, and how to take advantage of 2D appearance information and 3D structure in one coherent manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries and Notations</head><p>To conduct person re-identification in the 3D space, we first change the data structure of inputs. In particular, given one person re-id dataset, 2D images are mapped to the 3D space via the off-the-shelf 3D pose estimation <ref type="bibr" target="#b18">[Kanazawa et al., 2018]</ref>. We apply this mapping function to every image in the dataset to obtain 3D point clouds aligned with the 2D appearance. We denote the generated point sets and identity labels as S = {s n } N n=1 and Y = {y n } N n=1 , where N is the number of samples in the dataset, y n ∈ [1, K], and K is the number of the identity categories. We utilize the matrix format to illustrate the point cloud s n ∈ R m×6 , where m is the number of points, and 6 is the channel number. The former 3 channels contain 3D coordinates XYZ, while the latter 3 channels contain the corresponding RGB information. Given one 3D data s n ∈ R m×6 , our work intend to learn a mapping function F which projects the input s n to the identity-aware representation f n = F Θ (s n ) with learnable parameters Θ. Unlike the conventional image format, the 3D point clouds are unordered and Given the point cloud of (m × 6), we split the geometry location b 0 and the rgb color data a 0 . The 3D location information, i.e., (x,y,z), is to build the KNN graph, while the rgb data is to extract the appearance feature as the conventional 2D CNNs. We progressively downsample the number of selected points {m, 768, 384, 192, 96}, while increasing the appearance feature length {3, 64, 128, 256, 512}. For the last KNN Graph, we concatenate the position b 3 and the appearance feature a 3 to yield a non-local attention (see the red dash arrow). Finally, we concatenate the outputs of average pooling and max pooling layer, followed by one fully connected (FC) layer and one batch normalization (BN) layer. We adopt the conventional pretext task, i.e., identity classification L id , as the optimization objective to learn the pedestrian representation. When testing, we drop the last classifier and extract the compressed feature of 512 dimensions as the pedestrian representation for matching. discrete. We can not directly apply the traditional 2D convolutional layer on m × 6 to capture the local information, e.g., one 3 × 3 receptive field, since unordered neighbor points may have limited connections to the center point. To address the limitation, we follow the idea of graph neural networks <ref type="bibr" target="#b43">[Scarselli et al., 2008]</ref> to build the graph G based on the distance between points. Next we illustrate one basic component, i.e., dynamic graph convolution, to learn from the graph G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Graph Convolution</head><p>To model the relationship between neighbor points, we adopt the k-nearest neighbor (KNN) graph G = (V, E), where V denotes the vertex set, and E denotes the edge set (E ⊆ V × V). The KNN graph is directed, and includes self-loop, meaning (i, i) ∈ E. It is worth to noticing that the selection of the k-nearest neighbors is based on the value of vertexes (points) rather than the initial input order, evading the problem of unordered 3D point clouds. Besides, recent works  also show the dynamic graph is superior to the fixed graph structure during training GCN, which alleviates the over-smoothing problem and enlarges the receptive field of every node. Following the spirit of the dynamic graph, the KNN graph used in our work is not fixed, and we re-build the graph after every downsampling layer. The down-sampling layers are to pro-gressively remove redundant points (vertexes), and thus the computation cost of the proposed method is much less than the conventional implementation in .</p><p>To learn representation from the topology structure of the graph, we follow the spirit of the traditional 2D CNN and deploy one local convolutional layer based on neighbor points with connected edges. In particular, given one node feature x i , the output x i of the dynamic graph convolution could be formulated as:</p><formula xml:id="formula_0">x i = j:(i,j)∈E, j =i (θ i x i + θ j x j )<label>(1)</label></formula><p>where x j is the feature of neighbor points in the graph, and there is one edge from i to j. θ is the learnable parameter in Θ. The main difference with the traditional convolution is the definition of the neighbor set. In this work, we combine two kinds of neighbor choices, i.e., position similarity and feature similarity. If the graph G is based on the 3D coordinate similarity, dynamic graph convolution equals to the conventional 2D CNN to capture the local pattern based on the position. We note that this operation is translation invariant, since the global translation, such as ShiftX, ShiftY and Rotation, could not change the connected neighbors in E. On the other hand, if the graph G is built on the appearance feature, the dynamic graph convolution works as the non-local self-attention as <ref type="bibr" target="#b55">[Wang et al., 2018d</ref><ref type="bibr" target="#b71">, Zhang et al., 2018a</ref>, which ignores the  local position but pays attention to the area with similar appearance patterns. We next take advantage of the dynamic graph convolution function to build the basic module -Omni-scale module.</p><formula xml:id="formula_1">( ) ( ) (<label>) ( ) ( ) (</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Omni-scale Module</head><p>To leverage the rich multi-scale information as the prevailing 2D CNNs, we propose one basic Omni-scale module, which could be easily stacked to form the whole network. The module treats the 3D location and the RGB input differently (see <ref type="figure" target="#fig_2">Figure 4 (b)</ref>). We denote l ∈ [0, L − 1] as the layer index. The RGB input is the first appearance feature a 0 of m×3, while the initial 3D position is b 0 of m × 3. Different from the conventional graph CNN, the local k-nearest graph G l is dynamically generated according to the input location b l or the concatenation of a l and b l . Given the appearance feature a l of m × c, the location b l of m × 3 and the KNN graph G l , the Omni-scale module outputs the appearance feature a l+1 and the selected locations b l+1 . From the top to the bottom of the module, we first apply Dy-namic Graph Convolution to aggregate the k-nearest neighbor features, which is similar to the conventional convolutional layer. Dynamic Graph Convolution does not change the number of points, and thus the shape of the output feature is m × c . If down-sampling points is not applied, we will remain the channel number c = c following the conventional residual learning <ref type="bibr" target="#b11">[He et al., 2016</ref>] to obtain a l+1 followed by one batch normalization layer and one ReLU (see <ref type="figure" target="#fig_2">Figure 4 (a)</ref>). If downsampling points is applied, we generally set c = 2c to enlarge the feature channel before downsampling. Then we downsample the location according to the farthest point sampling (FPS) <ref type="bibr" target="#b35">[Qi et al., 2017b]</ref>. FPS selects the most distinguish points in the 3D space. We note that only the 3D position b l is used to calculate the distance and decide the selected points when downsampling. According to the selected location, we also downsample the appearance feature, and only keep the feature of the selected location. Therefore, the shape of the selected location is 1 2 m × 3, while the selected feature shape is 1 2 m × c . Next we deploy three branches with different grouping rates r = {8, 16, 32}, and the three branches do not share weights. In this way, we could capture the information with different receptive fields as the conventional 2D CNNs, i.e., InceptionNet <ref type="bibr" target="#b49">[Szegedy et al., 2017]</ref>. Each branch consists one grouping layer, two linear layers, two batch normalization (BN) layers, one squeeze-excitation (SE) block  and one group max pooling layer to aggregate the local information. Specifically, grouping-r layer is to sample and duplicate the r nearest points for each point, followed by the linear layers, batch normalization and the SE block.</p><p>We introduce SE-block  as one adaptive gate function to re-scale the weight of each branch before the summarization of three branches. Group max pooling layer is to maximize the feature within each group. Finally, we adopt the 'add' to calculate the sum of three branches rather than concatenation, so that the different scale pattern of the same part, such as cloth logos, could be accumulated. The shape of the new appearance feature a l+1 is 1 2 m × c , and the shape of the corresponding 3D position b l+1 is 1 2 m × 3. Alternatively, we could add the short-cut connection to take advantage of the identity representation as ResNet <ref type="bibr" target="#b11">[He et al., 2016]</ref>.</p><p>To summarize, the key of Omni-scale Module is two cross-point functions. The cross-point function indicates the function considers the neighbor points, while the pre-point function only considers the feature of one point itself. One cross-point function is the dynamic graph convolution before downsampling, which could be simply formulated as h(x i , x j ), where h denotes a linear function. It mimics the conventional 2D CNN to aggregate the local patterns according to the position. The other is the max group pooling layer in each branch, which could be simply formulated as max h(x i ). It maximizes neighbor features in each group as the new point feature. Now we have the Omni-scale module to learn from both of the appearance and the geometry structure information in a coherent manner, and next we will utilize Omni-scale modules to build the Omniscale Graph Network (OG-Net).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">OG-Net Architecture</head><p>The structure of OG-Net is as shown in <ref type="figure" target="#fig_1">Figure 3</ref>, consisting four Omni-scale modules. We progressively decrease the number of selected points as the conventional CNN. Every time the point number decreases, the channel number of the appearance feature is doubled. After four Omni-scale modules, we could obtain 96 points with 512-dim appearance feature. Similar to , we apply the max pooling as well as average pooling to aggregate the point feature, and concatenate the two outputs, yielding the 1024-dim feature. We add one fully-connected layer and one batch normalization layer to compress the feature to 512 dimensions as the pedestrian representation. When inference, we drop the last linear classifier for the pretext classification task, and extract the 512-dim feature to conduct image matching. Training Objective. We adopt the conventional identity classification as the pretext task to learn the identityaware feature. The vanilla cross-entropy loss could be formulated as:</p><formula xml:id="formula_2">L id = E[−log(p(y n |s n ))]<label>(2)</label></formula><p>where p(y n |s n ) is the predicted possibility of s n belonging to the ground-truth class y n . The training objective demands that the model could discriminate different identities according to the input points. Besides, other training objectives are orthogonal to our work. In this work, we intend to show the strong potential ability of the 3D space and the proposed OG-Net. We, therefore, only deploy the basic identification loss for a fair comparison with other networks.</p><p>Relation to Existing Methods. The main difference with existing GNN-based networks ] is three-fold: (1) We extract the multi-scale local information via the proposed Omniscale Block, which can deal with the common scale variants in 3D person data;</p><p>(2) We split the XYZ position information and RGB color information, and treat them differently. RGB inputs are used to extract appearance features, while the geometry position is to build the graph for local representation learning; (3) Due to a large number of points in 3D person, we progressively reduce the number of nodes in the graph, facilitating efficient training for 3D person data. On the other hand, compared with PointNet <ref type="bibr" target="#b34">[Qi et al., 2017a]</ref> and Point-Net++ <ref type="bibr" target="#b35">[Qi et al., 2017b]</ref>, the proposed OG-Net contains more cross-point functions, and provides topology information, enriching the representation power of the network. The graph could be built on the two kinds of neighbor choices, i.e., position similarity or feature similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>OG-Net is trained with a mini-batch of 16. We deploy Adam optimizer <ref type="bibr" target="#b19">[Kingma and Ba, 2014]</ref> and the initial learning rate is set to 4e − 4. We gradually decrease the learning rate via the cosine policy <ref type="bibr" target="#b30">[Loshchilov and Hutter, 2017]</ref>, and the model is trained for 1000 epochs.</p><p>To regularize the training, we transfer some traditional 2D data augmentation methods, such as random scale and position jittering, to the 3D space. For instance, position jittering is to add zero-mean Gaussian noise to every point. Following the setting in DGCNN , we set the neighbor number of KNNgraph to k = 20. The dynamic graph convolution in OG-Net can be any of the existing graph convolution operations, such as EdgeConv , SAGE <ref type="bibr" target="#b9">[Hamilton et al., 2017]</ref> and GAT . In practise, we adopt EdgeConv . Dropout with 0.7 drop probability is used before the last linear classification layer. Since the basic OG-Net is shallow, we do not use the short-cut connection. For the person reid task, the input image is resized to 128 × 64, and there are 8192 points with RGB color information. After mapping to the 3D space, we uniformly sample half points to train the OG-Net, and thus the number of input m in <ref type="figure" target="#fig_1">Figure 3</ref> equals to 4096. We note that, for other competitive 2D CNN methods, we still follow the common setting, and the 2D image input is resized to 256 × 128    <ref type="bibr">, 96, 96, 192, 192, 384, 384}</ref>. The short-cut connection is enabled. Further discussion on short-cut connection is provided in <ref type="table">Table 5</ref>. The parameter number is 2.47M . The models are trained from scratch on 3D point clouds. The whole training process costs about 2 days, with one NVIDIA 2080Ti. During testing, we extract the 512-dim feature before the classifier as the pedestrian representation. The feature is L2-normalized. Given one query image, we calculate the cosine similarity between the query feature and the candidate features of gallery images. We sort gallery images and return the ranking list according to the cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>We verify the effectiveness of the proposed method on four large-scale person re-id datasets, i.e., Market-1501 <ref type="bibr" target="#b78">[Zheng et al., 2015]</ref>, DukeMTMC-reID <ref type="bibr" target="#b39">[Ristani et al., 2016</ref>, <ref type="bibr">MSMT-17 [Wei et al., 2018]</ref>, and CUHK03-NP <ref type="bibr" target="#b84">, Zhong et al., 2017</ref>.</p><p>Market-1501 <ref type="bibr" target="#b78">[Zheng et al., 2015]</ref> is collected in a university campus by 6 cameras, containing 12, 936 training images of 751 identities, 3, 368 query images and 19, 732 gallery images of the other 750 identities. There are no overlapping identities (classes) between the training and test set. Every identity in the training set has 17.2 photos on average. All images are automatically detected by the DPM detector <ref type="bibr" target="#b5">[Felzenszwalb et al., 2009]</ref>.</p><p>DukeMTMC-reID <ref type="bibr" target="#b39">[Ristani et al., 2016</ref> consists 16, 522 training images of 702 identites, 2, 228 query images of the other 702 identities and 17, 661 gallery images, which is mostly collected in winter by eight high-resolution cameras. It is challenging in that most pedestrians are in the similar clothes, and may be occluded by cars or trees.</p><p>MSMT-17 <ref type="bibr" target="#b59">[Wei et al., 2018]</ref> is one of the newlyreleased large-scale datasets, including 126, 441 images collected in both indoor and outdoor scenarios with 15 cameras. It contains 32, 621 images of 1, 041 identities for training, 11, 659 query images with 82, 161 gallery images.</p><p>CUHK03-NP  is one of the early person re-identification datasets. We follow the new protocol in <ref type="bibr" target="#b84">[Zhong et al., 2017]</ref> to split 767 identities as the training set, and the rest 700 identities are deployed to verify the model. We utilize the pedestrian images detected by DPM <ref type="bibr" target="#b5">[Felzenszwalb et al., 2009]</ref> for training and testing, which is more close to the real-world scenario.</p><p>Evaluation Metrics. We report Rank-1 accuracy (R@1) and mean average precision (mAP). Rank-i denotes the probability of the true match in the top-i of the retrieval results, while AP denotes the area under the Precision-Recall curve. The mean of the average precision (mAP) for all query images reflects the precision and recall rate of the retrieval performance. Besides, we also provide the number of model parameters (#params).</p><p>Data Limitation. Before the experimental analysis, we would like to illustrate several data limitations. It is mainly due to lossy mapping in the 2D-to-3D process. Due to the restriction of the 3D human model, we could not build the 3D model for several body outliers, such as hair, bag, dress. However, these outliers contain discriminative identity information. For instance, as shown in <ref type="figure">Figure 5 (a) and (b)</ref>, the 3D model based on the visible part drops some part of hair and dress of the girl, which is not ideal for representation learning. We think it could be solved via the depth estimation de- <ref type="figure">Fig. 5 (a,b)</ref> Visualization of lossy compression in the 2Dto-3D mapping, which drops the body outliers, e.g., hair and dress. (c) We still introduce the 2D background to 3D space. vices, such as Kinect <ref type="bibr" target="#b10">[Han et al., 2013]</ref>, or more sophisticated human models in the future. In this paper, we do not solve the 3D human reconstruction problem, but focus on the person re-identification task. Therefore, as a trade-off, we still introduce the 2D background, and project the corresponding pixel to the XY plane (see <ref type="figure">Figure 5</ref> (c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Results</head><p>Comparisons to the 2D Space. We compare the results on three kinds of inputs, i.e., 2D input, 3D Visible Part and 3D Visible Part with 2D Background. For a fair comparison, the grid of the 2D input is also transformed to the point cloud format as (x, y, 0), while z is set to 0. We train OG-Net on three kinds of input data with the same hyper-parameters. As shown in <ref type="table" target="#tab_2">Table 1</ref>, we observe that the retrieval result of the pure 3D Visible Part input is inferior to that of 2D Image. As discussed in Section 4.2, we speculate that it is due to the lossy 2D-to-3D mapping, which drops several discriminative parts, such as hair, dress, and carrying. In contrast, the 3D Visible Part + 2D Background has achieved superior performance 85.90% Rank@1 and 66.93% mAP to the result of 2D Image (84.80% Rank@1 and 66.33% mAP), which shows that the 3D position information is complementary to 2D color information. The 3D space could ease the matching difficulty and highlight the geometry structure.</p><p>Person Re-id Performance. We compare the proposed method with three groups of competitive methods, i.e., prevailing 2D CNN models, light-weight CNN models, and popular point classification models. We note that the model pre-trained on the large-scale datasets, e.g., ImageNet <ref type="bibr" target="#b2">[Deng et al., 2009]</ref>, could yield the performance boost. For a fair comparison, models are trained from scratch with the same optimization objective, i.e., the cross-entropy loss. As shown in <ref type="table">Table 2</ref>, we can make the following observations:</p><p>(1) OG-Net has achieved competitive results of 68.09% mAP, 57.20% mAP, 22.82% mAP, and 35.88% mAP on four large-scale person re-id benchmarks with limited training parameters 1.95M . The mobile OG-Net-Small with less channel width also achieves a close result.</p><p>(2) Comparing with the point-based methods, such as PointNet++ <ref type="bibr" target="#b35">[Qi et al., 2017b]</ref> and DGCNN , both OG-Net and OG-Net-Small have surpassed this line of works by a clear margin, which validates the effectiveness of the proposed Omni-scale module in capturing multi-scale neighbor information on point clouds.</p><p>(3) Comparing with light-weight 2D CNN models, i.e., ShuffleNetV2 <ref type="bibr" target="#b75">[Zhang et al., 2018b]</ref> and MobileNetV2 <ref type="bibr" target="#b41">[Sandler et al., 2018]</ref>, OG-Net-Small has achieved competitive performance with fewer parameters (1.20M ).</p><p>(4) Comparing with prevailing 2D CNN models, i.e., ResNet-50 <ref type="bibr" target="#b11">[He et al., 2016]</ref> and DenseNet-121 <ref type="bibr" target="#b14">[Huang et al., 2017]</ref>, the proposed OG-Net surpasses these models. Furthermore, OG-Net-Deep with deeper structure has achieved better Rank@1 and mAP accuracy. Besides, we also observe that OG-Net is more robust than 2D CNNs, when facing the unseen data. We will discuss this aspect in the following section.</p><p>Transferring to Unseen Datasets. To verify the scalability of OG-Net, we train the model on dataset A and directly test the model on dataset B (with no adaptation), which is close to the real-world deployment. We denote the direct transfer learning protocol as A → B. Three groups of related works are considered. We observe that the modern CNN models are typically over-parameterized, which is prone to over-fit the training dataset. As shown in <ref type="table" target="#tab_4">Table 3</ref>, both ResNet-50 and DenseNet-121 do not perform well given more parameters. The 3D point cloud-based methods are competitive to the conventional 2D methods. It is worth noting that the proposed OG-Net has outperformed the point-based methods as well as prevailing 2D networks. The results suggest that the proposed method has the potential to adapt one new re-id dataset of unseen environments. <ref type="table">Table 2</ref> We mainly compare three groups of models trained from scratch on four large-scale person re-id datasets, i.e., Market-1501 <ref type="bibr" target="#b78">[Zheng et al., 2015]</ref>, DukeMTMC-reID <ref type="bibr" target="#b39">[Ristani et al., 2016</ref>, <ref type="bibr">MSMT-17 [Wei et al., 2018]</ref> and CUHK03-NP <ref type="bibr" target="#b84">, Zhong et al., 2017</ref>. We report Rank1(%), mAP(%) and the number of model paramters (M). The first group contains the point-based methods that we re-implemented. The second group contains the lightweight CNN models. The third group contains prevailing 2D CNN models with more parameters.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quantitative Results</head><p>Visualization of Retrieval Results. As shown in <ref type="figure" target="#fig_3">Figure 6</ref>, we provide the original query, the corresponding 3D query and the top-5 retrieved candidates. Two different cases are studied. One is the typical case that the 3D human reconstruction is relatively good. OG-Net can successfully retrieve the true-matches of differ-ent viewpoints (see <ref type="figure" target="#fig_3">Figure 6 (a)</ref>). On the other hand, we also show the challenging case, including the partially detected query and occlusion. Thanks to the prior knowledge of the human geometry structure, OG-Net can still provide reasonable retrieval results with large scale variants (see <ref type="figure" target="#fig_3">Figure 6 (b)</ref>). It also verifies the robustness of the proposed approach.  <ref type="figure">Fig. 7</ref> Sensitivity analysis on the different number of neighbors k. We provide the corresponding re-id performance on Market-1501 in terms of Rank@1(%) and mAP(%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Further Analysis and Discussions</head><p>Effect of Different Components. In this section, we intend to study the mechanism of the Omni-scale Module. First, we compare the OG-Net without KNN Graph, i.e., k = 1. For a fair comparison, we apply one linear layer to replace the dynamic graph convolution. As shown in the second and the third column of <ref type="table" target="#tab_5">Table 4</ref>, the performance of OG-Net without leveraging the KNN neighbor information drops from 65.99% mAP to 63.95% mAP. The result suggests that the dynamic graph captures effective local information, which could not be replaced by pre-point function, e.g., linear layer. On the other hand, if we include too many neighbors, e.g., k = 64, the model loses the discriminative feature of local patterns, thus compromising the   <ref type="figure">Figure 7)</ref>. The observation is consistent with the conventional k nearest neighbor algorithms <ref type="bibr" target="#b33">[Peterson, 2009]</ref> on the neighbor number. Next, we intend to verify the effectiveness of the last non-local graph. The last graph is built on the k-nearest neighbor of the appearance feature. (In practice, we append the 3-channel position to the appearance feature for building the graph, which prevents duplicate nodes with the same node attribute in the graph.) For a fair comparison, we replace the last non-local graph with the graph based on 3D position only. As shown in the third and the fourth column of <ref type="table" target="#tab_5">Table 4</ref>, OG-Net with the last non-local block has surpassed the model with position graph +0.94% mAP, indicating that the last non-local graph provides effective long-distance attention.</p><p>Finally, we study two alternative components, i.e., SE block and short-cut connection. By default, Omniscale Module deploys SE block but does not add the short-cut connection. As shown in the first and second column in <ref type="table" target="#tab_5">Table 4</ref>, we can observe that SE Block im-proves about +1.06% mAP from 62.89% to 63.95%. On the other hand, the short-cut connections do not provide significant improvement or performance drop on OG-Net, since OG-Net is relatively shallow with four Omni-scale blocks. As shown in <ref type="table">Table 5</ref>, we deploy the OG-Net-Deep to further validate this point. The observation is consistent with ResNet <ref type="bibr" target="#b11">[He et al., 2016]</ref>. The short-cut connection works well on the relatively deep network structure. The performance is improved from 58.81% mAP to 69.52% mAP, and the short-cut connections help the model optimization.</p><p>Sensitivity Analysis on the Point Density. Our model is trained with 50% points, i.e., 4096, and thus the best performance is achieved with 50% points remaining. In practice, different depth estimation devices may provide different scan point density. To verify the robustness of the proposed OG-Net on point density, we synthesize the data similar to that in <ref type="figure" target="#fig_4">Figure 8</ref> (left) and conduct the inference. When 25% points remain, OG-Net still could arrive at 85.04% Rank@1 and 65.50% mAP. When 100% points are used, OG-Net arrives at 84.89% Rank@1 and 65.41% mAP. It is because too low/high density impacts the distribution of the k-nearest neighbors, compromising the retrieval performance. Despite the density changes, the relative performance drop is small. The result verifies OG-Net is robust to different point density (see <ref type="figure" target="#fig_4">Figure 8 (right)</ref>).</p><p>Evaluation of Point Cloud Classification Task. We also evaluate the proposed OG-Net on the traditional point cloud classification benchmark, i.e., Mod-elNet <ref type="bibr" target="#b62">[Wu et al., 2015]</ref>. The ModelNet dataset contains 12,311 meshed CAD models of 40 categories. Following the train-test split in , 9,843 models are used for training, while the rest 2,468 models are for evaluation. Note that the ModelNet dataset does not provide RGB information. To verify the effectiveness of OG-Net, we duplicate the xyz input as the appearance input to train OG-Net. Following other competitive approaches <ref type="bibr" target="#b34">, Qi et al., 2017a</ref><ref type="bibr" target="#b35">, Qi et al., 2017b</ref>, the number of input points is fixed as 1024. As shown in <ref type="table" target="#tab_6">Table 6</ref>, we compare with prevailing models in terms of mean-class accuracy and overall accuracy. Although the proposed method is not designed for cloud point classification task, OG-Net-Small has achieved a competitive result of 90.5% meanclass accuracy and 93.3% overall accuracy with 1.22M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we provide an early attempt to learn the pedestrian representation in the 3D space, easing the part matching on 2D images. The 3D assumption is aligned with the human visual system of associating the 2D appearance with the 3D geometry structure. Different from existing CNN-based approaches, the proposed Omni-scale Graph Network (OG-Net) takes the advantage of 3D prior knowledge and 2D appearance information in an end-to-end manner, starting from 3D human point clouds. Given 3D points and the nearest neighbour graph, the basic Omni-scale module can aggregate different-scale neighbor information in the topology, enriching the representation ability. This allows the proposed OG-Net efficiently learns discriminative feature via limited network parameters. Extensive experiments suggest that OG-Net exploits the complementary information of 3D geometry information and the 2D appearance, yielding the competitive performance on four person re-id benchmarks. The 3D prior knowledge also benefits the model generalizability on the unseen pedestrian data, which is close to the application in realworld scenarios.</p><p>The proposed OG-Net still have room for futher improvements. In experiment, the proposed method learns the representation from the generated 3D point clouds mapping from 2D images. Although it works, the original 2D images are usually resized and compressed in most person re-id datasets, compromising the body shape, e.g., height. We may consider collecting a new 3D dataset in the future. Furthermore, the proposed method has the potential to many related fields. Similar graph-based models can be employed to other potential fields, e.g., objects with a rigid structure like vehicles <ref type="bibr" target="#b50">[Tang et al., 2019b</ref> and products <ref type="bibr" target="#b61">[Wei et al., 2019</ref><ref type="bibr" target="#b28">, Liu et al., 2012</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Person is a 3D non-rigid object. In this work, we conduct the person re-identification in the 3D space, and learn a new type of robust re-id feature. Given one 2D image (a), we first (b) estimate the 3D pose via the off-the-shelf model [Kanazawa et al., 2018], followed by (c) mapping the RGB color of visible surfaces to corresponding points. The invisible parts are made transparent for visualization. (d) The appearance information is aligned with the human structure. We make the person free from the 2D space, and thus ease the matching difficulty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>OG-Net Architecture. OG-Net is simply built via stacking Omni-scale Modules. (m × c) denotes the feature of m points with c-dim attribute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Visualization of Omni-scale Module. We provide the feature shape as the format of (·). For instance, (m × c) denotes the feature of m points with c-dim attribute. (a) We show the basic Omni-scale module without downsampling. (b) We show the Omni-scale module with downsampling, which is similar to the conventional pooling layer. The module distills the number of the points and improves the training efficiency. The dash line denotes the short-cut connection. Besides, we highlight two function types, i.e., cross-point functions and per-point functions, in red. The cross-point function aggregates the feature among neighbor points, while the per-point function only considers the single-point feature. The proposed Omni-scale module consists of these two kinds of functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>Visualization of Retrieval Results. (a) Given one 3D query, we show the original 2D images and the top-5 retrieval results. (b) We also show the challenging case, such as occlusion and the partially detected query. The green index indicates the true-matches, while the red index denotes the false-matches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8</head><label>8</label><figDesc>Sensitivity analysis on the point density. (left) We visualize point clouds with different proportion of the point number. (right) We provide the corresponding re-id performance in terms of Rank@1(%) and mAP(%) against the point number variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Ablation study of different inputs on Market-1501.</figDesc><table><row><cell cols="3">†: For a fair comparison, the model is trained on the tradi-</cell></row><row><cell cols="3">tional 2D image inputs with extra 3D coordinates (x, y, 0).</cell></row><row><cell>Inputs</cell><cell>R@1</cell><cell>mAP</cell></row><row><cell>2D Image  †</cell><cell>84.80</cell><cell>66.33</cell></row><row><cell>3D Visible Part</cell><cell>77.64</cell><cell>54.52</cell></row><row><cell cols="3">3D Visible Part + 2D Background 85.90 66.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Transferring to unseen datasets. Here we directly deploy the model trained on the dataset A to the unseen dataset B. We denote this setting as A → B, which could reflect the scalability of the model in different scenarios. We observe that OGNet is generally superior to the ResNet-50 and DenseNet-121 as well as lightweight models, such as ShuffleNetV2 and MobileNetV2.</figDesc><table><row><cell>Method</cell><cell>Input Type</cell><cell cols="12">Market→Duke Duke→Market Market→MSMT MSMT→Market Duke→MSMT MSMT→Duke R@1 mAP R@1 mAP R@1 mAP R@1 mAP R@1 mAP R@1 mAP</cell></row><row><cell>DGCNN [Wang et al., 2019a]</cell><cell cols="2">point clouds 7.4</cell><cell>2.9</cell><cell>13.4</cell><cell>4.4</cell><cell>1.4</cell><cell>0.4</cell><cell>10.0</cell><cell>3.7</cell><cell>1.8</cell><cell>0.5</cell><cell>7.3</cell><cell>2.7</cell></row><row><cell cols="3">PointNet++ (SSG) [Qi et al., 2017b] point clouds 18.6</cell><cell>8.4</cell><cell>28.8</cell><cell>11.3</cell><cell>3.9</cell><cell>1.2</cell><cell>32.4</cell><cell>13.3</cell><cell>5.5</cell><cell>1.7</cell><cell>29.0</cell><cell>15.4</cell></row><row><cell cols="3">PointNet++ (MSG) [Qi et al., 2017b] point clouds 23.2</cell><cell>11.0</cell><cell>32.8</cell><cell>12.6</cell><cell>5.0</cell><cell>1.5</cell><cell>30.6</cell><cell>12.9</cell><cell>6.5</cell><cell>1.9</cell><cell>24.3</cell><cell>12.4</cell></row><row><cell>ShuffleNetV2 [Zhang et al., 2018b]</cell><cell>images</cell><cell>17.2</cell><cell>7.2</cell><cell>36.4</cell><cell>13.9</cell><cell>2.8</cell><cell>0.8</cell><cell>36.5</cell><cell>14.1</cell><cell>5.8</cell><cell>1.5</cell><cell>29.3</cell><cell>15.3</cell></row><row><cell>MobileNetV2 [Sandler et al., 2018]</cell><cell>images</cell><cell>16.7</cell><cell>7.1</cell><cell>34.3</cell><cell>12.4</cell><cell>3.2</cell><cell>0.9</cell><cell>35.9</cell><cell>14.2</cell><cell>5.5</cell><cell>1.4</cell><cell>30.6</cell><cell>15.4</cell></row><row><cell>DenseNet-121 [Huang et al., 2017]</cell><cell>images</cell><cell>11.7</cell><cell>5.0</cell><cell>32.7</cell><cell>11.6</cell><cell>2.9</cell><cell>0.8</cell><cell>34.2</cell><cell>13.0</cell><cell>5.3</cell><cell>1.5</cell><cell>27.8</cell><cell>13.6</cell></row><row><cell>ResNet-50 [He et al., 2016]</cell><cell>images</cell><cell>12.1</cell><cell>5.2</cell><cell>34.3</cell><cell>13.5</cell><cell>2.7</cell><cell>0.7</cell><cell>34.7</cell><cell>13.5</cell><cell>5.4</cell><cell>1.5</cell><cell>28.1</cell><cell>14.4</cell></row><row><cell>OG-Net</cell><cell cols="2">point clouds 31.3</cell><cell>16.3</cell><cell>41.4</cell><cell>17.2</cell><cell>7.4</cell><cell>2.3</cell><cell>47.6</cell><cell>21.4</cell><cell>9.2</cell><cell>2.5</cell><cell>44.9</cell><cell>25.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Effectiveness of different components. We compare the network variants, including squeeze-excitation (SE), the usage of KNN Graph and the last non-local attention in the model.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">Performance</cell><cell></cell></row><row><cell cols="2">with Squeeze-excitation?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>with KNN Graph?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with Last Non-local?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rank@1</cell><cell>83.02</cell><cell>84.35</cell><cell>85.48</cell><cell>85.90</cell></row><row><cell>mAP(%)</cell><cell>62.89</cell><cell>63.95</cell><cell>65.99</cell><cell>66.93</cell></row><row><cell cols="5">Table 5 Effectiveness of the short-cut connection. We ob-</cell></row><row><cell cols="5">serve a similar result with [He et al., 2016] that the improve-</cell></row><row><cell cols="5">ment from the short-cut connection is not significant on the</cell></row><row><cell cols="5">"shallow" network, while it works well on the relatively deep</cell></row><row><cell>network structure.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Short-cut</cell><cell>R@1</cell><cell cols="2">mAP</cell></row><row><cell>OG-Net</cell><cell>×</cell><cell>86.19</cell><cell cols="2">68.09</cell></row><row><cell>OG-Net</cell><cell></cell><cell>85.63</cell><cell cols="2">66.85</cell></row><row><cell>OG-Net-Deep</cell><cell>×</cell><cell>81.56</cell><cell cols="2">58.81</cell></row><row><cell>OG-Net-Deep</cell><cell></cell><cell>87.74</cell><cell cols="2">69.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Classification results on ModelNet<ref type="bibr" target="#b62">[Wu et al., 2015]</ref>. We do not focus on the point cloud classification problem, but show the feasibility of the proposed OG-Net. † : We provide results based on our re-implementation, which is slightly higher than the reported result in<ref type="bibr" target="#b35">[Qi et al., 2017b]</ref>.</figDesc><table><row><cell>Method</cell><cell>#params(M)</cell><cell cols="2">Mean-class Overall Accuracy Accuracy</cell></row><row><cell>3DShapeNets [Wu et al., 2015]</cell><cell>-</cell><cell>77.3</cell><cell>84.7</cell></row><row><cell>VoxNet [Maturana and Scherer, 2015]</cell><cell>-</cell><cell>83.0</cell><cell>85.9</cell></row><row><cell>PointNet [Qi et al., 2017a]</cell><cell>3.50</cell><cell>86.0</cell><cell>89.2</cell></row><row><cell>SpecGCN [Wang et al., 2018a]</cell><cell>2.05</cell><cell>-</cell><cell>91.5</cell></row><row><cell>PointNet++(SSG)  † [Qi et al., 2017b]</cell><cell>1.62</cell><cell>89.5</cell><cell>92.0</cell></row><row><cell>PCNN by Ext [Atzmon et al., 2018]</cell><cell>1.40</cell><cell>-</cell><cell>92.2</cell></row><row><cell>PointNet++(MSG)  † [Qi et al., 2017b]</cell><cell>1.89</cell><cell>90.1</cell><cell>92.7</cell></row><row><cell>DGCNN [Wang et al., 2019a]</cell><cell>1.81</cell><cell>90.2</cell><cell>92.9</cell></row><row><cell>OG-Net-Small</cell><cell>1.22</cell><cell>90.5</cell><cell>93.3</cell></row><row><cell cols="4">retrieval performance as well. To validate this points, we</cell></row><row><cell cols="4">evaluate the sensitivity analysis on k = {4, 8, 16, 32, 64}</cell></row><row><cell>(see</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atzmon</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Looking beyond appearances: Synthetic training data for deep cnns in re-identification. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barbosa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="50" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning disentangled representation for robust person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ham</forename><surname>Eom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pose-guided visible part matching for occluded person reid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fd-gan: Pose-guided feature distilling gan for robust person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS. Gong et</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
	<note>The re-identification challenge. In Person re-identification</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhanced computer vision with microsoft kinect sensor: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1318" to="1334" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hermans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person reidentification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-pseudo regularized label for generated data in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1391" to="1403" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaderberg</surname></persName>
		</author>
		<title level="m">Spatial transformer networks. In NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalayeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ba ;</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Magnifiernet: Towards semantic adversary and fusion for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning of compact binary descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1501" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-domain complementary learning using pose for multi-person part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving person reidentification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="151" to="161" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pose transferrable person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hi, magic closet, tell me what to wear</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for realtime object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pose-guided feature alignment for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="542" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">K-nearest neighbor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Peterson ; Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scholarpedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1883</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-scale deep learning architectures for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Leader-based multi-scale attention deep architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="371" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Posenormalized image generation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Features for multi-target multi-camera tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasi ;</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6036" to="6046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sandler</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saquib</forename><surname>Sarfraz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dissecting person re-identification from the viewpoint of viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning part-based convolutional features for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pamtri: Pose-aware multi-task learning for vehicle re-identification using highly randomized synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<editor>AAAI. Tang et al.</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Inception-v4, inception-resnet and the impact of residual connections on learning</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attribute recognition by joint recurrent learning of context and correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2275" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to reduce dual-level discrepancy for infrared-visible person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Beyond intra-modality: A survey of heterogeneous person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>IJCAI</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07249</idno>
		<title level="m">Rpc: A large-scale retail product checkout dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Enhancing person re-identification in a self-trained subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Person reidentification via structural deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2987" to="2998" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Simulating content consistent vehicle datasets with attribute descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08855</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep learning for person reidentification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04193</idno>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
	<note>Deep metric learning for person re-identification</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">The devil is in the middle: Exploiting mid-level representations for cross-domain instance matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Part-guided attention learning for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TITS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Pose-invariant embedding for deep person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4500" to="4509" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Scalable person reidentification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<title level="m">Person re-identification: Past, present and future</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with kreciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning to adapt invariance in memory for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Joint disentangling and adaptation for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
