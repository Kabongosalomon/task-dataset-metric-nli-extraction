<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ‡ University of Technology Sydney</orgName>
								<orgName type="institution" key="instit2">University of Texas at San Antonio</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ‡ University of Technology Sydney</orgName>
								<orgName type="institution" key="instit2">University of Texas at San Antonio</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yee.i.yang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ‡ University of Technology Sydney</orgName>
								<orgName type="institution" key="instit2">University of Texas at San Antonio</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>qi.tian@utsa.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ‡ University of Technology Sydney</orgName>
								<orgName type="institution" key="instit2">University of Texas at San Antonio</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University ‡ University of Technology Sydney</orgName>
								<orgName type="institution" key="instit2">University of Texas at San Antonio</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Employing part-level features for pedestrian image description offers fine-grained information and has been verified as beneficial for person retrieval in very recent literature. A prerequisite of part discovery is that each part should be well located. Instead of using external cues, e.g., pose estimation, to directly locate parts, this paper lays emphasis on the content consistency within each part.</p><p>Specifically, we target at learning discriminative partinformed features for person retrieval and make two contributions. (i) A network named Part-based Convolutional Baseline (PCB). Given an image input, it outputs a convolutional descriptor consisting of several part-level features. With a uniform partition strategy, PCB achieves competitive results with the state-of-the-art methods, proving itself as a strong convolutional baseline for person retrieval. (ii) A refined part pooling (RPP) method. Uniform partition inevitably incurs outliers in each part, which are in fact more similar to other parts. RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency. Experiment confirms that RPP allows PCB to gain another round of performance boost. For instance, on the Market-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1 accuracy, surpassing the state of the art by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person retrieval, also known as person re-identification (re-ID), aims at retrieving images of a specified pedestrian in a large database, given a query person-of-interest. Presently, deep learning methods dominate this community, with convincing superiority against hand-crafted competitors <ref type="bibr" target="#b39">[40]</ref>. Deeply-learned representations provide high discriminative ability, especially when aggregated from deeply-learned part features. The latest state of the art on re- <ref type="bibr">Figure 1</ref>. Partition strategies of several deep part models in person retrieval. (a) to (e): Partitioned parts by GLAD <ref type="bibr" target="#b30">[31]</ref>, PDC <ref type="bibr" target="#b26">[27]</ref>, DPL <ref type="bibr" target="#b34">[35]</ref>, Hydra-plus <ref type="bibr" target="#b21">[22]</ref> and PAR <ref type="bibr" target="#b36">[37]</ref>, respectively, which are cropped from the corresponding papers. (f): Our method employs a uniform partition and then refines each stripe. Both PAR <ref type="bibr" target="#b36">[37]</ref> and our method conduct "soft" partition, but our method differs significantly from <ref type="bibr" target="#b36">[37]</ref>, as detailed in <ref type="bibr">Section 2.</ref> ID benchmarks are achieved with part-informed deep features <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>An essential prerequisite of learning discriminative part features is that parts should be accurately located. Recent state-of-the-art methods vary on their partition strategies and can be divided into two groups accordingly. The first group <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref> leverage external cues, e.g., assistance from the latest progress on human pose estimation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref>. They rely on external human pose estimation datasets and sophisticated pose estimator. The underlying datasets bias between pose estimation and person retrieval remains an obstacle against ideal semantic partition on person images. The other group <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b21">22]</ref> abandon cues from semantic parts. They require no part labeling and yet achieve competitive accuracy with the first group. Some partition strategies are compared in <ref type="figure">Fig. 1</ref>. Against this background of progress on learning part-level deep features, we rethink the problem of what makes well-aligned parts. Semantic partitions may offer stable cues to good alignment but are prone to noisy pose detections. This paper, from another perspective, lays emphasis on the consistency within each part, which we speculate is vital to the spatial alignment. Then we arrive at our motivation that given coarsely partitioned parts, we aim to refine them to reinforce within-part consistency. Specifically, we make the following two contributions:</p><p>First, we propose a network named Part-based Convolutional Baseline (PCB) which conducts uniform partition on the conv-layer for learning part-level features. It does not explicitly partition the images. PCB takes a whole image as the input and outputs a convolutional feature. Being a classification net, the architecture of PCB is concise, with slight modifications on the backbone network. The training procedure is standard and requires no bells and whistles. We show that the convolutional descriptor has much higher discriminative ability than the commonly used fully-connected (FC) descriptor. On the Market-1501 dataset, for instance, the performance increases from 85.3% rank-1 accuracy and 68.5% mAP to 92.3% (+7.0%) rank-1 accuracy and 77.4% (+8.9%) mAP, surpassing many state-of-the-art methods by a large margin.</p><p>Second, we propose an adaptive pooling method to refine the uniform partition. We consider the motivation that within each part the contents should be consistent. We observe that under uniform partition, there exist outliers in each part. These outliers are, in fact, closer to contents in some other part, implying within-part inconsistency. Therefore, we refine the uniform partition by relocating those outliers to the part they are closest to, so that the withinpart consistency is reinforced. An example of the refined parts is illustrated in <ref type="figure">Fig. 1(f)</ref>. With the proposed refined part pooling (RPP), performance on Market-1501 further increases to 93.8% (+1.5%) rank-1 accuracy and 81.6% (+4.2%) mAP. In Section 3 and 4, we describe the PCB and the refined part pooling, respectively.</p><p>In Section 5, we combine the two methods, which achieves a new state of the art in person retrieval. Importantly, we demonstrate experimentally that the proposed refined parts are superior to attentive parts, i.e., parts learned with attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Hand-crafted part features for person retrieval. Before deep learning methods dominated the re-ID research community, hand-crafted algorithms had developed approaches to learn part or local features. Gray and Tao <ref type="bibr" target="#b11">[12]</ref> partition pedestrians into horizontal stripes to extract color and texture features. Similar partitions have then been adopted by many works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20]</ref>. Some other works employ more sophisticated strategy. Gheissari et al. <ref type="bibr" target="#b10">[11]</ref> divide the pedestrian into several triangles for part feature extraction. Cheng et al. <ref type="bibr" target="#b3">[4]</ref> employ pictorial structure to parse the pedestrian into semantic parts. Das et al. <ref type="bibr" target="#b5">[6]</ref> apply HSV histograms on the head, torso and legs to capture spatial information.</p><p>Deeply-learned part features. The state of the art on most person retrieval datasets is presently maintained by deep learning methods <ref type="bibr" target="#b39">[40]</ref>. When learning part features for re-ID, the advantages of deep learning over hand-crafted algorithms are two-fold. First, deep features generically obtain stronger discriminative ability. Second, deep learning offers better tools for parsing pedestrians, which further benefits the part features. In particular, human pose estimation and landmark detection have achieved impressive progress <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b14">15]</ref>. Several recent works in re-ID employ these tools for pedestrian partition and report encouraging improvement <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>. However, the underlying gap between datasets for pose estimation and person retrieval remains a problem when directly utilizing these pose estimation methods in an off-the-shelf manner. Others abandon the semantic cues for partition. Yao et al. <ref type="bibr" target="#b34">[35]</ref> cluster the coordinates of max activations on feature maps to locate several regions of interest. Both Liu et al. <ref type="bibr" target="#b21">[22]</ref> and Zhao et al. <ref type="bibr" target="#b36">[37]</ref> embed the attention mechanism <ref type="bibr" target="#b33">[34]</ref> in the network, allowing the model to decide where to focus by itself.</p><p>Deeply-learned part with attention mechanism. A major contribution of this paper is the refined part pooling. We compare it with a recent work, PAR <ref type="bibr" target="#b34">[35]</ref> by Zhao et al. in details. Both works employ a part-classifier to conduct "soft" partition on pedestrian images, as shown in <ref type="figure">Fig.  1</ref>. Two works share the merit of requiring no part labeling for learning discriminative parts. However, the motivation, training methods, mechanism, and final performance of the two methods are quite different, to be detailed below.</p><p>Motivation: PAR aims at directly learning aligned parts while RPP aims to refine the pre-partitioned parts. Working mechanism: using attention method, PAR trains the part classifier in an unsupervised manner, while the training of RPP can be viewed as a semi-supervised process. Training process: RPP firstly trains an identity classification model with uniform partition and then utilizes the learned knowledge to induce the training of part classifier. Performance: the slightly more complicated training procedure rewards RPP with better interpretation and significantly higher performance. For instance on Market-1501, mAP achieved by PAR, PCB cooperating attention mechanism and the proposed RPP are 63.4%, 74.6% and 81.6%, respectively. In addition, RPP has the potential to cooperate with various partition strategies. <ref type="figure">Figure 2</ref>. Structure of PCB. The input image goes forward through the stacked convolutional layers from the backbone network to form a 3D tensor T . PCB replaces the original global pooling layer with a conventional pooling layer, to spatially down-sample T into p pieces of column vectors g. A following 1 × 1 kernel-sized convolutional layer reduces the dimension of g. Finally, each dimension-reduced column vector h is input into a classifier, respectively. Each classifier is implemented with a fully-connected (FC) layer and a sequential Softmax layer. During training, each classifier predicts the identity of the input image and is supervised by Cross-Entropy loss. During testing, either p pieces of g or h are concatenated to form the final descriptor of the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PCB: A Strong Convolutional Baseline</head><p>This section describes the structure of PCB and its comparison with several potential alternative structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Structure of PCB</head><p>Backbone network. PCB can take any network without hidden fully-connected layers designed for image classification as the backbone, e.g., Google Inception <ref type="bibr" target="#b28">[29]</ref> and ResNet <ref type="bibr" target="#b12">[13]</ref>. This paper mainly employs ResNet50 with consideration of its competitive performance as well as its relatively concise architecture.</p><p>From backbone to PCB. We reshape the backbone network to PCB with slight modifications, as illustrated in <ref type="figure">Fig.  2</ref>. The structure before the original global average pooling (GAP) layer is maintained exactly the same as the backbone model. The difference is that the GAP layer and what follows are removed. When an images undergoes all the layers inherited from the backbone network, it becomes a 3D tensor T of activations. In this paper, we define the vector of activations viewed along the channel axis as a column vector. Then, with a conventional average pooling, PCB partitions T into p horizontal stripes and averages all the column vectors in a same stripe into a single part-level column vector g i (i = 1, 2, · · · , p, the subscripts will be omitted unless necessary). Afterwards, PCB employs a convolutional layer to reduce the dimension of g. According to our preliminary experiment, the dimension-reduced column vectors h are set to 256-dim. Finally, each h is input into a classifier, which is implemented with a fully-connected (FC) layer and a following Softmax function, to predict the identity (ID) of the input.</p><p>During training, PCB is optimized by minimizing the sum of Cross-Entropy losses over p pieces of ID predictions. During testing, either p pieces of g or h are concatenated to form the final descriptor G or H, i.e., G =</p><formula xml:id="formula_0">[g 1 , g 2 , · · · , g p ] or H = [h 1 , h 2 , · · · , h p ].</formula><p>As observed in our experiment, employing G achieves slightly higher accuracy, but at a larger computation cost, which is consistent with the observation in <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Important Parameters.</head><p>PCB benefits from fine-grained spatial integration. Several key parameters, i.e., the input image size (i.e., [H, W ]), the spatial size of the tensor T (i.e., [M, N ]), and the number of pooled column vectors (i.e., p) are important to the performance of PCB. Note that [M, N ] is determined by the spatial down-sampling rate of the backbone model, given the fixed-size input. Some deep object detection methods, e.g., SSD <ref type="bibr" target="#b20">[21]</ref> and R-FCN <ref type="bibr" target="#b4">[5]</ref>, show that decreasing the down-sampling rate of the backbone network efficiently enriches the granularity of feature. PCB follows their success by removing the last spatial down-sampling operation in the backbone network to increase the size of T . This manipulation considerably increases person retrieval accuracy with only very light computation cost added. The details can be accessed in Section 5.4, which also provides insights to explain the phenomenon that partitioning tensor T into too many stripes (large p) compromises the discriminative ability of the learned feature.</p><p>Through our experiment, the optimized parameter settings for PCB are:</p><p>• The input images are resized to 384 × 128, with a height to width ratio of 3:1.</p><p>• The spatial size of T is set to 24 × 8.</p><p>• T is equally partitioned into 6 horizontal stripes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Potential Alternative Structures</head><p>Given a same backbone network, there exist several potential alternative structures to learn part-level features. We enumerate two structures for comparison with PCB.</p><p>• Variant 1. Instead of making an ID prediction based on each h i (i = 1, 2, · · · , p), it averages all h i into a single vector h, which is then fully connected to an ID prediction vector. During testing, it also concatenates g or h to form the final descriptor. Variant 1 is featured by learning a convolutional descriptor under a single loss.</p><p>• Variant 2. It adopts exactly the same structure as PCB in <ref type="figure">Fig. 2</ref>. However, all the branches of FC classifiers in Variant 2 share a same set of parameters.</p><p>Both variants are experimentally validated as inferior to PCB. The superiority of PCB against Variant 1 shows that not only the convolutional descriptor itself, but also the respective supervision on each part, is vital for learning discriminative part-level features. The superiority of PCB against Variant 2 shows that sharing weights for classifiers, while reducing the risk of over-fitting, compromises the discriminative ability of the learned part-level features. The experiment details are to be viewed in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Refined Part Pooling</head><p>Uniform partition for PCB is simple, effective, and yet to be improved. This section firstly explains the inconsistency phenomenon accompanying the uniform partition and then proposes the refined part pooling as a remedy to reinforce within-part consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Within-Part Inconsistency</head><p>With focus on the tensor T to be spatially partitioned, our intuition of within-part inconsistency is: column vectors f in a same part of T should be similar to each other and be dissimilar to column vectors in other parts; otherwise the phenomenon of within-part inconsistency occurs, implying that the parts are partitioned inappropriately.</p><p>After training PCB to convergence, we compare the similarities between each f and g i (i = 1, 2, · · · , p), i.e., the average-pooled column vector of each stripe, by measuring cosine distance. If f is closest to g i , f is inferred as closest to the ith part, correspondingly. By doing this, we find the closest part to each f , as exampled in <ref type="figure" target="#fig_0">Fig. 3</ref>. Each column vector is denoted by a small rectangle and painted in the color of its closest part.</p><p>Two phenomena are observed. First, most column vectors in a same horizontal stripe are clustered together (though there are no explicit constraints for this effect). Second, there exist many outliers, while designated to a specified horizontal stripe (part) during training, which are more similar to another part. The existence of these outliers suggests that they are inherently more consistent with column vectors in another part. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Relocating Outliers</head><p>We propose the refined part pooling to correct withinpart inconsistency. Our goal is to assign all the column vectors according to their similarities to each part, so that the outliers will be relocated.</p><p>To this end, we need to classify all the column vectors f in T on the fly. Based on the already-learned T , we use a linear layer followed by Softmax activation as a part classifier as follows:</p><formula xml:id="formula_1">P (P i |f ) = sof tmax(W T i f ) = exp(W T i f ) p j=1 exp(W T j f ) ,<label>(1)</label></formula><p>where P (P i |f ) is the predicted probability of f belonging to part P i , p is the number of pre-defined parts (i.e., p = 6 in PCB), and W is the trainable weight matrix of the part classifier, whose training procedure is to be detailed in Section 4.3.</p><p>Given a column vector f in T and the predicted probability of f belonging to part P i , we assign f to part P i with P (P i |f ) as the confidence. Correspondingly, each part P i (i = 1, 2, ..., p) is sampled from all column vectors f with P (P i |f ) as the sampling weight, i.e.,</p><formula xml:id="formula_2">P i = {P (P i |f ) × f, ∀f ∈ F },<label>(2)</label></formula><p>where F is the complete set of column vectors in tensor T , {•} denotes the sampling operation to form an aggregate. By doing this, the proposed refined part pooling conducts a "soft" and adaptive partition to refine the original "hard" and uniform partition, and the outliers originated from the uniform partition will be relocated. In combination with refined part pooling described above, PCB is further reshaped into <ref type="figure">Fig. 4</ref>. Refined part pooling, i.e., the part classifier along with the following sampling operation, replaces the original average pooling. The structure of all the other layers remain exactly the same as in <ref type="figure">Fig. 2</ref>. <ref type="figure">Figure 4</ref>. PCB in combination with refined part pooling. The 3D tensor T is denoted simply by a rectangle instead of a cube as we focus on the spatial partition. Layers before T are omitted as they remain unchanged compared with <ref type="figure">Fig. 2</ref>. A part classifier predicts the probability of each column vector belonging to p parts. Then each part is sampled from all the column vectors with the corresponding probability as the sampling weight. GAP denotes global average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Induced Training for Part Classifier</head><p>There lacks explicit supervisory information for learning W of the part classifier in Eq. 1. We design an induced training procedure instead, as illustrated in Alg. 1.</p><p>• First, a standard PCB model is trained to convergence with T equally partitioned.</p><p>• Second, we remove the original average pooling layer after T and append a p-category part classifier on T . New parts are sampled from T according to the prediction of the part classifier, as detailed in Section 4.2.</p><p>• Third, we set the all the already learned layers in PCB fixed, leaving only the part classifier trainable. Then we retrain the model on training set. In this condition, the model still expects the tensor T to be equally partitioned, otherwise it will predict incorrect about the identities of training images. So Step 3 penalizes the part classifier until it conducts partition close to the original uniform partition, whereas the part classifier is prone to categorize inherently similar column vectors into a same part. A state of balance will be reached as a result of Step 3.</p><p>• Finally, all the layers are allowed to be updated. The whole net, i.e., PCB along with the part classifier are fine-tuned for overall optimization.</p><p>In the above training procedure, PCB model trained in Step1 induces the training of the part classifier. Step3 and 4 converges very fast, requiring 10 more epochs in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussions on Refined Part Pooling</head><p>With step 1 in Alg. 1 skipped, the training can also converge. In this case, the training will be similar to PAR <ref type="bibr" target="#b36">[37]</ref> which employs attention mechanism to align parts, as introduced in Section 2. We compare both approaches, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Induced training for part classifier</head><p>Step 1. A standard PCB is trained to convergence with uniform partition. Step 2. A p-category part classifier is appended on the tensor T . Step 3. All the pre-trained layers of PCB are fixed. Only the part classifier is trainable. The model is trained until convergence again.</p><p>Step 4. The whole net is fine-tuned to convergence for overall optimization.</p><p>training part classifier with or without step 1, in experiments and find out that the induction procedure matters. Without the proposed induction, the performance turns out significantly lower. For example on Market-1501, when induction is applied, PCB in combination with refined part pooling achieves 80.9% mAP. When induction is removed, mAP decreases to 74.6%. It implies that the proposed induced training is superior to attention mechanism on PCB. The details can be accessed in Section 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Settings</head><p>Datasets. This paper uses three datasets for evaluation, i.e., Market-1501 <ref type="bibr" target="#b38">[39]</ref>, DukeMTMC-reID <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43]</ref>, and CUHK03 <ref type="bibr" target="#b17">[18]</ref>. The Market-1501 dataset contains 1,501 identities observed under 6 camera viewpoints, 19,732 gallery images and 12,936 training images detected by DPM <ref type="bibr" target="#b8">[9]</ref>. The DukeMTMC-reID dataset contains 1,404 identities, 16,522 training images, 2,228 queries, and 17,661 gallery images. With so many images captured by 8 cameras, DukeMTMC-reID manifests itself as one of the most challenging re-ID datasets up to now. The CUHK03 dataset contains 13,164 images of 1,467 identities. Each identity is observed by 2 cameras. CUHK03 offers both hand-labeled and DPM-detected bounding boxes, and we use the latter in this paper. CUHK03 originally adopts 20 random train/test splits, which is time-consuming for deep learning. So we adopt the new training/testing protocol proposed in <ref type="bibr" target="#b43">[44]</ref>. For Market-1501 and DukeMTMC-reID, we use the evaluation packages provided by <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b42">[43]</ref>, respectively. All the experiment evaluates the single-query setting. Moreover, for simplicity we do not use re-ranking algorithms which considerably improve mAP <ref type="bibr" target="#b43">[44]</ref>. Our results are compared with reported results without re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>Implementation of IDE for comparison. We note that the IDE model specified in <ref type="bibr" target="#b39">[40]</ref> is a commonly used baseline in deep re-ID systems <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>. In contrast to the proposed PCB, the IDE model learns a global descriptor. For comparison, we implement the IDE model on the same backbone network, i.e., ResNet50, and with several optimizations over the original one in <ref type="bibr" target="#b39">[40]</ref>, as follows. 1) After the "pool5" layer in ResNet50, we append a fully-connected layer followed by Batch Normalization and ReLU. The output dimension of the appended FC layer is set to 256-dim. 2) We apply dropout on "pool5" layer. Although there are no trainable parameters in "pool5" layer, there is evidence that applying Dropout on it, which outputs a high dimensional feature vector of 2048d, effectively avoids over-fitting and gains considerable improvement <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. We empirically set the dropout ratio to 0.5. On Market-1501, our implemented IDE achieves 85.3% rank-1 accuracy and 68.5% mAP, which is a bit higher than the implementation in <ref type="bibr" target="#b44">[45]</ref>.</p><p>Training. The training images are augmented with horizontal flip and normalization. We set batch size to 64 and train the model for 60 epochs with base learning rate initialized at 0.1 and decayed to 0.01 after 40 epochs. The backbone model is pre-trained on ImageNet <ref type="bibr" target="#b6">[7]</ref>. The learning rate for all the pre-trained layers are set to 0.1× of the base learning rate. When employing refined part pooling for boosting, we append another 10 epochs with learning rate set to 0.01. With two NVIDIA TITAN XP GPUs and Pytorch as the platform, training an IDE model and a standard PCB on Market-1501 (12,936 training images) consumes about 40 and 50 minutes, respectively. The increased training time of PCB is mainly caused by the cancellation of the last spatial down-sample operation in the Conv5 layer, which enlarges the tensor T by 4×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance evaluation</head><p>We evaluate our method on three datasets, with results shown in <ref type="table" target="#tab_0">Table 1</ref>. Both uniform partition (PCB) and refined part pooling (PCB+RPP) are tested.</p><p>PCB is a strong baseline. Comparing PCB and IDE, the prior commonly used baseline in many works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>, we clearly observe the significant advantage of PCB: mAP on three datasets increases from 68.5%, 52.8% and 38.9% to 77.4% (+8.9%), 66.1% (+13.3%) and 54.2% (+15.3%), respectively. This indicates that integrating part information increases the discriminative ability of the feature. The structure of PCB is as concise as that of IDE, and training PCB requires nothing more than training a canonical classification network. We hope it will serve as a baseline for person retrieval task.</p><p>Refined part pooling (RPP) improves PCB especially in mAP. From <ref type="table" target="#tab_0">Table 1</ref>, while PCB already has a high accuracy, RPP brings further improvement to it. On the three datasets, the improvement in rank-1 accuracy is +1.5%, +1.6%, and +3.1%, respectively; the improvement in mAP is +4.2%, +3.1%, and +3.5%, respectively. The improvement is larger in mAP than in rank-1 accuracy. In fact, rank-1 accuracy characterizes the ability to retrieve the easiest match in the camera network, while mAP indicates the ability to find all the matches. So the results indicate that RPP is especially beneficial in finding more challenging matches.</p><p>The benefit of using p losses. To validate the usage of p branches of losses in <ref type="figure">Fig. 2</ref>, we compare our method with Variant 1 which learns the convolutional descriptor under a single classification loss. <ref type="table" target="#tab_0">Table 1</ref> suggests that Variant 1 yields much lower accuracy than PCB, implying that employing a respective loss for each part is vital for learning discriminative part features.</p><p>The benefit of NOT sharing parameters among identity classifiers. In <ref type="figure">Fig. 2</ref>, PCB inputs each column vector h to a FC layer before the Softmax loss. We compare our proposal (not sharing FC layer parameters) with Variant 2 (sharing FC layer parameters). From <ref type="table" target="#tab_0">Table 1</ref>, PCB is higher than Variant 2 by 2.4%, 3.3%, and 7.4% on the three datasets, respectively. This suggests that sharing parameters among the final FC layers is inferior. Comparison with state of the art. We compare PCB and PCB+RPP with state of the art. Comparisons on Market-1501 are detailed in <ref type="table">Table 2</ref>. The compared methods are categorized into three groups, i.e., hand-crafted methods, deep learning methods with global feature and deep learning methods with part features. Relying on uniform partition only, PCB surpasses all the prior methods, including <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref> which require auxiliary part labeling to deliberately align parts. The performance lead is further enlarged by the proposed refined part pooling.</p><p>Comparisons on DukeMTMC-reID and CUHK03 (new training/testing protocol) are summarized in <ref type="table" target="#tab_3">Table 3</ref>. In the compared methods, PCB exceeds <ref type="bibr" target="#b2">[3]</ref> by +5.5% and 17.2% in mAP on the two datasets, respectively. PCB+RPP (refined part pooling) further surpasses it by a large margin of +8.6% mAP on DukeMTMC-reID and +20.5% mAP on CUHK03. PCB+RPP yields higher accuracy than "TriNet+Era" and "SVDNet+Era" <ref type="bibr" target="#b44">[45]</ref> which are enhanced by extra data augmentation.</p><p>In this paper, we report mAP = 81.6%, 69.2%, 57.5% and Rank-1 = 93.8%, 83.3% and 63.7% for Market-1501, Duke and CUHK03, respectively, setting new state of the art on the three datasets. All the results are achieved</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>DukeMTMC-reID CUHK03 rank-1 mAP rank-1 mAP BoW+kissme <ref type="bibr" target="#b38">[39]</ref> 25.   under the single-query mode without re-ranking. Reranking methods will further boost the performance especially mAP. For example, when "PCB+RPP" is combined with the method in <ref type="bibr" target="#b43">[44]</ref>, mAP and Rank-1 accuracy on Market-1501 increases to 91.9% and 95.1%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Parameters Analysis</head><p>We analyze some important parameters of PCB (and with RPP) introduced in Section 3.2 on Market-1501. Once optimized, the same parameters are used for all the three datasets.</p><p>The size of images and tensor T . We vary the image size from 192 × 64 to 576 × 192, using 96 × 32 as interval. Two down-sampling rates are tested, i.e., the original rate, and a halved rate (larger T ). We exhaustively train all these models on PCB and report their performance in <ref type="figure" target="#fig_1">Fig. 5</ref>. Two phenomena are observed.</p><p>First, a larger image size benefits the learned part feature. Both mAP and rank-1 accuracy increase with the image size until reaching a stable performance.</p><p>Second, a smaller down-sampling rate, i.e., a larger spatial size of tensor T enhances the performance, especially  when using relatively small images as input. In <ref type="figure" target="#fig_1">Fig. 5</ref>, PCB using 384 × 128 input and halved down-sampling rate achieves almost the same performance as PCB using 576 × 192 input and the original down-sampling rate. We recommend the manipulation of halving the down-sampling rate with consideration of the computing efficiency.</p><p>The number of parts p. Intuitively, p determines the granularity of the part feature. When p=1, the learned feature is a global one. As p increases, retrieval accuracy improves at first. However, accuracy does not always increase with p, as illustrated in <ref type="figure" target="#fig_2">Fig. 6</ref>. When p = 8 or 12, the performance drops dramatically, regardless of using refined part pooling. A visualization of the refined parts offers insights into this phenomenon, as illustrated in <ref type="figure" target="#fig_3">Fig. 7</ref>. When p increases to 8 or 12, some of the refined parts are very similar to others and some may collapse to an empty part. As a result, an over-increased p actually compromises the discriminative ability of the part features. In real-world applications, we would recommend to use p = 6 parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Market-1501 DukeMTMC rank-1 mAP rank-1 mAP PAR <ref type="bibr" target="#b36">[37]</ref> 81.0 63.  <ref type="table">Table 4</ref>. Ablation study of induction on Market-1501. PAR learns to focus on several parts to discriminate person with attention mechanisms. RPP (w/o induction) means no induction for learning the refined parts and the network learns to focus on several parts with attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Induction and Attention Mechanism</head><p>In this work, when training the part classifier in Alg. 1, a PCB pre-trained with uniform partition is required. The knowledge learned under uniform partition induces the subsequent training of the part classifier. Without PCB pretraining, the network learns to partition T under no induction and becomes similar to methods driven by attention mechanism. We conduct an ablation experiment on Market-1501 and DukeMTMC-reID to compare the two approaches. Results are presented in <ref type="table">Table 4</ref>, from which three observations can be drawn.</p><p>First, no matter which partition strategy is applied in PCB, it significantly outperforms PAR <ref type="bibr" target="#b36">[37]</ref>, which learns to partition through attention mechanism. Second, the attention mechanism also works based on the structure of PCB. Under the "RPP (w/o induction)" setting, the network learns to focus on several parts through attention mechanism, and achieves substantial improvement over IDE, which learns a global descriptor. Third, the induction procedure (PCB training) is critical. When the part classifier is trained without induction, the retrieval performance drops dramatically, compared with the performance achieved by "PCB+RPP". It implies that the refined parts learned through induction is superior to the parts learned through attention mechanism. Partitioned results with induction and attention mechanism are visualized in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper makes two contributions to solving the pedestrian retrieval problem. First, we propose a Part-based Convolutional Baseline (PCB) for learning part-informed features. PCB employs a simple uniform partition strategy and assembles part-informed features into a convolutional descriptor. PCB advances the state of the art to a new level, proving itself as a strong baseline for learning part-informed features. Despite the fact that PCB with uniform partition is simple and effective, it is yet to be improved. We propose the refined part pooling to reinforce the within-part consis-tency in each part. After refinement, similar column vectors are concluded into a same part, making each part more internally consistent. Refined part pooling requires no part labeling information and improves PCB considerably.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of within-part inconsistency. T . Left: T is equally partitioned to p = 6 horizontal stripes (parts) during training. Right: Every column vector in T is denoted with a small rectangle and painted in the color of its closest part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Impact of the size of the image and T . Rank-1 accuracy and mAP are compared. Using the original and halved downsampling rates, two different sizes of T are compared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Impact of p. Rank-1 accuracy and mAP are compared. We compare PCB both with and without the refined part pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of the refined parts under different p values. When p = 8 or 12, some parts repeat with others or become empty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>94.0 96.3 68.5 73.2 84.0 87.6 52.8 43.8 62.7 71.2 38.9 IDE FC 256 83.8 93.1 95.8 67.7 72.4 83.0 87.1 51.6 43.3 62.5 71.0 38.3 Variant 1 G 12288 86.7 95.2 96.5 69.4 73.9 84.6 88.1 53.2 43.6 62.9 71.3 38.Comparison of the proposed method with IDE and 2 variants. Both variants are described in Section 3.3. pool5: output of Pool5 layer in ResNet50. FC: output of the appended FC layer for dimension reduction. G (H): feature representation assembled with column vectors g (h). Both g and h are illustrated in Fig. 2.</figDesc><table><row><cell>Models</cell><cell>Feature</cell><cell>dim</cell><cell>Market-1501 R-1 R-5 R-10 mAP R-1 R-5 R-10 mAP R-1 R-5 R-10 mAP DukeMTMC-reID CUHK03</cell></row><row><cell>IDE</cell><cell>pool5</cell><cell cols="2">2048 85.3 8</cell></row><row><cell>Variant 1</cell><cell>H</cell><cell cols="2">1536 85.6 94.3 96.3 68.3 72.8 83.3 87.2 52.5 44.1 63.0 71.5 39.1</cell></row><row><cell>Variant 2</cell><cell>G</cell><cell cols="2">12288 91.2 96.6 97.7 75.0 80.2 88.8 91.3 62.8 52.6 72.4 80.9 45.8</cell></row><row><cell>Variant 2</cell><cell>H</cell><cell cols="2">1536 91.0 96.6 97.6 75.3 80.0 88.1 90.4 62.6 54.0 73.7 81.4 47.2</cell></row><row><cell>PCB</cell><cell>G</cell><cell cols="2">12288 92.3 97.2 98.2 77.4 81.7 89.7 91.9 66.1 59.7 77.7 85.2 53.2</cell></row><row><cell>PCB</cell><cell>H</cell><cell cols="2">1536 92.4 97.0 97.9 77.3 81.9 89.4 91.6 65.3 61.3 78.6 85.6 54.2</cell></row><row><cell>PCB+RPP</cell><cell>G</cell><cell cols="2">12288 93.8 97.5 98.5 81.6 83.3 90.5 92.5 69.2 62.8 79.8 86.8 56.7</cell></row><row><cell>PCB+RPP</cell><cell>H</cell><cell cols="2">1536 93.1 97.4 98.3 81.0 82.9 90.1 92.3 68.5 63.7 80.6 86.9 57.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Comparison with prior art on DukeMTMC-reID and</cell></row><row><cell>CUHK03. Rank-1 accuracy (%) and mAP (%) are shown.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>68.5 73.2 52.8 RPP (w/o induction) 88.7 74.6 78.8 60.9 PCB 92.3 77.4 81.7 66.1 PCB+RPP 93.8 81.6 83.3 69.2</figDesc><table><row><cell></cell><cell>4</cell><cell>-</cell><cell>-</cell></row><row><cell>IDE</cell><cell>85.3</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Looking beyond appearances: Synthetic training data for deep cnns in re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rognhaugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03153</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Person re-identification by deep learning multi-scale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, Workshop on Cross-Domain Human Identification (CHI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-FCN: object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Consistent Re-identification in a Camera Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person re-identification by support vector ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Nutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05244</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Person reidentification using spatiotemporal appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable metric learning via weighted approximate rank component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A comprehensive evaluation and benchmark for person re-identification: Features, metrics, and datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rates-Borras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09653</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Person re-identification by deep joint learning of multi-loss classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain transfer support vector ranking for person re-identification without target camera label information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multicamera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision workshop on Benchmarking Multi-Target Tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Posedriven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SVDNet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multiregion bilinear convolutional neural networks for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">GLAD: Global-local-alignment descriptor for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ACM Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep representation learning with part loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00798</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00384</idno>
		<title level="m">Deep mutual learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pose invariant embedding for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07732</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Person reidentification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reidentification by relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00408</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
