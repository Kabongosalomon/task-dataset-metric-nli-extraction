<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Open Set to Closed Set: Counting Objects by Spatial Divide-and-Conquer *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
							<email>hao.lu@adelaide.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
							<email>zgcao@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">From Open Set to Closed Set: Counting Objects by Spatial Divide-and-Conquer *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code has been made available at: https://github. com/xhp-hust-2018-2011/S-DCNet.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual counting, a task that predicts the number of objects from an image/video, is an open-set problem by nature, i.e., the number of population can vary in [0, +∞) in theory. However, the collected images and labeled count values are limited in reality, which means only a small closed set is observed. Existing methods typically model this task in a regression manner, while they are likely to suffer from an unseen scene with counts out of the scope of the closed set. In fact, counting is decomposable. A dense region can always be divided until sub-region counts are within the previously observed closed set. Inspired by this idea, we propose a simple but effective approach, Spatial Divide-and-Conquer Network (S-DCNet). S-DCNet only learns from a closed set but can generalize well to open-set scenarios via S-DC. S-DCNet is also efficient. To avoid repeatedly computing sub-region convolutional features, S-DC is executed on the feature map instead of on the input image. S-DCNet achieves the state-of-the-art performance on three crowd counting datasets (ShanghaiTech, UCF CC 50 and UCF-QNRF), a vehicle counting dataset (TRANCOS) and a plant counting dataset (MTC). Compared to the previous best methods, S-DCNet brings a 20.2% relative improvement on the ShanghaiTech Part B, 20.9% on the UCF-QNRF, 22.5% on the TRANCOS and 15.1% on the MTC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of visual counting in Computer Vision is to infer the number of objects (people, cars, maize tassels, etc.) from an image/video. It has wide applications, such as automatic crowd management <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, traffic monitoring <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref>, and crop yield estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref>. Extensive attention has been received in recent years.  <ref type="bibr" target="#b37">[38]</ref>. The orange curve denotes the relative mean absolute error (rMAE) of CSR-Net <ref type="bibr" target="#b19">[20]</ref> on local patches.</p><p>Counting is an open-set problem by nature as a count value can range from 0 to +∞ in theory. It is thus typically modeled in a regression manner. Benefiting from the success of convolutional neural networks (CNNs), state-ofthe-art deep counting networks often adopt a multi-branch architecture to enhance the feature robustness to dense regions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38]</ref>. However, the observed patterns in datasets are limited in practice, which means networks can only learn from a closed set. Are these counting networks still able to generate accurate predictions when the number of objects is out of the scope of the closed set? Meanwhile, observed local counts exhibit a long-tailed distribution shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Extremely dense patches are rare while sparse patches take up the majority. As what can be observed, the relative mean absolute error (rMAE) increases significantly with increased local density. Is it necessary to set the working range of CNN-based regressors to the maximum count value observed, even with a majority of samples are sparse such that the regressor works poorly in this range?</p><p>In fact, counting has an unique property-spatially decomposable. The above problem can be largely alleviated with the idea of spatial divide-and-conquer (S-DC). Suppose that a network has been trained to accurately predict a closed set of counts, say 0 ∼ 20. When facing an image with extremely dense objects, one can keep dividing the <ref type="bibr">Figure 2</ref>. An illustration of spatial divisions. Suppose that the closed set of counts is <ref type="bibr">[0,</ref><ref type="bibr" target="#b19">20]</ref>. In this example, dividing the image for one time is inadequate to ensure that all sub-region counts are within the closed set. For the top left sub-region, it needs a further division. <ref type="figure">Figure 3</ref>. Spatial divisions on the input image (left) and the feature map (right). Spatially dividing the input image is straightforward. The image is upsampled and fed to the same network to infer counts of local areas. The orange dashed line is used to connect the local feature map, the local count and the sub-image. S-DC on the feature map avoids redundant computations and is achieved by upsampling, decoding and dividing the feature map of high resolution.</p><p>image into sub-images until all sub-region counts are less than 20. Then the network can accurately count these subimages and sum over all local counts to obtain the global image count. <ref type="figure">Figure 2</ref> graphically depicts the idea of S-DC. A follow-up question is how to spatially divide the count. A naive way is to upsample the input image, divide it into sub-images and process sub-images with the same network. This way, however, is likely to blur the image and lead to exponentially-increased computation cost and memory consumption when repeatably extracting the feature map. Inspired by RoI pooling <ref type="bibr" target="#b11">[12]</ref>, we show that it is feasible to achieve S-DC on the feature map, as conceptually illustrated in <ref type="figure">Figure 3</ref>. By decoding and upsampling the feature map, the later prediction layers can focus on the feature of local areas and predict sub-region counts accordingly.</p><p>To realize the above idea, we propose a simple but effective Spatial Divide-and-Conquer Network (S-DCNet). S-DCNet learns from a closed set of count values but is able to generalize to open-set scenarios. Specifically, S-DCNet adopts a VGG16 <ref type="bibr" target="#b29">[30]</ref>-based encoder and an UNet <ref type="bibr" target="#b26">[27]</ref>like decoder to generate multi-resolution feature maps. All feature maps share the same counting predictor. Inspired by <ref type="bibr" target="#b18">[19]</ref>, in contrast to the conventional density map regression, we discretize continuous count values into a set of intervals and design the counting predictor to be a classifier. Further, a division decider is designed to decide which sub-region should be divided and to merge different levels of sub-region counts into the global image count. We show through a controlled toy experiment that, even given a closed training set, S-DCNet effectively generalizes to the open test set. The effectiveness of S-DCNet is further demonstrated on three crowd counting datasets (Shang-haiTech <ref type="bibr" target="#b37">[38]</ref>, UCF CC 50 <ref type="bibr" target="#b14">[15]</ref> and UCF-QNRF <ref type="bibr" target="#b15">[16]</ref>), a vehicle counting dataset (TRANCOS <ref type="bibr" target="#b13">[14]</ref>), and a plant counting dataset (MTC <ref type="bibr" target="#b22">[23]</ref>). Results show that S-DCNet indicates a clear advantage over other competitors and sets the new state-of-the-art across five datasets.</p><p>The main contribution of this work is that we propose to transform open-set counting into a closed-set problem. We show through extensive experiments that a model learned in a closed set can effectively generalize to the open set with the idea of S-DC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Current CNN-based counting approaches are mainly built upon the framework of local regression. According to their regression targets, they can be categorized into two categories: density map regression and local count regression. We first review these two types of regression. Since S-DCNet learns to classify counts, some works that reformulate the regression problem are also discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Density Map Regression</head><p>The concept of density map was introduced in <ref type="bibr" target="#b17">[18]</ref>. The density map contains the spatial distribution of objects, thus can be smoothly regressed. Zhang et al. <ref type="bibr" target="#b36">[37]</ref> first adopted a CNN to regress local density maps. Then almost all subsequent counting networks followed this idea. Among them, a typical network architecture is multi-branch. MCNN <ref type="bibr" target="#b37">[38]</ref> and Switching-CNN <ref type="bibr" target="#b1">[2]</ref> used three columns of CNNs with varying receptive fields to depict objects of different scales. SANet <ref type="bibr" target="#b3">[4]</ref> adopted Inception <ref type="bibr" target="#b33">[34]</ref>-liked modules to integrate extra branches. CP-CNN <ref type="bibr" target="#b31">[32]</ref> added two extra density-level prediction branches to combine global and local contextual information. AC-SCP <ref type="bibr" target="#b27">[28]</ref> inserted a child branch to match cross-scale consistency and an adversarial branch to attenuate the blurring effect of the density map. ic-CNN <ref type="bibr" target="#b25">[26]</ref> incorporated two branches to generate high-quality density maps in a coarse-to-fine manner. IG-CNN <ref type="bibr" target="#b0">[1]</ref> and D-ConvNet <ref type="bibr" target="#b28">[29]</ref> drew inspirations from ensemble learning and trained a series of networks or regressors to tackle different scenes. DecideNet <ref type="bibr" target="#b20">[21]</ref> attempted to selectively fuse the results of density map estimation and object detection for different scenes. Unlike multi-branch approaches, Idrees et al. <ref type="bibr" target="#b15">[16]</ref> employed a composition loss and simultaneously solved several counting-related tasks to assist counting. CSR-Net <ref type="bibr" target="#b19">[20]</ref> benefited from dilated convolution which effectively expanded the receptive field to capture contextual information.</p><p>Existing deep counting networks aim to generate highquality density maps. However, density maps are actually in the open set as well. Detailed discussion of the open set problem in density maps is provided in the Supplement.</p><p>Local Count Regression Local count regression directly predicts count values of local image patches. This idea first appeared in <ref type="bibr" target="#b6">[7]</ref> where a multi-output regression model was used to regress region-wise local counts simultaneously. <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b22">[23]</ref> introduced such an idea into deep counting. Local patches were first densely sampled in a slidingwindow manner with overlaps, and a local count was then assigned to each patch by the network. Inferred redundant local counts were finally normalized and fused to the global count. Stahl et al. <ref type="bibr" target="#b32">[33]</ref> regressed the counts for object proposals generated by Selective Search <ref type="bibr" target="#b35">[36]</ref> and combined local counts using an inclusion-exclusion principle. Inspired by subitizing, the ability for a human to quickly counting a few objects at a glance, Chattopadhyay et al. <ref type="bibr" target="#b4">[5]</ref> transferred their focus to the problem of counting objects in everyday scenes. The main challenge thus became large intra-class variances rather than the occlusions and perspective distortions in crowded scenes.</p><p>While some above methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref> also leverage the idea of spatial divisions, they still regress the open-set counts. Although local-region patterns are easier to be modelled than the whole image, the observed local patches are still limited. Since only finite local patterns (a closed set) can be observed, new scenes in reality have a high probability including objects out of the range (an open set). Moreover, dense regions with large count values are rare ( <ref type="figure" target="#fig_0">Figure 1</ref>) and the networks may suffer from sample imbalance. In this paper, we show that a counting network is able to learn from a closed set with a certain range of counts, say 0 ∼ 20, and then generalizes to an open set (including counts &gt; 20) via S-DC.</p><p>Beyond Naive Regression Regression is a natural way to estimating continuous variables, such as age and depth. However, some literatures suggest that regression is encouraged to be reformulated as an ordinal regression problem or a classification problem, which enhances performance and benefits optimization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>. Ordinal regression is usually implemented by modifying well-studied classification algorithms and has been applied to the problem of age estimation <ref type="bibr" target="#b23">[24]</ref> and monocular depth prediction <ref type="bibr" target="#b10">[11]</ref>. Li et al. <ref type="bibr" target="#b18">[19]</ref> further showed that directly reformulating regres- sion to classification was also a good choice. Since count values share a similar property like age and depth, it motivates us to follow such a reformulation. In this work, S-DCNet follows <ref type="bibr" target="#b18">[19]</ref> to discretize local counts and classify count intervals. Indeed, we observe in experiments that classification with S-DC works better than direct regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spatial Divide-and-Conquer Network</head><p>In this section, we describe the transformation from quantity to interval which transfers count values into a closed set. We also explain in detail our proposed S-DCNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">From Quantity to Interval</head><p>Instead of regressing an open set of count values, we follow <ref type="bibr" target="#b18">[19]</ref> to discretize local counts and classify count intervals. Specifically, we define an interval partition of [0, +∞) as {0}, (0, C 1 ], (C 2 , C 3 ], ... , (C M −1 , C M ] and (C M , +∞). These M + 1 sub-intervals are labeled to the 0-th to the Mth classes, respectively. For example, if a count value is within (C 2 , C 3 ], it is labeled as the 2-th class. In practice, C M should be not greater than the max local count observed in the training set.</p><p>The median of each sub-interval is adopted when recovering the count from the interval. Notice that, for the last sub-interval (C M , +∞], C M will be used as the count value if a region is classified into this interval. It is clear that adopting C M for the last class will cause a systematic error, but the error can be mitigated via S-DC as what we will show in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Single-Stage Spatial Divide-and-Conquer</head><p>As shown in <ref type="figure" target="#fig_1">Figure 4</ref>, S-DCNet includes a VGG16 <ref type="bibr" target="#b29">[30]</ref> feature encoder, an UNet <ref type="bibr" target="#b26">[27]</ref>-like decoder, a count-interval classifier and a division decider. The structure of the classifier and the division decider are shown in <ref type="table" target="#tab_0">Table 1</ref>. Notice that, the first average pooling layer in the classifier has a stride of 2, so the final prediction has an output stride of 64.</p><p>The feature encoder removes fully-connected layers from the pre-trained VGG16. Suppose that the input patch is of size 64 × 64. Given the feature map F 0 (extracted  <ref type="figure">Figure 3</ref>. A shared classifier and a division decider receive divided feature maps, and respectively, generate division counts Cis and division masks Wis, for i = 1, 2, ... After obtaining these results, Ci and Wi are merged to the i-th division count DIVi shown in the right sub-figure. Specially, we average each count of low resolution into the corresponding 2 × 2 area of high resolution before merging (avg shown in the figure). "•" denotes the Hadamard product. Note that, the 64 × 64 local patch is only used as an example for readers to understand the pipeline of S-DCNet. Since S-DCNet is a fully convolutional network, it can process images of arbitrary sizes M × N and return DIV2s of size M 64 × N 64 . The structures for the classifier and the division decider are presented in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>from the Conv5 layer) with 1 32 resolution of the input image, the classifier predicts the class label of the count interval CLS 0 conditioned on F 0 . The local count C 0 , which denotes the count value of the 64 × 64 input patch, can be recovered from CLS 0 . Note that C 0 is the local count without S-DC, which is also the final output of previous approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>We execute the first-stage S-DC on the fused feature map F 1 . F 1 is divided and sent to the shared classifier to produce the division count C 1 ∈ R 2×2 . Concretely, F 0 is upsampled by ×2 in an UNet-like manner to F 1 . Given F 1 , the classifier fetches the local features that correspond to spatially divided sub-regions, and predicts the first-level division counts C 1 . Each of the 2 × 2 elements in C 1 denotes a sub-count of the corresponding 32 × 32 sub-region.</p><p>With local counts C 0 and C 1 , the next question is to decide where to divide. We learn such decisions with another network module, division decider, as depicted in the right part of <ref type="figure" target="#fig_1">Figure 4</ref>. At the first stage of S-DC, the division decider generates a soft division mask W 1 of the same size as C 1 conditioned on F 1 such that for any w ∈ W 1 , w ∈ [0, 1]. w = 0 means no division is required at this position, and the value in C 0 is used. w = 1 implies that here the initial prediction should be replaced with the division count in C 1 . Since W 1 and C 1 are both 2 times larger than C 0 , C 0 is upsampled by ×2 toĈ 0 , and the count is averaged into the 2 × 2 local area inĈ 0 . The first-stage division result DIV 1 can thus be computed as</p><formula xml:id="formula_0">DIV 1 = (1 − W 1 ) • avg(C 0 ) + W 1 • C 1 ,<label>(1)</label></formula><p>where 1 denotes a matrix filled with 1 and is with the same size of W 1 . "•" denotes the Hadamard product. avg is an averaging re-distribution operator (equally dividing a count value into a 2 × 2 region).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-Stage Spatial Divide-and-Conquer</head><p>S-DCNet can execute multi-stage S-DC by further decoding, dividing the feature map until reaching the output of the first convolutional block. In this sense, the maximum division time is 4 in VGG16 (actually we show later in experiments that a two-stage division is adequate to guarantee satisfactory performance). In multi-stage S-DC, DIV i (i ≥ 2) is merged in a recursive manner as</p><formula xml:id="formula_1">DIV i = (1 − W i ) • avg(DIV i−1 ) + W i • C i . (2)</formula><p>We employ two types of standard loss functions to train S-DCNet: several cross-entropy losses L i C s that correspond to different classification outputs CLS i s, and a 1 loss L N R for the final division output DIV N (N denotes the division time). S-DCNet is learned in a multi-task manner where the overall loss L is a summation of all losses, i.e., L = N i=0 L i C + L N R . Note that, L N R is essential to provide an implicit supervision signal for learning W i s. Multi-stage S-DCNet is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Open Set or Closed Set? A Toy-Level Justification</head><p>As aforementioned, counting is an open-set problem while the model is learned in a closed set. Can a closedset counting model really generalize to open-set scenarios? Here we show through a controlled toy experiment that, the answer is no. Inspired by <ref type="bibr" target="#b17">[18]</ref>, we synthesize a cell counting dataset to explore the counting performance outside a closed training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthesized Cell Counting Dataset</head><p>We first generate 500 256 × 256 images with 64 × 64 sub-regions containing only 0 ∼ 10 cells to construct the training set (a closed set). To generate an open testing set, we further synthesize 500 images with sub-region counts evenly distributed in the range of [0, 20].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines and Protocols</head><p>We adopt three approaches for comparisons, they are: i) a regression baseline with pretrained VGG16 as the backbone and the classifier module used in S-DCNet as the backend except that the output channel is modified to 1. 1 loss is used. This approach directly regresses the open-set counts; ii) a classification baseline with the same VGG16 and the classifier settings as S-DCNet, without S-DC; iii) our proposed S-DCNet, which learns from a closed set but adapts to the open set via S-DC.</p><p>Regarding the discretization of count intervals, we choose 0.5 as the step because cells can be partially presented in local patches. As a consequence, we have a partition of {0}, (0.0.5],(0.5, 1], ... ,(9.5, 10] and (10, +∞). All approaches are trained with standard stochastic gradient descent (SGD). The learning rate is initially set to 0.001 and is decreased by ×10 when the training error stagnates.</p><p>Observations According to <ref type="figure" target="#fig_2">Figure 5</ref>, it can be observed that both regression and classification baselines work well in the range of the closed set (0 ∼ 10), but the counting error increases rapidly when counts are larger than 10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on Real-World Datasets</head><p>Extensive experiments are further conducted to demonstrate the effectiveness of S-DCNet on real-world datasets. We first describe some essential implementation details. After that, an ablation study is conducted on the ShanghaiTech Part A <ref type="bibr" target="#b37">[38]</ref> dataset to highlight the benefit of S-DC. Finally, we compare S-DCNet against current state-of-the-art methods on five public datasets. Mean Absolute Error (MAE) and Root Mean Squared Error (MSE) are used as the evaluation metrics following <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>Interval Partition We generate ground-truth counts of local patches by integrating over the density maps. The counts are usually not integers, because objects can partly present in cropped local patches. We evaluate two different partition strategies. In the first partition, we choose 0.5 as the step and generate partitions as {0}, (0.0.5],(0.5, 1], ... ,(C max − 0.5, C max ] and (C max , +∞), where C max denotes the maximum count of the closed set. This partition is named as One-Linear Partition.</p><p>In the second partition, we further finely divide the subinterval (0.0.5], because this interval contains a sudden change from no object to part of an object, and a large proportion of objects lie in this sub-interval. A small step of 0.05 is used to divide this interval. We call this partition Two-Linear Partition.</p><p>Data Augmentation We follow the same data augmentation used in <ref type="bibr" target="#b19">[20]</ref>, except for the UCF-QNRF dataset <ref type="bibr" target="#b15">[16]</ref>. In particular, 9 sub-images of 1 the original image. The first 4 sub-images are from four corners, and the remaining 5 are randomly cropped. Random scaling and mirroring are also performed. For the UCF-QNRF dataset <ref type="bibr" target="#b15">[16]</ref>, we follow the same setting as in <ref type="bibr" target="#b15">[16]</ref> and crop the original image into 224 × 224 sub-images.</p><p>Training Details S-DCNet is implemented with PyTorch. We train S-DCNet using standard SGD. The encoder in S-DCNet is directly adopted from convolutional layers of VGG16 <ref type="bibr" target="#b29">[30]</ref> pretrained on ImageNet, and the other layers employ random Gaussian initialization with a standard deviation of 0.01. The learning rate is initially set to 0.001 and is decreased by ×10 when the training error stagnates. We keep training until convergence. For the ShanghaiTech, UCF CC 50, TRANCOS and MTC datasets, the batch size is set to 1. For the UCF-QNRF dataset, the batch size is set to 16 following <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study on the ShanghaiTech Part A</head><p>Is S-DCNet Robust to C max ? When reformulating the counting problem into classification, a critical issue is how to choose C max , which defines the closed set. Hence, it is important that S-DCNet is robust to the choice of C max .</p><p>We conduct a statistical analysis on count values of local patches in the training set, and then set C max with the quantiles ranging from 100% to 80% (decreased by 5%). Twostage S-DCNet is evaluated. Another baseline of classification without S-DC is also used to explore whether counting can be simply modeled in a closed-set classification manner. To be specific, we reserve the VGG16 encoder and the classifier in this classification baseline.</p><p>Results are presented in <ref type="figure">Figure 6</ref>. It can be observed that the MAE of the classification baseline increases rapidly with decreased C max . This result is not surprising, because the model is constrained to be visible to count values not greater than C max . This suggests that counting cannot be simply transformed into closed-set classification. However, with the help of S-DC, S-DCNet exhibits strong robustness to the changes of C max . It seems the systematic error brought by C max can somewhat be alleviated with S-DC. Regarding how to choose concrete C max , the maximum count of the training set seems not the best choice, while some small quantiles even deliver better performance. Perhaps a model is only able to count objects accurately within a certain degree of denseness. We also notice Two-Linear Partition is slightly better than One-Linear Partition, which indicates that the fine division to the (0, 0.5] sub-interval has a positive effect.</p><p>According to the above results, S-DCNet is robust to C max in a wide range of values, and C max is generally encouraged to be set less than the maximum count value observed. In addition, there is no significant difference between two kinds of partitions. For simplicity, we set <ref type="figure">Figure 6</ref>. The influence of Cmax to S-DCNet on the ShanghaiTech Part A dataset <ref type="bibr" target="#b37">[38]</ref>. The numbers in the brackets denote quantiles of the training set, for example, 22 (95%) means the 95% quantile is 22. 'VGG16 Encoder' is the classification baseline without S-DC. 'One-Linear' and 'Two-Linear' are defined in Section 5.1.</p><p>C max to be the 95% quantile and adopt Two-Linear Partition in the following experiments.</p><p>How Many Times to Divide? S-DCNet can apply S-DC up to 4 times, but how many times are sufficient? Here we evaluate S-DCNet with different division stages. Quantitative results are listed in <ref type="table">Table 2</ref>. It can be observed that applying two-stage S-DC is clearly adequate.</p><p>The Effect of S-DC To highlight the effect of S-DC, we compare S-DCNet against several regression and classification baselines. These baselines adopt the same architecture of VGG16 encoder and the classifier in S-DCNet. classification is the result of C 0 without S-DC, and C max is set to be the 95% quantile <ref type="bibr" target="#b21">(22)</ref>. For all regression baselines, we modify the output channel of the classifier to be 1 and employ the 1 loss. We set three regression baselines. regression predicts counts without S-DC. To justify whether S-DC can also work in regression, we adapt the S-DC idea to regression under both open-set and closed-set settings. openset regression + S-DC is straight-forward. We do not limit the output range, and it can vary from 0 to +∞. closed-set regression + S-DC indicates that the output range is constrained within [0, C max ] (C max is set to 22 for a fair comparison), and large outputs will be clipped to C max .</p><p>Results are shown in <ref type="table" target="#tab_3">Table 3</ref>. We can see that counting by classification without S-DC suffers from the limitation of C max and performs even worse than regression. In addition, regression can also benefit from S-DC, and it is encouraged to limit the output range of the regressor in a closed set. Moreover, with S-DC, S-DCNet significantly reduces the counting error and outperforms both the classification and regression baselines by a large margin. This verifies our argument that it is more effective to reformulate counting in classification than in regression. Perhaps the optimization is easier and less sensitive to sample imbalance in classification than in regression. Whatever, at least one thing is   <ref type="table">Table 4</ref>. Effect of different loss functions. Note that, multi-stage predictions are averaged if L 2 R is not applied, because the division decider cannot receive supervision signal during training. The best performance is boldfaced. <ref type="figure">Figure 7</ref>. Counting errors of 64 × 64 local patches on the test set of ShanghaiTech Part A <ref type="bibr" target="#b37">[38]</ref>. regression denotes direct local counts regression using VGG16. C0, C1 and C2 are single-branch predictions conditioned on F0, F1 and F2, respectively. DIV2 denotes two-stage S-DCNet, which fuses the predictions of C0, C1 and C2 with S-DC. made clear: a counting model can learn from a closed set and generalize well to a open set via S-DC.</p><p>We further analyze the counting error of 64 × 64 local patches in detail. As shown in <ref type="figure">Figure 7</ref>, we observe that the direct single-branch prediction without S-DC (predicting C 0 , C 1 and C 2 from F 0, F 1 and F 2, respectively) performs worse than the regression baseline, which can be attributed to the limited C max of the classifier. After embedding the S-DC strategy to divide and merge the count map of multiple resolutions, counting errors significantly reduce. Such a benefit is even much obvious in dense patches with local counts greater than 100. It justifies our argument that, instead of regressing a large count value directly, it is more accurate to count dense patches through S-DC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Functions Also Matter</head><p>We further validate the effect of different loss functions used in S-DCNet and report the results in <ref type="table">Table 4</ref>. S-DCNet works poorly when trained with only L 2 R . This is not surprising, because no supervision signal is provided to multi-stage division results. In addition, it seems necessary for the division decider to decide where to divide, because S-DCNet greatly benefits from the help of merging loss L 2 R . Through the visualizations of W i s in <ref type="figure" target="#fig_0">Fig. 10</ref>, we observe that reasonably good divisions can be achieved with the supervision of L 2 R . This has another benefit, the network can <ref type="figure">Figure 8</ref>. Visualizations of Wis in S-DCNet. The brighter the image is, the greater the values are. In the input image, count values greater than Cmax are indicated by yellow regions. It is clear that Wi appropriately identifies regions to be divided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Cmax max Gaussian kernel SH Part A <ref type="bibr" target="#b37">[38]</ref> 22.0 148.5 Geometry-Adaptive UCF CC 50 <ref type="bibr" target="#b14">[15]</ref> − − UCF-QNRF <ref type="bibr" target="#b15">[16]</ref> 8.0 131.5 SH Part B <ref type="bibr" target="#b37">[38]</ref> 7.0 83.  <ref type="table">Table 5</ref>. Overall configurations of S-DCNet. max denotes the maximum count of local patch in the training set, while Cmax is the maximum count set for the closed set in S-DCNet. Gaussian kernel is used to generate density maps from dotted annotations. Specially, since UCF CC 50 adopts 5-fold crossvalidation, max and Cmax are set differently for each fold.</p><p>learn when to divide not just in counts larger than C max .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State of the Art</head><p>According to the ablation study, the final configurations for S-DCNet are summarized in <ref type="table">Table 5</ref>. Qualitative results are shown in the Supplement.</p><p>The ShanghaiTech Dataset The ShanghaiTech crowd counting dataset <ref type="bibr" target="#b37">[38]</ref> is consisted of two parts: Part A and Part B. Part A includes 300 images for training and 182 for testing. This part represents highly congested scenes. Part B contains 716 images in relatively sparse scenes, where 400 images are used for training and 316 for testing. Quantitative results are listed in <ref type="table">Table 6</ref>. Our method outperforms the previous state-of-the-art SPN <ref type="bibr" target="#b7">[8]</ref> and SANet <ref type="bibr" target="#b3">[4]</ref> with a 5.5% relative improvement in Part A and 20.2% in Part B, respectively. These results suggest S-DCNet is able to adapt to both sparse and crowded scenes.</p><p>The UCF CC 50 Dataset UCF CC 50 <ref type="bibr" target="#b14">[15]</ref> is a tiny crowd counting dataset with 50 images in extremely  <ref type="table">Table 7</ref>. Comparison with state-ofthe-art approaches on the test set of UCF CC 50 <ref type="bibr" target="#b14">[15]</ref> dataset. The best performance is boldfaced.</p><p>Method MAE MSE Idreeset al. <ref type="bibr" target="#b14">[15]</ref> 315 508 MCNN <ref type="bibr" target="#b37">[38]</ref> 277 426 Encoder-Decoder <ref type="bibr" target="#b2">[3]</ref> 270 478 CMTL <ref type="bibr" target="#b30">[31]</ref> 252 514 Switching-CNN <ref type="bibr" target="#b1">[2]</ref> 228 445 Base Network <ref type="bibr" target="#b15">[16]</ref> 163 227 Composition Loss <ref type="bibr" target="#b15">[16]</ref> 132 191 S-DCNet 104.4 176.1 <ref type="table">Table 8</ref>. Comparison with state-ofthe-art approaches on the test set of UCF-QNRF <ref type="bibr" target="#b15">[16]</ref> dataset. The best performance is boldfaced.</p><p>Method GAME(0) GAME(1) GAME(2) GAME <ref type="formula">(</ref>   crowded scenes. The number of people within an images varies from 96 to 4633. We follow the 5-fold crossvalidation as in <ref type="bibr" target="#b14">[15]</ref>. Results are shown in <ref type="table">Table 7</ref>. Our method surpasses the previous best method, DRSAN <ref type="bibr" target="#b21">[22]</ref>, with a 6.8% relative improvement in MAE.</p><p>The UCF-QNRF Dataset UCF-QNRF <ref type="bibr" target="#b15">[16]</ref> is a large crowd counting dataset with 1535 high-resolution images and 1.25 million head annotations. There are 1201 training images and 334 test images. It contains extremely congested scenes where the maximum count of an image can reach 12865. We follow the same image processing as in <ref type="bibr" target="#b15">[16]</ref> and report results in <ref type="table">Table 8</ref>. Our method reaches the state-of-the-art performance and surpasses the previous best method with a 20.9% boost in MAE. We surprisingly notice that S-DCNet only learn from a closed set with C max = 8.0, which is only 6% of the maximum count 131.5 according to <ref type="table">Table 5</ref>. S-DCNet, however, generalizes to large counts effectively and predicts accurate counts.</p><p>The TRANCOS Dataset Aside from crowd counting, we also evaluate S-DCNet on a vehicle counting dataset, TRANCOS <ref type="bibr" target="#b13">[14]</ref>, to see its generalization ability. TRAN-COS contains 1244 images of congested traffic scenes in various perspectives. It adopts the Grid Average Mean Absolute Error (GAME) <ref type="bibr" target="#b13">[14]</ref> as the evaluation metric. GAM E(L) divides an image into 2 L ×2 L non-overlapping sub-regions and accumulates of the M AE over sub-regions. Larger L implies better local predictions. In particular, GAM E(0) downgrades to M AE. Results are listed in <ref type="table" target="#tab_7">Table 9</ref>. S-DCNet surpasses other methods under all GAM E(L) metrics, and particularly, delivers a 22.5% relative improvement on GAM E(3). This suggests S-DCNet not only achieves accurate global predictions but also be-haves well in local regions.</p><p>The MTC Dataset We further evaluate our method on a plant counting dataset, i.e., the MTC dataset <ref type="bibr" target="#b22">[23]</ref>. The MTC dataset contains 361 high-resolution images of maize tassels collected from 2010 to 2015 in the wild field. In contrast to people or vehicles that have similar physical sizes, maize tassels are with heterogeneous physical sizes and are self-changing over time. We think this dataset is suitable for justifying the robustness of S-DCNet to object-size variations. We follow the same setting as in <ref type="bibr" target="#b22">[23]</ref> and report quantitative results in <ref type="table" target="#tab_0">Table 10</ref>. Although the previous best method, TasselNet <ref type="bibr" target="#b22">[23]</ref>, already exhibits accurate results, S-DCNet still shows a certain degree of improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Counting is an open-set problem in theory, but only a finite closed set can be observed in reality. This is particularly true because any dataset is always a sampling of the real world. Inspired by the decomposable property of counting, we propose to transform the open-set counting into a closedset problem, and address the problem with the idea of S-DC. We realize S-DC in a deep counting network termed S-DCNet. We show through a toy experiment and extensive evaluations on standard benchmarks that, even given a closed training set, S-DCNet can effectively generalize to open-set scenarios.</p><p>For future work, we will test the adaptability of S-DC on other network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary Materials</head><p>In this Supplement, we provide further clarifications and discussions on the motivation of S-DCNet, compare S-DCNet with other related ideas, and show qualitative results on evaluated datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">The Open-Set Problem of Density Maps</head><p>Density maps are actually in the open set as well. As shown in <ref type="figure" target="#fig_3">Fig. 9(b) (top)</ref>, for a single point, different kernel sizes lead to different density values. When multiple objects exist and are close, density patterns are even much diverse as in <ref type="figure" target="#fig_3">Fig. 9(b) (bottom)</ref>. Since observed samples are limited, density maps are certainly in an open set.</p><p>We add another baseline of CSRNet <ref type="bibr" target="#b19">[20]</ref> to the toy experiment in <ref type="figure" target="#fig_3">Fig. 9(a)</ref>. CSRNet also performs worse than S-DCNet in the open set (&gt; 10), which implies the openset problem also exists in density map based methods.</p><p>Furthermore, density map cannot be used in S-DCNet, because it is not spatially divisible. This is determined by its physical definition. However, local counts can. Thus we adopt local counts in S-DCNet rather than density maps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Relation to Other Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IG-CNN [1]</head><p>IG-CNN drew inspirations from ensemble learning and trained a series of networks to tackle different scenes. While our S-DCNet focuses on inducing and utilizing physical laws, such as the open set problem in counting and the spatial divisibility of local counts. We propose to transform the open-set counting into a closed-set problem via spatial divide-and-conquer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Mechanisms</head><p>Despite it is possible to provide explicit supervision to W i , we find that S-DCNet already can produce reasonably good divisions with the implicit supervision provided by L 2 R . This has another benefit, the network can learn when to divide not just in counts larger than C max . The visualizations of W i s in <ref type="figure" target="#fig_0">Fig. 10</ref> further justify our point. To highlight the difference against attention, we remove the division decider and generate a three-channel output conditioned on F 2 , then process it with softmax to obtain W att 0 , W att 1 , W att 2 . The final count is merged as W att 0 * upsample(C 0 )+W att 1 * upsample(C 1 )+W att 2 * C 2 . In SHTech PartA, it has 64.1 M AE and 109.9 M SE (worse than S-DCNet). As per the visualization of W att i in <ref type="figure" target="#fig_0">Fig. 10</ref>, we find the attention only focuses on the highest resolution and no effect of division is observed. In addition, S-DCNet executes fusion progressively, while attention fuses the prediction in a single step. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Further Discussions on S-DCNet</head><p>The necessity to distinguish counting task into open set and closed set scenarios One may raise the concern like: the relevance of distinguishing counting to an open set and closed set is unnecessary if each data point (head) is treated separately and the network learns to count each data point. If the network can count each head well, counting should already be addressed by detection networks. However, detection performs poorly when objects seriously overlap. This is why the notion of density map is introduced in <ref type="bibr" target="#b17">[18]</ref>, and density-based networks beat detection networks in counting. It is thus not suitable to treat each point separately, and distinguishing counting to an open set and closed set makes sense.</p><p>Generating ground-truth local counts Generating local counts directly from point annotations does not take partial objects cropped in patches into account. Density maps naturally tackle this situation. Thus we generate groundtruth counts of local patches by integrating over the density maps. This strategy is only utilized during training, while the point annotations are still used to calculate errors during validation.</p><p>If one position in W 1 is 0, which means the initial prediction should not be replaced. Is it possible that the same position in W 2 is 1? In theory, it is possible, because each division decision is independent. However, in practice, we do not observe such a behaviour of W <ref type="figure" target="#fig_0">(Fig. 10</ref>). Even this situation appears, we do not think it will be a problem. W 2 gives the second chance for division if the division decider makes a wrong decision in W 1 .</p><p>Why C 2 is performing much worse than C 1 and C 0 in S-DCNet? C 0 , C 1 and C 2 are trained jointly in S-DCNet and greatly influenced by the loss of L 2 R . As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, W 2 focus on local patches with high density, which means L 2 R will push C 2 to predict well on these patches and ignore others. High density patches, however, only occupy a small fraction. C 2 thus tends to predict worse than C 0 and C 1 . This may also explain why three-stage/four-stage S-DCNet performs worse than two-stage S-DCNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Qualitative Results of S-DCNet</head><p>We present some qualitative results of two-stage S-DCNet on five benchmarks (ShanghaiTech, UCF CC 50, UCF-QNRF, TRANCOS and MTC) in <ref type="figure" target="#fig_0">Fig. 11 to 16</ref>. S-DCNet predicts the local count map conditioned on the input image, where each element denotes a count value of the corresponding 16×16 local area. Meanwhile, since the output stride of S-DCNet is 64, we pad the original image with zeros to ensure that the length and width are multiples of 64.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The histogram of count values of 64 × 64 local patches on the test set of ShanghaiTech Part A dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The architecture of S-DCNet (left) and a two-stage S-DC process (right). S-DCNet adopts all convolutional layers in VGG16<ref type="bibr" target="#b29">[30]</ref> while the first two convolutional blocks are simplified as Conv in the figure. An UNet<ref type="bibr" target="#b26">[27]</ref>-like decoder is employed to upsample and divide the feature map as per</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>A toy-level justification. (a) Some 256 × 256 images in the simulated cell counting dataset. The numbers denote the range of local counts of 64×64 sub-regions. (b) The mean absolute error (MAE) of different methods versus 64 × 64 sub-region counts. S-DCNet(N) means N -stage S-DCNet. This suggests a conventional counting model learned in a closed set cannot generalize to the open set. However, S-DCNet can achieve accurate predictions even on the open set, which confirms the advantage of S-DC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 .</head><label>9</label><figDesc>(a) The toy-level experiment with an extra "CSRNet" baseline. (b) Density values along one axis with various kernels (top), and with two kernels with different relative distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 .</head><label>10</label><figDesc>Visualization of Wi for S-DCNet (top) and the attention baseline (bottom). The lighter the image is, the greater the values are. In the input image, count values greater than Cmax are indicated by yellow regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 .</head><label>11</label><figDesc>Some samples generated by S-DCNet from the test set of ShanghaiTech Part A dataset. The left column shows the original images, while the middle and right columns display the ground truth and predicted count maps respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 .</head><label>12</label><figDesc>Some samples generated by S-DCNet from the test set of ShanghaiTech Part B dataset. The left column shows the original images, while the middle and right columns display the ground truth and predicted count maps respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 .</head><label>13</label><figDesc>Some samples generated by S-DCNet from the test set of UCF CC 50 dataset. The left column shows the original images, while the middle and right columns display the ground truth and predicted count maps respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 .</head><label>14</label><figDesc>Some samples generated by S-DCNet from the test set of UCF-QNRF dataset. The left column shows the original images, while the middle and right columns display the ground truth and predicted count maps respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 15 .</head><label>15</label><figDesc>Some samples generated by S-DCNet from the test set of TRANCOS dataset. The left column shows the original images, while the middle and right columns display the ground truth and predicted count maps respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 16 .</head><label>16</label><figDesc>Some samples generated by S-DCNet from the test set of MTC dataset. The left column shows the original images, while the middle and right columns display the ground truth and predicted count maps respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Conv, 512, s 1 1 × 1 Conv, 512, s 1 1 × 1 Conv, class num, s 1 1 × 1 Conv, 1, s 1 − Sigmoid The architecture of classifier and division decider.</figDesc><table><row><cell>classifier</cell><cell>division decider</cell></row><row><cell>2 × 2 AvgPool, s 2</cell><cell>2 × 2 AvgPool, s 2</cell></row><row><cell cols="2">1 × 1 AvgP ool denotes average pooling. Convolutional layers are de-</cell></row><row><cell cols="2">fined in the format: Conv size×size, output channel, s stride.</cell></row><row><cell cols="2">Each convolutional layer is followed by a ReLU function except</cell></row><row><cell cols="2">the last layer. In particular, a sigmoid function is employed at the</cell></row><row><cell cols="2">end of division decider to generate soft division masks.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Multi-Stage S-DC Input: Image I and division time N Output: Image count C 1 Extract F0 from I; 2 Generate CLS0 given F0 with the classifier, and recover C0 from CLS0; 3 Initialize DIV0 = C0; 4 for i ← 1 to N do Decode Fi−1 to Fi;Process Fi with the classifier and the division decider to obtain CLSi and the division mask Wi;</figDesc><table /><note>567 Recover Ci from CLSi;8 Update DIVi as per Eq. 2 ;9 Integrate over DIVN to obtain the image count C;10 return C</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>2 i=0 L i C</cell><cell>L 2 R</cell><cell>MAE</cell><cell>MSE</cell></row><row><cell></cell><cell></cell><cell>301.4</cell><cell>396.9</cell></row><row><cell></cell><cell></cell><cell>88.4</cell><cell>128.8</cell></row><row><cell></cell><cell></cell><cell>58.3</cell><cell>95.0</cell></row><row><cell>Effect of S-DC. Two classification and</cell><cell></cell><cell></cell><cell></cell></row><row><cell>regression baselines are compared against S-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DCNet. S-DCNet (2) denotes two-stage S-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DCNet. The best performance is boldfaced.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Comparison with state-of-the-art approaches on the test set of TRANCOS<ref type="bibr" target="#b13">[14]</ref> dataset. The best performance is boldfaced.</figDesc><table><row><cell>Method</cell><cell>MAE</cell><cell>MSE</cell></row><row><cell>GlobalReg [35]</cell><cell>19.7</cell><cell>23.3</cell></row><row><cell>DensityReg [18]</cell><cell>11.9</cell><cell>14.8</cell></row><row><cell>CCNN [25]</cell><cell>21.0</cell><cell>25.5</cell></row><row><cell>TasselNet [23]</cell><cell>6.6</cell><cell>9.6</cell></row><row><cell>S-DCNet</cell><cell>5.6</cell><cell>9.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>Comparison with state-of-the-art approaches on the test set of MTC<ref type="bibr" target="#b22">[23]</ref> dataset. The best performance is boldfaced.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by the Natural Science Foundation of China under Grant No. 61876211.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Divide and grow: Capturing huge diversity in crowd images with incrementally growing cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak Babu</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><forename type="middle">N</forename><surname>Sajjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukundhan</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak Babu</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5744" to="5752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scale aggregation network for accurate and efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Counting everyday objects in everyday scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithvijit</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasaath</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scale pyramid network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanrui</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Count-ception: Counting by fully convolutional redundant counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Paul Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><forename type="middle">A</forename><surname>Glastonbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><forename type="middle">Z</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision Workshop (ICCVW)</title>
		<meeting>IEEE International Conference on Computer Vision Workshop (ICCVW)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wheat ear counting in-field conditions: high throughput and low-cost approach using RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><forename type="middle">C</forename><surname>Fernandez-Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nieves</forename><surname>Kefauver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">María</forename><forename type="middle">Teresa</forename><surname>Aparicio Gutiérrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José Luis</forename><surname>Nieto-Taladriz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Araus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Methods</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="33" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to count leaves in rosette plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Valerio Giuffrida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotirios</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference Workshops (BMVCW)</title>
		<meeting>British Machine Vision Conference Workshops (BMVCW)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extremely overlapping vehicle counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Guerrerogómezolmedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Torrejiménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Lópezsastre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saturnino</forename><surname>Maldonadobascón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Oñororubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Image Analysis</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="423" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Composition loss for counting, density map estimation and localization in dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhmmad</forename><surname>Tayyab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishan</forename><surname>Athrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somaya</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="532" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Where are the blobs: Counting by localization with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Issam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="547" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep attention-based classification network for robust depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Hang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decidenet: Counting varying density crowds through attention guided detection and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Crowd counting using deep recurrent spatialaware network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TasselNet: counting maize tassels in the wild via local counts regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Methods</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="95" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ordinal regression with multiple output cnn for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4920" to="4928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Oñoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="615" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Iterative crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="270" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crowd counting via adversarial cross-scale consistency pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5245" to="5254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Crowd counting with deep negative correlation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenglin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyan</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5382" to="5390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generating high-quality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1861" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Divide and count: Generic object counting by image divisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Stahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Silvia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1035" to="1044" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Counting in dense crowds using deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karunya</forename><surname>Tota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<idno>CRCV. 8</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
