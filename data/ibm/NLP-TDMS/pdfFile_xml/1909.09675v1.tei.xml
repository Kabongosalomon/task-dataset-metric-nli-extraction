<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Dataset Person Re-Identification via Unsupervised Pose Disentanglement and Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">ASUS Intelligent Cloud Services</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ci-Siang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Bo</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang</forename><forename type="middle">Frank</forename><surname>Wang</surname></persName>
							<email>ycwang@ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">ASUS Intelligent Cloud Services</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Dataset Person Re-Identification via Unsupervised Pose Disentanglement and Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification (re-ID) aims at recognizing the same person from images taken across different cameras. On the other hand, cross-dataset/domain re-ID focuses on leveraging labeled image data from source to target domains, while target-domain training data are without label information. In order to introduce discriminative ability and to generalize the re-ID model to the unsupervised target domain, our proposed Pose Disentanglement and Adaptation Network (PDA-Net) learns deep image representation with pose and domain information properly disentangled. Our model allows pose-guided image recovery and translation by observing images from either domain, without predefined pose category nor identity supervision. Our qualitative and quantitative results on two benchmark datasets confirm the effectiveness of our approach and its superiority over state-of-the-art cross-dataset re-ID approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given a query image containing a person (e.g., pedestrian, suspect, etc.), person re-identification (re-ID) <ref type="bibr" target="#b58">[59]</ref> aims at matching images with the same identity across nonoverlapping camera views. Person re-ID has been among active research topics in computer vision due to its practical applications to smart cities and large-scale surveillance systems. In order to tackle the challenges like visual appearance changes or occlusion in practical re-ID scenarios, several works have been proposed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b61">62]</ref>. However, such approaches require a large amount of labeled data for training, and this might not be applicable for realwork applications.</p><p>Since it might be computationally expensive to collect identity labels for the dataset of interest, one popular solution is to utilize an additional yet distinct source-domain dataset. This dataset contains fully labeled images (but <ref type="figure">Figure 1</ref>: Existing cross-dataset re-ID methods like <ref type="bibr" target="#b11">[12]</ref> perform style transfer followed by feature extraction for re-ID, which might limit image variants to be observed. We choose to perform pose disentanglement and adaption with domain-invariant features jointly learned, alleviating the above issue with improved image representation. with different identities) captured by a different set of cameras. Thus, the goal of cross-domain/dataset person re-ID is to extract and adapt useful information from source to the target-domain data of interest, so that re-ID at the targetdomain can be addressed accordingly. Since no label is observed for the target-domain data during training, one typically views the aforementioned setting as a unsupervised learning task.</p><p>Several methods for cross-dataset re-ID have been proposed <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b60">61]</ref>. For example, Deng et al. <ref type="bibr" target="#b12">[13]</ref> employ CycleGAN to covert labeled images from source to target domains, followed by performing re-ID at the target domain. Similarly, Zhong et al. <ref type="bibr" target="#b60">[61]</ref> utilize Star-GAN <ref type="bibr" target="#b10">[11]</ref> to learn camera invariance and domain connectedness simultaneously. On the other hand, Lin et al. <ref type="bibr" target="#b34">[35]</ref> employ Maximum Mean Discrepancy (MMD) for learning mid-level feature alignment across data domains for cross-dataset re-ID. However, as shown in <ref type="figure">Fig. 1</ref>, existing crossdomain re-ID approaches generally adapt style information across datasets, and thus pose information cannot be easily be described or preserved in such challenging scenarios.</p><p>To overcome the above limitations, we propose a novel deep learning framework for cross-dataset person re-ID. Without observing any ground truth label and pose information in the target domain, our proposed Pose Disentanglement and Adaptation Network (PDA-Net) learns domaininvariant features with the ability to disentangle pose information. This allows one to extract, adapt, and manipulate images across datasets without supervision in identity or label. More importantly, this allows us to learn domain and pose-invariant image representation using our proposed network (as depicted in <ref type="figure">Fig. 1</ref>). With label information observed from the source-domain images for enforcing the re-ID performance, our PDA-Net can be successfully applied to cross-dataset re-ID. Compare to prior unsupervised cross-dataset re-ID approaches which lack the ability to describe pose and content features, our experiments confirm that our model is able to achieve improved performances and thus is practically preferable.</p><p>We now highlight the contributions of our work below:</p><p>• To the best of our knowledge, we are among the first to perform pose-guided yet dataset-invariant deep learning models for cross-domain person re-ID.</p><p>• Without observing label information in the target domain, our proposed PDA-Net learns deep image representation with pose and domain information properly disentangled.</p><p>• The above disentanglement abilities are realized by adapting and recovering source and target-domain images in a unified framework, simply based on pose information observed from either domain image data.</p><p>• Experimental results on two challenging unsupervised cross-dataset re-ID tasks quantitatively and qualitatively confirm that our method performs favorably against state-of-the-art re-ID approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Supervised Person Re-ID. Person re-ID has been widely studied in the literature. Existing methods typically focus on tackling the challenges of matching images with viewpoint and pose variations, or those with background clutter or occlusion presented <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>. For example, Liu et al. <ref type="bibr" target="#b36">[37]</ref> develop a posetransferable deep learning framework based on GAN <ref type="bibr" target="#b18">[19]</ref> to handle image pose variants. Chen et al. <ref type="bibr" target="#b3">[4]</ref> integrate conditional random fields (CRF) and deep neural networks with multi-scale similarity metrics. Several attention-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> are further proposed to focus on learning the discriminative image features to mitigate the effect of background clutter. While promising results have been observed, the above approaches cannot easily be applied for cross-dataset re-ID due to the lack of ability in suppressing the visual differences across datasets.</p><p>Cross-dataset Person Re-ID. To handle cross-dataset person re-ID, a range of hand-crafted features have been considered, so that re-ID at the target domain can be performed in an unsupervised manner <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b57">58]</ref>.</p><p>To better exploit and adapt visual information across data domains, methods based on domain adaptation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref> have been utilized <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b60">61]</ref>. However, since the identities, viewpoints, body poses and background clutter can be very different across datasets, plus no label supervision is available at the target domain, the performance gains might be limited. For example, Fan et al. <ref type="bibr" target="#b13">[14]</ref> propose a progressive unsupervised learning method iterating between K-means clustering and CNN fine-tuning. Li et al. <ref type="bibr" target="#b28">[29]</ref> consider spatial and temporal information to learn tracklet association for re-ID. <ref type="bibr">Wang</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations and Problem Formulation</head><p>For the sake of completeness, we first define the notations to be used in this paper. Assume that we have the access to a set of</p><formula xml:id="formula_0">N S images X S = {x s i } N S i=1 with the as- sociated label set Y S = {y s i } N S i=1</formula><p>, where x s i ∈ R H×W ×3 and y s i ∈ R represent the i th image in the source-domain dataset and its corresponding identity label, respectively. Another set of N T target-domain dataset images X T = {x t j } N T j=1 without any label information are also available during training, where x t j ∈ R H×W ×3 represent the j th image in the target-domain dataset. To extract the pose information from source and target-domain data, we apply the pose estimation model <ref type="bibr" target="#b0">[1]</ref> on the above images to generate source/target-domain pose outputs</p><formula xml:id="formula_1">P S = {p s i } N S i=1 and P T = {p t j } N T j=1</formula><p>, respectively. Note that p s i ∈ R H×W ×N L and p t j ∈ R H×W ×N L represent the i th and j th pose maps in the corresponding domains, respectively. Following <ref type="bibr" target="#b0">[1]</ref>, we set the number of pose landmarks N L = 18 in our work.</p><p>To achieve cross-dataset person re-ID, we present an end-to-end trainable network, Pose Disentanglement and Adaptation Network (PDA-Net). As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, our PDA-Net aims at learning domain-invariant deep representation v c ∈ R d (d denotes the dimension of the feature), while pose information is jointly disentangled from this feature space. To achieve this goal, a pair of encoders E C and E P for encoding the input images and pose maps into v c and v p ∈ R h (h denotes the dimension of the fea-ture), respectively. Guided by the encoded pose features (from either domain), our domain specific generators (G S and G T for source and target-domain datasets, respectively) would recover/synthesize the desirable outputs in the associated data domain. We will detail the properties of each component in the following subsections.</p><p>To perform person re-ID of the target-domain dataset in the testing phase, our network encodes the query image by E C for deriving the domain and pose-invariant representation v c , which is applied for matching the gallery ones via nearest neighbor search (in Euclidean distances).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pose Disentanglement and Adaptation Network (PDA-Net)</head><p>As depicted in <ref type="figure" target="#fig_0">Figure 2</ref>, our proposed Pose Disentanglement and Adaptation Network consists of a number of network components. The content encoder E C encodes input images across different domains/datasets and produces content feature v c for person re-ID. The pose encoder E P encodes the pose maps and produce pose feature v p for pose disentanglement. The two domain-specific generators, G S and G T , output images in source and target domains respectively (by feeding both v c and v p ). The two domain specific discriminators, D S and D T , are designed to enforce the two domain-specific generators G S and G T produce perceptually realistic and domain-specific images. Finally, the pose discriminator D P aims at enforcing the generators to output realistic images conditioned on the given pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Domain-invariant representation for re-ID</head><p>We encourage the content encoder E C to generate similar feature distributions when observing both X S and X T . To accomplish this, we apply the Maximum Mean Discrepancy (MMD) measure <ref type="bibr" target="#b21">[22]</ref> to calculate the difference between the associated feature distributions for the content feature v c between the source and target domains. Given an source image x s ∈ X S and an target image x t ∈ X T 1 , we first forward x s and x t to the content encoder E C to obtain their content feature v s c and v t c . Then we can formulate our MMD loss L MMD as:</p><formula xml:id="formula_2">L MMD = 1 n s ns g=1 φ(v s c,g ) − 1 n t nt l=1 φ(v t c,l ) 2 H ,<label>(1)</label></formula><p>where φ is a map operation which project the distribution into a reproducing kernel Hilbert space H <ref type="bibr" target="#b20">[21]</ref>. n s and n t are the batch sizes of the images in the associated domains. The arbitrary distribution of the features can be represented by using the kernel embedding technique. It has been proven that if the kernel is characteristic, then the mapping to the space H is injective while the injectivity indicates that the arbitrary probability distribution is uniquely represented by and element in the space H. It is also worth noting that, we do not consider the adversarial learning strategy for deriving domain-invariant features (e.g., <ref type="bibr" target="#b16">[17]</ref>) in our work. This is because that this technique might produce pose-invariant features instead of domain-invariant ones for re-ID datasets, and thus the resulting features cannot perform well in cross-dataset re-ID.</p><p>Next, to utilize label information observed from sourcedomain training data, we impose a triplet loss L tri on the derived feature vector v c . This would maximize the interclass discrepancy while minimizing intra-class distinctness. To be more specific, for each input source image x s , we sample a positive image x s pos with the same identity label and a negative image x s neg with different identity labels to form a triplet tuple. Then, the distance between x s and x s pos (or x s neg ) can be calculated as:</p><formula xml:id="formula_3">d pos = v s c − v s c,pos 2 ,<label>(2)</label></formula><formula xml:id="formula_4">d neg = v s c − v s c,neg 2 ,<label>(3)</label></formula><p>where v s c , v s c,pos , and v s c,neg represent the feature vectors of images x s , x s pos , and x s neg , respectively. With the above definitions, the triplet loss L tri is</p><formula xml:id="formula_5">L tri = E (x s ,y s )∼(X S ,Y S ) max(0, m + d pos − d neg ),<label>(4)</label></formula><p>where m &gt; 0 is the margin enforcing the separation between positive and negative image pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Pose-guided cross-domain image translation</head><p>To ensure our derived content feature is domain-invariant in cross-domain re-ID tasks, we need to perform additional image translation during the learning of our PDA-Net. That is, we have the pose encoder E P in <ref type="figure" target="#fig_0">Fig. 2</ref> encodes the inputs from source pose set inputs P S and the target pose set P T into pose features v s p and v t p . As a result, both content and pose features would be produced in the latent space.</p><p>We enforce the two generators G S and G T for generating the person images conditioned on the encoded pose feature. For the source domain, we have the source generator G S take the concatenated source-domain content and pose feature pair (v s p , v s c ) and output the corresponding image x s→s . Similarly, we have G T take</p><formula xml:id="formula_6">(v t p , v t c ) for pro- ducing x t→t . Note that x s→s = G S ((v s p , v s c )), x t→t = G T (v t p , v t c )</formula><p>denote the reconstructed images in source and target domains, respectively. Since this can be viewed as image recovery in each domain, reconstruction loss can be applied as the objective during learning.</p><p>Since we have ground truth labels (i.e., image pair correspondences) for the source-domain data, we can further perform a unique image recovery task for the source-domain images. To be more precise, given two source-domain images x s and x s of the same person but with different poses p s and p s , we expect that they share the same content feature v s c but with pose features as v s p and v s p . Given the desirable pose v s p , we then enforce G S to output the source domain image x s using the content feature v s c which is originally associated with v s p . This is referred to as pose-guided image recovery.</p><p>With the above discussion, image reconstruction loss for the source-domain data L S rec can be calculated as:</p><formula xml:id="formula_7">L S rec = E x s ∼X S ,p s ∼P S [ x s→s − x s 1 ] + E {x s ,x s }∼X S ,p s ∼P S [ x s→s p →p − x s 1 ],<label>(5)</label></formula><p>where x s→s p →p = G S (v s p , v s c |v s p ) denotes the generated image from the input x s and v s c describe the content feature of the same identity (i.e., x s , and x s of the same person by with different poses p and p). As for the target-domain reconstruction loss, we have</p><formula xml:id="formula_8">L T rec = E x t ∼X T ,p t ∼P T [ x t→t − x t 1 ].<label>(6)</label></formula><p>Note that we adopt the L1 norm in the above reconstruction loss terms as it preserves image sharpness <ref type="bibr" target="#b24">[25]</ref>.</p><p>In addition to image recovery in either domain, our model also perform pose-guided image translation. That is, our decoders G S and G T allow input feature pairs whose content and pose representation are extracted from different domains. Thus, we would observe x t→s = G S (v t p , v t c ) and</p><formula xml:id="formula_9">x s→t = G T (v s p , v s c )</formula><p>as the outputs, with the goal of having these translated images as realistic as possible.</p><p>To ensure G S and G T produce perceptually realistic outputs in the associated domains, we have the image discriminator D S discriminate between the real source-domain images x s and the synthesized/translated ones (i.e., x s→s , x t→s ). Thus, the source-domain discriminator loss L S domain as</p><formula xml:id="formula_10">L S domain = E x s ∼X S [log(D S (x s ))] + E x s ∼X S ,p s ∼P S [log(1 − D S (x s→s ))] + E x t ∼X T ,p t ∼P T [log(1 − D S (x t→s ))].<label>(7)</label></formula><p>Similarly, the target domain discriminator loss L T domain is defined as</p><formula xml:id="formula_11">L T domain = E x t ∼X T [log(D T (x t ))] + E x t ∼X T ,p t ∼P T [log(1 − D T (x t→t ))] + E x s ∼X S ,p s ∼P S [log(1 − D T (x s→t ))].<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Unsupervised pose disentanglement across data domains</head><p>With the above pose-guided image translation mechanism, we have our PDA-Net learn domain-invariant content features across data domains. However, to further ensure the pose encoder describes and disentangles the pose information observed from the input images, we need additional network modules for completing this goal. To achieve this object, we introduce a pose discriminator D P in <ref type="figure" target="#fig_0">Fig. 2</ref>, which focuses on distinguishing between real and recovered images, conditioned on the given pose inputs. Following previous FD-GAN <ref type="bibr" target="#b17">[18]</ref>, we adopt the PatchGAN <ref type="bibr" target="#b25">[26]</ref> structure as our D P . That is, the input to D P is concatenation of the real/recovered image and the given pose map, which is processed by Gaussian-like heat-map transformation. Then, D P produces a image-pose matching confidence map, each location of this output confidence map represents the matching degree between the input image and the associated pose map.</p><p>It can be seen that, the two generators G S and G T in PDA-Net tend to fool the pose discriminator D P to obtain high matching confidences for the generated images. Intuitively, since only source-domain data are with ground truth labels, our D P is designed to authenticate the recovered images in each corresponding domain but not the translated ones across domains. In other words, the adversarial loss of D P is formulated as:</p><formula xml:id="formula_12">L pose = L S pose + L T pose ,<label>(9)</label></formula><p>where </p><formula xml:id="formula_13">L S pose = E x s ∼X S ,p s ∼P S [log(D P (p s , x s ))] + E x s ∼X S ,p s ∼P S [log(1 − D P (p s , x s→s ))] + E x s ∼X S ,p s ∼P S [log(1 − D P (p s , x s ))] + E {x s ,x s }∼X S ,p s ∼P S [log(1 − D P (p s , x s→s p →p ))]<label>(10)</label></formula><formula xml:id="formula_14">x s , p s , y s , x t , p t , x s , p s ← sample from XS, PS, YS, XT , PT 4 v s c , v t c ← obtain by EC (x s /x s ), EC (x t ) 5 v s p , v t p ← obtain by EP (p s ), EP (p t ) 6</formula><p>LMMD, Ltri ← calculate by (1), (4)</p><formula xml:id="formula_15">7 θE C + ← − −∇ θ E C (LMMD + λtriLtri) 8 x s→s , x t→s ← obtain by GS(v s p , v s c ), GS(v t p , v t c ) 9 x s→t , x t→t ← obtain by GT (v s p , v s c ), GT (v t p , v t c ) 10 x s→s p →p ← obtain by GS(v s p , v s c |v s p ) 11 L S rec , L T rec , L S domain , L T domain ,</formula><p>Lpose ← calculate by (5), (6), <ref type="bibr" target="#b6">(7)</ref>, (8), (9) <ref type="bibr" target="#b11">12</ref> for Iters. of updating generator do <ref type="bibr" target="#b14">15</ref> for Iters. of updating discriminator do</p><formula xml:id="formula_16">13 θE C ,E P ,G S + ← − −∇ θ E C ,E P ,G S (λrecL S rec − L S domain − λposeLpose) 14 θE C ,E P ,G T + ← − −∇ θ E C ,E P ,G T (λrecL T rec − L T domain − λposeLpose)</formula><formula xml:id="formula_17">16 θD S + ← − −∇ θ D S L S domain 17 θD T + ← − −∇ θ D T L T domain 18 θD P + ← − −∇ θ D P Lpose and L T pose = E x t ∼X T ,p t ∼P T [log(D P (p t , x t ))] + E x t ∼X T ,p t ∼P T [log(1 − D P (p t , x t→t ))].<label>(11)</label></formula><p>Note that x s→s p →p = G S (v s p , v s c |v s p ) represents the synthesized image from the input x s (with the same content feature v s c with x s but with a different pose feature v s p ). From <ref type="bibr" target="#b8">(9)</ref>, we see that while our pose disentanglement loss enforces the matching between the output image and its conditioned pose in each domain, additional guidance is available in the source domain to update our D P . That is, as shown in <ref type="formula" target="#formula_10">(7)</ref>, we are able to verify the authenticity of the source-domain output image which is given by the input image of the same person but with a different pose (i.e., p instead of p). While our decoder is able to output such a image with its ground truth source-domain image observed (as noted in <ref type="bibr" target="#b4">(5)</ref>, the introduced D P would further improve our capability of pose disentanglement and poseguided image recovery.</p><p>It is worth repeating that the goal of PDA-Net is to perform cross-dataset re-ID without observing label information in the target domain. By introducing the aforementioned network module, our PDA-Net would be capable </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Experimental Settings</head><p>To evaluate our proposed method, we conduct experiments on Market-1501 <ref type="bibr" target="#b57">[58]</ref> and DukeMTMC-reID <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b59">60]</ref>, both are commonly considerd in recent re-ID tasks. Market-1501. The Market-1501 <ref type="bibr" target="#b57">[58]</ref> is composed of 32,668 labeled images of 1,501 identities collected from 6 camera views. The dataset is split into two non-overlapping fixed parts: 12,936 images from 751 identities for training and 19,732 images from 750 identities for testing. In testing, 3368 query images from 750 identities are used to retrieve the matching persons in the gallery. DukeMTMC-reID. The DukeMTMC-reID <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b59">60]</ref> is also a large-scale Re-ID dataset. It is collected from 8 cameras and contains 36,411 labeled images belonging to 1,404 identities. It also consists of 16,522 training images from 702 identities, 2,228 query images from the other 702 identities, and 17,661 gallery images.</p><p>Evaluation Protocol. We employ the standard metrics as in most person Re-ID literature, namely the cumulative matching curve (CMC) used for generating ranking accuracy, and the mean Average Precision (mAP). We report rank-1 accuracy and mean average precision (mAP) for evaluation on both datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Configuration of PDA-Net. We implement our model using PyTorch. Following Section 3, we use ResNet-50 pretrained on ImageNet as our backbone of cross-domain encoder E C . Given an input image x (all images are resized to size 256 × 128 × 3, denoting width, height, and channel respectively.), E C encodes the input into 2048-dimension content feature v c . As mentioned in the Section. 3.1, the pose-map is represented by an 18-channel map, where each channel represents the location of one pose landmark. Such landmark location is converted to a Gaussian heat map. The pose encoder E P then employs 4 convolution blocks to produce the 256-dimension pose feature vector v p from these pose-maps. The structure of the both the domain generators (G S , G T ) are 6 convolution-residual blocks similar to that proposed by Miyato et al. <ref type="bibr" target="#b40">[41]</ref>. The structure of the both the domain discriminator (D S , D T ) employ the ResNet-18 as backbone while the architecture of shared pose disciminator D P adopts PatchGAN structure following FD-GAN <ref type="bibr" target="#b17">[18]</ref> and is composed of 5 convolution blocks in our PDA-Net. Domain generators (G S , G T ), domain discriminator (D S , D T ), shared pose discriminator D P are all randomly initialized. The margin for the L tri is set as 0.5, and we fix λ tri , λ rec , and λ pose as 1.0, 10.0, 0.1, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Comparisons</head><p>Market-1501. In <ref type="table" target="#tab_1">Table 1</ref>, we compare our proposed model with the use of Bag-of-Words (BoW) <ref type="bibr" target="#b57">[58]</ref> for matching (i.e., no transfer), four unsupervised re-ID approaches, including UMDL <ref type="bibr" target="#b41">[42]</ref>, PUL <ref type="bibr" target="#b14">[15]</ref>, CAMEL <ref type="bibr" target="#b53">[54]</ref> and TAUDL <ref type="bibr" target="#b28">[29]</ref>, and seven cross-dataset re-ID methods, including PTGAN <ref type="bibr" target="#b50">[51]</ref>, SPGAN <ref type="bibr" target="#b11">[12]</ref>, TJ-AIDL <ref type="bibr" target="#b48">[49]</ref>, MMFA <ref type="bibr" target="#b34">[35]</ref>, HHL <ref type="bibr" target="#b60">[61]</ref>, CFSM <ref type="bibr" target="#b2">[3]</ref> and ARN <ref type="bibr" target="#b31">[32]</ref>. From this table, we see that our model achieved very promising  <ref type="bibr" target="#b11">[12]</ref> and HHL <ref type="bibr" target="#b60">[61]</ref>, we note that our model is able to generate cross-domain images conditioned on various poses rather than few camera styles. Compared to MMFA <ref type="bibr" target="#b34">[35]</ref>, our model further disentangles the pose information and learns a pose invariant cross-domain latent space. Compared to the second best method, i.e., TAUDL <ref type="bibr" target="#b28">[29]</ref>, our results were higher by 11.5% in Rank-1 accuracy and by 11.4% in mAP, while no additional spatial and temporal information is utilized (but TAUDL did). DukeMTMC-reID. We now consider the DukeMTMC-reID as the target-domain dataset of interest, and list the comparisons in <ref type="table" target="#tab_3">Table 2</ref>. From this table, we also see that our model performed favorably against baseline and stateof-art unsupervised/cross-domain re-ID methods. Take the single query setting for example, we achieved Rank-1 ac-curacy=63.2% and mAP=45.1%. Compared to the second best method, our results were higher by 1.5% in Rank-1 accuracy and by 1.6% in mAP. From the experiments on the above two datasets, the effectiveness of our model for crossdomain re-ID can be successfully verified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies and Visualization</head><p>Analyzing the network modules in PDA-Net. As shown in <ref type="table" target="#tab_4">Table 3</ref>, we start from two baseline methods, i.e., naive Resnet-50 (w/o L MMD ) and advanced Resnet-50 (w/ L MMD ), showing the standard re-ID performances. We then utilize ResNet-50 as the backbone CNN model to derive representations for re-ID with only triplet loss L tri , while the advanced one includes the MMD loss L MMD . We observe that our full model (the last row) improved the performance by a large margin (roughly 20 ∼ 25%) at Rank-1 on both two benchmark datasets. The performance gain can be ascribed to the unique design of our model for deriving both domain-invariant and pose-invariant representation.</p><p>Loss functions To further analyze the importance of each introduced loss function, we conduct an ablation study from third row to seventh rows shown in <ref type="table" target="#tab_4">Table 3</ref>. Firstly, the reconstruction loss L rec is shown to be vital to our PDA-Net, since we observe 23% and 20% drops on Market-1501 and DukeMTMC-reID, respectively when the loss was excluded. This is caused by no explicit supervision to guide our PDA-Net to generate human-perceivable images, and thus the resulting model would suffer from image-level information loss.</p><p>Secondly, without the pose loss L pose on both domains, our model would not be able to perform pose matching based on each generated image, causing failure on the pose disentanglement process and resulting in the re-ID performance drop (about 20% on both settings). Thirdly, when L S/T domain is turned off, our model is not able to preserve the domain information, indicating that only pose information would be observed. We credited such a 10% performance drop to the negative effect in learning pose-invariant feature, which resulted in unsatisfactory pose disentanglement. Lastly, the MMD loss L MMD is introduced to our PDA-NET to mitigate the domain shift due to dataset differences. Its effectiveness is also confirmed by our studies.</p><p>Shared pose discriminator D P . To demonstrate the effectiveness and necessity of the pose discriminator D P introduced to our PDA-Net, we first consider replacing D P by two separate pose discriminators D S P and D T P , and report the re-ID performance in the fifth row of <ref type="table" target="#tab_4">Table 3</ref>. With a clear performance drop observed, we see that the resulting PDA-Net would not be able to transfer the substantiated pose-matching knowledge from source to target domains. In other words, a shared pose discriminator would be preferable since pose guidance can be provided by both domains. <ref type="figure">Figure 3</ref>: Visualization examples of our PDA-Net for pose-guided image translation across datasets. Given six pose conditions (the first row) and the input image (x s or x t ), we present the six generated images for each dataset pair: x s→s (the second row), x t→s (the third row), x t→t (the fourth row) and x s→t (the fifth row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4:</head><p>Visualization of cross-dataset or pose-guided re-ID. Note that SPGAN <ref type="bibr" target="#b12">[13]</ref> performs style-transfer for converting images across datasets but lacks the ability to exhibit pose variants, while FD-GAN <ref type="bibr" target="#b17">[18]</ref> disentangles pose information but cannot take cross-domain data.</p><p>Visualization comparisons of cross-dataset and poseguided re-ID models. In <ref type="figure">Figure 3</ref>, we visualize the generated images: x s→s , x s→t , x t→s , and x t→t in two crossdomain settings. Given an input from either domain with pose conditions, our model was able to produce satisfactory pose-guided image synthesis within or across data domains.</p><p>In <ref type="figure">Figure 4</ref>, we additionally consider the crossdataset re-ID appoach of SPGAN <ref type="bibr" target="#b12">[13]</ref> and the posedisentanglement re-ID method of FD-GAN <ref type="bibr" target="#b17">[18]</ref>. We see that, since SPGAN performed style transfer for synthesizing cross-domain images, pose variants cannot be exploited in the target domain. While FD-GAN was able to generate pose-guided image outputs with supervision on target target domain, their model is not designed to handle cross-domain data so that cannot produce images across datasets with satisfactory quality. From the above qualitative evaluation and comparison, we confirm that our PDA-Net is able to perform pose-guided single-domain image recovery and crossdomain image translation with satisfactory image quality, which would be beneficial to cross-domain re-ID tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we presented a novel Pose Disentanglement and Adaptation Network (PDA-Net) for cross-dataset re-ID. The main novelty lies in the unique design of our PDA-Net, which jointly learns domain-invariant and posedisentangled visual representation with re-ID guarantees. By observing only image input (from either domain) and any desirable pose information, our model allows poseguided singe-domain image recovery and cross-domain image translation. Note that only label information (image correspondence pairs) is available for the source-domain data, any no pre-defined pose category is utilized during training. Experimental results on the two benchmark datasets showed remarkable improvements over existing works, which support the use of our proposed approach for cross-dataset re-ID. Qualitative results also confirmed that our model is capable of performing cross-domain image translation with pose properly disentangled/manipulated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The overview of our Pose Disentanglement and Adaptation Network (PDA-Net). The content encoder E C learns domain-invariant features v c for input images from either domain. The pose encoder E P transforms the pose maps (p s and p t ) into the latent features v p for pose guidance and disentanglement purposes. The generators G T and G S output domainspecific images via single-domain recovery or cross-domain translation (x s→s p →p , x s→s , x t→s , x t→t and x s→t ), conditioned on the pose maps (p s and p t ). The domain discriminators D S and D T preserve image perceptual quality, while the pose discriminator D P is employed for pose disentanglement guarantees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Learning of PDA-Net Data: Source domain: XS, PS, and YS; Target domain: XT and PT Result: Configurations of PDA-Net 1 θE C , θE P , θG S , θG T , θD S , θD T , θD P ← initialize 2 for Num. of training Iters. do 3</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance comparisons on Market-1501 with cross-dataset/unsupervised Re-ID methods. The number in bold indicates the best result.</figDesc><table><row><cell>Method</cell><cell cols="4">Source: DukeMTMC, Target: Market Rank-1 Rank-5 Rank-10 mAP</cell></row><row><cell>BOW [58]</cell><cell>35.8</cell><cell>52.4</cell><cell>60.3</cell><cell>14.8</cell></row><row><cell>UMDL [42]</cell><cell>34.5</cell><cell>52.6</cell><cell>59.6</cell><cell>12.4</cell></row><row><cell>PTGAN [51]</cell><cell>38.6</cell><cell>-</cell><cell>66.1</cell><cell>-</cell></row><row><cell>PUL [15]</cell><cell>45.5</cell><cell>60.7</cell><cell>66.7</cell><cell>20.5</cell></row><row><cell>CAMEL [54]</cell><cell>54.5</cell><cell>-</cell><cell>-</cell><cell>26.3</cell></row><row><cell>SPGAN [13]</cell><cell>57.7</cell><cell>75.8</cell><cell>82.4</cell><cell>26.7</cell></row><row><cell>TJ-AIDL [49]</cell><cell>58.2</cell><cell>74.8</cell><cell>81.1</cell><cell>26.5</cell></row><row><cell>MMFA [35]</cell><cell>56.7</cell><cell>75.0</cell><cell>81.8</cell><cell>27.4</cell></row><row><cell>HHL [61]</cell><cell>62.2</cell><cell>78.8</cell><cell>84.0</cell><cell>31.4</cell></row><row><cell>CFSM [3]</cell><cell>61.2</cell><cell>-</cell><cell>-</cell><cell>28.3</cell></row><row><cell>ARN [32]</cell><cell>70.3</cell><cell>80.4</cell><cell>86.3</cell><cell>39.4</cell></row><row><cell>TAUDL [29]</cell><cell>63.7</cell><cell>-</cell><cell>-</cell><cell>41.2</cell></row><row><cell>PDA-Net (Ours)</cell><cell>75.2</cell><cell>86.3</cell><cell>90.2</cell><cell>47.6</cell></row><row><cell cols="5">of performing cross-dataset re-ID via pose-guided cross-</cell></row><row><cell cols="5">domain image translation. More precisely, with the joint</cell></row><row><cell cols="5">training of cross-domain encoders/decoders and the pose</cell></row><row><cell cols="5">disentanglement discriminators, our model allows learning</cell></row><row><cell cols="5">of domain-invariant and pose-disentangled feature repre-</cell></row><row><cell cols="5">sentation. The pseudo code for training our PDA-Net is</cell></row><row><cell cols="2">summarized in Algorithm 1.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparisons on DukeMTMC-reID with cross-dataset/unsupervised Re-ID methods. The number in bold indicates the best result.</figDesc><table><row><cell>Method</cell><cell cols="4">Source: Market, Target: DukeMTMC Rank-1 Rank-5 Rank-10 mAP</cell></row><row><cell>BOW [58]</cell><cell>17.1</cell><cell>28.8</cell><cell>34.9</cell><cell>8.3</cell></row><row><cell>UMDL [42]</cell><cell>18.5</cell><cell>31.4</cell><cell>37.6</cell><cell>7.3</cell></row><row><cell>PTGAN [51]</cell><cell>27.4</cell><cell>-</cell><cell>50.7</cell><cell>-</cell></row><row><cell>PUL [15]</cell><cell>30.0</cell><cell>43.4</cell><cell>48.5</cell><cell>16.4</cell></row><row><cell>SPGAN [13]</cell><cell>46.4</cell><cell>62.3</cell><cell>68.0</cell><cell>26.2</cell></row><row><cell>TJ-AIDL [49]</cell><cell>44.3</cell><cell>59.6</cell><cell>65.0</cell><cell>23.0</cell></row><row><cell>MMFA [35]</cell><cell>45.3</cell><cell>59.8</cell><cell>66.3</cell><cell>24.7</cell></row><row><cell>HHL [61]</cell><cell>46.9</cell><cell>61.0</cell><cell>66.7</cell><cell>27.2</cell></row><row><cell>CFSM [3]</cell><cell>49.8</cell><cell>-</cell><cell>-</cell><cell>27.3</cell></row><row><cell>ARN [32]</cell><cell>60.2</cell><cell>73.9</cell><cell>79.5</cell><cell>33.4</cell></row><row><cell>TAUDL [29]</cell><cell>61.7</cell><cell>-</cell><cell>-</cell><cell>43.5</cell></row><row><cell>PDA-Net (Ours)</cell><cell>63.2</cell><cell>77.0</cell><cell>82.5</cell><cell>45.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies of the proposed PDA-Net under two experimental settings. "Share D P " incidates whether to build separate pose discriminators, i.e. D S P and D T P , instead of one shared D P .</figDesc><table><row><cell>Experimental setting</cell><cell cols="4">Loss functions and component</cell><cell cols="2">Source: DukeMTMC-reID Target: Market-1501</cell><cell>Source: Market-1501 Target: DukeMTMC-reID</cell></row><row><cell cols="2">Ltri LMMD L</cell><cell>S/T rec</cell><cell>L S/T domain</cell><cell cols="2">Lpose Share DP Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>Baseline (ResNet-50)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>44.2</cell><cell>18.1</cell><cell>33.5</cell><cell>16.3</cell></row><row><cell>Baseline (ResNet-50 w/ MMD )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50.4</cell><cell>22.6</cell><cell>39.5</cell><cell>23.1</cell></row><row><cell>PDA-Net (w/o L S rec ,L T rec )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52.3</cell><cell>24.7</cell><cell>42.5</cell><cell>24.0</cell></row><row><cell>PDA-Net (w/o Lpose)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55.1</cell><cell>25.2</cell><cell>45.5</cell><cell>26.1</cell></row><row><cell>PDA-Net (w/o share DP )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>59.4</cell><cell>27.8</cell><cell>50.9</cell><cell>29.7</cell></row><row><cell>PDA-Net (w/o L S domain , L T domain )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>65.3</cell><cell>30.7</cell><cell>56.5</cell><cell>31.2</cell></row><row><cell>PDA-Net (w/o MMD)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>71.2</cell><cell>39.8</cell><cell>60.1</cell><cell>35.8</cell></row><row><cell>PDA-Net (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>75.2</cell><cell>47.6</cell><cell>63.2</cell><cell>45.1</cell></row><row><cell cols="4">results in Rank-1, Rank-5, Rank-10, and mAP, and observed</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">performance margins over recent approaches. For exam-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ple, in the single query setting, we achieved Rank-1 accu-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>racy=75.2% and mAP=52.6%.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Compared to SPGAN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For simplicity, we would omit the subscript i and j, denote source and target images as x s and x t , and represent the corresponding labels for source images as y s in this paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is supported by the Ministry of Science and Technology of Taiwan under grant MOST 108-2634-F-002-018.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Disjoint label space transfer learning with common factorised space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Group consistent similarity learning via deep crf for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Saliency aware: Weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting>the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep semantic matching with foreground detection and cycleconsistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Hsiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Yu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning resolution-invariant deep representation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crdoco: Pixel-level domain transfer with cross-domain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Show, match and segment: Joint learning of semantic matching and object co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Person re-identification by multichannel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domaindissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domaindissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Person reidentification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michela</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fd-gan: Poseguided feature distilling gan for robust person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A fast, consistent kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sriperumbudur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emrah</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhittin</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by deep learning tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minxian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recover and identify: A generative dual model for cross-resolution person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adaptation and re-identification network: An unsupervised deep transfer learning approach to person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-En</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ying</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning volumetric segmentation for lung tumor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Sheng</forename><surname>Jhih-Yuan Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Cheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Te</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Ting</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICIP VIP Cup Tech</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-task mid-level feature alignment network for unsupervised cross-dataset person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Tsun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Chichung</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pose transferrable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Covariance descriptor based on bio-inspired features for person reidentification and face verification. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Disentangled person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchical gaussian descriptor for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsu</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einoshin</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">cgans with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pose-normalized image generation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision workshop on Benchmarking Multi-Target Tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep groupshuffling random walk for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlou</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Resource aware person reidentification across multiple resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequn</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Glad: Global-local-alignment descriptor for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Multimedia (MM)</title>
		<meeting>the ACM Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep representation learning with part loss for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Pose invariant embedding for deep person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Scalable person reidentification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
