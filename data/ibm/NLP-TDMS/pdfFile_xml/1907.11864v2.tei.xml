<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty in Model-Agnostic Meta-Learning using Variational Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-30">30 Oct 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong</forename><surname>Nguyen</surname></persName>
							<email>cuong.nguyen@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
							<email>thanh-toan.do@liverpool.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<email>gustavo.carneiro@adelaide.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Uncertainty in Model-Agnostic Meta-Learning using Variational Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-30">30 Oct 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new, rigorously-formulated Bayesian meta-learning algorithm that learns a probability distribution of model parameter prior for few-shot learning. The proposed algorithm employs a gradient-based variational inference to infer the posterior of model parameters to a new task. Our algorithm can be applied to any model architecture and can be implemented in various machine learning paradigms, including regression and classification. We show that the models trained with our proposed meta-learning algorithm are well calibrated and accurate, with state-of-the-art calibration and classification results on two few-shot classification benchmarks (Omniglot, mini-ImageNet and tiered-ImageNet), and competitive results in a multi-modal task-distribution regression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning, in particular deep learning, has thrived during the last decade, producing results that were previously considered to be infeasible in several areas. For instance, outstanding results have been achieved in speech and image understanding <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b2">[2]</ref><ref type="bibr" target="#b3">[3]</ref><ref type="bibr" target="#b4">[4]</ref>, and medical image analysis <ref type="bibr" target="#b5">[5]</ref>. However, the development of these machine learning methods typically requires a large number of training samples to achieve notable performance. Such requirement contrasts with the human ability of quickly adapting to new learning tasks using few "training" samples. This difference may be due to the fact that humans tend to exploit prior knowledge to facilitate the learning of new tasks, while machine learning algorithms often do not use any prior knowledge (e.g., training from scratch with random initialisation) <ref type="bibr" target="#b6">[6]</ref> or rely on weak prior knowledge to learn new tasks (e.g., training from pre-trained models) <ref type="bibr" target="#b7">[7]</ref>. This challenge has motivated the design of machine learning methods that can make more effective use of prior knowledge to adapt to new learning tasks using few training samples <ref type="bibr" target="#b8">[8]</ref>.</p><p>Such methods assume the existence of a latent distribution over classification or regression tasks that share a common structure. This common structure means that solving many tasks can be helpful for solving a new task, sampled from the same task distribution, even if it contains a limited number of training samples. For instance, in multitask learning <ref type="bibr" target="#b9">[9]</ref>, an agent simultaneously learns the shared representation of many related tasks and a main task that are assumed to come from the same domain. The extra information provided by this multi-task training tends to regularise the main task training, particularly when it contains few training samples. In domain adaptation <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref>, a learner transfers the shared knowledge of many training tasks drawn from one or several source domains to perform well on tasks (with small training sets) drawn from a target domain. Bayesian learning <ref type="bibr" target="#b12">[12]</ref> has also been explored, where prior knowledge is represented by a probability density function on the parameters of the visual classes' probability models. In learning to learn or meta-learning <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref>, a meta-learner extracts relevant knowledge from many tasks learned in the past to facilitate the learning of new future tasks.</p><p>From the methods above, meta-learning currently produces state-of-the-art results in many benchmark few-shot learning datasets <ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref>. Such success can be attributed to the way meta-learning leverages prior knowledge from several training tasks drawn from a latent distribution of tasks, where the objective is to perform well on unseen tasks drawn from the same distribution. However, a critical issue arises with the limited amount of training samples per task combined with the fact that most of these approaches <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b23">23]</ref> do not try to estimate model uncertaintythis may result in overfitting. This issue has been recently addressed with Laplace approximation to estimate model uncertainty, involving the computationally hard estimation of a high-dimensional covariance matrix <ref type="bibr" target="#b24">[24]</ref>, and with vari-ational Bayesian learning <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b25">25]</ref> containing sub-optimal point estimate of model parameters and inefficient optimisation.</p><p>In this work, we propose a new variational Bayesian learning by extending model-agnostic meta-learning (MAML) <ref type="bibr" target="#b19">[19]</ref> based on a rigorous formulation that is efficient and does not require any point estimate of model parameters. In particular, compared to MAML <ref type="bibr" target="#b19">[19]</ref>, our approach explores probability distributions over possible values of meta-parameters, rather than having a fixed value. Learning and prediction using our proposed method are, therefore, more robust due to the perturbation of learnt meta-parameters that coherently explains data variability. Our evaluation shows that the models trained with our proposed meta-learning algorithm is at the same time well calibrated and accurate, with competitive results in terms of Expected Calibration Error (ECE) and Maximimum Calibration Error (MCE), while outperforming state-of-theart methods in some few-shot classification benchmarks (Omniglot, mini-ImageNet and tiered-ImageNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Meta-learning has been studied for a few decades <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b26">26]</ref>, and recently gained renewed attention with the use of deep learning methods. As meta-learning aims at the unique ability of learning how to learn, it has enabled the development of training methods with limited number of training samples, such as few-shot learning. Some notable meta-learning approaches include memory-augmented neural networks <ref type="bibr" target="#b15">[15]</ref>, deep metric learning <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23]</ref>, learn how to update model parameters <ref type="bibr" target="#b16">[16]</ref> and learn good prior using gradient descent update <ref type="bibr" target="#b19">[19]</ref>. These approaches have generated some of the most successful meta-learning results, but they lack the ability to estimate model uncertainty. Consequently, their performances may suffer in uncertain environments and real world applications.</p><p>Bayesian meta-learning techniques have, therefore, been developed to incorporate uncertainty into model estimation. Among those, MAML-based meta-learning has attracted much of research interest due to the straightforward use of gradient-based optimisation of MAML. Grant et al. <ref type="bibr" target="#b24">[24]</ref> use Laplace approximation to improve the robustness of MAML, but the need to estimate and invert the Hessian matrix makes this approach computationally challenging, particularly for large-scale models, such as the ones used by deep learning methods. Variational inference (VI) addresses such scalability issue -remarkable examples of VI-based methods are PLATIPUS <ref type="bibr" target="#b25">[25]</ref>, BMAML <ref type="bibr" target="#b20">[20]</ref> and the methods similar to our proposal, Amortised metalearner <ref type="bibr" target="#b27">[27]</ref> and VERSA [28] 1 . However, PLATIPUS optimises the lower bound of data prediction, leading to the need to approximate a joint distribution between the taskspecific and meta parameters. This approximation complicates the implementation and requires a point estimate of the task-specific parameters to reduce the complexity of the estimation of this joint distribution. Employing point estimate may, however, reduce its ability to estimate model uncertainty. BMAML uses a closed-form solution based on Stein Variational Gradient Descent (SVGD) that simplifies the task adaptation step, but it relies on the use of a kernel matrix, which increases its computational complexity. Amortised meta-learner applies variational approximation on both the meta-parameters and task-specific parameters, resulting in a challenging optimisation. VERSA takes a slightly different approach by employing an external neural network to learn the variational distribution for certain parameters, while keeping other parameters shared across all tasks. Another inference-based method is Neural Process <ref type="bibr" target="#b29">[29]</ref> that employs the train-ability of neural networks to model a Gaussian-Process-like distribution over functions to achieve uncertainty quantification in few-shot learning. However, due to the prominent weakness of Gaussian Process that suffers from cubic complexity to data size, this might limit the scalability of Neural Process and makes it infeasible for large-scale datasets.</p><p>Our approach, in contrast, employs a straightforward variational approximation for the distribution of only the task-specific parameters, where we do not require the use of point estimate of any term, nor do we need to compute Hessian or kernel matrices or depend on an external network. Our proposed algorithm can be considered a rigorous and computationally efficient Bayesian meta-learning algorithm. A noteworthy non-meta-learning method that employs Bayesian methods is the neural statistician <ref type="bibr" target="#b30">[30]</ref> that uses an extra variable to model data distribution within each task, and combines that information to solve few-shot learning problems. Our proposed algorithm, instead, does not introduce additional parameters, while still being able to extract relevant information from a small number of examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first define and formulate the fewshot meta-learning problem. We then describe MAML, derive our proposed algorithm, and mention the similarities and differences between our method and recently proposed meta-learning methods that are relevant to our proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Few-shot Learning Problem Setup</head><p>While conventional machine learning paradigm is designed to optimise the performance on a single task, fewshot learning is trained on a set of conditional independent and identically distributed (i.i.d.) tasks given metaparameters. The notation of "task environment" was formulated in <ref type="bibr" target="#b31">[31]</ref>, where tasks are sampled from an unknown <ref type="figure" target="#fig_2">Figure 1</ref>: (a) Hierarchical model of the few-shot meta-learning, aiming to learn θ that parameterises prior p(w i ; θ), so that given a few data points z</p><formula xml:id="formula_0">p(w; θ * ) p(w 1 |Y (t) 1 , θ * ) p(w 2 |Y (t) 2 , θ * ) p(w 3 |Y (t) 3 , θ * ) (c) VAMPIRE</formula><formula xml:id="formula_1">(t) ij = (x (t) ij , y (t)</formula><p>ij ) from the support set of task T i , the model can quickly adapts and accurately predicts the output for the query set z task distribution D over a family of tasks. Each task T i in this family is indexed by i ∈ {1, ..., T } and consists of a support set {X</p><formula xml:id="formula_2">(v) ij = (x (v) ij , y (v)</formula><formula xml:id="formula_3">(t) i , Y (t) i } and a query set {X (v) i , Y (v) i }, with X (t) i = {x (t) ij } M j=1 and Y (t) i = {y (t) ij } M j=1 (X (v) i and Y (v) i</formula><p>are similarly defined). The aim of few-shot learning is to predict the output y</p><formula xml:id="formula_4">(v) ij of the query input x (v)</formula><p>ij given the small support set for task T i (e.g. M ≤ 20). We rely on a Bayesian hierarchical model <ref type="bibr" target="#b24">[24]</ref> to model the few-shot meta-learning problem. In the graphical model shown in <ref type="figure" target="#fig_2">Figure 1a</ref>, θ denotes the meta parameters of interest, and w i represents the task-specific parameters for task T i . One typical example of this modelling approach is MAML <ref type="bibr" target="#b19">[19]</ref>, where w i are the neural network weights adapted to task T i by performing truncated gradient descent using the data from the support set {X</p><formula xml:id="formula_5">(t) i , Y (t)</formula><p>i } and the initial weight values θ.</p><p>The objective function of few-shot learning is, therefore, to find a meta-learner, parameterised by θ, across tasks sampled from D, as follows:</p><formula xml:id="formula_6">θ * = arg min θ − 1 T T i=1 ln p(Y (v) i |Y (t) i , θ)<label>(1)</label></formula><p>where T denotes the number of tasks, and, hereafter, we simplify the notation by dropping the explicit dependence on X (t) i and X</p><p>(v) i from the set of conditioning variables. Each term of the predictive probability on the right hand side of (1) can be expanded by applying the sum rule of probability and lower-bounded by Jensen's inequality:</p><formula xml:id="formula_7">ln p(Y (v) i |Y (t) i , θ) = ln E p(wi|Y (t) i ,θ) p(Y (v) i |w i ) ≥ L (v) i ,</formula><p>where:</p><formula xml:id="formula_8">L (v) i (θ) = E p(wi|Y (t) i ,θ) ln p(Y (v) i |w i ) .<label>(2)</label></formula><p>Hence, instead of minimising the negative log-likelihood in (1), we minimise the upper-bound of the corresponding negative log-likelihood which can be presented as:</p><formula xml:id="formula_9">L (v) (θ) = − 1 T T i=1 L (v) i .<label>(3)</label></formula><p>If each task-specific posterior, p(w i |Y</p><formula xml:id="formula_10">(t) i , θ)</formula><p>, is wellbehaved, we can apply Monte Carlo to approximate the expectation in (3) by sampling model parameters w i from p(w i |Y (t) i , θ). Thus, depending on the formulation of the task-specific posterior p(w i |Y (t) i , θ), we can formulate different algorithms to solve the problem of few-shot learning. We review a deterministic method widely used in the literature in subsection 3.2, and present our proposed approach in subsection 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Point Estimate -MAML</head><p>A simple way is to approximate p(w i |Y (t) i , θ) by a Dirac delta function at its local mode:</p><formula xml:id="formula_11">p(w i |Y (t) i , θ) = δ(w i − w MAP i ),<label>(4)</label></formula><p>where the local mode w MAP i can be obtained by using maximum a posterior (MAP):</p><formula xml:id="formula_12">w MAP i = arg max wi ln p(Y (t) |w i ) + ln p(w i ; θ).<label>(5)</label></formula><p>In the simplest case where the prior is also assumed to be a Dirac delta function: p(w i ; θ) = δ(w i − θ), and gradient descent is used, the local mode can be determined as:</p><formula xml:id="formula_13">w MAP i = θ − α∇ wi − ln p(Y (t) i |w i ) ,<label>(6)</label></formula><p>where α is the learning rate, and the truncated gradient descent consists of a single step of (6) (the extension to a larger number of steps is trivial). Given the point estimate assumption in (4), the upper-bound of the negative log-likelihood in (3) can be simplified to:</p><formula xml:id="formula_14">L (v) (θ) = 1 T T i=1 − ln p(Y (v) i |w MAP i ).<label>(7)</label></formula><p>Minimising the upper-bound of the negative loglikelihood in (7) w.r.t. θ represents the MAML algorithm <ref type="bibr" target="#b19">[19]</ref>. This derivation also explains the intuition behind MAML, which finds a good initialisation of model parameters as illustrated in <ref type="figure" target="#fig_2">Figure 1b</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Gradient-based Variational Inference</head><p>In contrast to the deterministic method presented in subsection 3.2, we use a variational distribution q(w i ;</p><formula xml:id="formula_15">λ i ), pa- rameterized by λ i = λ i (Y (t) i , θ), to approximate the task- specific posterior p(w i |Y (t) i , θ).</formula><p>In variational inference, q(w i ; λ i ) can be obtained by minimising the following Kullback-Leibler (KL) divergence:</p><formula xml:id="formula_16">λ * i = arg min λi KL q(w i ; λ i ) p(w i |Y (t) i , θ) = arg min λi q(w i ; λ i ) ln q(w i ; λ i )p(Y (t) i |θ) p(Y (t) i |w i )p(w i ; θ) dw i = arg min λi L (t) i (λ i , θ) + ln p(Y (t) |θ) const. wrt λi . (8) where: L (t) i (λ i , θ) = KL [q(w i ; λ i ) p(w i ; θ)] + E q(wi;λi) − ln p(Y (t) i |w i ) . (9)</formula><p>The resulting cost function (excluding the constant term) L (t) i is often known as the variational free energy (VFE). The first term of VFE can be considered as a regularisation that penalises the difference between the prior p(w i ; θ) and the approximated posterior q(w i ; λ i ), while the second term is referred as data-dependent part or likelihood cost. Exactly minimising the cost function in (9) is computationally challenging, so gradient descent is used with θ as the initialisation of λ i :</p><formula xml:id="formula_17">λ i ← θ − α∇ λi L (t) i (λ i , θ) ,<label>(10)</label></formula><p>where α is the learning rate. Given the approximated posterior q(w i ; λ i ) with parameter λ i updated according to <ref type="bibr" target="#b10">(10)</ref>, we can calculate and optimise the upper-bound in (3) to find a local-optimal metaparameter θ.</p><p>In Bayesian statistics, the prior p(w i |θ) represents a modelling assumption, and the variational posterior for each task T i do 5:</p><formula xml:id="formula_18">λ i ← θ 6: draw L t samplesŵ (lt) i ∼ q(w i ; λ i ), l t = 1 : L t 7: update: λ i ← λ i − α Lt ∇ λi L (t) i (λ i , θ) {Eq (10)} 8: draw L v samplesŵ (lv ) i ∼ q(w i ; λ i ), l v = 1 : L v 9: L (v) i (θ) = 1 Lv Lv lv =1 ln p Y (v) i |ŵ (lv ) i {Eq. (2)} 10: end for 11: meta-update: θ ← θ + γ T ∇ θ T i=1 L (v) i (θ) 12: end while q(w i ; λ i )</formula><p>is a flexible function that can be adjusted to achieve a good trade-off between performance and complexity. For simplicity, we assume that both q(w i ; λ i ) and p(w i ; θ) are Gaussian distributions with diagonal covariance matrices:</p><formula xml:id="formula_19">p(w i ; θ) = N w i |µ θ , Σ θ = diag(σ 2 θ ) q(w i ; λ i ) = N w i |µ λi , Σ λi = diag(σ 2 λi ) ,<label>(11)</label></formula><p>where µ θ , µ λi , σ θ , σ λi ∈ R d , with d denoting the number of model parameters, and the operator diag(.) returns a diagonal matrix using the vector in the parameter. Given the prior p(w i |θ) and the posterior q(w i ; λ i ) in (11), we can compute the KL divergence of VFE shown in (9) by using either Monte Carlo sampling or a closedform solution. According to <ref type="bibr" target="#b32">[32]</ref>, sampling model parameters from the approximated posterior q(w i ; λ i ) to compute the KL divergence term and optimise the cost function in <ref type="bibr" target="#b9">(9)</ref> does not perform better or worse than using the closed-form of the KL divergence between two Gaussian distributions. Therefore, we employ the closed-form formula of the KL divergence to speed up the training process.</p><p>For numerical stability, we parameterise the standard deviation point-wisely as σ = exp(ρ) when performing gradient update for the standard deviations of model parameters. The meta-parameters θ = (µ θ , exp(ρ θ )) are the initial mean and standard deviation of neural network weights, and the variational parameters λ i = (µ λi , exp(ρ λi )) are the mean and standard deviation of those network weights optimised for task T i .</p><p>We also implement the reparameterisation trick <ref type="bibr" target="#b33">[33]</ref> when sampling the network weights from the approximated posterior to compute the expectation of the data log-likelihood in <ref type="formula">(9)</ref>:</p><formula xml:id="formula_20">w i = µ λi + ǫ ⊙ exp(ρ λi ),<label>(12)</label></formula><p>where ǫ ∼ N (0, I d ), and ⊙ is the element-wise multiplication. Given this direct dependency, the gradients of the cost function L (t) i in (9) with respect to λ i can be derived as:</p><formula xml:id="formula_21">         ∇ µ λ i L (t) i = ∂L (t) i ∂w i + ∂L (t) i ∂µ λi ∇ ρ λ i L (t) i = ∂L (t) i ∂w i ǫ ⊙ exp(ρ λi ) + ∂L (t) i ∂ρ λi .<label>(13)</label></formula><p>After obtaining the variational parameters λ i in <ref type="formula" target="#formula_6">(10)</ref>, we can apply Monte Carlo approximation by sampling L v sets of model parameters from the approximated posterior q(w i ; λ i ) to calculate and optimise the upper-bound in <ref type="formula" target="#formula_9">(3)</ref> w.r.t. θ. This approach leads to the general form of our proposed algorithm, named Variational Agnostic Modelling that Performs Inference for Robust Estimation (VAMPIRE), shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Differentiating VAMPIRE and Other Bayesian</head><p>Meta-learning Methods VAMPIRE is different from the "probabilistic MAML" -PLATIPUS <ref type="bibr" target="#b25">[25]</ref> in several ways. First, PLATIPUS uses VI to approximate the joint distribution p(w i , θ|Y</p><formula xml:id="formula_22">(t) i , Y (v) i ),</formula><p>while VAMPIRE uses VI to approximate the task-specific posterior p(w i |Y (t) i , θ). To handle the complexity of sampling from a joint distribution, PLATIPUS relies on the same point estimate of the task-specific posterior as MAML, as shown in <ref type="bibr" target="#b4">(4)</ref>. Second, to adapt to task T i , PLATIPUS learns only the mean, without change the variance. In contrast, VAMPIRE learns both µ θ and Σ θ for each task T i . Lastly, when adapting to a task, PLATIPUS requires 2 additional gradient update steps, corresponding to steps 7 and 10 of Algorithm 1 in <ref type="bibr" target="#b25">[25]</ref>, while VAM-PIRE needs only 1 gradient update step as shown in step 7 of Algorithm 1. Hence, VAMPIRE is based on a simpler formulation that does not rely on any point estimate, and it is also more flexible and efficient because it allows all meta-parameters to be learnt while performing less gradient-based steps.</p><p>VAMPIRE is also different from the PAC-Bayes metalearning method designed for multi-task learning <ref type="bibr" target="#b34">[34]</ref> at the relation between the shared prior p(w i ; θ) and the variational task-specific posterior q(w i ; λ i ). While the PAC-Bayes meta-learning method does not relate the "posterior" to the "prior" as in the standard Bayesian analysis, VAM-PIRE relates these two probabilities through a likelihood function by performing a fixed number of gradient updates as shown in <ref type="bibr" target="#b10">(10)</ref>. Due to this discrepancy, the PAC-Bayes meta-learning needs to maintain all the task-specific posteriors, requiring more memory storage, consequently resulting in an un-scalable approach, especially when the number of tasks is very large. In contrast, VAMPIRE learns only the shared prior, and hence, is a more favourable method for large-scaled applications, such as few-shot learning.</p><p>Our proposed algorithm is different from BMAML <ref type="bibr" target="#b20">[20]</ref> at the methods used to approximate task-specific posterior p(w i |Y (t) i , θ): BMAML is based on SVGD, while VAM-PIRE is based on a variant of amortised inference. Although SVGD is a non-parametric approach that allows a flexible variational approximation, its downside is the computational complexity due to need to compute the kernel matrix, and high memory usage when increasing the number of particles. In contrast, our approach uses a straightforward variational method without any transformation of variables. One advantage of BMAML compared to our method in Algorithm 1 is the use of Chaser Loss, which may be an effective way of preventing overfitting. Nevertheless, in principle, we can also implement the same loss for our proposed algorithm.</p><p>VAMPIRE is different from Amortised Metalearner <ref type="bibr" target="#b27">[27]</ref> at the data subset used to update the meta-parameters θ: whole data set of task T i in Amortised Meta-learner versus only the query subset {X</p><formula xml:id="formula_23">(v) i , Y (v) i } in VAMPIRE.</formula><p>This discrepancy is due to the differences in the objective function. In particular, Amortised Meta-learner maximises the lower bound of marginal likelihood, while VAMPIRE maximises the predictive probability in (1). Moreover, when deriving a lower bound of marginal loglikelihood using VI [27, Derivation right before Eq. (1)], the variational distribution q must be strictly greater than zero for all θ and variational parameters. The assumption that approximates the variational distribution q(θ; ψ) by a Dirac delta function made in Amortised ML <ref type="bibr" target="#b27">[27,</ref><ref type="bibr">Eq. (4)</ref>] is, therefore, arguable.</p><p>Another Bayesian meta-learning approach similar to VAMPIRE is VERSA <ref type="bibr" target="#b28">[28]</ref>. The two methods are different at the methods modelling the parameters of interest θ. VAMPIRE relies on gradient update to relate the prior and posterior through likelihood function, while VERSA is based on an amortisation network to output the parameters of the variational distributions. To scale up to deep neural network models, VERSA models only the parameters of the last fully connected network, while leaving other parameters as point estimates that are shared across all tasks. As a result, VAMPIRE is more flexible since it does not need to define which parameters are shared or not shared, nor does it require any additional network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>The goal of our experiments is to present empirical evaluation of VAMPIRE compared to state-of-art meta-learning approaches. Our experiments include both regression and few-shot classification problems. The experiments are carried out using the training procedure shown in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Regression</head><p>We evaluate VAMPIRE using a multi-modal task distribution where half of the data is generated from sinusoidal functions, while the other half is from linear functions <ref type="bibr" target="#b25">[25]</ref>. A detailed configuration of the problem setup and the model used as well as additional visualisation results can be referred to Appendix.</p><p>The results in <ref type="figure" target="#fig_3">Figures 2a and 2b</ref> show that VAMPIRE can effectively reason which underlying function generates the training data points as the predictions are all sinusoidal or linear. In addition, VAMPIRE is able to vary the prediction variance, especially when there is more uncertainty in the training data. In contrast, due to the deterministic nature, MAML can only output a single value at each input.</p><p>To quantitatively compare the performance between VAMPIRE and other few-shot meta-learning methods, we use the reliability diagram which is based on the quantile calibration for regression <ref type="bibr" target="#b36">[36]</ref>. A model is perfectly cal-ibrated when its predicted probability equals to the actual probability, resulting in a curve that is well-aligned with the diagonal y = x. We re-implement some few-shot meta-learning methods, train until convergence, and plot their reliability diagram for 1000 tasks in <ref type="figure" target="#fig_3">Figure 2c</ref>. To have a fair comparison, BMAML is trained without Chaser Loss, and Amortised Meta-learner is trained with a uniform hyper-posterior. Due to the deterministic nature, MAML is presented as a single point connecting with the two extreme points. For a further quantitative comparison, we also plot the expected calibration error (ECE), which averages the absolute errors measuring from the diagonal, and the maximum calibration error (MCE), which returns the maximum of absolute errors in <ref type="figure" target="#fig_3">Figure 2d</ref>. Overall, in terms of ECE and MCE, the model trained with VAMPIRE is better than BMAML and Amortised Meta-learner, while competitive with PLATIPUS. The performance of BMAML could be higher if more particles and Chaser Loss are used. Another observation is that Amortised Meta-learner has slightly lower performance than MAML, although the training procedures of the two methods are very similar. We hypothesise that this is due to overfitting induced by using the whole training data subset that includes {X</p><formula xml:id="formula_24">(t) i , Y (t)</formula><p>i }, while MAML and VAMPIRE use only the query data subset {X</p><formula xml:id="formula_25">(v) i , Y (v)</formula><p>i } to train the meta-parameters, which is consistent between the training and testing scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Few-shot Classification</head><p>The experiments in this sub-section are based on the Nway k-shot learning task, where a meta learner is trained on many related tasks containing N classes and small training sets of k samples for each class (i.e., this is the size of Y (t) i ). We benchmark our results against the state of the art on the data sets Omniglot <ref type="bibr" target="#b8">[8]</ref>, mini-ImageNet <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b23">23]</ref> and tiered-ImageNet <ref type="bibr" target="#b37">[37]</ref>.</p><p>Omniglot contains 1623 different handwritten characters from 50 different alphabets, where each one of the characters was drawn online via Amazon's Mechanical Turk by 20 different people <ref type="bibr" target="#b8">[8]</ref>. Omniglot is often split by randomly picking 1200 characters for training and the remaining for testing <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref>. However, for language character classification, this random split may be unfair since knowing a character of an alphabet may facilitate the learning of other characters in the same alphabet. The original train-test split defined in <ref type="bibr" target="#b8">[8]</ref> suggests 30 alphabets for training and 20 alphabets for testing -such split clearly avoids potential information leakage from the training set to the testing set. We run experiments using both splits to compare with state-ofthe-art methods and to perform testing without any potential data leakage. As standardly done in the literature, our training includes a data augmentation based on rotating the samples by multiples of 90 degrees, as proposed in <ref type="bibr" target="#b15">[15]</ref>.  down-sampled to 28-by-28 pixels to be consistent with the reported works in the meta-learning literature <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b23">23]</ref>. Mini-ImageNet was proposed in <ref type="bibr" target="#b23">[23]</ref> as an evaluation for few-shot learning. It consists of 100 different classes, each having 600 colour images taken from the original ImageNet data set <ref type="bibr" target="#b41">[41]</ref>. We use the train-test split reported in <ref type="bibr" target="#b16">[16]</ref> that consists of 64 classes for training, 16 for validation, and 20 for testing. Similarly to Omniglot, the examples in mini-ImageNet are pre-processed by down-sampling the images to 84-by-84 pixels to be consistent with previous works in the literature.</p><p>Tiered-ImageNet <ref type="bibr" target="#b37">[37]</ref> is a larger subset of ImageNet that has 608 classes grouped into 34 high-level categories. We use the standard train-test split that consists of 20, 6, and 8 categories for training, validation and testing. The experiments on tiered-ImageNet is carried with input as features extracted by a residual network that was pre-trained on data and classes from training meta-set <ref type="bibr" target="#b22">[22,</ref><ref type="bibr">Section 4.2.2]</ref>.</p><p>For Omniglot and mini-ImageNet, we use the same network architecture of state-of-the-art methods <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b23">23]</ref>. The network consists of 4 hidden convolution modules, each containing 64 3-by-3 filters, followed by batch normalisation <ref type="bibr" target="#b42">[42]</ref>, ReLU activation, and a 2-by-2 strided convolution. For the mini-ImageNet, the strided convolution is replaced by a 2-by-2 max-pooling layer, and only 32 filters are used on each convolution layer to avoid over-fitting <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b19">19]</ref>. For tiered-ImageNet, we use a 2-hidden-layer fullyconnected network with 128 and 32 hidden units. Please refer to Appendixfor detailed description on the configuration and the hyperparameters used.</p><p>The N -way k-shot classification accuracy measured on Omniglot, and mini-ImageNet, tiered-ImageNet data sets are shown in Tables 1 and 2, respectively. Overall, the results of VAMPIRE are competitive to the state-of-the-art methods that use the same network architecture <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b23">23]</ref>.</p><p>On Omniglot, our results on a random train-test split are competitive in most scenarios. Our proposed method outperforms some previous works in few-shot learning, such as siamese networks <ref type="bibr" target="#b38">[38]</ref>, matching networks <ref type="bibr" target="#b23">[23]</ref> and memory models <ref type="bibr" target="#b39">[39]</ref>, although they are designed with a focus on few-shot classification. Our result on the 20-way 1shot is slightly lower than prototypical networks <ref type="bibr" target="#b18">[18]</ref> and VERSA <ref type="bibr" target="#b28">[28]</ref>, but prototypical networks need more classes (higher "way") per training episode to obtain advantageous results and VERSA requires an additional amortised networks to learn the variational distributions. Our results are also slightly lower than MAML, potentially due to the difference of train-test split. To obtain a fair comparison, we run the public code provided by MAML's authors, and measure its accuracy on the original split suggested in <ref type="bibr" target="#b8">[8]</ref>. Us- <ref type="bibr" target="#b2">2</ref> Trained with 60-way episodes.  <ref type="bibr" target="#b37">[37]</ref> 53.31 ± 0.89 72.69 ± 0.74 RELATION NET <ref type="bibr" target="#b44">[44]</ref> 54.48 ± 0.93 71.32 ± 0.78 TRNS. PRP. NETS <ref type="bibr" target="#b44">[44]</ref> 57.41 ± 0.94 71.55 ± 0.74 LEO <ref type="bibr" target="#b22">[22]</ref> 66.33 ± 0.05 81.44 ± 0.09 METAOPTNET <ref type="bibr" target="#b45">[45]</ref> 65.81 ± 0.74 81.75 ± 0.53 VAMPIRE 69.87 ± 0.29 82.70 ± 0.21 <ref type="table">Table 2</ref>: The few-shot 5-way classification accuracy results (in percentage) of VAMPIRE averaged over 600 mini-ImageNet tasks and 5000 tiered-ImageNet tasks are competitive to the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MINI-IMAGENET</head><p>ing this split, VAMPIRE achieves competitive performance, and outperforms MAML in some cases. On mini-ImageNet, VAMPIRE outperforms all reported methods that use the standard 4-layer CNN architecture on the 1-shot tests, while being competitive on the 5-shot episodes. Prototypical Networks achieve a higher accuracy on the 5-shot tests due to, again, the use of extra classes during training. Although our work does not aim to achieve the state-of-the-art results in few-shot learning, we also run an experiment using as input features extracted by a residual network that was pre-trained on data and classes from training meta-set <ref type="bibr" target="#b22">[22,</ref><ref type="bibr">Sect. 4.2.2]</ref>, and present the results, including the state-of-the-art methods that employ much deeper networks with various architectures, in Appendix. Note that deeper networks tend to reduce intra-class variation, resulting in a smaller gap of performance among many meta-learning methods <ref type="bibr" target="#b43">[43]</ref>.</p><p>On tiered-ImageNet, VAMPIRE outperforms many methods published previously by a large margin on both 1and 5-shot settings.</p><p>To evaluate the predictive uncertainty of the models, we <ref type="bibr" target="#b2">2</ref> Trained with 30-way episodes for 1-shot classification and 20-way episodes for 5-shot classification <ref type="bibr" target="#b3">3</ref>   <ref type="figure">Figure 3</ref>: (a) Uncertainty evaluation between different meta-learning methods using reliability diagrams, and (b) expected calibration error (ECE) and maximum calibration error (MCE), in which the evaluation is carried out on 5way 1-shot setting for <ref type="bibr">20 5</ref> = 15504 unseen tasks sampled from mini-ImageNet dataset.</p><p>show in <ref type="figure">Figure 3a</ref> the reliability diagrams <ref type="bibr" target="#b46">[46]</ref> averaged over many unseen tasks to compare different meta-learning methods. A perfectly calibrated model will have its values overlapped with the identity function y = x, indicating that the probability associated with the label prediction is the same as the true probability. To have a fair comparison, we train all the methods of interest under the same configuration, e.g. network architecture, number of gradient updates, while keeping all method-specific hyper-parameters the same as the reported values. Due to the constrain of GPU memory, BMAML is trained with only 8 particles, while PLATIPUS, Amortimised Meta-learner and VAM-PIRE are trained with 10 Monte Carlo samples. According to the reliability graphs, the model trained with VAM-PIRE shows a much better calibration than the ones trained with the other methods used in the comparison. To further evaluate, we compute the expected calibration error (ECE) and maximum calibration error (MCE) <ref type="bibr" target="#b46">[46]</ref> of each models trained with these methods. Intuitively, ECE is the weighted average error, while MCE is the largest error. The results plotted in <ref type="figure">Figure 3b</ref> show that the model trained with VAM-PIRE has smaller ECE and MCE compared to all the stateof-the-art meta-learning methods. The slightly low performance of Amortised Meta-learner might be due to the usage of the whole task-specific dataset, potentially overfitting to the training data. Another factor contributed might be the arguable Dirac-delta hyper-prior used, which can be also the cause for the low prediction accuracy shown in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce and formulate a new Bayesian algorithm used for few-shot meta-learning. The proposed algorithm, VAMPIRE, employs variational inference to optimise a well-defined cost function to learn a distribution of model parameters. The uncertainty, in the form of the learnt distribution, can introduce more variability into the decision made by the model, resulting in well-calibrated and highlyaccurate prediction. The algorithm can be combined with different models that are trainable with gradient-based optimisation, and is applicable in regression and classification. We demonstrate that the algorithm can make reasonable predictions about unseen data in a multi-modal 5-shot learning regression problem, and achieve state-of-the-art calibration and classification results with only 1 or 5 training examples per class on public image data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Regression experiments A.1. Training configuration</head><p>As mentioned in section 4.1, the experiment is carried out on a multi-modal structured data, where a half of tasks are generated from sinusoidal functions, while the other half of tasks are from linear functions. The sinusoidal functions are in the form of A sin(x+ϕ), where the amplitude A and the phase ϕ are uniformly sampled from [0.1, 5] and [0, π], respectively. The linear functions are in the form of ax + b, where the slope a and the intercept b are sampled from the uniform distribution on <ref type="bibr">[-3, 3]</ref>. The input x is uniformly sampled from <ref type="bibr">[-5, 5]</ref>. In addition, a Gaussian noise with zero-ed mean and a standard deviation of 0.3 is added to the output.</p><p>The model used in this experiment is a 3-hidden fully connected neural network with 100 hidden units per each hidden layer. Output from each layer is activated by ReLU without batch normalisation. The optimisation for the objective function in (3) is carried out by Adam. Note that for regression, there is we do not place any weighting factor for the KL divergence term of VFE. Please refer to <ref type="table" target="#tab_5">Table 3</ref> for the details of hyperparameters used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters</head><p>Notation Value  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Additional results</head><p>In addition to the results in <ref type="figure" target="#fig_3">Figure 2</ref>, we also provide more qualitative visualisation from the multi-modal task distribution in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>We also implement many Bayesian meta-learning methods, such as PLATIPUS, BMAML and Amortised Meta-learner, to compare with VAMPIRE using reliability diagram. We train all the methods of interest in the same setting used for VAMPIRE to obtain a fair comparison. The mean-squared error (MSE) of each method after training can be referred to <ref type="table" target="#tab_7">Table 4</ref>. Please note that for probabilistic methods, MSE is the average value across many Monte Carlo samples or particles sampled from the posterior distribution of model parameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classification experiments</head><p>This section describes the detailed setup to train and validate the few-shot learning on Omniglot and mini-ImageNet presented in Sec. 4.2. Following the notation used in Sec. 3.1, each task or episode i has N classes, where the support set Y (t) i has k samples per class, and the query set Y (v) i has 15 samples per class. This is to be consistent with the previous works in the literature <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b19">19]</ref>. The training is carried out by using Adam to minimise the cross-entropy loss of the softmax output. The learning rate of the meta-parameters θ is set to be γ = 10 −3 across all trainings, and decayed by a factor of 0.99 after every 10,000 tasks. Other hyperparameters used are specified in <ref type="table" target="#tab_9">Table 5</ref>. We select the number of ensemble models L t and L v to fit into the memory of one Nvidia 1080 Ti GPU. Higher values of L t and L v are desirable to achieve a better Monte Carlo approximation.  For the experiments using extracted features <ref type="bibr" target="#b22">[22]</ref> presented in <ref type="table">Table 6</ref> for mini-ImageNet, and the bottom part of <ref type="table">Table 2</ref> for tiered-ImageNet, we used a 2-hidden fully connected layer with 128 and 32 hidden units. The learning rate α is set as 0.01 and 5 gradient updates were carried out. The learning rate for meta-parameters was γ = 0.001.</p><p>Both the experiments for classification re-weight the KL divergence term of VFE by a factor of 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Model calibration for classification -ECE and MCE</head><p>We provide the results of model calibration, in particular, ECE and MCE in the numeric form. We also include the 95% confidence interval in <ref type="table">Table 7</ref>, although they are extremely small due to the large number of unseen tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MINI-IMAGENET [16] 1-SHOT 5-SHOT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NON-STANDARD CNN</head><p>RELATION NETS <ref type="bibr" target="#b40">[40]</ref> 50.44 ± 0.82 65.32 ± 0.70 VERSA <ref type="bibr" target="#b28">[28]</ref> 53.40 ± 1.82 67.37 ± 0.86 SNAIL <ref type="bibr" target="#b47">[47]</ref> 55.71 ± 0.99 68.88 ± 0.92 ADARESNET <ref type="bibr" target="#b48">[48]</ref> 56.88 ± 0.62 71.94 ± 0.57 TADAM <ref type="bibr" target="#b49">[49]</ref> 58.5 ± 0.30 76.7 ± 0.30 LEO <ref type="bibr" target="#b22">[22]</ref> 61.76 ± 0.08 77.59 ± 0.12 METAOPTNET <ref type="bibr" target="#b45">[45]</ref> 64.09 ± 0.62 80.00 ± 0.45 VAMPIRE 62.16 ± 0.24 76.72 ± 0.37 <ref type="table">Table 6</ref>: Accuracy for 5-way classification on mini-ImageNet tasks (in percentage) of many methods which uses extra parameters, deeper network architectures or different training settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method ECE MCE</head><p>MAML 0.0410 ± 0.005 0.124 PLATIPUS 0.032 ± 0.005 0.108 BMAML 0.025 ± 0.006 0.092 Amortised Meta-learner 0.026 ± 0.003 0.058 VAMPIRE 0.008 ± 0.002 0.038 <ref type="table">Table 7</ref>: Results of ECE and MCE of several meta-learning methods that are tested in 5-way 1-shot setting over 15504 unseen tasks sampled from mini-ImageNet dataset. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ik ); (b) and (c) Visualisation between MAML and VAMPIRE, respectively, where VAMPIRE extends the deterministic prior p(w i ; θ) and posterior p(w i |Y (t) i , θ) in MAML by using probabilistic distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>VAMPIRE training Require: task distribution D Require: Hyper-parameters: T, L t , L v , α and γ 1: initialise θ 2: while θ not converged do 3: sample a mini-batch of tasks T i ∼ D, i = 1 : T 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative and quantitative results on multimodal data -half of the tasks are generated from sinusoidal functions, and the other half are from linear functions: (a) and (b) visualisation of MAML and VAMPIRE, where the shaded area is the prediction made by VAMPIRE ± 2× standard deviation, (c) reliability diagram of various metalearning methods averaged over 1000 tasks, and (d) ECE and MCE of the Bayesian meta-learning methods.All implementations of VAMPIRE use PyTorch<ref type="bibr" target="#b35">[35]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Additional qualitative results with tasks generated from either sinusoidal or linear function. The shaded area is the prediction made by VAMPIRE ± 1× standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>C. Pseudo-code for evaluation Algorithm 2</head><label>2</label><figDesc>VAMPIRE testingRequire: a new task T T +1 , θ, L t , L v , α and β 1:λ T +1 ← θ 2: sampleŵ (l) T +1 ∼ q(w T +1 |λ T +1 ), where l t = 1 : L t 3: update: λ i ← λ i − α Lt ∇ λi L (t) i | Y (t) T +1 4: draw L v ensemble model parametersŵ (lv) i ∼ q(w i ; λ i ) 5: compute predictionŶ (v) iusing L v ensemble models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ORIGINAL SPLIT,STANDARD  4-LAYER CNN MAML 96.68 ± 0.57 98.33 ± 0.22 84.38 ± 0.64 96.32 ± 0.17 VAMPIRE 96.27 ± 0.38 98.77 ± 0.27 86.60 ± 0.24 96.14 ± 0.10</figDesc><table><row><cell></cell><cell cols="2">5-WAY</cell><cell cols="2">20-WAY</cell></row><row><cell></cell><cell>1-SHOT</cell><cell>5-SHOT</cell><cell>1-SHOT</cell><cell>5-SHOT</cell></row><row><cell cols="3">OMNIGLOT [8] -OMNIGLOT [8] -RANDOM SPLIT, STANDARD 4-LAYER CNN</cell><cell></cell><cell></cell></row><row><cell>MATCHING NETS [23]</cell><cell>98.1</cell><cell>98.9</cell><cell>93.8</cell><cell>98.5</cell></row><row><cell>PROTO. NETS [18] 2</cell><cell>98.8</cell><cell>99.7</cell><cell>96.0</cell><cell>98.9</cell></row><row><cell>MAML [19]</cell><cell>98.7 ± 0.4</cell><cell>99.9 ± 0.1</cell><cell>95.8 ± 0.3</cell><cell>98.9 ± 0.2</cell></row><row><cell>VAMPIRE</cell><cell>98.43 ± 0.19</cell><cell>99.56 ± 0.08</cell><cell>93.20 ± 0.28</cell><cell>98.52 ± 0.13</cell></row><row><cell cols="3">OMNIGLOT [8] -RANDOM SPLIT, NON-STANDARD CNNS</cell><cell></cell><cell></cell></row><row><cell>SIAMESE NETS [38]</cell><cell>97.3</cell><cell>98.4</cell><cell>88.2</cell><cell>97.0</cell></row><row><cell>NEURAL STATISTICIAN [30]</cell><cell>98.1</cell><cell>99.5</cell><cell>93.2</cell><cell>98.1</cell></row><row><cell>MEMORY MODULE [39]</cell><cell>98.4</cell><cell>99.6</cell><cell>95.0</cell><cell>98.6</cell></row><row><cell>RELATION NETS [40]</cell><cell>99.6 ± 0.2</cell><cell>99.8 ± 0.1</cell><cell>97.6 ± 0.2</cell><cell>99.1 ± 0.1</cell></row><row><cell>VERSA [28]</cell><cell>99.70 ± 0.20</cell><cell>99.75 ± 0.13</cell><cell>97.66 ± 0.29</cell><cell>98.77 ± 0.18</cell></row><row><cell></cell><cell></cell><cell cols="3">Before performing experiments, all Omniglot images are</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Few-shot classification accuracy (in percentage) on Omniglot, tested on 1000 tasks and reported with 95% confidence intervals. The results of VAMPIRE are competitive to the state-of-the-art baselines which are carried out on a standard 4- convolution-layer neural networks. The top of the table contains methods trained on the original split defined in [8], while the middle part contains methods using a standard 4-layer CNN trained on random train-test split. The bottom part presents results of different methods using different network architectures, or requiring external modules and additional parameters trained on random split. Note that the Omniglot results on random split cannot be fairly compared.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameters used in the regression experiments on multi-modal structured data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Mean squared error of many meta-learning methods after being trained in the same setting are tested on 1000 tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters used in the few-shot classification presented in Sec. 4.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Amortised meta-learner<ref type="bibr" target="#b27">[27]</ref> and VERSA<ref type="bibr" target="#b28">[28]</ref> have been developed in parallel to our proposed VAMPIRE.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">To transfer or not to transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Rosenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2005 workshop on transfer learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">898</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recnorm: Simultaneous normalisation and classification applied to speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Bridle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="234" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Evolutionary principles in selfreferential learning (on learning how to learn: The meta-meta-... hook),&quot; Diploma thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Meta-learning with memoryaugmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7343" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Metagan: An adversarial approach to fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2371" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Metalearning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recasting gradient-based meta-learning as hierarchical bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic modelagnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9537" to="9548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Meta-neural networks that learn by learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mammone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks, IEEE</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Amortized bayesian metalearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beatson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta-learning probabilistic inference for prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshop on Theoretical Foundations and Applications of Deep Generative Models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards a neural statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A model of inductive bias learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="149" to="198" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Meta-learning by adjusting priors based on extended PAC-Bayes theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Meir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Automatic Differentiation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distribution calibration for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Diethe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="5897" to="5906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to remember rare events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3661" to="3670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="719" to="729" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
