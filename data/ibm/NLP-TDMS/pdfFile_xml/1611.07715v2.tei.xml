<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Feature Flow for Video Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
							<email>jifdai@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<email>luyuan@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
							<email>yichenw@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Feature Flow for Video Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional neutral networks have achieved great success on image recognition tasks. Yet, it is nontrivial to transfer the state-of-the-art image recognition networks to videos as per-frame evaluation is too slow and unaffordable. We present deep feature flow, a fast and accurate framework for video recognition. It runs the expensive convolutional sub-network only on sparse key frames and propagates their deep feature maps to other frames via a flow field. It achieves significant speedup as flow computation is relatively fast. The end-to-end training of the whole architecture significantly boosts the recognition accuracy. Deep feature flow is flexible and general. It is validated on two video datasets on object detection and semantic segmentation. It significantly advances the practice of video recognition tasks. Code is released at https:// github.com/msracver/Deep-Feature-Flow. * This work is done when Xizhou Zhu and Yuwen Xiong are interns at Microsoft Research Asia</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have witnessed significant success of deep convolutional neutral networks (CNNs) for image recognition, e.g., image classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b15">16]</ref>, semantic segmentation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b51">52]</ref>, and object detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27]</ref>. With their success, the recognition tasks have been extended from image domain to video domain, such as semantic segmentation on Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref>, and object detection on ImageNet VID dataset <ref type="bibr" target="#b36">[37]</ref>. Fast and accurate video recognition is crucial for high-value scenarios, e.g., autonomous driving and video surveillance. Nevertheless, applying existing image recognition networks on individual video frames introduces unaffordable computational cost for most applications.</p><p>It is widely recognized that image content varies slowly over video frames, especially the high level semantics <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b20">21]</ref>. This observation has been used as means of regularization in feature learning, considering videos as unsu-pervised data sources <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b20">21]</ref>. Yet, such data redundancy and continuity can also be exploited to reduce the computation cost. This aspect, however, has received little attention for video recognition using CNNs in the literature.</p><p>Modern CNN architectures <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b15">16]</ref> share a common structure. Most layers are convolutional and account for the most computation. The intermediate convolutional feature maps have the same spatial extent of the input image (usually at a smaller resolution, e.g., 16× smaller). They preserve the spatial correspondences between the low level image content and middle-to-high level semantic concepts <ref type="bibr" target="#b47">[48]</ref>. Such correspondence provides opportunities to cheaply propagate the features between nearby frames by spatial warping, similar to optical flow <ref type="bibr" target="#b16">[17]</ref>.</p><p>In this work, we present deep feature flow, a fast and accurate approach for video recognition. It applies an image recognition network on sparse key frames. It propagates the deep feature maps from key frames to other frames via a flow field. As exemplifed in <ref type="figure">Figure 1</ref>, two intermediate feature maps are responsive to "car" and "person" concepts. They are similar on two nearby frames. After propagation, the propagated features are similar to the original features.</p><p>Typically, the flow estimation and feature propagation are much faster than the computation of convolutional features. Thus, the computational bottleneck is avoided and significant speedup is achieved. When the flow field is also estimated by a network, the entire architecture is trained end-to-end, with both image recognition and flow networks optimized for the recognition task. The recognition accuracy is significantly boosted.</p><p>In sum, deep feature flow is a fast, accurate, general, and end-to-end framework for video recognition. It can adopt most state-of-the-art image recognition networks in the video domain. Up to our knowledge, it is the first work to jointly train flow and video recognition tasks in a deep learning framework. Extensive experiments verify its effectiveness on video object detection and semantic segmentation tasks, on recent large-scale video datasets. Compared to per-frame evaluation, our approach achieves unprecedented speed (up to 10× faster, real time frame rate) with moderate accuracy loss (a few percent). The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>To our best knowledge, our work is unique and there is no previous similar work to directly compare with. Nevertheless, it is related to previous works in several aspects, as discussed below.</p><p>Image Recognition Deep learning has been successful on image recognition tasks. The network architectures have evolved and become powerful on image classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16]</ref>. For object detection, the region-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8]</ref> have become the dominant paradigm. For semantic segmentation, fully convolutional networks (FCNs) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b51">52]</ref> have dominated the field. However, it is computationally unaffordable to directly apply such image recognition networks on all the frames for video recognition. Our work provides an effective and efficient solution.</p><p>Network Acceleration Various approaches have been proposed to reduce the computation of networks. To name a few, in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b11">12]</ref> matrix factorization is applied to decompose large network layers into multiple small layers. In <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b17">18]</ref>, network weights are quantized. These tech-niques work on single images. They are generic and complementary to our approach.</p><p>Optical Flow It is a fundamental task in video analysis. The topic has been studied for decades and dominated by variational approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b1">2]</ref>, which mainly address small displacements <ref type="bibr" target="#b43">[44]</ref>. The recent focus is on large displacements <ref type="bibr" target="#b2">[3]</ref>, and combinatorial matching (e.g., Deep-Flow <ref type="bibr" target="#b44">[45]</ref>, EpicFlow <ref type="bibr" target="#b35">[36]</ref>) has been integrated into the variational approach. These approaches are all hand-crafted.</p><p>Deep learning and semantic information have been exploited for optical flow recently. FlowNet <ref type="bibr" target="#b8">[9]</ref> firstly applies deep CNNs to directly estimate the motion and achieves good result. The network architecture is simplified in the recent Pyramid Network <ref type="bibr" target="#b32">[33]</ref>. Other works attempt to exploit semantic segmentation information to help optical flow estimation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b18">19]</ref>, e.g., providing specific constraints on the flow according to the category of the regions.</p><p>Optical flow information has been exploited to help vision tasks, such as pose estimation <ref type="bibr" target="#b31">[32]</ref>, frame prediction <ref type="bibr" target="#b30">[31]</ref>, and attribute transfer <ref type="bibr" target="#b48">[49]</ref>. This work exploits optical flow to speed up general video recognition tasks.</p><p>Exploiting Temporal Information in Video Recognition T-CNN <ref type="bibr" target="#b21">[22]</ref> incorporates temporal and contextual information from tubelets in videos. The dense 3D CRF <ref type="bibr" target="#b23">[24]</ref> proposes long-range spatial-temporal regularization in semantic video segmentation. STFCN <ref type="bibr" target="#b9">[10]</ref> considers a spatial-temporal FCN for semantic video segmentation. These works operate on volume data, show improved recognition accuracy but greatly increase the computational cost. By contrast, our approach seeks to reduce the computation by exploiting temporal coherence in the videos. The network still runs on single frames and is fast.</p><p>Slow Feature Analysis High level semantic concepts usually evolve slower than the low level image appearance in videos. The deep features are thus expected to vary smoothly on consecutive video frames. This observation has been used to regularize the feature learning in videos <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b40">41]</ref>. We conjecture that our approach may also benefit from this fact.</p><p>Clockwork Convnets <ref type="bibr" target="#b38">[39]</ref> It is the most related work to ours as it also disables certain layers in the network on certain video frames and reuses the previous features. It is, however, much simpler and less effective than our approach.</p><p>About speed up, Clockwork only saves the computation of some layers (e.g., 1/3 or 2/3) in some frames (e.g., every other frame). As seen later, our method saves that on most layers (task network has only 1 layer) in most frames (e.g., 9 out of 10 frames). Thus, our speedup ratio is much higher.</p><p>About accuracy, Clockwork does not exploit the correspondence between frames and simply copies features. It only reschedules the computation of inference in an offthe-shelf network and does not perform fine-tuning or retraining. Its accuracy drop is pretty noticeable at even small speed up. In <ref type="table">Table 4</ref> and 6 of their arxiv paper, at 77% full runtime (thus 1.3 times faster), Mean IU drops from 31.1 to 26.4 on NYUD, from 70.0 to 64.0 on Youtube, from 65.9 to 63.3 on Pascal, and from 65.9 to 64.4 on Cityscapes. By contrast, we re-train a two-frame network with motion considered end-to-end. The accuracy drop is small, e.g., from 71.1 to 70.0 on Cityscape while being 3 times faster <ref type="figure">(</ref> About generality, Clockwork is only applicable for semantic segmentation with FCN. Our approach transfers general image recognition networks to the video domain. <ref type="table">Table 1</ref> summarizes the notations used in this paper. Our approach is briefly illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Feature Flow</head><p>Deep Feature Flow Inference Given an image recognition task and a feed-forward convolutional neutral network N that outputs result for input image I as y = N (I). Our goal is to apply the network to all video frames I i , i = 0, ..., ∞, fast and accurately.</p><p>Following the modern CNN architectures <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b15">16]</ref> and applications <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8]</ref>, without loss of generality, we decompose N into two consecutive subnetworks. The first sub-network N f eat , dubbed feature network, is fully convolutional and outputs a number of intermediate feature maps, f = N f eat (I). The second sub- network N task , dubbed task network, has specific structures for the task and performs the recognition task over the feature maps, y = N task (f ). Consecutive video frames are highly similar. The similarity is even stronger in the deep feature maps, which encode high level semantic concepts <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b20">21]</ref>. We exploit the similarity to reduce computational cost. Specifically, the feature network N f eat only runs on sparse key frames. The feature maps of a non-key frame I i are propagated from its preceding key frame I k .</p><p>The features in the deep convolutional layers encode the semantic concepts and correspond to spatial locations in the image <ref type="bibr" target="#b47">[48]</ref>. Examples are illustrated in <ref type="figure">Figure 1</ref>. Such spatial correspondence allows us to cheaply propagate the feature maps by the manner of spatial warping.</p><p>Let M i→k be a two dimensional flow field. It is obtained by a flow estimation algorithm F such as <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b8">9]</ref>,</p><formula xml:id="formula_0">M i→k = F(I k , I i ).</formula><p>It is bi-linearly resized to the same spatial resolution of the feature maps for propagation. It projects back a location p in current frame i to the location p + δp in key frame k, where δp = M i→k (p).</p><p>As the values δp are in general fractional, the feature warping is implemented via bilinear interpolation</p><formula xml:id="formula_1">f c i (p) = q G(q, p + δp)f c k (q),<label>(1)</label></formula><p>where c identifies a channel in the feature maps f , q enumerates all spatial locations in the feature maps, and G(·, ·) denotes the bilinear interpolation kernel. Note that G is two dimensional and is separated into two one dimensional kernels as</p><formula xml:id="formula_2">G(q, p + δp) = g(q x , p x + δp x ) · g(q y , p y + δp y ), (2) where g(a, b) = max(0, 1 − |a − b|).</formula><p>We note that Eq. (1) is fast to compute as a few terms are non-zero.</p><p>The spatial warping may be inaccurate due to errors in flow estimation, object occlusion, etc. To better approximate the features, their amplitudes are modulated by a "scale field" S i→k , which is of the same spatial and channel dimensions as the feature maps. The scale field is obtained by applying a "scale function" S on the two frames,</p><formula xml:id="formula_3">S i→k = S(I k , I i ).</formula><p>Finally, the feature propagation function is defined as</p><formula xml:id="formula_4">f i = W(f k , M i→k , S i→k ),<label>(3)</label></formula><p>where W applies Eq.(1) for all locations and all channels in the feature maps, and multiples the features with scales S i→k in an element-wise way. The proposed video recognition algorithm is called deep feature flow. It is summarized in Algorithm 1. Notice that any flow function F, such as the hand-crafted low-level flow (e.g., SIFT-Flow <ref type="bibr" target="#b25">[26]</ref>), is readily applicable. Training the flow function is not obligate, and the scale function S is set to ones everywhere.</p><p>Deep Feature Flow Training A flow function is originally designed to obtain correspondence of low-level image pixels. It can be fast in inference, but may not be accurate enough for the recognition task, in which the high-level feature maps change differently, usually slower than pixels <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">39]</ref>. To model such variations, we propose to also Algorithm 1 Deep feature flow inference algorithm for video recognition.</p><formula xml:id="formula_5">1: input: video frames {I i } 2: k = 0;</formula><p>initialize key frame <ref type="bibr">3:</ref> </p><formula xml:id="formula_6">f 0 = N f eat (I 0 ) 4: y 0 = N task (f 0 ) 5: for i = 1 to ∞ do 6:</formula><p>if is key f rame(i) then key frame scheduler <ref type="bibr">7:</ref> k = i update the key frame 8:</p><formula xml:id="formula_7">f k = N f eat (I k ) 9: y k = N task (f k ) 10:</formula><p>else use feature flow <ref type="bibr" target="#b10">11</ref>:</p><formula xml:id="formula_8">f i = W(f k , F(I k , I i ), S(I k , I i )) propagation 12: y i = N task (f i ) 13:</formula><p>end if 14: end for 15: output: recognition results {y i } use a CNN to estimate the flow field and the scale field such that all the components can be jointly trained end-to-end for the task.</p><p>The architecture is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(b). Training is performed by stochastic gradient descent (SGD). In each mini-batch, a pair of nearby video frames, {I k , I i } 1 , 0 ≤ i − k ≤ 9, are randomly sampled. In the forward pass, feature network N f eat is applied on I k to obtain the feature maps f k . Next, a flow network F runs on the frames I i , I k to estimate the flow field and the scale field. When i &gt; k, feature maps f k are propagated to f i as in Eq. (3). Otherwise, the feature maps are identical and no propagation is done. Finally, task network N task is applied on f i to produce the result y i , which incurs a loss against the ground truth result. The loss error gradients are back-propagated throughout to update all the components. Note that our training accommodates the special case when i = k and degenerates to the per-frame training as in <ref type="figure" target="#fig_1">Figure 2</ref>(a).</p><p>The flow network is much faster than the feature network, as will be elaborated later. It is pre-trained on the Flying Chairs dataset <ref type="bibr" target="#b8">[9]</ref>. We then add the scale function S as a sibling output at the end of the network, by increasing the number of channels in the last convolutional layer appropriately. The scale function is initialized to all ones (weights and biases in the output layer are initialized as 0s and 1s, respectively). The augmented flow network is then fine-tuned as in <ref type="figure" target="#fig_1">Figure 2</ref></p><formula xml:id="formula_9">(b).</formula><p>The feature propagation function in Eq.(3) is unconventional. It is parameter free and fully differentiable. In backpropagation, we compute the derivative of the features in f i with respect to the features in f k , the scale field S i→k , and the flow field M i→k . The first two are easy to compute us-ing the chain rule. For the last, from Eq. <ref type="formula" target="#formula_1">(1)</ref> and <ref type="formula" target="#formula_4">(3)</ref>, for each channel c and location p in current frame, we have</p><formula xml:id="formula_10">∂f c i (p) ∂M i→k (p) = S c i→k (p) q ∂G(q, p + δp) ∂δp f c k (q). (4)</formula><p>The term ∂G(q,p+δp) ∂δp can be derived from Eq. <ref type="bibr" target="#b1">(2)</ref>. Note that the flow field M(·) is two-dimensional and we use ∂δp to denote ∂δp x and ∂δp y for simplicity.</p><p>The proposed method can easily be trained on datasets where only sparse frames are annotated, which is usually the case due to the high labeling costs in video recognition tasks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b5">6]</ref>. In this case, the per-frame training <ref type="figure" target="#fig_1">(Figure 2</ref>(a)) can only use annotated frames, while DFF can easily use all frames as long as frame I i is annotated. In other words, DFF can fully use the data even with sparse ground truth annotation. This is potentially beneficial for many video recognition tasks.</p><p>Inference Complexity Analysis For each non-key frame, the computational cost ratio of the proposed approach (line 11-12 in Algorithm 1) and per-frame approach (line 8-9) is</p><formula xml:id="formula_11">r = O(F) + O(S) + O(W) + O(N task ) O(N f eat ) + O(N task ) ,<label>(5)</label></formula><p>where O(·) measures the function complexity.</p><p>To understand this ratio, we firstly note that the complexity of N task is usually small. Although its split point in N is kind of arbitrary, as verified in experiment, it is sufficient to keep only one learnable weight layer in N task in our implementation (see <ref type="bibr">Sec. 4</ref> </p><formula xml:id="formula_12">r ≈ O(F) O(N f eat ) .<label>(6)</label></formula><p>It is mostly determined by the complexity ratio of flow network F and feature network N f eat , which can be precisely measured, e.g., by their FLOPs. <ref type="table" target="#tab_3">Table 2</ref> shows its typical values in our implementation.</p><p>Compared to the per-frame approach, the overall speedup factor in Algorithm 1 also depends on the sparsity of key frames. Let there be one key frame in every l consecutive frames, the speedup factor is</p><formula xml:id="formula_13">s = l 1 + (l − 1) * r .<label>(7)</label></formula><p>Key Frame Scheduling As indicated in Algorithm 1 (line 6) and Eq. <ref type="formula" target="#formula_13">(7)</ref>, a crucial factor for inference speed is when to allocate a new key frame. In this work, we use a simple fixed key frame scheduling, that is, the key frame duration length l is a fixed constant. It is easy to implement and tune. However, varied changes in image content may require a varying l to provide a smooth tradeoff between accuracy and speed. Ideally, a new key frame should be allocated when the image content changes drastically.</p><p>How to design effective and adaptive key frame scheduling can further improve our work. Currently it is beyond the scope of this work. Different video tasks may present different behaviors and requirements. Learning an adaptive key frame scheduler from data seems an attractive choice. This is worth further exploration and left as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Network Architectures</head><p>The proposed approach is general for different networks and recognition tasks. Towards a solid evaluation, we adopt the state-of-the-art architectures and important vision tasks.</p><p>Flow Network We adopt the state-of-the-art CNN based FlowNet architecture (the "Simple" version) <ref type="bibr" target="#b8">[9]</ref> as default. We also designed two variants of lower complexity. The first one, dubbed FlowNet Half, reduces the number of convolutional kernels in each layer of FlowNet by half and the complexity to <ref type="bibr">1 4</ref> . The second one, dubbed FlowNet Inception, adopts the Inception structure <ref type="bibr" target="#b42">[43]</ref> and reduces the complexity to <ref type="bibr">1 8</ref> of that of FlowNet. The architecture details are reported in Appendix A.</p><p>The three flow networks are pre-trained on the synthetic Flying Chairs dataset in <ref type="bibr" target="#b8">[9]</ref>. The output stride is 4. The input image is half-sized. The resolution of flow field is therefore 1 8 of the original resolution. As the feature stride of the feature network is 16 (as described below), the flow field and the scale field is further down-sized by half using bilinear interpolation to match the resolution of feature maps. This bilinear interpolation is realized as a parameter-free layer in the network and also differentiated during training.</p><p>Feature Network We use ResNet models <ref type="bibr" target="#b15">[16]</ref>, specifically, the ResNet-50 and ResNet-101 models pre-trained for ImageNet classification as default. The last 1000-way classification layer is discarded. The feature stride is reduced from 32 to 16 to produce denser feature maps, following the practice of DeepLab <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> for semantic segmentation, and R-FCN <ref type="bibr" target="#b7">[8]</ref> for object detection. The first block of the conv5 layers are modified to have a stride of 1 instead of 2. The holing algorithm <ref type="bibr" target="#b3">[4]</ref> is applied on all the 3×3 convolutional kernels in conv5 to keep the field of view (dilation=2). A randomly initialized 3×3 convolution is appended to conv5 to reduce the feature channel dimension to 1024, where the holing algorithm is also applied (dilation=6). The resulting 1024-dimensional feature maps are the intermediate feature maps for the subsequent task. <ref type="table" target="#tab_3">Table 2</ref> presents the complexity ratio Eq. (6) of feature networks and flow networks.</p><p>Semantic Segmentation A randomly initialized 1 × 1 convolutional layer is applied on the intermediate feature maps to produce (C +1) score maps, where C is the number of categories and 1 is for background category. A following softmax layer outputs the per-pixel probabilities. Thus, the task network only has one learnable weight layer. The overall network architecture is similar to DeepLab with large field-of-view in <ref type="bibr" target="#b4">[5]</ref>.</p><p>Object Detection We adopt the state-of-the-art R-FCN <ref type="bibr" target="#b7">[8]</ref>. On the intermediate feature maps, two branches of fully convolutional networks are applied on the first half and the second half 512-dimensional of the intermediate feature maps separately, for sub-tasks of region proposal and detection, respectively.</p><p>In the region proposal branch, the RPN network <ref type="bibr" target="#b34">[35]</ref> is applied. We use n a = 9 anchors (3 scales and 3 aspect ratios). Two sibling 1 × 1 convolutional layers output the 2n a -dimensional objectness scores and the 4n adimensional bounding box (bbox) regression values, respectively. Non-maximum suppression (NMS) is applied to generate 300 region proposals for each image. Intersectionover-union (IoU) threshold 0.7 is used.</p><p>In the detection branch, two sibling 1 × 1 convolutional layers output the position-sensitive score maps and bbox regression maps, respectively. They are of dimensions (C + 1)k 2 and 4k 2 , respectively, where k banks of classifiers/regressors are employed to encode the relative position information. See <ref type="bibr" target="#b7">[8]</ref> for details. On the position-sensitive score/bbox regression maps, position-sensitive ROI pooling is used to obtain the per-region classification score and bbox regression result. No free parameters are involved in the per-region computation. Finally, NMS is applied on the scored and regressed region proposals to produce the detection result, with IoU threshold 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Unlike image datasets, large scale video dataset is much harder to collect and annotate. Our approach is evaluated on the two recent datasets: Cityscapes <ref type="bibr" target="#b5">[6]</ref> for semantic segmentation, and ImageNet VID <ref type="bibr" target="#b36">[37]</ref> for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment Setup</head><p>Cityscapes It is for urban scene understanding and autonomous driving. It contains snippets of street scenes collected from 50 different cities, at a frame rate of 17 fps. The train, validation, and test sets contain 2975, 500, and 1525 snippets, respectively. Each snippet has 30 frames, where the 20 th frame is annotated with pixel-level ground-truth labels for semantic segmentation. There are 30 semantic categories. Following the protocol in <ref type="bibr" target="#b4">[5]</ref>, training is performed on the train set and evaluation is performed on the validation set. The semantic segmentation accuracy is measured by the pixel-level mean intersection-over-union (mIoU) score.</p><p>In both training and inference, the images are resized to have shorter sides of 1024 and 512 pixels for the feature network and the flow network, respectively. In SGD training, 20K iterations are performed on 8 GPUs (each GPU holds one mini-batch, thus the effective batch size ×8), where the learning rates are 10 −3 and 10 −4 for the first 15K and the last 5K iterations, respectively.</p><p>ImageNet VID It is for object detection in videos. The training, validation, and test sets contain 3862, 555, and 937 fully-annotated video snippets, respectively. The frame rate is 25 or 30 fps for most snippets. There are 30 object categories, which are a subset of the categories in the ImageNet DET image dataset 2 . Following the protocols in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>, evaluation is performed on the validation set, using the standard mean average precision (mAP) metric.</p><p>In both training and inference, the images are resized to have shorter sides of 600 pixels and 300 pixels for the feature network and the flow network, respectively. In SGD training, 60K iterations are performed on 8 GPUs, where the learning rates are 10 −3 and 10 −4 for the first 40K and the last 20K iterations, respectively.</p><p>During training, besides the ImageNet VID train set, we also used the ImageNet DET train set (only the same 30 category labels are used), following the protocols in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>. Each mini-batch samples images from either ImageNet VID or ImageNet DET datasets, at 2 : 1 ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Methodology and Results</head><p>Deep feature flow is flexible and allows various design choices. We evaluate their effects comprehensively in the experiment. For clarify, we fix their default values throughout the experiments, unless specified otherwise. For feature network N f eat , ResNet-101 model is default. For flow network F, FlowNet (section 4) is default. Key-frame duration length l is 5 for Cityscapes <ref type="bibr" target="#b5">[6]</ref> segmentation and 10 for Im-ageNet VID <ref type="bibr" target="#b36">[37]</ref> detection by default, based on different frame rate of videos in the datasets..</p><p>For each snippet we evaluate l image pairs, (k, i), k = i − l + 1, ..., i, for each frame i with ground truth anno-method training of image recognition network N training of flow network F Frame (oracle baseline) trained on single frames as in <ref type="figure" target="#fig_1">Fig. 2 (a</ref> DFF trained on frame pairs as in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref> init. on Flying Chairs <ref type="bibr" target="#b8">[9]</ref>, fine-tuned in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref> DFF fix N same as Frame, then fixed in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref> same as DFF DFF fix F same as DFF init. on Flying Chairs <ref type="bibr" target="#b8">[9]</ref>, then fixed in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref> DFF separate same as Frame init. on Flying Chairs <ref type="bibr" target="#b8">[9]</ref>   <ref type="table">Table 4</ref>. Comparison of accuracy and runtime (mostly in GPU) of various approaches in <ref type="table" target="#tab_5">Table 3</ref>. Note that, the runtime for SFF consists of CPU runtime of SIFT-Flow and GPU runtime of Frame, since SIFT-Flow only has CPU implementation.</p><p>tation. Time evaluation is on a workstation with NVIDIA K40 GPU and Intel Core i7-4790 CPU.</p><p>Validation of DFF Architecture We compared DFF with several baselines and variants, as listed in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>• Frame: train N on single frames with ground truth.</p><p>• SFF: use pre-computed large-displacement flow (e.g., SIFT-Flow <ref type="bibr" target="#b25">[26]</ref>). SFF-fast and SFF-slow adopt different parameters.</p><p>• DFF: the proposed approach, N and F are trained end-to-end. Several variants include DFF fix N (fix N in training), DFF fix F (fix F in training), and DFF seperate (N and F are separately trained). <ref type="table">Table 4</ref> summarizes the accuracy and runtime of all approaches. We firstly note that the baseline Frame is strong enough to serve as a reference for comparison. Our implementation resembles the state-of-the-art DeepLab <ref type="bibr" target="#b4">[5]</ref> for semantic segmentation and R-FCN <ref type="bibr" target="#b7">[8]</ref> for object detection. In DeepLab <ref type="bibr" target="#b4">[5]</ref>, an mIoU score of 69.2% is reported with DeepLab large field-of-view model using ResNet-101 on Cityscapes validation dataset. Our Frame baseline achieves slightly higher 71.1%, based on the same ResNet model. For object detection, Frame baseline has mAP 73.9% using R-FCN <ref type="bibr" target="#b7">[8]</ref> and ResNet-101. As a reference, a comparable mAP score of 73.8% is reported in <ref type="bibr" target="#b21">[22]</ref>, by combining CRAFT <ref type="bibr" target="#b46">[47]</ref> and DeepID-net <ref type="bibr" target="#b29">[30]</ref> object detectors trained on the ImageNet data, using both VGG-16 <ref type="bibr" target="#b39">[40]</ref> and GoogleNet-v2 [20] models, with various tricks (multi-scale training/testing, adding context information, model ensemble). We do not adopt above tricks as they complicate the comparison and obscure the conclusions.</p><p>SFF-fast has a reasonable runtime but accuracy is significantly decreased. SFF-slow uses the best parameters for flow estimation. It is much slower. Its accuracy is slightly improved but still poor. This indicates that an off-the-shelf flow may be insufficient.</p><p>The proposed DFF approach has the best overall performance. Its accuracy is slightly lower than that of Frame and it is 3.7 and 5.0 times faster for segmentation and detection, respectively. As expected, the three variants without using joint training have worse accuracy. Especially, the accuracy drop by fixing F is significant. This indicates a jointing end-to-end training (especially flow) is crucial.</p><p>We also tested another variant of DFF with the scale function S removed (Algorithm 1, Eq (3), Eq. (4)). The accuracy drops for both segmentation and detection (less than one percent). It shows that the scaled modulation of features is slightly helpful.</p><p>Accuracy-Speedup Tradeoff We investigate the tradeoff by varying the flow network F, the feature network N f eat , and key frame duration length l. Since Cityscapes and ImageNet VID datasets have different frame rates, we tested l = 1, 2, ..., 10 for segmentation and l = 1, 2, ..., 20 for detection.</p><p>The results are summarized in <ref type="figure" target="#fig_2">Figure 3</ref>. Overall, DFF achieves significant speedup with decent accuracy drop. It smoothly trades in accuracy for speed and fits different application needs flexibly. What flow F should we use? From <ref type="figure" target="#fig_2">Figure 3</ref>, the smallest FlowNet Inception is advantageous. It is faster than its two counterparts at the same accuracy level, most of the times.</p><p>What feature N f eat should we use? In high-accuracy zone, an accurate model ResNet-101 is clearly better than ResNet-50. In high-speed zone, the conclusions are different on the two tasks. For detection, ResNet-101 is still advantageous. For segmentation, the performance curves intersect at around 6.35 fps point. For higher speed, ResNet-50 becomes better than ResNet-101. The seemingly different conclusions can be partially attributed to the different video frame rates, the extents of dynamics on the two datasets. The Cityscapes dataset not only has a low frame rate 17 fps, but also more quick dynamics. It would be hard to utilize temporal redundancy for a long propagation. To achieve the same high speed, ResNet-101 needs a larger key frame length l than ResNet-50. This in turn significantly increases the difficulty of learning.</p><p>Above observations provide useful recommendations for practical applications. Yet, they are more heuristic than general, as they are observed only on the two tasks, on limited data. We plan to explore the design space more in the future. Split point of N task Where should we split N task in N ? Recall that the default N task keeps one layer with learning weight (the 1 × 1 conv over 1024-d feature maps, see Section 4). Before this is the 3 × 3 conv layer that reduces dimension to 1024. Before this is series of "Bottleneck" unit in ResNet <ref type="bibr" target="#b15">[16]</ref>, each consisting of 3 layers. We back move the split point to make different N task s with 5, 12, and 21 layers, respectively. The one with 5 layers adds the dimension reduction layer and one bottleneck unit (conv5c). The one with 12 layers adds two more units (conv5a and conv5b) at the beginning of conv5. The one with 21 layers adds three more units in conv4. We also move the only layer in default N task into N f eat , leaving N task with 0 layer (with learnable weights). This is equivalent to directly propagate the parameter-free score maps, in both semantic segmentation and object detection. <ref type="table" target="#tab_6">Table 5</ref> summarizes the results. Overall, the accuracy variation is small enough to be neglected. The speed be-  comes lower when N task has more layers. Using 0 layer is mostly equivalent to using 1 layer, in both accuracy and speed. We choose 1 layer as default as that leaves some tunable parameters after the feature propagation, which could be more general.</p><p>Example results of the proposed method are presented in <ref type="figure" target="#fig_4">Figure 4</ref> and <ref type="figure" target="#fig_5">Figure 5</ref> for video segmentation on CityScapes and video detection on ImageNet VID, respectively. More example results are available at https: //www.youtube.com/watch?v=J0rMHE6ehGw.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future Work</head><p>Several important aspects are left for further exploration. It would be interesting to exploit how the joint learning affects the flow quality. We are unable to evaluate as there lacks ground truth. Current optical flow works are also limited to either synthetic data <ref type="bibr" target="#b8">[9]</ref> or small real datasets, which is insufficient for deep learning.</p><p>Our method can further benefit from improvements in flow estimation and key frame scheduling. In this paper, we adopt FlowNet <ref type="bibr" target="#b8">[9]</ref> mainly because there are few choices. Designing faster and more accurate flow network will cer-     <ref type="table" target="#tab_3">conv1  7x7 conv  2  64  conv2  5x5 conv  2  128  conv3  5x5 conv  2  256  conv3 1  3x3 conv  256  conv4  3x3 conv  2  512  conv4 1  3x3 conv  512  conv5  3x3 conv  2  512  conv5 1  3x3 conv  512  conv6</ref> 3x3 conv 2 1024 conv6 1 3x3 conv 1024 tainly receive more attention in the future. For key frame scheduling, a good scheduler may well significantly improve both speed and accuracy. And this problem is definitely worth further exploration. We believe this work opens many new possibilities. We hope it will inspire more future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. FlowNet Inception Architecture</head><p>The architectures of FlowNet, FlowNet Half follow that of <ref type="bibr" target="#b8">[9]</ref> (the "Simple" version), which are detailed in <ref type="table">Table 6</ref> and <ref type="table">Table 7</ref>, respectively. The architecture of FlowNet Inception follows the design of the Inception structure <ref type="bibr" target="#b42">[43]</ref>, which is detailed in <ref type="table" target="#tab_3">Table 8.  layer   type  stride  # output   Inception/Reduction   #1x1  #1x1-#3x3  #1x1-#3x3-#3x3  #pool   conv1  7x7 conv  2  32  pool1  3x3 max pool  2  32  conv2  Inception  64  24-32  24-32-32  conv3 1  3x3 conv  2  128  conv3 2  Inception  128  48  32-64  8-16-16  conv3 3  Inception  128  48</ref>   <ref type="table">Table 8</ref>. The FlowNet Inception network architecture, following the design of the Inception structure <ref type="bibr" target="#b42">[43]</ref>. "Inception/Reduction" modules consist of four branches: 1x1 conv (#1x1), 1x1 conv-3x3 conv (#1x1-#3x3), 1x1 conv-3x3 conv-3x3 conv (#1x1-#3x3-#3x3), and 3x3 max pooling followed by 1x1 conv (#pool, only for stride=2).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 3, bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of video recognition using per-frame network evaluation (a) and the proposed deep feature flow (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>(better viewed in color) Illustration of accuracy-speed tradeoff under different implementation choices on ImageNet VID detection (top) and on Cityscapes segmentation (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Semantic segmentation results on Cityscapes validation dataset. The first column corresponds to the images and the results on the key frame (the k th frame). The following four columns correspond to the k + 1 st , k + 2 nd , k + 3 rd and k + 4 th frames, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Object detection results on ImageNet VID validation dataset. The first column corresponds to the images and the results on the key frame (the k th frame). The following four columns correspond to the k + 2 nd , k + 4 th , k + 6 th and k + 8 th frames, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1611.07715v2 [cs.CV] 5 Jun 2017</figDesc><table><row><cell></cell><cell>filter #183</cell><cell>filter #289</cell></row><row><cell>key frame</cell><cell>key frame feature maps</cell></row><row><cell></cell><cell>filter #183</cell><cell>filter #289</cell></row><row><cell>current frame</cell><cell>current frame feature maps</cell></row><row><cell></cell><cell>filter #183</cell><cell>filter #289</cell></row><row><cell>flow field</cell><cell>propagated feature maps</cell></row></table><note>Figure 1. Motivation of proposed deep feature flow approach. Here we visualize the two filters' feature maps on the last convolutional layer of our ResNet-101 model (see Sec. 4 for details). The convolutional feature maps are similar on two nearby frames. They can be cheaply propagated from the key frame to current frame via a flow field.high performance facilitates video recognition tasks in practice. Code is released at https://github.com/ msracver/Deep-Feature-Flow.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and S are very simple. Thus, the ratio in Eq. (5) is approximated as</figDesc><table><row><cell>We also have O(W)</cell><cell>O(F) and O(S)</cell><cell>O(F) be-</cell></row><row><cell>cause W</cell><cell></cell><cell></cell></row></table><note>). While both N f eat and F have considerable complexity (Section 4), we have O(N task ) O(N f eat ) and O(N task ) O(F).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>The approximated complexity ratio in Eq. (6) for different feature network N f eat and flow network F, measured by their FLOPs. See Section 4. Note that r 1 and we use 1 r here for clarify. A significant per-frame speedup factor is obtained.</figDesc><table><row><cell></cell><cell>FlowNet</cell><cell>FlowNet Half</cell><cell>FlowNet Inception</cell></row><row><cell>ResNet-50</cell><cell>9.20</cell><cell>33.56</cell><cell>68.97</cell></row><row><cell>ResNet-101</cell><cell>12.71</cell><cell>46.30</cell><cell>95.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Description of variants of deep feature flow (DFF), shallow feature flow (SFF), and the per-frame approach (Frame).</figDesc><table><row><cell>Methods</cell><cell cols="2">Cityscapes (l = 5)</cell><cell cols="2">ImageNet VID (l = 10)</cell></row><row><cell></cell><cell cols="4">mIoU(%) runtime (fps) mAP(%) runtime (fps)</cell></row><row><cell>Frame</cell><cell>71.1</cell><cell>1.52</cell><cell>73.9</cell><cell>4.05</cell></row><row><cell>SFF-slow</cell><cell>67.8</cell><cell>0.08</cell><cell>70.7</cell><cell>0.26</cell></row><row><cell>SFF-fast</cell><cell>67.3</cell><cell>0.95</cell><cell>69.7</cell><cell>3.04</cell></row><row><cell>DFF</cell><cell>69.2</cell><cell>5.60</cell><cell>73.1</cell><cell>20.25</cell></row><row><cell>DFF fix N</cell><cell>68.8</cell><cell>5.60</cell><cell>72.3</cell><cell>20.25</cell></row><row><cell>DFF fix F</cell><cell>67.0</cell><cell>5.60</cell><cell>68.8</cell><cell>20.25</cell></row><row><cell>DFF separate</cell><cell>66.9</cell><cell>5.60</cell><cell>67.4</cell><cell>20.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>For example, in detection, it improves 4.05 fps of ResNet-101 Frame to 41.26 fps of ResNet-101 + FlowNet Inception. The 10× faster speed is at the cost of moderate accuracy drop from 73.9% to 69.5%. In segmentation, it improves 2.24 fps of ResNet-50 Frame # layers in N task Results of using different split points for N task . to 17.48 fps of ResNet-50 FlowNet Inception, at the cost of accuracy drop from 69.7% to 62.4%.</figDesc><table><row><cell></cell><cell cols="2">Cityscapes (l=5)</cell><cell cols="2">ImageNet VID (l=10)</cell></row><row><cell></cell><cell cols="4">mIoU(%) runtime (fps) mAP(%) runtime (fps)</cell></row><row><cell>21</cell><cell>69.1</cell><cell>2.87</cell><cell>73.2</cell><cell>7.23</cell></row><row><cell>12</cell><cell>69.1</cell><cell>3.14</cell><cell>73.3</cell><cell>8.04</cell></row><row><cell>5</cell><cell>69.2</cell><cell>3.89</cell><cell>73.2</cell><cell>9.99</cell></row><row><cell>1 (default)</cell><cell>69.2</cell><cell>5.60</cell><cell>73.1</cell><cell>20.25</cell></row><row><cell>0</cell><cell>69.5</cell><cell>5.61</cell><cell>72.7</cell><cell>20.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>The FlowNet network architecture. The FlowNet Half network architecture.</figDesc><table><row><cell>layer</cell><cell>type</cell><cell>stride</cell><cell># output</cell></row><row><cell>conv1</cell><cell>7x7 conv</cell><cell>2</cell><cell>32</cell></row><row><cell>conv2</cell><cell>5x5 conv</cell><cell>2</cell><cell>64</cell></row><row><cell>conv3</cell><cell>5x5 conv</cell><cell>2</cell><cell>128</cell></row><row><cell>conv3 1</cell><cell>3x3 conv</cell><cell></cell><cell>128</cell></row><row><cell>conv4</cell><cell>3x3 conv</cell><cell>2</cell><cell>256</cell></row><row><cell>conv4 1</cell><cell>3x3 conv</cell><cell></cell><cell>256</cell></row><row><cell>conv5</cell><cell>3x3 conv</cell><cell>2</cell><cell>256</cell></row><row><cell>conv5 1</cell><cell>3x3 conv</cell><cell></cell><cell>256</cell></row><row><cell>conv6</cell><cell>3x3 conv</cell><cell>2</cell><cell>512</cell></row><row><cell>conv6 1</cell><cell>3x3 conv</cell><cell></cell><cell>512</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The same notations are used for consistency although there is no longer the concept of "key frame" during training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.image-net.org/challenges/LSVRC/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting semantic information and deep matching for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: Descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flownet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">STFCN: spatio-temporal FCN for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A unified video segmentation benchmark: Annotation, metrics and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shankar Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Jimenez</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint optical flow and temporally consistent semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV CVRSUAD Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Slow and steady feature analysis: higher order temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">T-cnn: Tubelets with convolutional neural networks for object detection from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature space optimization for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multi-class multi-object tracking using changing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdenee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rhee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sift flow: dense correspondence across difference scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepid-net: Deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06309</idno>
		<title level="m">Spatio-temporal video autoencoder with differentiable memory</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015. 1, 6</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optical flow with semantic segmentation and localized layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Clockwork convnets for video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dlsfa: deeply-learned slow feature analysis for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey on variational optic flow methods for small displacements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical models for registration and applications to medical imaging</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DeepFlow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Slow feature analysis: Unsupervised learning of invariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Craft objects from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Discriminative image warping with attribute flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Accelerating very deep convolutional networks for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Slow feature analysis for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep learning of invariant features via simulated fixations in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
