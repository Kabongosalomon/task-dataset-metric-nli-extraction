<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatial Aggregation of Holistically-Nested Convolutional Neural Networks for Automated Pancreas Localization and Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Le</forename><forename type="middle">Lu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Lay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">P</forename><surname>Harrison</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><surname>Farag</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Sohn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
						</author>
						<title level="a" type="main">Spatial Aggregation of Holistically-Nested Convolutional Neural Networks for Automated Pancreas Localization and Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate and automatic organ segmentation from 3D radiological scans is an important yet challenging problem for medical image analysis. Specifically, as a small, soft, and flexible abdominal organ, the pancreas demonstrates very high inter-patient anatomical variability in both its shape and volume. This inhibits traditional automated segmentation methods from achieving high accuracies, especially compared to the performance obtained for other organs, such as the liver, heart or kidneys. To fill this gap, we present an automated system from 3D computed tomography (CT) volumes that is based on a twostage cascaded approach-pancreas localization and pancreas segmentation. For the first step, we localize the pancreas from the entire 3D CT scan, providing a reliable bounding box for the more refined segmentation step. We introduce a fully deep-learning approach, based on an efficient application of holistically-nested convolutional networks (HNNs) on the three orthogonal axial, sagittal, and coronal views. The resulting HNN per-pixel probability maps are then fused using pooling to reliably produce a 3D bounding box of the pancreas that maximizes the recall. We show that our introduced localizer compares favorably to both a conventional non-deep-learning method and a recent hybrid approach based on spatial aggregation of superpixels using random forest classification. The second, segmentation, phase operates within the computed bounding box and integrates semantic mid-level cues of deeply-learned organ interior and boundary maps, obtained by two additional and separate realizations of HNNs. By integrating these two mid-level cues, our method is capable of generating boundarypreserving pixel-wise class label maps that result in the final pancreas segmentation. Quantitative evaluation is performed on a publicly available dataset of 82 patient CT scans using 4-fold cross-validation (CV). We achieve a (mean ± std. dev.) Dice similarity coefficient (DSC) of 81.27±6.27% in validation, which significantly outperforms both a previous state-of-the art method and a preliminary version of this work that report DSCs of 71.80±10.70% and 78.01±8.20%, respectively, using the same dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Accurate and automatic organ segmentation from 3D radiological scans is an important yet challenging problem for medical image analysis. Specifically, as a small, soft, and flexible abdominal organ, the pancreas demonstrates very high inter-patient anatomical variability in both its shape and volume. This inhibits traditional automated segmentation methods from achieving high accuracies, especially compared to the performance obtained for other organs, such as the liver, heart or kidneys. To fill this gap, we present an automated system from 3D computed tomography (CT) volumes that is based on a twostage cascaded approach-pancreas localization and pancreas segmentation. For the first step, we localize the pancreas from the entire 3D CT scan, providing a reliable bounding box for the more refined segmentation step. We introduce a fully deep-learning approach, based on an efficient application of holistically-nested convolutional networks (HNNs) on the three orthogonal axial, sagittal, and coronal views. The resulting HNN per-pixel probability maps are then fused using pooling to reliably produce a 3D bounding box of the pancreas that maximizes the recall. We show that our introduced localizer compares favorably to both a conventional non-deep-learning method and a recent hybrid approach based on spatial aggregation of superpixels using random forest classification. The second, segmentation, phase operates within the computed bounding box and integrates semantic mid-level cues of deeply-learned organ interior and boundary maps, obtained by two additional and separate realizations of HNNs. By integrating these two mid-level cues, our method is capable of generating boundarypreserving pixel-wise class label maps that result in the final pancreas segmentation. Quantitative evaluation is performed on a publicly available dataset of 82 patient CT scans using 4-fold cross-validation (CV). We achieve a (mean ± std. dev.) Dice similarity coefficient (DSC) of 81.27±6.27% in validation, which significantly outperforms both a previous state-of-the art method and a preliminary version of this work that report DSCs of 71.80±10.70% and 78.01±8.20%, respectively, using the same dataset. performance with Dice similarity coefficients (DSCs) of &gt;90% <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, the pancreas' variable shape, size, and location in the abdomen limits segmentation accuracy to &lt;73% DSC being reported in the literature <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Previous pancreas segmentation work <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> are all based on performing volumetric multiple atlas registration <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> and executing robust label fusion methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> to optimize the per-pixel organ labeling process. This type of organ segmentation strategy is widely used for many organ segmentation problems, such as the brain <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, heart <ref type="bibr" target="#b11">[12]</ref>, lung <ref type="bibr" target="#b13">[14]</ref>, and pancreas <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. These methods can be referred as a top-down model fitting approach, or more specifically, MALF (Multi-Atlas Registration &amp; Label Fusion). Another group of top-down frameworks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> leverages statistical model detection, e.g., generalized Hough transform <ref type="bibr" target="#b14">[15]</ref> or marginal space learning <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, for organ localization; and deformable statistical shape models for object segmentation. However, due to the intrinsic huge 3D shape variability of the pancreas, statistical shape modeling has not been applied for pancreas segmentation.</p><p>Recently, a new bottom-up pancreas segmentation representation has been proposed in <ref type="bibr" target="#b5">[6]</ref>, which uses dense binary image patch labeling confidence maps that are aggregated to classify image regions, or superpixels <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, into pancreas and non-pancreas label assignments. This method's motivation is to improve segmentation accuracy of highly deformable organs, such as the pancreas, by leveraging midlevel visual representations of image segments. This work was advanced further by Roth et al. <ref type="bibr" target="#b6">[7]</ref>, who proposed a probabilistic bottom-up approach using a set of multi-scale and multi-level deep convolutional neural networks (CNNs) to capture the complexity of pancreas appearance in CT images. The resulting system improved upon the performance of <ref type="bibr" target="#b5">[6]</ref> with a reported DSC of 71.8±10.7% against 68.8±25.6%. Compared to the MALF based pancreas segmentation work <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> that are evaluated using "leave-one-patientout" (LOO) protocol, the bottom-up approaches using superpixel representation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> have reported comparable or higher DSC accuracy measurements, under more challenging 6-fold or 4-fold cross-validation <ref type="bibr" target="#b0">1</ref> . Comparing the two bottom-up approaches, the usage of deep CNN models has noticeably improved the performance stability, which is evident by the significantly smaller standard deviation <ref type="bibr" target="#b6">[7]</ref> than all other topdown or bottom-up works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>Deep CNNs have successfully been applied to many highlevel tasks in medical imaging, such as recognition and object detection <ref type="bibr" target="#b21">[22]</ref>. The main advantage of CNNs comes from the fact that end-to-end learning of salient feature representations for the task at hand is more effective than handcrafted features with heuristically tuned parameters <ref type="bibr" target="#b22">[23]</ref>. Similarly, CNNs demonstrate promising performance for pixellevel labeling problems, e.g., semantic segmentation in recent computer vision and medical imaging analysis work, e.g., fully convolutional neural networks (FCN) <ref type="bibr" target="#b23">[24]</ref>, DeepLab <ref type="bibr" target="#b24">[25]</ref> and U-Net <ref type="bibr" target="#b25">[26]</ref>. These approaches have all garnered significant improvements in performance over previous methods by applying state-of-the-art CNN-based image classifiers and representation to the semantic segmentation problem in both domains.</p><p>Semantic organ segmentation involves assigning a label to each pixel in the image. On one hand, features for classification of single pixels (or patches) play a major role, but on the other hand, factors such as edges, i.e., organ boundaries, appearance consistency, and spatial consistency, could greatly impact the overall system performance <ref type="bibr" target="#b22">[23]</ref>. Furthermore, there are indications of semantic vision tasks requiring hierarchical levels of visual perception and abstraction <ref type="bibr" target="#b26">[27]</ref>. As such, generating rich feature hierarchies for both the interior and the boundary of the organ could provide important "mid-level visual cues" for semantic segmentation. Subsequent spatial aggregation of these mid-level cues then has the prospect of improving semantic segmentation methods by enhancing the accuracy and consistency of pixel-level labeling.</p><p>A preliminary version of this work appears as <ref type="bibr" target="#b27">[28]</ref>, where we demonstrate that a two-stage bottom-up localization and segmentation approach can improve upon the state of the art. In this work, the major extension is that we describe an improved pancreas localization method by replacing the initial super-pixel based one, with a new general deep learning based approach. This methodological component is designed to optimize or maximize the pancreas spatial recall criterion while reducing the non-pancreas volume as much as possible. Specifically, we generate the per-pixel pancreas class probability maps (or "heat maps") through an efficient combination of holistically-nested convolutional networks (HNNs) in the three orthogonal axial, sagittal, and coronal CT views. We fuse the three HNN outputs to produce a 3D bounding box covering the underlying, yet latent in testing, pancreas volume by nearly 100%. In addition, we show that exactly the same HNN model architecture can be effective for the subsequent pancreas segmentation stage by integrating both deeply learned boundary and appearance cues. This also results in a simpler overall pancreas localization and segmentation system using HNNs only, rather than the previous hybrid setup involving non-deep-and deep-learning method components <ref type="bibr" target="#b27">[28]</ref>. Lastly, our current method reports an overall improved DSC performance compared to <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b6">[7]</ref>: DSC of 81.14±7.3% versus 78.0±8.2% and 71.8±10.7% <ref type="bibr" target="#b6">[7]</ref>, respectively.</p><p>The proposed two-stage process essentially performs 3D spatial aggregation and assembling on the HNN-produced per-pixel pancreas probability maps that run on 2D axial, coronal, and sagittal CT planes. This process operates exhaustively for pancreas localization and selectively for pancreas segmentation. Therefore, this work inherits a hierarchical and compositional visual representation of computing 3D object information aggregated from 2D image slices or parts, in a similar spirit of <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Alternatively, there are recent studies on directly using 3D convolutional neural networks for liver, brain segmentation <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> and volumetric vascular boundary detection <ref type="bibr" target="#b33">[34]</ref>. Due to CNN memory restrictions, these 3D CNN approaches adopt padded sliding windows or volumes to process the original CT scans, such as 96×96×48 segments <ref type="bibr" target="#b33">[34]</ref>, 160×160×72 subvolumes <ref type="bibr" target="#b31">[32]</ref> and 80×80×80 windows <ref type="bibr" target="#b32">[33]</ref>, which may cause segmentation discontinuities or inconsistencies at overlapped window boundaries. We argue that learning shareable lower-dimensional 2D CNN models may be more generalizable and handle the "curse-ofdimensionality" issue better than their fully 3D counterparts, especially when used to parse complex 3D anatomical structures, e.g., lymph node clusters <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> and the pancreas <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Analogous examples of comparing compositional multi-view 2D CNNs versus direct 3D deep models can be found in other computer vision problems: 1) video based action recognition where a two-stream 2D CNN model <ref type="bibr" target="#b36">[37]</ref>, capturing the image intensity and motion cues, significantly improves upon the 3D CNN method <ref type="bibr" target="#b37">[38]</ref>; 2) the advantageous performance of multi-view CNNs over volumetric CNNs in 3D Shape Recognition <ref type="bibr" target="#b38">[39]</ref>. The rest of this paper is organized as follows. We describe the technical motivation and details of the proposed approach in Sec. II. Experimental results and comparison with related work are addressed in Sec. III. We conclude the paper, an with extended discussion, in Sec. IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head><p>In this work, we present a two-phased approach for automated pancreas localization and segmentation. The pancreas localization step aims to robustly compute a bounding box which, at the desirable setting, should cover the entire pancreas while pruning the high majority volumetric space from any input CT scan without any manual pre-processing. The second stage of pancreas segmentation incorporates deeply learned organ interior and boundary mid-level cues with subsequent spatial aggregation, focusing only on the properly zoomed or cascaded pancreas location and spatial extents that are generated after the first phase. In Sec. II-A we introduce the HNN model that proves effective for both stages. Afterwards, we focus on localization in Sec. II-B, which discusses and contrasts a conventional approach to localization with newer CNN-based ones-a hybrid and a fully deep-learning approach. We show how the latter approach, which relies on HNNs, provides a simple, yet state-of-the-art, localization method. Importantly, it relies on the same HNN architecture as the later segmentation step. With localization discussed, we explain our segmentation approach in Sec. II-C, which relies on combining semantic mid-level cues produced from HNNs.</p><p>Our approach to organ segmentation is based on simple, reproducible, yet effective, machine-learning principles. In particular, we demonstrate the most effective configuration of our system is simply composed of cascading and aggregating outputs from six HNNs trained at three orthogonal views and two spatial scales. No multi-atlas registration or multi-label fusion techniques are employed. <ref type="figure" target="#fig_1">Fig. 1</ref> provides a flowchart depicting the makeup of our system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pancreas Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pancreas Localization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II-B2 Random Forest on Superpixels for Organ Region Proposal</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II-B3 Mutli-view Max-pooled HNNs for Pancreas Localization (Preferred)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.C1 Mid-level Cues from HNNs and Multiview Fusion for Pancreas Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.C2 Learning Organ-specific Segmentation Proposals via Boundary Cue</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.C3 Spatial Aggregation using Random Forests on Segmentation Proposal Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning Mid-level Cues via Holistically-Nested Networks for Localization and Segmentation</head><p>In this work, we use the HNN architecture, to learn the pancreas' interior and boundary image-labeling maps, for both localization and segmentation. Object-level interior and boundary information are referred to as mid-level visual cues. Note that this type of CNN architecture was first proposed by <ref type="bibr" target="#b26">[27]</ref> under the name "holistically-nested edge detection" as a deep learning based general image edge detection method. It has been used successfully for extracting "edge-like" structures like blood vessels in 2D retina images <ref type="bibr" target="#b39">[40]</ref>. We however would argue and validate that it can serve as a suitable deep representation to learn general raw pixel-in and label-out mapping functions, i.e., to perform semantic segmentation. We use these principles to segment the interior of organs. HNN is designed to address two important issues: (1) training and prediction on the whole image end-to-end, i.e, holistically, using a per-pixel labeling cost; and (2) incorporating multi-scale and multi-level learning of deep image features <ref type="bibr" target="#b26">[27]</ref> via auxiliary cost functions at each convolutional layer. HNN computes the imageto-image or pixel-to-pixel prediction maps from any input raw image to its annotated labeling map, building on fully convolutional neural networks <ref type="bibr" target="#b23">[24]</ref> and deeply-supervised nets <ref type="bibr" target="#b40">[41]</ref>. The per-pixel labeling cost function <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref> makes it feasible that HNN/FCN can be effectively trained using only several hundred annotated image pairs. This enables the automatic learning of rich hierarchical feature representations and contexts that are critical to resolve spatial ambiguity in the segmentation of organs. The network structure is initialized based on an ImageNet pre-trained VGGNet model <ref type="bibr" target="#b41">[42]</ref>. It has been shown that fine-tuning CNNs pre-trained on general image classification tasks is helpful for low-level tasks, e.g., edge detection <ref type="bibr" target="#b26">[27]</ref>. Furthermore, we can utilize pre-trained edge-detection networks (trained on BSDS500 <ref type="bibr" target="#b26">[27]</ref>) to segment organ-specific boundaries. Network formulation:</p><formula xml:id="formula_0">Our training data S I/B = (X n , Y I/B n ), n = 1, . . . , N where X n denotes cropped axial CT images X n , rescaled to within [0, . . . , 255] with a soft-tissue window of [−160, 240] HU. Y I n ∈ {0, 1} and Y B n ∈ {0,</formula><p>1} denote the binary ground truths of the interior and boundary map of the pancreas, respectively, for any corresponding X n . Each image is considered holistically and independently as in <ref type="bibr" target="#b26">[27]</ref>. The network is able to learn features from these images alone from which interior and boundary prediction maps can be produced, which we denote as HNN-I and HNN-B, respectively. HNN can efficiently generate multi-level image features due to its deep architecture. Furthermore, multiple stages with different convolutional strides can capture the inherent scales of organ edge/interior labeling maps. However, due to the difficulty of learning such deep neural networks with multiple stages from scratch, we use the pre-trained network provided by <ref type="bibr" target="#b26">[27]</ref> and fine-tuned to our specific training data sets S I/B with a relatively smaller learning rate of 10 −6 . We use the HNN network architecture with 5 stages, including strides of 1, 2, 4, 8 and 16, respectively, and with different receptive field sizes as suggested by the authors 2 . In addition to standard CNN layers, a HNN network has M side-output layers as shown in <ref type="figure">Fig. 2</ref>. These side-output layers are also realized as classifiers in which the corresponding weights are w = (w <ref type="bibr" target="#b0">(1)</ref> , . . . , w (M ) ). For simplicity, all standard network layer parameters are denoted as W. Hence, the following objective function can be defined <ref type="bibr" target="#b2">3</ref> :</p><formula xml:id="formula_1">L side (W, w) = M m=1 α m l (m) side (W, w m ).<label>(1)</label></formula><p>Here, l side denotes an image-level loss function for sideoutputs, computed over all pixels in a training image pair X and Y . Because of the heavy bias towards non-labeled pixels in the ground truth data, <ref type="bibr" target="#b26">[27]</ref> introduces a strategy to automatically balance the loss between positive and negative classes via a per-pixel class-balancing weight β. This offsets the imbalances between edge/interior (y = 1) and nonedge/exterior (y = 0) samples. Specifically, a class-balanced cross-entropy loss function can be used in Eq. (1) with j iterating over the spatial dimensions of the image:</p><formula xml:id="formula_2">l (m) side (W, w (m) ) = −β j∈Y+ log P r y j = 1|X; W, w (m) − (1 − β) j∈Y− log P r y j = 0|X; W, w (m) . (2)</formula><p>Here, β is simply |Y − |/|Y | and 1 − β = |Y + |/|Y |, where |Y − | and |Y + | denote the ground truth set of negatives and positives, respectively. In contrast to <ref type="bibr" target="#b26">[27]</ref>, where β is computed for each training image independently, we use a constant balancing weight computed on the entire training set. This is because some training slices might have no positives at all and otherwise would be ignored in the loss function. The class probability P r(</p><formula xml:id="formula_3">y j = 1|X; W, w (m) ) = σ(a (m) j ) ∈ [0, 1]</formula><p>is computed on the activation value at each pixel j using the sigmoid function σ(.). Now, organ edge/interior map predictionsŶ (m) side = σ(Â(m) side ) can be obtained at each side-output layer, whereÂ(m) side ≡ {a (m) j , j = 1, . . . , |Y |} are activations of the side-output of layer m. Finally, a "weighted-fusion" layer is added to the network that can be simultaneously learned during training. The loss function at the fusion layer L fuse is defined as</p><formula xml:id="formula_4">L fuse (W, w, h) = Dist Y,Ŷ fuse ,<label>(3)</label></formula><p>whereŶ fuse = σ M m=1 hÂ side m with h = (h 1 , . . . , h M ) being the fusion weight. Dist(., .) is a distance measure between the fused predictions and the ground truth label map. We use cross-entropy loss for this purpose. Hence, the following objective function can be minimized via standard stochastic gradient descent and back propagation:</p><formula xml:id="formula_5">(W, w, h) = argmin (L side (W, w) + L fuse (W, w, h))</formula><p>(4) Testing phase: Given image X, we obtain both interior (HNN-I) and boundary (HNN-B) predictions from the models' side output layers and the weighted-fusion layer as in <ref type="bibr" target="#b26">[27]</ref>:</p><formula xml:id="formula_6">Ŷ I fuse ,Ŷ I1) side , . . . ,Ŷ I M side = HNN-I (X, (W, w, h))<label>(5)</label></formula><formula xml:id="formula_7">Ŷ B fuse ,Ŷ B1) side , . . . ,Ŷ B M side = HNN-B (X, (W, w, h))<label>(6)</label></formula><p>Here, HNN-I/B(·) denotes the interior/boundary prediction maps estimated by the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pancreas Localization</head><p>Segmentation performance can be enhanced if irrelevant regions of the CT volume are pruned out. Conventional organ localization methods using random forest regression <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, which we explain in Sec. II-B1, may not guarantee that the regressed organ bounding box contains the targeted organ with extremely high sensitivities on the pixel-level coverage. In Sec. II-B2 we outline a superpixel based approach <ref type="bibr" target="#b5">[6]</ref>, based on hand-crafted and CNN features, that is able to provide improved performance. While this is effective, the complexity involved motivates our own development of a simpler and more accessible newly proposed multi-view HNN fusion based procedure. This is explained in Sec. II-B3. The output of the localization method will later feed into a more detailed and accurate segmentation method combining multiple mid-level cues from HNNs as illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>1) Regression Forest: Object localization by regression has been studied extensively in the literature including <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b43">[44]</ref>. The general idea is to predict an offset vector ∆x ∈ R 3 for a given image patch I(x) centered about x ∈ R 3 . The predicted object position is then given as x + ∆x. This is repeated for many examples of image patches and then aggregated to produce a final predicted position. Aggregation can be done with non-maximum suppression on prediction voting maps, mean aggregation <ref type="bibr" target="#b42">[43]</ref>, cluster medoid aggregation <ref type="bibr" target="#b44">[45]</ref>, and the use of local appearance with discriminative models to accept or reject predictions <ref type="bibr" target="#b43">[44]</ref>. The pancreas can be localized by regression due to their locations in the body in correlation to other anatomical structures. The objective is to predict bounding boxes (x center , ∆x lower , ∆x upper ) ∈ R 3×3 where x center is the center of the pancreas and x center +∆x lower and x center +∆x upper are the lower and upper corner of the pancreas bounding box respectively. The addition of the extra three parameters follows from the observation that the center of the bounding box is not necessarily the center of the localized object. The pancreas Regression Forest predicts (∆x, ∆x lower , ∆x upper ) for a given image patch I(x). This produces pancreas bounding box candidates of the form (x + ∆x, ∆x lower , ∆x upper ). We additionally use a discriminative model to accept or reject predictions x+∆x as in <ref type="bibr" target="#b43">[44]</ref>. Finally, accepted predictions are aggregated using non-maximum suppression over probability scores and then the bounding boxes are ranked by the count of accepted predictions within the box. The box with the highest count of predictions is kept as the final prediction.</p><p>2) Random Forest on Superpixels: As a form of initialization, we alternatively employ a previously proposed method based on random forest (RF) classification <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> using both hand-crafted and deep CNN derived image features to compute a candidate bounding box regions. We only operate the RF labeling at a low probability threshold of &gt;0.5 which is sufficient to reject the vast amount of non-pancreas from the CT images. This initial candidate generation is sufficient to extract bounding box regions that nearly surround the pancreases completely in all patient cases with ∼ 97% recall. All candidate regions are computed during the testing phase of cross-validation (CV) as in <ref type="bibr" target="#b6">[7]</ref>. As we will see next, candidate generation can be done even more efficiently by using the same HNN architectures, which are based on convolutional neural networks. The technical details of HNNs were described in Sec. II-A.</p><p>3) Multi-view Aggregated HNNs: Alternatively to the candidate region generation process described in Sec. II-B2 that uses hybrid deep and non-deep learning techniques, we employ HNN-I (interior, see Sec. II-A) as a building block for pancreas localization, inspired by the effectiveness of HNN being able to capture the complex pancreas appearance in CT images <ref type="bibr" target="#b27">[28]</ref>. This enables us to drastically discard large negative volumes of the CT scan, while operating HNN-I on a conservative probability threshold of &gt;=0.5 that retains high sensitivity/recall (&gt;99%). The constant balancing weight on β during training HNN-I is critical in this step since the high majority of CT slices have empty pancreas appearance and are indeed included for effective training of HNN-I models, in order to successfully suppress the pancreas probability values from appearing in background. Furthermore, we perform a largest connected-component analysis to remove outlier "blobs" of high probabilities. To get rid of small incorrect connections between high-probability blobs, we first perform an erosion step with radius of 1 voxel, and then select the largest connected-component, and subsequently dilate the region again ( <ref type="figure" target="#fig_2">Fig. 3)</ref>. HNN-I models are trained in axial, coronal, and sagittal planes in order to make use of the multi-view representation of 3D image context. Empirically, we found a max-pooling operation across the 3D models to give the highest sensitivity/recall while still being sufficient to reject the vast amount of non-pancreas from the CT images (see <ref type="table" target="#tab_2">Table II</ref>). One illustrative example is demonstrated in <ref type="figure" target="#fig_3">Fig. 4</ref>. This initial candidate generation is sufficient to extract bounding box regions that completely surround the pancreases with nearly 100% recall. All candidate regions are computed during the testing phase of cross-validation (CV) with the same split as in <ref type="bibr" target="#b6">[7]</ref>. Note that this candidate region proposal is a crucial step for further processing. It removes "easy" nonpancreas tissue from further analysis and allows HNN-I and HNN-B to focus on the more difficult distinction of pancreas versus its surrounding tissue. The fact that we can use exactly the same HNN model architecture for both stages though is noteworthy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pancreas Segmentation</head><p>With pancreas localized, the next step is to produce a reliable segmentation. Our segmentation pipeline consists of three steps. We first use HNN probability maps to generate mid-level boundary and interior cues. These are then used to produce superpixels, which are then aggregated together into a final segmentation using RF classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Combining Mid-level Cues via HNNs:</head><p>We now show that organ segmentation can benefit from multiple mid-level cues, like organ interior and boundary predictions. We investigate deep-learning based approaches to independently learn the pancreas' interior and boundary mid-level cues. Combining both cues via learned spatial aggregation can elevate the overall performance of this semantic segmentation system. Organ boundaries are a major mid-level cue for defining and delineating the anatomy of interest. It could prove to be essential for accurate semantic segmentation of an organ.</p><p>2) Learning Organ-specific Segmentation Proposals: Multiscale combinatorial grouping (MCG) <ref type="bibr" target="#b18">[19]</ref> is one of the state-of-the-art methods for generating segmentation object proposals in computer vision. We utilize this approach, and publicly available code 4 , to generate organ-specific superpixels based on the learned boundary predication maps HNN-B. Superpixels are extracted via continuous oriented watershed transform at three different scales, denoted (Ŷ B2 side ,Ŷ B3 side ,Ŷ B fuse ), supervisedly learned by HNN-B. This allows the computation of a hierarchy of superpixel partitions at each scale, and merges superpixels across scales, thereby efficiently exploring their combinatorial space <ref type="bibr" target="#b18">[19]</ref>. This, then, allows MCG to group the merged superpixels toward object proposals. We find that the first two levels of object MCG proposals are sufficient to achieve ∼ 88% DSC (see <ref type="table" target="#tab_2">Table IV</ref> and <ref type="figure" target="#fig_4">Fig. 5)</ref>, with the optimally computed superpixel labels using their spatial overlapping ratios against the segmentation ground truth map. All merged superpixels S from the first two levels are used for the subsequent spatial aggregation step. Note that HNN-B can only be trained using axial slices where the manual annotation was performed. Pancreas boundary maps in coronal and sagittal views can display strong artifacts. 3) Spatial Aggregation with Random Forest: We use the superpixel set S generated previously to extract features for spatial aggregation via random forest classification <ref type="bibr" target="#b4">5</ref> . Within any superpixel s ∈ S we compute simple statistics including the 1st-4th order moments, and 8 percentiles [20%, 30%, . . . , 90%] on the CT intensities, and a per-pixel element-wise pooling function of multi-view HNN-Is and HNN-B. Additionally, we compute the mean x, y, and z coordinates normalized by the range of the 3D candidate region (Sec. II-B3). This results in 39 features describing each superpixel and are used to train a RF classifier on the training positive or negative superpixels at each round of 4-fold CV. Empirically, we find 50 trees to be sufficient to model our feature set. A final 3D pancreas segmentation is simply obtained by stacking each slice prediction back into the original CT volume space. No further post-processing is employed. This complete pancreas segmentation model is denoted as HNN-RF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data</head><p>Manual tracings of the pancreas for 82 contrast-enhanced abdominal CT volumes are provided by a publicly available dataset 6 <ref type="bibr" target="#b6">[7]</ref>, for the ease of comparison. Our experiments are conducted on random splits of ∼60 patients for training and ∼20 for unseen testing, in 4-fold cross-validation throughout in this section, unless otherwise mentioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation</head><p>We perform extensive quantitative evaluation on different configurations of our method and compare to the previous state-of-the-art work with in-depth analysis. <ref type="bibr" target="#b4">5</ref> Using MATLAB's TreeBagger() class. <ref type="bibr" target="#b5">6</ref>  1) Localization: From our empirical study, the candidate region bounding box generation based on multi-view maxpooled HNN-Is (Sec. II-B3) or previous hybrid methods (Sec. II-B2 <ref type="bibr" target="#b5">[6]</ref>) works comparably in terms of addressing the requirement to produce spatially-truncated 3D regions that maximally cover the pancreas in the pixel-to-pixel level and reject as much as possible the background spaces. An average reduction of absolute volume of 90.36% (range [80.45%-96.26%]) between CT scan and candidate bounding box is achieved during this step, while keeping a mean recall of 99.93%, ranging [94.54%-100.00%] <ref type="table" target="#tab_2">Table I</ref> shows the test performance of pancreas localization and bounding box prediction using regression forests in DSC and average Euclidean distance against the gold standard bounding boxes. As illustrated in <ref type="figure" target="#fig_6">Fig. 7</ref>, regression forest based localization generates 16 out of 82 bounding boxes that lie below 60% in the pixel-to-pixel recall against the ground-truth pancreas masks. Nevertheless we obtain nearly 100% recall for all scans (except for two cases ≥94.54%) through the multi-view max-pooled HNN-Is. An example of detected pancreas can be seen in <ref type="figure" target="#fig_5">Fig. 6</ref>   2) HNN Spatial Aggregation for Pancreas Segmentation: The interior HNN models trained on the axial (AX), coronal (CO) or sagittal (SA) CT images in Sec. II-B3 can be straightforwardly used to generate pancreas segmentation masks. We exploit different spatial aggregation or pooling functions on the AX, CO, and SA viewed HNN-I probability maps, denoted as AX, CO, SA (any single view HNN-I probability map simply used); mean(AX,CO), mean(AX,SA), mean(CO,SA) and mean(AX,CO,SA) (element-wise mean of two or three view HNN-I probability maps); max(AX,CO,SA) (elementwise maximum of three view HNN-I probability maps); and finally meanmax(AX,CO,SA) (element-wise mean of the maximal two scores from three view HNN-I probability maps). After the optimal thresholding calibrated using the training folds on these pooled HNN-I maps, the resulting binary segmentation masks are further refined by 3D connected component process and simple morphological oper-ations (as in Sec. II-B3). <ref type="table" target="#tab_2">Table II</ref> demonstrates the DSC pancreas segmentation accuracy performance by investigating different spatial aggregation functions. We observe that the element-wise multi-view (mean or max) pooling operations on HNN-I probabilities maps generally outperform their single view counterparts. max(AX,CO,SA) performs slightly better than mean(AX,CO,SA). The configuration of meanmax(AX,CO,SA) produces the most superior performance in mean DSC which may behave as a robust fusion function by rejecting the smallest probability value and averaging the remained two HNN-I scores per pixel location. After the   <ref type="table" target="#tab_2">Table III</ref>. We find consistent empirical observations as above when comparing multi-view HNN pooling operations. The meanmax(AX,CO,SA) operation again reports the best mean DSC performance at 81.14% which is increased considerably from 76.79% in <ref type="table" target="#tab_2">Table II</ref>. We denote this system configuration as HNN meanmax . This result validates our two staged pancreas segmentation framework of proposing candidate region generation for organ localization followed by "Zoomed"</p><p>deep HNN models to refine segmentation. <ref type="table" target="#tab_2">Table IV</ref> shows the improvement from the meanmax-pooled HNN-Is (i.e., HNN meanmax ) to the HNN-RF based spatial aggregation, using DSC and average minimum surface-to-surface distance (AVGDIST). The average DSC is increased from 81.14% to 81.27%, However, this improvement is not statistically significantly with p &gt; 0.05 using Wilcoxon signed rank test. In contrast, using dense CRF (DCRF) optimization <ref type="bibr" target="#b24">[25]</ref> (with HNN-I as the unary term and the pairwise term depending on the CT values) as a means of introducing spatial consistency does not improve upon HNN-I noticeably as shown in <ref type="bibr" target="#b27">[28]</ref>).</p><p>Comparing to the performance of previous state-of-the-art methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref> at mean DSC scores of 71.4% and 78.01% respectively, both variants of HNN meanmax and HNN-RF demonstrate superior quantitative segmentation accuracy in DSC and AVGDIST metrics. We have the following two observations. 1, The main performance gain from <ref type="bibr" target="#b27">[28]</ref> (similar to HN N AX in <ref type="table" target="#tab_2">Table III)</ref> is found by the multi-view aggregated HNN pancreas segmentation probability maps (e.g., HNN meanmax ), which also serve in HNN-RF. 2, The new candidate region bounding box generation method (Sec. II-B3) works comparably to the hybrid technique (Sec. II-B2 <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref>) based on our empirical evaluation. However the proposed pancreas localization via multi-view max-pooled HNNs greatly simplified our overall pancreas segmentation system which may also help the generality and reproducibility. The variant of HNN meanmax produces competitive segmentation accuracy but merely involves evaluating two sets of multi-view HNN-Is at two spatial scales: whole CT slices or truncated bounding boxes. There is no need to compute any handcrafted image features <ref type="bibr" target="#b5">[6]</ref> or train other external machine learning classifiers. As shown in <ref type="figure" target="#fig_6">Fig. 7</ref>, the conventional organ localization framework using regression forest <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> does not address well the purpose of candidate region generation for segmentation where extremely high pixel-to-pixel recall is required since it is mainly designed for organ detection. In <ref type="table" target="#tab_6">Table V</ref>, the quantitative pancreas segmentation performance of two method variants, HNN meanmax , HNN-RF spatial aggregation, are evaluated using four metrics of DSC (%), Jaccard Index (%) <ref type="bibr" target="#b46">[47]</ref>, Hausdorff distance (HDRFDST [mm]) <ref type="bibr" target="#b47">[48]</ref> and AVGDIST <ref type="bibr">[mm]</ref>. Note that there is no statistical significance when comparing the performance of two variants in three measures of DSC, JACARD, and AVGDIST, except for HDRFDIST with p &lt; 0.001 under Wilcoxon signed rank test. Since Hausdorff distance represents the maximum deviation between two point sets or surfaces, this observation indicates that HNN-RF may be more robust than HNN meanmax in the worst case scenario. Pancreas segmentation on illustrative patient cases are shown in <ref type="figure" target="#fig_8">Fig. 9</ref>. Furthermore, we applied our trained HNN-I model on a different CT data set 7 with 30 patients, and achieve a mean DSC of 62.26% without any re-training on the new data cases, but if we average the outputs of our 4 HNN-I models from cross-validation, we achieve 65.66% DSC. This demonstrates that HNN-I may be highly generalizable in cross-dataset evaluation. Performance on that dataset will <ref type="bibr" target="#b6">7</ref> 30 training data sets at https://www.synapse.org/#!Synapse:syn3193805/wiki/217789. likely improve with further fine-tuning. Last, we collected an additional dataset of 19 unseen CT scans using the same patient data protocol <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Here, HNN meanmax achieves a mean DSC of 81.2%.  <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref>) is not directly possible due to different datasets utilized. Our holistic segmentation approach with multi-view pooling and spatial aggregation advances the current stateof-the-art quantitative performance to an average DSC of 81.27% in testing. Previous notable results for CT images range from ∼68% to ∼78% <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b48">[49]</ref>, all under the "leave-one-patient-out" (LOO) cross-validation scheme. In particular, DSC drops from 68% (150 patients) to 58% (50 patients) as reported in <ref type="bibr" target="#b2">[3]</ref>. Our methods also perform with the better statistical stability, i.e., comparing 7.3% or 6.27% versus 18.6% <ref type="bibr" target="#b0">[1]</ref>, 15.3% <ref type="bibr" target="#b1">[2]</ref> in the standard deviation of DSC scores. The minimal DSC values are 44.69% with HNN meanmax and 50.69% for HNN-RF whereas <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref> all report patient cases with DSC &lt;10%. Recent work that explores the direct application of 3D convolutional filters with fully convolutional architectures also shows promise <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b33">[34]</ref>. It has to be established whether 2D or 3D implementations are more suited for certain tasks. There is some evidence that deep networks representations with direct 3D input suffer from the curse-ofdimensionality and are more prone to overfitting <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b49">[50]</ref>. Volumetric object detection might require more training data and might suffer from scalability issues. However, proper hyper-parameter tuning of the CNN architecture and enough training data (including data augmentation) might help eliminate these problems. In the mean time, spatial aggregation in multiple 2D views (as proposed here) might be a very efficient (and computationally less expensive) way of diminishing the curse-of-dimensionality. Furthermore, using 2D views has the advantage that networks trained on much larger databases of natural images (e.g. ImageNet, BSDS500) can be used for finetuning to the medical domain. It has been shown that transfer for comparison. Note that the DSC performance remains much more stable after RF aggregation with respect to the probability threshold. The percentage of total cases that lie above a certain DSC with RF are shown (right): 80% of the cases have a DSC of 78.05%, and 90% of the cases have a DSC of 74.22% and higher.  learning is a viable approach when the medical imaging data set size is limited <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b50">[51]</ref>. 3D CNN approaches often adopt padded spatially-local sliding volumes to parse any CT scan, e.g., 96×96×48 <ref type="bibr" target="#b33">[34]</ref>, 160×160×72 <ref type="bibr" target="#b31">[32]</ref> or 80×80×80 <ref type="bibr" target="#b32">[33]</ref>, which may cause the segmentation discontinuity or inconsistency at overlapped window boundaries. Ensemble of several neural networks trained with random configuration variations is found to be advantageous comparing a single CNN model in object recognition <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Our pancreas segmentation method can be indeed considered as ensembles of multiple correlated HNN models but good complementary information gain since they are trained from orthogonal axial, coronal or sagittal CT views. In conclusion, we present a holistic deep CNN approach for pancreas localization and segmentation in abdominal CT scans, exploiting multi-view spatial pooling and combining interior and boundary mid-level cues. The robust fusion of HNN meanmax aggregating on interior holistically-nested networks (HNN-I) alone already achieve good performance at DSC of 81.14%±7.30% in 4-fold CV. The other method variant HNN-RF incorporates the organ boundary responses from the HNN-B model and significantly improves the worst case pancreas segmentation accuracy in Hausdorff distance (p&lt;0.001). The highest reported DSCs of 81.27%±6.27% is achieved, at the computational cost of 2∼3 minutes, not hours as in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Our deep learning based organ segmentation approach could be generalizable to other segmentation problems with large variations and pathologies, e.g., pathological organs and tumors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>P</head><label></label><figDesc>ANCREAS segmentation in computed tomography (CT) challenges current computer-aided diagnosis (CAD) systems. While automatic segmentation of numerous other organs in CT scans, such as the liver, heart or kidneys, achieves good All authors are with the Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Clinical Image Processing Service, Radiology and Imaging Sciences Department, National Institutes of Health Clinical Center, Bethesda, MD 20892-1182, USA. This work was supported by the Intramural Research Program of National Institutes of Health Clinical Center. This version was submitted to IEEE Trans. on Medical Imaging on Dec. 18th, 2016. The content of this article is covered by US Patent Applications of 62/345,606# and 62/450,681#. Contact emails: {le.lu, rms}@nih.gov.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Flowchart of the proposed two-stage pancreas localization and segmentation framework. Sec. II-B2 and Sec. II-B3 are the alternative means of bottom-up organ localization. The remaining modules are for pancreas segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Candidate bounding box region generation pipeline (left to right). Gold standard pancreas in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Candidate bounding box region generation. Gold standard pancreas in red, blobs of ≥ 0.5 probabilities in green, the selected largest 3D connected component in purple, the resulting candidate bounding box in yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Multiscale combinatorial grouping (MCG)<ref type="bibr" target="#b18">[19]</ref> on three different scales of learned boundary predication maps from HNN-B:Ŷ B 2 side , Y B 3side , andŶ B fuse using the original CT image on far left as input (with ground truth delineation of pancreas in red). MCG computes superpixels at each scale and produces a set of merged superpixel-based object proposals. We only visualize the boundary probabilities whose values are greater than .10 (Figure reproduced from<ref type="bibr" target="#b27">[28]</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>http://dx.doi.org/10.7937/K9/TCIA.2016.tNB1kqBU. (a) RF: Axial (d) HNN-I: Axial (b) RF: Sagittal (e) HNN-I: Sagittal (c) RF: Coronal (f) HNN-I: Coronal An example for comparison of regression forest (RF, a-c) and HNN-I (d-f) for pancreas localization. Green and red boxes are ground truth and detected bounding boxes respectively. The green dot denotes the ground truth center. This case demonstrates a case in the 90th percentile in RF localization distance and serves as a representative of poorly performing localization. In contrast, HNN-I includes all of the pancreas with nearly 100% recall in this case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Histogram plots (Y-Axis) of regression forest based bounding boxes (a) and HNN-I's generated bounding boxes (b) in recalls (Xaxis) covering the ground-truth pancreas masks in 3D. Note that Regression Forest produces 16 out of 82 bounding boxes that lie below 60% in pixel-to-pixel recall while HNN-I produces 100% recalls, except for two cases ≥94.54%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Average DSC performance as a function of pancreas probability using HNNmeanmax (left) and spatial aggregation via RF (middle)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Examples of our HNN-RF pancreas segmentation results (green) comparing with the ground-truth annotation (red). The best performing case (a), two cases with DSC scores close to the data set mean (b,c) and the worst case are shown (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Metrics</cell><cell>Mean</cell><cell>Std.</cell><cell>10%</cell><cell>50%</cell><cell>90%</cell><cell>Min</cell><cell>Max</cell></row><row><cell>Distance (mm)</cell><cell>14.9</cell><cell>9.4</cell><cell>6.4</cell><cell>11.7</cell><cell>29.3</cell><cell>2.8</cell><cell>48.7</cell></row><row><cell>Dice</cell><cell>0.71</cell><cell>0.11</cell><cell>0.56</cell><cell>0.74</cell><cell>0.83</cell><cell>0.33</cell><cell>0.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell>: Test performance of pancreas localization and bounding</cell></row><row><cell>box prediction using regression forests in Dice and average Euclidean</cell></row><row><cell>distance against the gold standard bounding boxes, in 4-fold cross</cell></row><row><cell>validation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Four-fold cross-validation: DSC [%] pancreas segmentation performance of various spatial aggregation functions on AX, CO, and SA viewed HNN-I probability maps in the candidate region generation stage (the best results in bold).</figDesc><table><row><cell>DSC</cell><cell>Mean</cell><cell>Std</cell><cell>Min</cell><cell>Max</cell></row><row><cell>AX</cell><cell>73.46</cell><cell>11.63</cell><cell>1.88</cell><cell>85.97</cell></row><row><cell>CO</cell><cell>70.19</cell><cell>9.81</cell><cell>39.72</cell><cell>83.84</cell></row><row><cell>SA</cell><cell>72.42</cell><cell>11.26</cell><cell>14.00</cell><cell>84.92</cell></row><row><cell>mean(AX,CO)</cell><cell>74.65</cell><cell>11.21</cell><cell>5.08</cell><cell>86.87</cell></row><row><cell>mean(AX,SA)</cell><cell>75.08</cell><cell>12.29</cell><cell>2.31</cell><cell>86.97</cell></row><row><cell>mean(CO,SA)</cell><cell>73.70</cell><cell>11.40</cell><cell>18.96</cell><cell>86.64</cell></row><row><cell>mean(AX,CO,SA)</cell><cell>75.07</cell><cell>12.08</cell><cell>4.26</cell><cell>87.19</cell></row><row><cell>max(AX,CO,SA)</cell><cell>75.67</cell><cell>10.32</cell><cell>16.11</cell><cell>87.65</cell></row><row><cell>meanmax(AX,CO,SA)</cell><cell>76.79</cell><cell>11.07</cell><cell>8.97</cell><cell>88.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Four-fold cross-validation: DSC [%] pancreas segmentation performance of various spatial aggregation functions on AX, CO, and SA viewed HNN-I probability maps in the second cascaded stage (the best results in bold).</figDesc><table><row><cell>DSC</cell><cell>Mean</cell><cell>Std</cell><cell>Min</cell><cell>Max</cell></row><row><cell>AX</cell><cell>78.99</cell><cell>7.70</cell><cell>44.25</cell><cell>88.69</cell></row><row><cell>CO</cell><cell>76.16</cell><cell>8.67</cell><cell>45.29</cell><cell>88.11</cell></row><row><cell>SA</cell><cell>76.53</cell><cell>9.35</cell><cell>40.60</cell><cell>88.34</cell></row><row><cell>mean(AX,CO)</cell><cell>79.02</cell><cell>7.96</cell><cell>42.64</cell><cell>88.82</cell></row><row><cell>mean(AX,SA)</cell><cell>79.29</cell><cell>8.21</cell><cell>42.32</cell><cell>89.38</cell></row><row><cell>mean(CO,SA)</cell><cell>77.61</cell><cell>8.92</cell><cell>44.14</cell><cell>89.11</cell></row><row><cell>mean(AX,CO,SA)</cell><cell>80.40</cell><cell>7.30</cell><cell>45.18</cell><cell>89.11</cell></row><row><cell>max(AX,CO,SA)</cell><cell>80.55</cell><cell>6.89</cell><cell>45.66</cell><cell>89.92</cell></row><row><cell>meanmax(AX,CO,SA)</cell><cell>81.14</cell><cell>7.30</cell><cell>44.69</cell><cell>89.98</cell></row><row><cell cols="5">pancreas localization stage, we train a new set of multi-view</cell></row><row><cell cols="5">HNN-Is with the spatially truncated scales and extents. This</cell></row><row><cell cols="5">serves a desirable "Zoom Better to See Clearer" effect for</cell></row><row><cell cols="5">deep neural network segmentation models [46] where cascaded</cell></row><row><cell cols="5">HNN-Is only focus on discriminating or parsing the remained</cell></row><row><cell cols="5">organ candidate regions. Similarly, DSC [%] pancreas seg-</cell></row><row><cell cols="5">mentation accuracy results of various spatial aggregation or</cell></row><row><cell cols="5">pooling functions on AX, CO, and SA viewed HNN-I proba-</cell></row><row><cell cols="5">bility maps (trained in the second cascaded stage) are shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Four-fold cross-validation: The DSC [%] and average surface-to-surface minimum distance (AVGDIST [mm]) performance of<ref type="bibr" target="#b6">[7]</ref>,<ref type="bibr" target="#b27">[28]</ref>, HNNmeanmax, HNN-RF spatial aggregation, and optimally achievable superpixel assignments (italic). Best performing method in bold.</figDesc><table><row><cell>DSC</cell><cell>[7]</cell><cell></cell><cell>[28]</cell><cell>HNNmeanmax</cell><cell>HNN-RF</cell><cell>Opt.</cell></row><row><cell>Mean</cell><cell>71.42</cell><cell cols="2">78.01</cell><cell>81.14</cell><cell>81.27</cell><cell>87.67</cell></row><row><cell>Std</cell><cell>10.11</cell><cell></cell><cell>8.20</cell><cell>7.30</cell><cell>6.27</cell><cell>2.21</cell></row><row><cell>Min</cell><cell>23.99</cell><cell cols="2">34.11</cell><cell>44.69</cell><cell>50.69</cell><cell>81.59</cell></row><row><cell>Max</cell><cell>86.29</cell><cell cols="2">88.65</cell><cell>89.98</cell><cell>88.96</cell><cell>91.71</cell></row><row><cell cols="2">AVGDIST</cell><cell>[7]</cell><cell>[28]</cell><cell>HNNmeanmax</cell><cell>HNN-RF</cell><cell>Opt.</cell></row><row><cell>Mean</cell><cell cols="2">1.53</cell><cell>0.60</cell><cell>0.43</cell><cell>0.42</cell><cell>0.16</cell></row><row><cell>Std</cell><cell cols="2">1.60</cell><cell>0.55</cell><cell>0.32</cell><cell>0.31</cell><cell>0.04</cell></row><row><cell>Min</cell><cell cols="2">0.20</cell><cell>0.15</cell><cell>0.12</cell><cell>0.14</cell><cell>0.10</cell></row><row><cell>Max</cell><cell cols="2">10.32</cell><cell>4.37</cell><cell>1.88</cell><cell>2.26</cell><cell>0.26</cell></row><row><cell></cell><cell cols="5">IV. DISCUSSION &amp; CONCLUSION</cell></row><row><cell cols="7">To the best of our knowledge, our result comprises the</cell></row><row><cell cols="7">highest reported average DSC in testing folds under 4-fold CV</cell></row><row><cell cols="7">evaluation metric. Strict comparison to other methods (except</cell></row><row><cell>for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Four-fold cross-validation: The quantitative pancreas segmentation performance results of our two method variants, HNNmeanmax, HNN-RF spatial aggregation, in four metrics of DSC (%), Jaccard Index (%), Hausdorff distance (HDRFDST [mm]), and AVGDIST [mm]. Best performing methods are shown in bold. Note that there is no statistical significance when comparing the performance by two variants in three measures of DSC, JACARD, and AVGDIST, except for HDRFDIST with p &lt; 0.001 (Wilcoxon Signed Rank Test). This indicates that HNN-RF may be more robust than HNNmeanmax in the worst case scenario.</figDesc><table><row><cell></cell><cell>DSC</cell><cell></cell><cell>Jaccard</cell><cell></cell><cell cols="2">HDRFDST</cell><cell>AVGDIST</cell><cell></cell></row><row><cell></cell><cell>HNNmeanmax</cell><cell>HNN-RF</cell><cell>HNNmeanmax</cell><cell>HNN-RF</cell><cell>HNNmeanmax</cell><cell>HNN-RF</cell><cell>HNNmeanmax</cell><cell>HNN-RF</cell></row><row><cell>Mean</cell><cell>81.14</cell><cell>81.27</cell><cell>68.82</cell><cell>68.87</cell><cell>22.24</cell><cell>17.71</cell><cell>0.43</cell><cell>0.42</cell></row><row><cell>Std</cell><cell>7.30</cell><cell>6.27</cell><cell>9.27</cell><cell>8.12</cell><cell>13.90</cell><cell>10.40</cell><cell>0.32</cell><cell>0.31</cell></row><row><cell>Median</cell><cell>82.98</cell><cell>82.75</cell><cell>70.92</cell><cell>70.57</cell><cell>18.03</cell><cell>14.88</cell><cell>0.32</cell><cell>0.32</cell></row><row><cell>Min</cell><cell>44.69</cell><cell>50.69</cell><cell>28.78</cell><cell>33.95</cell><cell>5.83</cell><cell>5.20</cell><cell>0.12</cell><cell>0.14</cell></row><row><cell>Max</cell><cell>89.98</cell><cell>88.96</cell><cell>79.52</cell><cell>80.12</cell><cell>79.52</cell><cell>69.14</cell><cell>1.88</cell><cell>2.26</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">As discussed in<ref type="bibr" target="#b20">[21]</ref>, LOO can be considered as an extreme case of Mfold cross-validation with M = N when N patient datasets are available for experiments. When M is decreasing and significantly smaller than N , Mfold CV becomes more challenging since there are less data for training and more patient cases on testing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/s9xie/hed.<ref type="bibr" target="#b2">3</ref> We follow the notation of<ref type="bibr" target="#b26">[27]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/jponttuset/mcg.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Geodesic patch-based segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marvao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dawes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="666" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-organ segmentation based on spatially-divided probabilistic atlas from 3D abdominal CT images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kitasaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<editor>MICCAI</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automated abdominal multi-organ segmentation with subject-specific atlas generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1723" to="1730" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative dictionary learning for abdominal multi-organ segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="104" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abdominal multi-organ segmentation from ct images using conditional shape-location and unsupervised intensity priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Linguraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A bottom-up approach for automatic pancreas segmentation in abdominal CT scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI Abdominal Imaging workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<editor>MICCAI</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="556" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lung registration using the niftyreg package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Analysis for the Clinic-A Grand Challenge</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Advanced normalization tools (ants)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Insight J</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A reproducible evaluation of ants similarity metric performance in brain image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2033" to="2044" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiatlas segmentation with joint label fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pluta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Craige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yushkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A probabilistic patch-based label fusion model for multi-atlas segmentation with registration refinement: application to cardiac mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>O&amp;apos;regan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jamil-Copley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1302" to="1315" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Segmentation of neonatal brain mr images using patch-driven level sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evaluation of registration methods on thoracic ct: The empire10 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1901" to="1920" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic model-based segmentation of the heart in ct images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ecabert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schramm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vembar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Subramanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1189" to="1201" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Four-chamber heart modeling and automatic segmentation for 3d cardiac ct volumes using marginal space learning and steerable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scheuering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1668" to="1681" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical, learning-based automatic liver segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suehling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00848</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for computeraided detection: Cnn architectures, dataset characteristics and transfer learnings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nogues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mollura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bodypart recognition using multi-stage deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="449" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
	<note>IEEE ICCV</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatial aggregation of holistically-nested networks for automated pancreas segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A new 2.5D representation for lymph node detection using random sets of deep convolutional neural network observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<editor>MICCAI</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="520" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accurate polyp segmentation for 3d ct colonography using multi-staged probabilistic binary learning and compositional model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">3d deeply supervised network for automatic liver segmentation from ct volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00582</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">MICCAI, arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Voxresnet: Deep voxelwise residual networks for volumetric brain segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05895</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dense volumeto-volume vascular boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Merkow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08401</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">MICCAI, arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Automatic lymph node cluster segmentation using holistically-nested networks and structured optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nogues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">MICCAI</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving computer-aided detection using convolutional neural networks and random view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1170" to="1181" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Retinal vessel segmentation via deep learning network and fully-connected conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5185</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Regression forests for efficient anatomy detection and localization in computed tomography scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1293" to="1303" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rapid multi-organ segmentation using context integration and discriminative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing in Medical Imaging</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="450" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Automatic detection and segmentation of kidneys in 3d ct images using random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cuingnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prevost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lesage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ardon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Zoom better to see clearer: Human and object parsing with hierarchical auto-zoom net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00582</idno>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Distance between sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levandowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Winter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">234</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="35" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Variational analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rockafellar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Regression forest-based atlas localization and direction specific atlas generation for pancreas segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karasawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kitasaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="556" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for medical image analysis: Full training or fine tuning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1299" to="1312" />
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
