<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-shot Natural Language Generation for Task-Oriented Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
							<email>bapeng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
							<email>chezhu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
							<email>nzeng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Few-shot Natural Language Generation for Task-Oriented Dialog</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a crucial component in task-oriented dialog systems, the Natural Language Generation (NLG) module converts a dialog act represented in a semantic form into a response in natural language. The success of traditional template-based or statistical models typically relies on heavily annotated data, which is infeasible for new domains. Therefore, it is pivotal for an NLG system to generalize well with limited labelled data in real applications.</p><p>To this end, we present FEWSHOTWOZ, the first NLG benchmark to simulate the fewshot learning setting in task-oriented dialog systems. Further, we develop the SC-GPT 1 model. It is pre-trained on a large set of annotated NLG corpus to acquire the controllable generation ability, and fine-tuned with only a few domain-specific labels to adapt to new domains. Experiments on FEWSHOTWOZ and the large Multi-Domain-WOZ datasets show that the proposed SC-GPT significantly outperforms existing methods, measured by various automatic metrics and human evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidential TACL submission. DO NOT DISTRIBUTE.</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Task-oriented dialog systems are becoming increasingly popular, as they can assist users in various daily activities such as ticket booking and restaurant reservations. In a typical task-oriented dialog system, the Natural Language Generation (NLG) module plays a crucial role: it converts a system action (e.g., often specified in a semantic form selected by a dialog policy) into a final response in natural language. Hence, the response should be adequate to represent semantic dialog actions, and fluent to engage users' attention. As the ultimate interface to interacts with users, NLG plays a significant impact on the users' experience. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantically-Conditioned Generative Pre-Training</head><p>Existing methods for NLG can be broadly summarized into two major categories. (i) Templatebased methods require domain experts to handcraft templates for each domain, and the system fills in slot-values afterward <ref type="bibr" target="#b4">(Cheyer and Guzzoni, 2014;</ref><ref type="bibr" target="#b11">Langkilde and Knight, 1998)</ref>. Thus, the produced responses are often adequate to contain the required semantic information, but not always fluent and nature, hurting users' experiences. (ii) Statistical language models such as neural networks  learn to generate fluent responses via training from labelled corpus. One canonical model is semantically conditioned LSTM (SC-LSTM) <ref type="bibr" target="#b25">(Wen et al., 2015b)</ref>, which encodes dialog acts with onehot representations and uses it as an extra feature to inform the sentence generation process. Despite its good performance on simple domains, it requires large amounts of domain-specific annotated data which is not available for many domains in realworld applications. Even worse, this renders severe scalability issues when the number of possible combinations of dialog acts grows exponentially with the number of slots in more complex domains.</p><p>We revisit the current research benchmarks for NLG, and notice that each dialog domain is extensively labelled to favor model training. However, this is in contrast to the real-world application scenarios, where only very limited amounts of labelled data are available for new domains. To simulate such a few-shot learning setting, we have developed a new benchmark dataset, called FEWSHOT-WOZ, based on the MultiWOZ <ref type="bibr" target="#b2">(Budzianowski et al., 2018)</ref> and Cambridge NLG datasets <ref type="bibr" target="#b24">(Wen et al., 2016a)</ref>. FEWSHOTWOZ consists of dialog utterances from 7 domains. For each domain, we provide less than 50 labeled utterances for finetuning. We believe that FEWSHOTWOZ can better inspire research to address the challenge of learning data-hungry statistical models with very limited amounts of labelled data in real-world scenarios.</p><p>To deal with the challenge of few-shot learning, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response</head><p>Let me confirm that you are searching for Hinton hotel in the center area (a) The overall framework of a task-oriented dialog system (b) Dialog act &amp; Response <ref type="figure">Figure 1</ref>: Illustration of the NLG module in the overall task-oriented dialog system. (a) The NLG module is highlighted with glowing black bounding boxes. (b) One example of dialog act (including intent and slot-value pairs) and its corresponding natural language response.</p><p>we develop the SC-GPT model. SC-GPT is a multilayer Transformer neural language model, trained in three steps: (i) Pre-trained on plain text, similar to GPT-2 <ref type="bibr">(Radford et al.)</ref>; (ii) Continuously pretrained on large amounts of dialog-act labeled utterances corpora to acquire the ability of controllable generation; (iii) Fine-tuned for a target domain using very limited amounts of domain labels. Unlike GPT-2, SC-GPT generates semantically controlled responses that are conditioned on the given semantic form, similar to SC-LSTM but requiring much less domain labels to generalize to new domains.</p><p>In summary, our key contributions are three-fold:</p><p>• A new benchmark FEWSHOTWOZ is introduced to simulate the few-shot adaptation setting where only a handful of training data from each domain is available.</p><p>• We propose a new model SC-GPT. To our best knowledge, this work is the first study of exploiting state-of-the-art pre-trained language models for NLG in task-oriented dialog systems.</p><p>• On the MultiWOZ dataset, SC-GPT creates a new SOTA, outperforming previous models by 4 points in BLEU. On FEWSHOT-WOZ, SC-GPT outperforms several strong baselines such as SC-LSTM and HDSA , showing that SC-GPT adapts to new domain much more effectively, requiring much smaller amounts of in-domain labels. We release our code 2 and dataset 3 for reproducible research.</p><p>2 https://github.com/pengbaolin/ SC-GPT 3 Project website: https://aka.ms/scgpt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>A typical task-oriented spoken dialog system uses a pipeline architecture, as shown in <ref type="figure">Figure 1</ref> (a), where each dialog turn is processed using a fourstep procedure. (i) Transcriptions of users input are first passed to the natural language understanding (NLU) module, where the users intention and other key information are extracted. (ii) This information is then formatted as the input to dialog state tracking (DST), which maintains the current state of the dialog. (iii) Outputs of DST are passed to the dialog policy module, which produces a dialog act based on the facts or entities retrieved from external resources (such as a database or a knowledge base). (iv) The dialog act emitted by the dialog policy module serves as the input to the NLG, through which a system response in natural language is generated. In this paper, we focus on the NLG component of task-oriented dialog systems, i.e., how to produce natural language responses conditioned on dialog acts. Specifically, dialog act A is defined as the combination of intent I and slot-value pairs</p><formula xml:id="formula_0">{(s i , v i )} P i=1 : A = [ I Intent , (s 1 , v 1 ), · · · , (s P , v P ) Slot-value pairs ]<label>(1)</label></formula><p>where P is the number of pairs 4 , which varies in different dialog acts.</p><p>• Intents are usually used to distinguish different System Response Dialog Act <ref type="bibr">[BOS]</ref> Let me confirm that you are searching for Hinton hotel in the center area <ref type="bibr">[EOS]</ref> Figure 2: Illustration of SC-GPT. In this example, SC-GPT generates a new word token (e.g., "confirm" or "center") by attending the entire dialog act and word tokens on the left within the response.</p><p>• Slot-value pairs indicate the category and content of the information to express in the utterance, respectively.</p><p>The goal of NLG is to translate A into a natural language response x = [x 1 , · · · , x T ], where T is the sequence length. In <ref type="figure">Figure 1</ref> (b), we show an example of the dialog act:</p><p>confirm (name=Hilton, area=center), and the corresponding natural language response is "Let me confirm that you are searching for Hilton in the center area".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Semantically Conditioned GPT</head><p>We tackle this generation problem using conditional neural language models. Given training data of N samples D = {(A n , x n )} N n=1 , our goal is to build a statistical model parameterized by θ to characterize p θ (x|A). To leverage the sequential structure of response, one may further decompose the joint probability of x using the chain rule, casting an auto-regressive generation process as follows:</p><formula xml:id="formula_1">p θ (x|A) = T t=1 p θ (x t |x &lt;t , A)<label>(2)</label></formula><p>where x &lt;t indicates all tokens before t.</p><p>Learning θ is performed via maximizing the loglikelihood (MLE) of the conditional probabilities in (2) over the entire training dataset:</p><formula xml:id="formula_2">L θ (D) = |D| n=1 Tn t=1 log p θ (x t,n |x &lt;t,n , A n ) (3)</formula><p>In this paper, we employ the Transformers <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> to parameterize the conditionals in (2). To enable strong generalization and controllable ability for the learned model, we propose the following three-stage procedure as the training recipe.</p><p>Massive Plain Language Pre-training. Large models trained on massive training corpus usually generalize better to new domains. Inspired by this, we inherit the GPT-2 architecture <ref type="bibr">(Radford et al.)</ref> as the backbone language model. GPT-2 is an auto-regressive language model that leverages 12-24 layers of masked, multi-head self-attention Transformers. GPT-2 is pre-trained on extremely massive text data OpenWebText <ref type="bibr">(Radford et al.)</ref>. It has demonstrated superior performance on characterizing human language data distribution and knowledge transfer. Given text prompts, GPT-2 can often generate realistic sentences.</p><p>Dialog-Act Controlled Pre-training. To enable the guidance of dialog act in response generation, we propose to continuously pre-train the GPT-2 model on large amounts of annotated (dialog act, response) pairs. The pre-training dataset 5 includes annotated training pairs from Schema-Guided Dialog corpus, MultiWOZ corpus, Frame corpus, and Facebook Multilingual Dialog Corpus. The total size of the pre-training corpus is around 400k examples.</p><p>We firstly pre-process dialog act A into a sequence of control codes using the following format:</p><formula xml:id="formula_3">A = [ I ( s 1 = v 1 , · · · s P = v P ) ] (4)</formula><p>Meanwhile, the output sequence x is preprocessed via appending x with a special start token [BOS] and an end token <ref type="bibr">[EOS]</ref>. Finally, the sequentialized dialog act A is concatenated with its augmented response x , and then fed into GPT-2. During training, the prediction loss is only computed for x , and A provides the attended conditions. Since the dialog act represents the semantics of the generated sentences, we follow the naming convention of SC-LSTM, and term our model as Semantically Conditioned Generative Pre-training (SC-GPT). The overall architecture of SC-GPT is illustrated in <ref type="figure">Figure 2</ref>.</p><p>Fine-tuning. For a new domain, a dialog act usually contains novel intents or slot-value pairs, and annotated training samples are often limited. We fine-tune SC-GPT on limited amounts of domainspecific labels for adaptation. The fine-tuning follows the same procedure of dialog-act controlled pre-training, as described above, but uses only a few dozens of domain labels. It is worth noticing that the above recipe has several favorable properties:</p><p>• Flexibility. SC-GPT operates on a sequence of tokens without delexicalization, which means that SC-GPT does not assume a fixed onehot or tree-structured dialog act representation vectors. Hence, it has great flexibility in extending to novel dialog acts.</p><p>• Controllability. In contrast to GPT-2 that generates natural sentences without high-level semantic guidance, SC-GPT can generate sentences with adequate intent and slot-value information and maintain its fluency.</p><p>• Generalizability. SC-GPT is able to generalize significantly better than SC-LSTM, due to the pre-training on massive plain text corpora and annotated dialog datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset: FEWSHOTWOZ</head><p>Revisiting NLG Benchmarks. The three commonly used NLG datasets in developing and evaluating task-oriented dialog systems are E2E NLG <ref type="bibr" target="#b15">(Novikova et al., 2017)</ref> BAGEL <ref type="bibr" target="#b14">(Mairesse et al., 2010)</ref> and RNNLG (Wen et al., 2016a), as summarized in <ref type="table">Table 1</ref>. We observe two issues from their shared statistics: (i) All the datasets contain a large number of labelled training samples for each domain, ranging from hundreds to tens of thousands. However, the cost of labeling is high in practice, e.g., labeling 50 utterances is 5 hours per domain. Creating such an extensively annotated dataset for each new domain is prohibitively expensive. (ii) The percentage of distinct delexicalised dialog acts between training and testing data is quite small. For example, the delexicalised dialog acts in testing is 100% covered by the training set for the E2E NLG dataset. It renders difficulties in evaluating the model's generalization ability for new domains.</p><p>FEWSHOTWOZ. To build a setting for more pragmatic NLG scenarios, we introduce a new dataset FEWSHOTWOZ to better reflect real application complexity, and encourage the community to develop algorithms that are capable of generalizing with only a few domain-specific labels for each (new) domain. The dataset statistics are shown in the last column of <ref type="table">Table 1</ref>. We see that FEW-SHOTWOZ is different from the other datasets in three aspects: (i) More domains. FEWSHOTWOZ contains seven domains in total, which is larger than any existing NLG datasets. (ii) Less training instances. Importantly, FEWSHOTWOZ has a much smaller number of training instances per domain, aiming to evaluate the few-shot learning ability. (iii) Lower training/testing overlap. FEW-SHOTWOZ has only 8.82% overlap, significantly smaller than the other datasets, which amount to more than 90% overlap. The average number of intents per instance in Attraction/ Taxi/ Train domain is 2, 1.33, and 2.05, respectively. In contrast, there is only one intent for each example in the other datasets. The NLG task defined on FEWSHOTWOZ requires the models to learn to generalize over new compositions of intents.  details of FEWSHOTWOZ is shown in <ref type="table" target="#tab_4">Table 2</ref>.</p><p>Collection Protocols. We construct FEWSHOT-WOZ via re-organizing data samples from RNNLG and MultiWOZ datasets <ref type="bibr" target="#b2">(Budzianowski et al., 2018)</ref>. For each domain in RNNLG, we first group utterances according to their delexicalised dialog acts, and keep only one utterance as the target sentence. To ensure diversity, we consider three domains from MultiWOZ: Attraction, Taxi, and</p><p>Train. Since MultiWOZ is a cross-domain dataset, the dialog act of an utterance may exist in multiple domains. We choose to keep utterances whose dialog act appears only in one domain. Similar delexicalising processing is applied to ensure that each dialog act has only one target utterance. Finally, to simulate the few-shot learning in practice, we randomly sample 50 training examples for each domain, except the Taxi domain, which has 40 examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Pre-trained Models. Pre-trained language models (PLMs) have substantially advanced the stateof-the-art across a variety of natural language processing (NLP) tasks <ref type="bibr" target="#b16">(Peters et al., 2018;</ref><ref type="bibr" target="#b6">Devlin et al., 2019;</ref><ref type="bibr" target="#b9">Keskar et al., 2019;</ref><ref type="bibr" target="#b18">Raffel et al., 2019)</ref>. PLMs are often trained to predict words based on their context on massive text data, and the learned models can be fine-tuned to adapt to various downstream tasks. The closest line of research to ours are GPT-2 <ref type="bibr">(Radford et al.)</ref>, <ref type="bibr">CTRL (Keskar et al., 2019)</ref> and Grover <ref type="bibr" target="#b29">(Zellers et al., 2019)</ref>. GPT-2 first investigated missive Transformer-based auto-regressive language models with large-scale text data for pretraining. After fine-tuning, GPT-2 achieves drastic improvements on several generation tasks. One drawback of GPT-2 is the lack of high-level semantic controlling ability in language generation. To alleviate this issue, CTRL (Keskar et al., 2019) was introduced to train the model based on pre-defined codes such as text style, content description, and task-specific behavior, meanwhile Grover <ref type="bibr" target="#b29">(Zellers et al., 2019)</ref> was proposed to generate news articles conditioned on authors, dates etc. Although conceptually similar to our SC-GPT, CTRL and Grover cannot be readily applied to NLG in taskoriented dialog systems, as the conditioning codes are quite different. Another controllable generation work for GPT-2 is PPLM <ref type="bibr" target="#b5">(Dathathri et al., 2019)</ref>, which provides a decoding scheme to guide the generation process using key-words or classifiers, without re-training the model. In this paper, we focus on pre-training an NLG model conditioned on finer-grained semantic dialog acts, which are more desirable for dialog systems.      <ref type="bibr" target="#b27">(Wolf et al., 2019)</ref>. We use GPT2-Medium with 345M parameters 7 as the initial checkpoint, and byte pair encodings <ref type="bibr" target="#b19">(Sennrich et al., 2015)</ref> for the tokenization. Linear rate scheduler with start rate as 5e-5 was used for both pre-training and fine-tuning. Adam (Kingma and Ba, 2014) with weight decay was used to optimize the parameters. For pretraining, the model was trained with a mini-batch of 8 on an 8 Nvidia V100 machine until observing no significant progress on validation loss or up to 20 epochs, whichever is earlier. For fine-tuning on FEWSHOTWOZ, models were trained on each domain separately with 5 epochs.</p><formula xml:id="formula_4">BLEU " ERR # BLEU " ERR # BLEU " ERR # BLEU " ERR # BLEU " ERR # BLEU " ERR # BLEU " ERR # SC-LSTM</formula><formula xml:id="formula_5">BLEU " ERR # BLEU " ERR # BLEU " ERR # BLEU " ERR # BLEU " ERR # BLEU " ERR # BLEU " ERR # SC-LSTM</formula><p>Automatic metrics. Following Wen et al.</p><p>(2015c), BLEU scores and the slot error rate (ERR) are reported. BLEU score evaluates how natural the generated utterance is compared with human readers. ERR measures the exact matching the slot tokens in the candidate utterances. ERR = (p + q)/M , where M is the total number of slots in the dialog act, and p, q is the number of missing and redundant slots in the given realisation. For each dialogue act, we generate 5 utterances and select the top one with the lowest ERR as the final output.</p><p>Human evaluation. We conducted human evaluation using Amazon Mechanical Turk to assess subjective quality. To do this, we recruit master level worker (who has good prior approval rate) to perform a human comparison between generated responses from two systems (which are randomly <ref type="bibr">7</ref> We also experimented using GPT2 with 117M parameters but observed significant poor performance. sampled from comparison systems). The workers are required to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness. Informativeness indicates the extent to which generated utterance contains all the information specified in the dialogue act. Naturalness denotes whether the utterance is as natural as a human does. To reduce the bias in the workers, for each question, we distribute to three different workers. Finally, we collected back approximately 5800 judges.</p><p>Baselines. We compare with three baseline methods.  <ref type="table" target="#tab_7">Table 3</ref> reports the automatic evaluation performance of different methods on FEWSHOTWOZ. SC-LSTM fails to learn the generation effectively in this few-shot learning setting. The generated utterances are poor in quality and suffer from inaccurate slot rendering. In addition, GPT-2 performs consistently better than SC-LSTM in all the domains. It reveals the feasibility of using a pretrained language model for NLG, though only limited annotations are available. Importantly, SC-GPT performs significantly better than GPT and SC-LSTM in terms of both BLEU and ERR. In all the domains, SC-GPT reduces the ERR to a significantly lower level, revealing its strong controllability power. This verifies the importance of pre-training on large annotated dialog data, as SC-GPT learns how to generate utterances specified by  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">FEWSHOTWOZ</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we evaluate the proposed SC-GPT on the FEWSHOTWOZ and MultiWoz datasets to answer two research questions: (i) Is SC-GPT an effective model for strong generalization and controlability in dialog response generation? (ii) Does FEWSHOTWOZ meet the goal of effectively evaluating the generalization of NLG models in the few-shot learning setting?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Implementation details. The model was built upon Huggingface Pytorch Transformer <ref type="bibr" target="#b27">(Wolf et al., 2019)</ref>. We use GPT2-Medium with 345M parameters 8 as the initial checkpoint, and byte pair encodings <ref type="bibr" target="#b19">(Sennrich et al., 2015)</ref> for the tokenization. Linear rate scheduler with start rate as 5e- <ref type="bibr">8</ref> We also experimented using GPT2 with 117M parameters but observed significant poor performance. 5 was used for both pre-training and fine-tuning. Adam (Kingma and Ba, 2014) with weight decay was used to optimize the parameters. For pretraining, the model was trained with a mini-batch of 8 on an 8 Nvidia V100 machine until observing no significant progress on validation loss or up to 20 epochs, whichever is earlier. For fine-tuning on FEWSHOTWOZ, models were trained on each domain separately with 5 epochs.</p><p>Automatic metrics. Following Wen et al. (2015b), BLEU scores and the slot error rate (ERR) are reported. BLEU score evaluates how natural the generated utterance is compared with human readers. ERR measures the exact matching the slot tokens in the candidate utterances. ERR = (p + q)/M , where M is the total number of slots in the dialog act, and p, q is the number of missing and redundant slots in the given realisation. For each dialog act, we generate 5 utterances and select the top one with the lowest ERR as the final output.</p><p>Human evaluation. We conducted human evaluation using Amazon Mechanical Turk to assess subjective quality. We recruit master level workers (who have good prior approval rates) to perform a human comparison between generated responses from two systems (which are randomly sampled from comparison systems). The workers are required to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness. Informativeness indicates the extent to which generated utterance contains all the information specified in the dialog act. Naturalness denotes whether the utterance is as natural as a human does. To reduce  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we evaluate the proposed SC-GPT on the FEWSHOTWOZ and MultiWOZ datasets to answer two research questions: (i) Is SC-GPT an effective model for strong generalization and controllability in dialog response generation? (ii) Does FEWSHOTWOZ meet the goal of effectively evaluating the generalization of NLG models in the few-shot learning setting?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Implementation details. The model was built upon Huggingface Pytorch Transformer <ref type="bibr" target="#b27">(Wolf et al., 2019)</ref>. We use GPT2-Medium with 345M parameters 9 as the initial checkpoint, and byte pair encodings <ref type="bibr" target="#b19">(Sennrich et al., 2015)</ref> for the tokenization. Linear rate scheduler with start rate as 5e-5 was used for both pre-training and fine-tuning. Adam (Kingma and Ba, 2014) with weight decay was used to optimize the parameters. For pretraining, the model was trained with a mini-batch <ref type="bibr">9</ref> We also experimented using GPT2 with 117M parameters but observed significant poor performance. of 8 on an 8 Nvidia V100 machine until observing no significant progress on validation loss or up to 20 epochs, whichever is earlier. For fine-tuning on FEWSHOTWOZ, models were trained on each domain separately with five epochs.</p><p>Automatic metrics. Following <ref type="bibr" target="#b25">Wen et al. (2015b)</ref>, BLEU scores and the slot error rate (ERR) are reported. BLEU score evaluates how natural the generated utterance is compared with human readers. ERR measures the exact matching of the slot tokens in the candidate utterances. ERR = (p + q)/M , where M is the total number of slots in the dialog act, and p, q is the number of missing and redundant slots in the given realisation. For each dialog act, we generate five utterances and select the top one with the lowest ERR as the final output.</p><p>Human evaluation. We conducted the human evaluation using Amazon Mechanical Turk to assess subjective quality. We recruit master level workers (who have good prior approval rates) to perform a human comparison between generated responses from two systems (which are randomly sampled from comparison systems). The workers are required to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness. Informativeness indicates the extent to which generated utterance contains all the information specified in the dialog act. Naturalness denotes whether the utterance is as natural as a human does. To reduce judgement bias, we distribute each question to three different workers. Finally, we collected in total of 5800 judges.</p><p>Baselines. We compare with three baseline methods.        <ref type="table" target="#tab_16">Table 6</ref> shows the human assessment on FEW-SHOTWOZ. The results exhibit the same trend with automatic evaluation. SC-GPT outperforms GPT and SC-LSTM significantly in both metrics, i.e., SC-GPT can better control the generation to convey information in the dialogue act while maintaining good fluency. Note that the gap between SC-GPT and human annotation is still large, indicating that the proposed FEWSHOTWOZ exhibits an under-explored research area, and provides a large space to encourage future research for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">MultiWoz</head><p>The results on MultiWoz are shown in <ref type="table" target="#tab_10">Table 4</ref>. Again, SC-GPT achieves the best performance on BLEU score. We exclude GPT in this table since MultiWoz contains 57k utterances in total; it is large enough for GPT to achieve good performance. The results also confirm that with enough annotated data, conditional language model formulation performs significantly better than HDSA, a strong competitor that leverages graph/tree-structure in-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Informativeness Naturalness   formation to encode dialogue acts.</p><p>To investigate how SC-GPT performs with different training data sizes. We further conduct experiments with varying percentage of training data on MultiWoz, ranging from 0.1% (50 examples) to 50%. As shown in <ref type="table" target="#tab_11">Table 5</ref>, the observations are consistent with FEWSHOTWOZ. For the small data size, GPT outperforms HDSA by a large margin. Further, SC-GPT performs consistently better than HDSA and SC-LSTM. The improvement is more obvious in the fewer data samples setting, which validates our observation on FEWSHOTWOZ that SC-GPT is more effective on controlled generation. <ref type="table" target="#tab_17">Table 7</ref> shows the human assessment results on MultiWoz. The results are consistent with the automatic evaluation. It is interesting to observe that (i) the gap between the state-of-the-art method (i.e., SC-GPT ) and human performance on FEWSHOT-WOZ is much larger than that on MultiWoz; (ii) the human rating on the naturalness of SC-GPT is even higher than humans on MultiWoz, while poses a marginal gap on FEWSHOTWOZ. These results demonstrate that there is an abundant research space to explore with FEWSHOTWOZ, and SG-GPT serves as a simple and strong baseline to evaluate a model's ability to generalize and generate adequate and fluent responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Analysis</head><p>Example dialogue acts and their generated utterance from different methods are shown in <ref type="table" target="#tab_23">Table 8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we have made two major contributions towards developing a more pragmatic NLG module in task-oriented dialogue systems: (i) A new benchmark FEWSHOTWOZ is introduced to simulate the few-shot learning scenarios with  <ref type="table" target="#tab_28">Table 9</ref>: Human evaluation on MultiWoz. Statistical significance was computed with a two-tailed t-test between SC-GPT and HDSA.</p><p>2016b) is used to evaluate the entity coverage accuracy (including all slot values, days, numbers, and reference, etc.). Again, SC-GPT achieves the best performance on BLEU score. Note that GPT-2 performs similarly with SC-GPT on the full Mul-tiWoz dataset, this is because MultiWoz contains 57k utterances, which is large enough for GPT-2 to achieve good performance. The results also confirm that with enough annotated data, conditional language model formulation performs significantly better than HDSA, a strong competitor that leverages graph/tree-structure information to encode dialog acts.</p><p>To study how SC-GPT performs with different  <ref type="table" target="#tab_7">Table 3</ref> reports the automatic evaluation performance of different methods on FEWSHOTWOZ. SC-LSTM fails to learn the generation effectively in this few-shot learning setting. The generated utterances are poor in quality and suffer from inaccurate slot rendering. In addition, GPT-2 performs consistently better than SC-LSTM in all the domains. It reveals the feasibility of using a pretrained language model for NLG, though only limited annotations are available for fine-tuning. Importantly, SC-GPT performs significantly better than GPT and SC-LSTM in terms of both BLEU and ERR. In all the domains, SC-GPT reduces the ERR to a significantly lower level, revealing its strong controllability power. This verifies the importance of pre-training on large annotated dialog data, as SC-GPT learns how to generate utterances specified by the dialog acts accurately. <ref type="table" target="#tab_10">Table 4</ref> shows the human assessment on FEW-SHOTWOZ. The results exhibit the same trend with automatic evaluation. SC-GPT outperforms GPT-2 and SC-LSTM significantly in both metrics, i.e., SC-GPT can better control the generation to convey information in the dialog act while maintaining good fluency. Note that the gap between SC-GPT and human annotation is still large, indicating that the proposed FEWSHOTWOZ exhibits an under-explored research area, and provides a large space to encourage future research for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">FEWSHOTWOZ</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Informativeness Naturalness  <ref type="table" target="#tab_17">Table 7</ref>: Human evaluation on MultiWOZ. Statistical significance was computed with a two-tailed t-test between SC-GPT and HDSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">MultiWOZ</head><p>The results on MultiWOZ are shown in <ref type="table" target="#tab_11">Table 5</ref>. Following <ref type="bibr">), Entity F1 (Wen et al., 2016b</ref> is used to evaluate the entity coverage accuracy (including all slot values, days, numbers, and reference, etc.). Again, SC-GPT achieves the best performance on BLEU score. Note that GPT-2 performs similarly with SC-GPT on the full Multi-WOZ dataset, this is because MultiWOZ contains 57k utterances, which is large enough for GPT-2 to achieve good performance. The results also confirm that with enough annotated data, conditional language model formulation performs significantly better than HDSA, a strong competitor that leverages graph/tree-structure information to encode dialog acts.</p><p>To study how SC-GPT performs with different training data sizes. We further conduct experiments with varying percentages of training data on Mul-tiWOZ, ranging from 0.1% (50 examples) to 50%. As shown in <ref type="table" target="#tab_16">Table 6</ref>, the observations are consistent with FEWSHOTWOZ. SC-GPT performs consistently better than GPT-2, HDSA, and SC-LSTM for a wide range of dataset sizes, and the improvement is more substantial when the fewer numbers of in-domain labels are used for fine-tuning. <ref type="table" target="#tab_17">Table 7</ref> shows the human assessment results on MultiWOZ. The results are consistent with the automatic evaluation. It is interesting to see that (i) the gap between the new state-of-the-art method (i.e., SC-GPT ) and human performance on FEW-SHOTWOZ (as shown in <ref type="table" target="#tab_10">Table 4</ref>) is much larger than that on MultiWOZ; (ii) the human rating on the naturalness of SC-GPT is even higher than humans on MultiWOZ, while there is a visible gap on FEWSHOTWOZ. These results demonstrate that FEWSHOTWOZ presents a challenging few-shot learning setting, SG-GPT serves as a simple and strong baseline in this setting, and the combined   provides a platform for researchers to develop NLG models that are able to generalize to new domains and generate semantically conditioned and fluent responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Analysis</head><p>We perform detailed analysis to investigate SG-GPT's flexibility, controllability and generalizability. The test set is split into two subsets -seen and unseen. If a dialog act of an example appears in the training set, the example is marked as seen; otherwise, it is marked as unseen. <ref type="table" target="#tab_28">Table 9</ref> compares different models on the seen and unseen subsets in the restaurant domain. SC-GPT yields higher BLEU and lower ERR, and the improvement is more significant on the unseen set. For example, SC-GPT reduces ERR to 4.96, an order of magnitude lower than SC-LSTM and only 1/3 of GPT-2. This demonstrates that SC-GPT generalizes well to novel dialog acts, and is able to precisely ground in them to compose fluent responses. This is further confirmed by the quantitative comparison in <ref type="table" target="#tab_23">Table 8</ref>, where we compare the generated utterance examples of different models. While the baseline methods prone to over-generate or miss important slots, SC-GPT can successfully generate fluent natural language utterances that share precise semantic conditions with the ground-truth references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Generated Responses from Different Models</head><p>Original DA ack makereservation ( price = moderate ; restaurantcusine = chinese ; location = center ; peoplecount = 2 ) Reference ok making a reservation for restaurant moderate chinese in center for two people?</p><p>1 Input DA ack makereservation ( price = moderate ; restaurantcusine = japanese ; location = bellevue square ; peoplecount = 2 ) GPT-2 ok making reservation for moderate japanese restaurant in bellevue square for 2 people? SC-GPT ok making a reservation for a moderate japanese cuisine restaurant in bellevue square for 2 people?</p><p>2 Input DA ack makereservation ( restaurantname = sakura ; price = moderate ; restaurantcusine = japanese ; location = bellevue sqare ; peoplecount = 2 ) GPT-2 ok making a reservation for 2 people sakura moderate japanese restaurant located bellevue sqare for 10 minutes?</p><p>SC-GPT ok making a reservation for sakura, moderate japanese cuisine in bellevue square for 2 people?</p><p>3 Input DA ack makereservation ( price = expensive ; restaurantcusine = japanese ; location = center ; peoplecount = 2 ) GPT-2 ok making reservation for japanese cuisine expensive? SC-GPT ok making reservation for expensive japanese cuisine? #</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Generated Responses from Different Models</head><p>Original DA ack makereservation ( childrenfriendly = true ) Reference make reservation for a children friendly restaurant ?</p><p>1 Input DA ack makereservation ( restaurantname = sakura ; childrenfriendly = true ; price = moderate ) GPT-2 make reservation at sakura child friendly restaurant and make table moderate price? SC-GPT make reservation for restaurant sakura moderate price and children friendly restaurant? <ref type="table">Table 10</ref>: Examples of generated utterances with novel dialog acts. SC-GPT produces better utterances than GPT-2 for with edited dialog acts. Since both models produce similar responses to references for the original dialog act, the results are not shown here. (Better viewed in color. insert a slot , substitute a slot value , delete a slot ).</p><p>We further simulate the process when deploying SC-GPT for a new domain, using the examples provided in the RASA dialog toolkit 10 . We first fine-tune SC-GPT using a few training examples (only 16 instances in this new domain), and then generate utterances based on novel dialog acts that are unseen in training data, shown in <ref type="table">Table 10</ref>. In practice, it is desirable for an NLG system to deal with an extending domain whose dialog acts change dynamically. We simulate the setting by editing the original input dialog acts, such as inserting or deleting a slot, or substituting a slot value.</p><p>Since SC-LSTM is infeasible in the setting of an extending domain, we compare SC-GPT with GPT-2. Results show that SC-GPT produces better utterances than GPT-2. SC-GPT can generate reasonably good natural language responses with different combinations of editing operations, showing its high flexibility to generalize to new dialog 7 Conclusion and Future Work In this paper, we have made two major contributions towards developing a more pragmatic NLG module for task-oriented dialog systems: (i) A new benchmark FEWSHOTWOZ is introduced to simulate the few-shot learning scenarios with scarce labelled data in real-world applications. (ii) A new model SC-GPT is proposed to endow the NLG module with strong semantically controlling and generalization ability. Empirical results on both FEWSHOTWOZ and MultiWOZ show that SC-GPT achieves the best overall performance in both automatic and human evaluations.</p><p>There are two interesting directions for future work. The first is to design mechanisms to generate more interpersonal responses which are proven to help improve user experiences <ref type="bibr" target="#b12">(Li et al., 2016;</ref>. The other is to generalize the generative pre-training idea to all four modules in the dialog system pipeline for end-to-end training. Since these four modules process information in order, one may organize their input/output as segments, and pre-train a segment-level autoregressive model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(i) SC-LSTM (Wen et al., 2015c) is a canonical model and a strong baseline that uses an additional dialogue act vector and a reading gate to inform utterance generation. (ii) GPT-2 (Radford et al.) is used to directly fine-tune on the domain-specific corpus, without pre-training on the large-scale corpus of (dialog act, response) pairs. (iii) HDSA (Chen et al., 2019) is a state-of-theart model on MultiWoz. It leverages dialogue act structures to enable knowledge transfer in the multidomain setting, showing superior performance than SC-LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(i) SC-LSTM (Wen et al., 2015b) is a canonical model and a strong baseline that uses an additional dialog act vector and a reading gate to guide the utterance generation. (ii) GPT-2 (Radford et al.) is used to directly fine-tune on the domain-specific labels, without pre-training on the large-scale corpus of (dialog act, response) pairs. (iii) HDSA (Chen et al., 2019) is a state-of-the-art</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>[Chunyuan: Waiting for results to show flexibility and controllability.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2002.12328v1 [cs.CL] 27 Feb 2020</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Task Oriented Dialog System</cell><cell>Dialog Act</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Intent: Confirm</cell></row><row><cell>User Input</cell><cell cols="2">Natural Language Understanding (NLU) 1</cell><cell>Intent &amp; Slot</cell><cell>2</cell><cell>Dialog State Tracking</cell><cell>Slot-value pairs: [ name = Hilton ], [ area = center ]</cell></row><row><cell>System</cell><cell>4</cell><cell>Natural Language</cell><cell>Dialog</cell><cell>3</cell><cell>Dialog Policy</cell></row><row><cell>Response</cell><cell></cell><cell>Generation (NLG)</cell><cell>Act</cell><cell></cell><cell>Learning</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>types of system actions. Typical examples include inform, request, confirm, select etc.</figDesc><table><row><cell>Confirm ( name = Hinton</cell><cell>, area = center</cell><cell>) [BOS] Let me confirm that you are searching for Hinton hotel in</cell><cell>the center area [EOS]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>FEWSHOTWOZ statistics over 7 different domains.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>15.90 48.02 21.98 80.48 31.30 31.54 22.39 64.62 7.76 367.12 6.08 189.88 11.61 61.45 GPT-2 29.48 13.47 27.43 11.26 35.75 11.54 28.47 9.44 16.11 21.10 13.72 19.26 16.27 9.52 SC-GPT 38.08 3.89 32.73 3.39 38.25 2.75 32.95 3.38 20.69 12.72 17.21 7.74 19.70 3.57</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Performance of different methods on FEWSHOTWOZ 6</figDesc><table><row><cell>Model</cell><cell>Restaurant</cell><cell>Laptop</cell><cell>Hotel</cell><cell>TV</cell><cell>Attraction</cell><cell>Train</cell><cell>Taxi</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>15.90 48.02 21.98 80.48 31.30 31.54 22.39 64.62 7.76 367.12 6.08 189.88 11.61 61.45  GPT  29.48 13.47 27.43 11.26 35.75 11.54 28.47 9.44 16.11 21.10 13.72 19.26 16.</figDesc><table><row><cell>SC-GPT</cell><cell>38.08 3.89 32.73 3.39 38.25 2.75 32.95 3.38 20.69 12.72 17.21</cell><cell>7.74</cell><cell>27 9.52 19.70 3.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Performance of different methods on FEWSHOTWOZ</cell></row><row><cell>swer two research questions: (i) Is SC-GPT an</cell></row><row><cell>effective model for strong generalization and con-</cell></row><row><cell>trolability in dialog response generation? (ii) Is</cell></row><row><cell>FEWSHOTWOZ an encouraging new setting to</cell></row><row><cell>evaluate NLG research?</cell></row><row><cell>6.1 Experimental Setup</cell></row><row><cell>Implementation details. The model was built upon Huggingface Pytorch Transformer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Performance of different methods on FEWSHOTWOZ</figDesc><table><row><cell>Model</cell><cell cols="2">Informativeness Naturalness</cell></row><row><cell>SC-LSTM GPT-2 SC-GPT</cell><cell>2.29 2.54 * 2.64 * †</cell><cell>2.13 2.38 * 2.47 * †</cell></row><row><cell>Human</cell><cell>2.92</cell><cell>2.72</cell></row></table><note>* p &lt; 0.005, comparison with SC-LSTM† p &lt; 0.05, comparison with GPT</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Human evaluation on FEWSHOTWOZ. Statistical significance is computed with a two- tailed t-test.mally set up such a research scenario by proposing a new dataset FEWSHOTWOZ, and a new model SC-GPT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 3 :</head><label>3</label><figDesc>Performance of different methods on FEWSHOTWOZ</figDesc><table><row><cell>Model</cell><cell cols="2">Informativeness Naturalness</cell></row><row><cell>SC-LSTM GPT-2 SC-GPT</cell><cell>2.29 2.54 * 2.64 * †</cell><cell>2.13 2.38 * 2.47 * †</cell></row><row><cell>Human</cell><cell>2.92</cell><cell>2.72</cell></row></table><note>* p &lt; 0.005, comparison with SC-LSTM† p &lt; 0.05, comparison with GPT</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Human evaluation on FEWSHOTWOZ.</cell></row><row><cell>Statistical significance is computed with a two-</cell></row><row><cell>tailed t-test.</cell></row><row><cell>labeling and improve the sample efficiency of mod-</cell></row><row><cell>els, This is especially important when deploying</cell></row><row><cell>the models to new domains, where dialog acts need</cell></row><row><cell>to be labelled from scratch. Our paper aims to for-</cell></row><row><cell>mally set up such a research scenario by proposing</cell></row><row><cell>a new dataset FEWSHOTWOZ, and a new model</cell></row><row><cell>SC-GPT.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>h question to</cell><cell>Model</cell><cell>Entity F1</cell><cell>BLEU</cell></row><row><cell>collected in baseline meth-</cell><cell>SC-LSTM (Wen et al., 2015b) HDSA (Chen et al., 2019) GPT-2 SC-GPT</cell><cell>80.42 87.30 87.70 88.37</cell><cell>21.6 26.48 30.71 30.76</cell></row><row><cell>b) is a canon-at uses an ad-</cell><cell cols="3">: Performance on MultiWOZ</cell></row><row><cell>ading gate to</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPT-2 (Rad--tune on the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>raining on the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>sponse) pairs.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>tate-of-the-art</cell><cell></cell><cell></cell><cell></cell></row><row><cell>log act struc-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>omain setting,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>C-LSTM.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ation perfor-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SHOTWOZ.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>on effectively</cell><cell></cell><cell></cell><cell></cell></row><row><cell>he generated</cell><cell></cell><cell></cell><cell></cell></row><row><cell>uffer from in-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>, GPT-2 per-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TM in all the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>f using a pre-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ugh only lim-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>e-tuning. Im-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>cantly better</cell><cell></cell><cell></cell><cell></cell></row><row><cell>f both BLEU</cell><cell></cell><cell></cell><cell></cell></row><row><cell>T reduces the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>revealing its</cell><cell></cell><cell></cell><cell></cell></row><row><cell>erifies the im-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>otated dialog</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ate utterances</cell><cell></cell><cell></cell><cell></cell></row><row><cell>y.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ent on FEW-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>e same trend</cell><cell></cell><cell></cell><cell></cell></row><row><cell>outperforms</cell><cell></cell><cell></cell><cell></cell></row><row><cell>both metrics,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>generation to</cell><cell></cell><cell></cell><cell></cell></row><row><cell>t while main-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>gap between</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ill large, indi-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OZ exhibits</cell><cell></cell><cell></cell><cell></cell></row><row><cell>d provides a</cell><cell></cell><cell></cell><cell></cell></row><row><cell>earch for im-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Table 6. Fol-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 (Wen et al.,</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 6 :</head><label>6</label><figDesc>Performance on MultiWoz</figDesc><table><row><cell>Model</cell><cell></cell><cell>Data size</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1% 0.5%</cell><cell>1%</cell><cell>5%</cell><cell>10%</cell><cell>20%</cell><cell>50%</cell></row><row><cell cols="6">SC-LSTM 9.05 15.15 15.38 18.26 18.97 19.99 21.07 HDSA 9.40 15.32 18.27 22.19 22.89 24.16 25.01 GPT-2 11.96 18.88 20.29 24.18 25.39 26.25 27.40 SC-GPT 12.70 19.65 20.67 24.45 25.67 26.37 27.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 7 :</head><label>7</label><figDesc>BLEU score of different models on Multi-Woz using training data of different sizes.</figDesc><table><row><cell>Model</cell><cell>Error</cell><cell>BLEU</cell></row><row><cell>SC-LSTM (Wen et al., 2015c) HDSA (Chen et al., 2019) SC-GPT</cell><cell></cell><cell>21.6 26.48 30.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 4 :</head><label>4</label><figDesc>Performance on MultiWoz</figDesc><table><row><cell>Model</cell><cell></cell><cell>Data size</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1% 0.5%</cell><cell>1%</cell><cell>5%</cell><cell>10%</cell><cell>20%</cell><cell>50%</cell></row><row><cell cols="6">SC-LSTM 9.05 15.15 15.38 18.26 18.97 19.99 21.07 HDSA 9.40 15.32 18.27 22.19 22.89 24.16 25.01 GPT 11.96 18.88 20.29 24.18 25.39 26.25 27.40 SC-GPT 12.70 19.65 20.67 24.45 25.67 26.37 27.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>BLEU score of different models on Multi- Woz using training data of different sizes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="2">Informativeness Naturalness</cell></row><row><cell>SC-LSTM</cell><cell>2.14</cell><cell>2.33</cell></row><row><cell>HDSA SC-GPT</cell><cell>2.34 2.71 *</cell><cell>2.42 2.69 *</cell></row><row><cell>Human</cell><cell>2.77</cell><cell>2.61</cell></row></table><note>Human evaluation on FEWSHOTWOZ. Statistical significance is computed with a two- tailed t-test.* p &lt; 0.005</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Human evaluation on MultiWoz. Statis-</cell></row><row><cell>tical significance was computed with a two-tailed</cell></row><row><cell>t-test between SC-GPT and HDSA.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 8 :</head><label>8</label><figDesc>BLEU score of different models on Multi-Woz using training data of different sizes.</figDesc><table><row><cell>Model</cell><cell cols="2">Informativeness Naturalness</cell></row><row><cell>SC-LSTM</cell><cell>2.14</cell><cell>2.33</cell></row><row><cell>HDSA SC-GPT</cell><cell>2.34 2.71 *</cell><cell>2.42 2.69 *</cell></row><row><cell>Human</cell><cell>2.77</cell><cell>2.61</cell></row></table><note>* p &lt; 0.005</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: BLEU score of different models on Multi-</cell></row><row><cell>WOZ using training data of different sizes.</cell></row><row><cell>model on MultiWOZ. It leverages dialog act struc-</cell></row><row><cell>tures to enable transfer in the multi-domain setting,</cell></row><row><cell>showing superior performance than SC-LSTM.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head></head><label></label><figDesc>Referencethe satellite proteus 84 is a laptop with a 1 tb drive , 8 gb memory and weighs in at 2.3 kg SC-LSTM the satellite proteus 84 is a laptop with 8 gb , with a 1 tb drive , and is for business computing , and is for business computing % [businesscomputing=true] [weight=2.3kg] hotels that meet your criteria . the trip will last 50 minutes . do you have an area preference or star rating you would like ?</figDesc><table><row><cell>#</cell><cell>Model</cell><cell>Generated Responses from Different Models</cell></row><row><cell>1</cell><cell>Input DA</cell><cell>Laptop{inform(name=satellite proteus 84; type=laptop; memory=8 gb; drive=1 tb; weight=2.3 kg)}</cell></row><row><cell></cell><cell>GPT-2</cell><cell>the satellite proteus 84 laptop has a 1 tb drive , weighs 2.3 kg and is used for business computing</cell><cell>%</cell></row><row><cell></cell><cell></cell><cell>[businesscomputing=true] [memory=8 gb]</cell></row><row><cell></cell><cell>SC-GPT</cell><cell>the satellite proteus 84 is a laptop with 8 gb memory , 1 tb drive , and a weight of 2.3 kg</cell></row><row><cell>2</cell><cell>Input DA</cell><cell cols="2">Restaurant{inform only match(name=marnee thai and thanh long; pricerange=moderate; near=outer sun-set; goodformeal=dinner)}</cell></row><row><cell></cell><cell>Reference</cell><cell cols="2">marnee thai and thanh long are the only 2 moderately priced restaurants near outer sunset that are good for dinner</cell></row><row><cell></cell><cell cols="2">SC-LSTM marnee thai and thanh long is is moderate restaurant restaurant and good good good dinner</cell><cell>%</cell></row><row><cell></cell><cell></cell><cell>[near=outer sunset] [inform only match]</cell></row><row><cell></cell><cell>GPT-2</cell><cell cols="2">there is a moderately priced restaurant called marnee thai and thanh long that is near the outer sunset area</cell></row><row><cell></cell><cell></cell><cell>% [goodformeal=dinner] [inform only match]</cell></row><row><cell></cell><cell>SC-GPT</cell><cell cols="2">marnee thai and thanh long is the only restaurant that serves moderately priced food near outer sunset and good for dinner</cell></row><row><cell>3</cell><cell>Input DA</cell><cell>Train{inform(time=50 minutes)}, hotel{request(stars=?; area=?), inform(choice=5)}</cell></row><row><cell></cell><cell>Reference</cell><cell cols="2">travel time is 50 minutes . i found 5 hotels you may like . do you have a star rating request or an area you prefer</cell></row><row><cell></cell><cell cols="2">SC-LSTM there are 5 trains that are 50 minutes . do you have a preferred departure location ?</cell></row><row><cell></cell><cell></cell><cell cols="2">% [train{inform(choice=5),request(departure=?)}] [hotel{request(stars=?; area=?), inform(choice=5)}]</cell></row><row><cell></cell><cell>HDSA</cell><cell cols="2">there are 5 trains that meet your criteria . do you have a preference on the area or star rating ? %</cell></row><row><cell></cell><cell></cell><cell>[train{inform(choice=5, time=50 minutes)}] [hotel{inform(choice=5)}]</cell></row><row><cell></cell><cell>SC-GPT</cell><cell>there are 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 8 :</head><label>8</label><figDesc>Examples of generated utterances from different models, along with its corresponding dialog acts (DAs) and references. The first two examples are sampled from FEWSHOTWOZ and the last one is from MultiWOZ. Each generated utterance is followed by a brief description explaining the errors (starting with "%"). (Better viewed in color. wrong , redundant , missing information)</figDesc><table><row><cell>Model</cell><cell>Seen</cell><cell>Unseen</cell></row><row><cell cols="3">BLEU ↑ ERR ↓ BLEU ↑ ERR ↓ SC-LSTM 23.05 40.82 12.83 51.98</cell></row><row><cell>GPT-2 SC-GPT</cell><cell cols="2">30.43 3.26 27.92 17.36 40.28 1.09 36.69 4.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 9 :</head><label>9</label><figDesc>Performance of different methods on seen DAs and unseen DAs in restaurant domain.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In some literature, dialog act denotes only the type of system actions, slot-value pairs are defined as meaning representations. Throughout this paper, we follow the usage in<ref type="bibr" target="#b2">Budzianowski et al. (2018)</ref> and use dialog acts to indicate system action and associated slot-value pairs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The domains appearing in fine-tuning are excluded.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/RasaHQ/rasa/tree/master /examples/restaurantbot acts with very limited training data, and produce controllable responses.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Confidential TACL submission. DO NOT DISTRIBUTE</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<title level="m">Towards a human-like open-domain chatbot</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiwoz -a large-scale multi-domain wizard-ofoz dataset for task-oriented dialogue modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paweł</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gašić</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00278</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantically conditioned dialog response generation via hierarchical disentangled self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengda</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3696" to="3709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Method and apparatus for building an intelligent automated assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Cheyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Guzzoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">377</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Plug and play language models: a simple approach to controlled text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02164</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequenceto-sequence generation for spoken dialogue via deep syntax trees and strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurčíček</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-2008</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural approaches to conversational ai. Foundations and Trends R in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="127" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05858</idno>
		<title level="m">Ctrl: A conditional transformer language model for controllable generation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generation that exploits corpus-based statistical knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Langkilde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="704" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A diversitypromoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phrase-based statistical language generation using graphical models and active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurčíček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">Young</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1552" to="1561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09254</idno>
		<title level="m">The e2e dataset: New challenges for end-to-end generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Natural language generation for spoken dialogue system using RNN encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Minh</forename><surname>Van-Khanh Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-1044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="442" to="451" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural-based natural language generation in dialogue using RNN encoder-decoder with semantic aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Minh</forename><surname>Van-Khanh Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tojo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5528</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-4639</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multidomain neural network language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01232</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantically conditioned LSTM-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1199</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04562</idno>
		<title level="m">Stefan Ultes, and Steve Young. 2016b. A network-based end-to-end trainable task-oriented dialogue system</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R&amp;apos;emi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Defending against neural fake news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dialogpt: Large-scale generative pre-training for conversational response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00536</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The design and implementation of xiaoice, an empathetic social chatbot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="1" to="62" />
		</imprint>
	</monogr>
	<note>Just Accepted</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-task learning for natural language generation in task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1123</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1261" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
