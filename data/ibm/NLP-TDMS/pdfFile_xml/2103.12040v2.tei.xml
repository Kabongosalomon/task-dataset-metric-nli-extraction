<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LaneAF: Robust Multi-Lane Detection with Affinity Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hala</forename><surname>Abualsaud</surname></persName>
							<email>habualsa@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Intelligent &amp; Safe Automobiles</orgName>
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Intelligent &amp; Safe Automobiles</orgName>
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lu</surname></persName>
							<email>dblu@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Intelligent &amp; Safe Automobiles</orgName>
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Situ</surname></persName>
							<email>ksitu@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Intelligent &amp; Safe Automobiles</orgName>
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Rangesh</surname></persName>
							<email>arangesh@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Intelligent &amp; Safe Automobiles</orgName>
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
							<email>mtrivedi@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory for Intelligent &amp; Safe Automobiles</orgName>
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LaneAF: Robust Multi-Lane Detection with Affinity Fields</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This study presents an approach to lane detection involving the prediction of binary segmentation masks and perpixel affinity fields. These affinity fields, along with the binary masks, can then be used to cluster lane pixels horizontally and vertically into corresponding lane instances in a post-processing step. This clustering is achieved through a simple row-by-row decoding process with little overhead; such an approach allows LaneAF to detect a variable number of lanes without assuming a fixed or maximum number of lanes. Moreover, this form of clustering is more interpretable in comparison to previous visual clustering approaches, and can be analyzed to identify and correct sources of error. Qualitative and quantitative results obtained on popular lane detection datasets demonstrate the model's ability to detect and cluster lanes effectively and robustly. Our proposed approach performs on par with stateof-the-art approaches on the limited TuSimple benchmark, and sets a new state-of-the-art on the challenging CULane dataset. † authors contributed equally Code: https://github.com/sel118/LaneAF The authors are with the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Lane detection is the process of automatically perceiving the shape and position of marked lanes and is a crucial component of autonomous driving systems, directly influencing the guidance and steering of vehicles while also aiding the interaction between numerous agents on the road. As the number of drivers on the roads has increased, autonomous driving systems have received considerable attention in the automotive and tech industries as well as in academia <ref type="bibr" target="#b1">[1]</ref>. According to the Insurance Institute for Highway Safety (IIHS), in the US alone, car accidents claimed 36,560 lives in 2018, underscoring the importance of any technology that can help prevent crashes.</p><p>Since roads commonly have different types of lane lines (solid white, broken white, solid yellow, etc.), each of which have specific implications with regards to how vehicles may interact with them, automated lane detection systems can also help alert drivers when there are changes in lane topology on the road. Furthermore, there are several factors that make lane detection a challenging task. Firstly, there is a wide variety of road infrastructure in use around the world. Additionally, the lane detection system must be able to identify instances where lanes are ending, merging, and splitting. Finally, the lane detection system must possess the ability to discern worn or unclear lane markings. Precise detection <ref type="figure">Fig. 1</ref>: In our approach, we propose to train a model that outputs binary segmentation masks and affinity fields, which can then be decoded together to produce multiple lane instances. This is opposed to the standard approach to (anchor-free) lane detection that treats each lane as a separate class and trains a model to perform multi-class segmentation.</p><p>of lanes also enable more robust trajectory prediction of surrounding vehicles; as discussed in <ref type="bibr" target="#b2">[2]</ref>, this is critical for successful path planning in autonomous driving. Therefore, while lane detection is a significant and complex task, it is a key factor in developing any autonomous vehicle system. While binary classification is used for the detection of lanes in our approach, a limitation of this type of classification is that it produces a single-channel output, which does not allow for the identification of separate lane entities. To dissociate different lane instances, we propose a novel clustering scheme based on affinity fields (see <ref type="figure">Figure 1</ref>). Affinity fields were originally introduced in <ref type="bibr" target="#b3">[3]</ref> for the purpose of multi-person 2D pose estimation, and are comprised of unit vectors that encode location and orientation. This technique was also used for the detection of hands inside a vehicle, as demonstrated in <ref type="bibr" target="#b4">[4]</ref>. In this paper, we have defined two types of affinity fields, the horizontal affinity field (HAF) and vertical affinity field (VAF). It is these affinity fields that enable unique lane instances to be identified and segmented. Since these affinity fields are present wherever there are foreground lane pixels, they are not bound to a predetermined number of lanes. The model is therefore agnostic to the number of lanes present on the road.</p><p>The main contributions of this paper are as follows: 1) We show that using an off-the-shelf convolutional neural network (CNN) backbone <ref type="bibr" target="#b5">[5]</ref> that intrinsically aggregates and refines multi-scale features can result in superior performance when compared to other bespoke architectures and losses previously proposed for lane detection. 2) We propose affinity fields that are suitable for clustering and associating pixels belonging to amorphous entities like lanes. 3) We detail the procedure and losses to train models that predict binary segmentation masks and affinity fields for the purpose of lane instance segmentation. 4) We introduce efficient methods for generating and decoding such affinity fields into an unknown number of clustered lane instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED RESEARCH</head><p>Lane detection has traditionally been tackled by featurebased approaches which then evolved to model-based approaches to detect lane boundaries. However, these are not practical in real world scenarios since they require ideal road scenes to work effectively. Currently, data-driven approaches are commonly used to detect both lane boundaries as well as lane regions. While several shortcomings of the traditional lane detection methods (i.e. lane segmentation via hand-crafted features) have been resolved with more robust methods in recent years, there is still room for improvement. In more recent times, deep learning and large-scale datasets have provided solutions to many of these issues. However, lane detection in unconstrained environments and complex scenarios remain a challenge.</p><p>Lane detection nowadays is typically modelled as a semantic segmentation problem to extract features using deep learning methods. New approaches tackle lane detection as a multi-class segmentation problem, where each lane forms a separate class. Some of these approaches include: <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>, and <ref type="bibr" target="#b10">[10]</ref>. In <ref type="bibr" target="#b8">[8]</ref>, the authors combine a recurrent neural network (RNN) with a CNN for lane prediction and detection. The use of an embedding loss was introduced in <ref type="bibr" target="#b9">[9]</ref> which uses generative adversarial networks (GANs) to better preserve the structure of lanes and to mitigate the problem of complex post-processing for the output of semantic segmentation; 96% accuracy on the TuSimple dataset was obtained. In <ref type="bibr" target="#b10">[10]</ref>, a sequential prediction network has been used to avoid heuristic-based clustering post-processing. Another network architecture was presented in <ref type="bibr" target="#b11">[11]</ref> with two elements: a deep network which generates weighted pixel coordinates in addition to a differentiable weighted least-squares fitting module. In <ref type="bibr" target="#b12">[12]</ref>, the authors introduced Self Attention Distillation (SAD) loss to avoid models that propagate data sequentially and to decrease inference time. However, the fully connected layer that the SAD model employs is computationally expensive and cannot adapt to any number of lanes.</p><p>Other lane detection approaches choose to first perform binary segmentation of all lanes, followed by a clustering stage to separate each individual lane instance as in <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b15">[14]</ref>, and <ref type="bibr" target="#b16">[15]</ref>. Lane detection is posed as an instance segmentation problem in <ref type="bibr" target="#b13">[13]</ref> so that each lane can be detected in an end-to-end manner, adapting to changing numbers of lanes on the road. In <ref type="bibr" target="#b15">[14]</ref>, a combination of instance segmentation and classification was used as an end-to-end deep learning real-time method to avoid reliance on twostep detection networks. Although recent methods of lane detection show high accuracy when applied to the popular published datasets, some drawbacks of these current methods are that they are not robust when encountering occlusion and that they require a fixed number of lanes in a scene; thus, they cannot work for a random number of lanes present on the road. Acknowledging this problem is <ref type="bibr" target="#b16">[15]</ref>, which uses a key points estimation approach to allow for lane detection of an arbitrary numbers of lanes regardless of orientation.</p><p>More recently, some approaches have modelled lane detection as an anchor-based object detection problem such as <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b19">[18]</ref>, and <ref type="bibr" target="#b20">[19]</ref>. One of the approaches that aimed to enhance lane estimation was to use contextual cues to improve computational efficiency and accuracy of lane detection <ref type="bibr" target="#b20">[19]</ref>. The authors in <ref type="bibr" target="#b21">[20]</ref> introduced a spatial CNN with learned spatial kernels. This method has improves performance over conventional CNN methods since it provides spatial information by computing slice by slice convolution in feature maps, enabling information to be transferred between pixels within each layer. In <ref type="bibr" target="#b19">[18]</ref>, a spatio-temporal deep learning method was proposed to mitigate the errors that can occur when experiencing harsh weather or other complex problems in the road, jeopardizing the accuracy of detecting a lane in the scene. Meanwhile, in <ref type="bibr" target="#b17">[16]</ref>, lane markers were tracked temporally. Additionally, <ref type="bibr" target="#b22">[21]</ref> presents an anchor-based single-stage deep lane detection model using anchors for feature pooling. In <ref type="bibr" target="#b18">[17]</ref>, the authors developed 3D-LaneNet, a network that predicts the 3D layout of lanes using a single image. A combination of LiDAR and camera sensors were used in <ref type="bibr" target="#b23">[22]</ref> for their network to obtain accurate lane detection in 3D space directly.</p><p>The methods in this paper were inspired by <ref type="bibr" target="#b3">[3]</ref>, which presented an approach to 2D pose estimation of multiple people in an image through Part Affinity Fields (PAFs). The technique introduced takes an input image, passes it to a two-branch CNN, obtains the confidence maps to detect body parts, utilizes PAFs for parts association, parses using a greedy algorithm, and finally assembles them into the final image with estimated poses. The main takeaway from <ref type="bibr" target="#b3">[3]</ref> is that parsing based on PAFs adds robustness with regards to part detection and association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>Our proposed methodology involves a feed-forward CNN that is trained to predict binary lane segmentation masks and per-pixel affinity fields. More specifically, the model is trained to predict two affinity fields, which we call the horizontal affinity field (HAF) and vertical affinity field (VAF), respectively. Affinity fields can be thought of as vector fields that map any 2D location on the image plane to a unit vector in 2D. A unit vector in the VAF encodes the direction in which the next set of lane pixels above it is located. On the other hand, a unit vector in the HAF points toward the center of the lane in the current row -thereby allowing us to cluster lanes of arbitrary widths. These two affinity fields in conjunction with the predicted binary segmentation can then be used to cluster foreground pixels into lanes as a postprocessing step. In the next few subsections, we discuss each individual block in our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Backbone</head><p>Recent lane detection approaches have made use of a variety of backbone architectures; but most popular among them are usually the ResNet family of architectures <ref type="bibr" target="#b25">[23]</ref>, ENet <ref type="bibr" target="#b6">[6]</ref>, and ERFNet <ref type="bibr" target="#b26">[24]</ref>. Although these architectures have proven benefits across a variety of tasks, we believe that more recent developments in the field can be leveraged for the purpose of lane detection. To this end, we make use of the DLA-34 backbone presented in <ref type="bibr" target="#b5">[5]</ref>.</p><p>The DLA family of models make use of deep layer aggregation, which unifies semantic and spatial fusion for better localization and semantic interpretation. In particular, this architecture extends densely connected networks <ref type="bibr" target="#b27">[25]</ref> and feature pyramid networks with hierarchical and iterative skip connections that deepen the representation and refine resolution. They employ two forms of aggregation: iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA). IDA focuses on fusing resolutions and scales while HDA focuses on merging features from all modules and channels. These architectures also incorporate deformable convolution operations <ref type="bibr" target="#b28">[26]</ref> that can adapt the spatial sampling grid for convolutions based on their inputs. We believe these are desirable properties for the tasks of lane detection and instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Affinity Fields</head><p>In addition to binary lane segmentation masks, our model is trained to predict horizontal and vertical affinity fields (HAFs and VAFs respectively). For any given image, the HAF and VAF can be thought of as vector fields #» H(·, ·) and #» V (·, ·), that assign a unit vector to each (x, y) location in the image. As we alluded to earlier, the HAF enables us to cluster lane pixels horizontally and the VAF vertically. With the predicted affinity fields and binary mask, clustering lane pixels is achieved through a simple row-by-row decoding process from bottom to top. The rest of this subsection provides details on how to create such affinity fields using the ground truth and how to use the predicted affinity fields to decode individual lanes. Creating HAFs and VAFs: Affinity fields are created using ground truth segmentation masks on the fly as detailed in Algorithm 1. This proceeds row-by-row, from bottom to top.</p><p>For any row y in the image, the HAF vectors are computed for each lane point (x l i , y) using the ground truth vector field mapping #» H g t(·, ·) as follows:</p><formula xml:id="formula_0">#» H gt (x l i , y) = x l y − x l i |x l y − x l i | , y − y |y − y| = x l y − x l i |x l y − x l i | , 0 ,<label>(1)</label></formula><p>where x l y is the mean x-coordinate of all points belonging to lane l in row y. This process is illustrated in <ref type="figure" target="#fig_0">Figure 2a</ref>, where pixels in green and blue represent points belonging to lanes l and l + 1 respectively. Similarly, the VAF vectors are computed for each lane point (x l i , y) in row y using the ground truth vector field mapping #» V g t(·, ·) as follows:</p><formula xml:id="formula_1">#» V gt (x l i , y) = x l y−1 − x l i |x l y−1 − x l i | , y − 1 − y |y − 1 − y| = x l y−1 − x l i |x l y−1 − x l i | , −1 ,<label>(2)</label></formula><p>where x l y−1 is the mean x-coordinate of all points belonging to lane l in row y −1. This process is illustrated in <ref type="figure" target="#fig_0">Figure 2c</ref>, where pixels in green represent points belonging to lanes l. Note that unlike the HAF, unit vectors in the VAF point to the mean location of the lane in the previous row. Decoding HAFs and VAFs: After a model is trained to predict the HAFs and VAFs detailed above, a decoding procedure is carried out to cluster foreground pixels into lanes during testing. This procedure is presented in Algorithm 2, and similarly operates row-by-row, from bottom to top.</p><p>Assuming #» H pred (·, ·) is the vector field corresponding to the predicted HAF, foreground pixels in a row y − 1 are first assigned to clusters based on the following rule:</p><formula xml:id="formula_2">c * haf (x f g i , y −1) =      C k+1 if #» H pred (x f g i−1 , y − 1) 0 ≤ 0 ∧ #» H pred (x f g i , y − 1) 0 &gt; 0, C k otherwise,<label>(3)</label></formula><p>where c * haf (x f g i , y − 1) denotes the optimal cluster assignment for a foreground pixel (x f g i , y − 1); C k and C k+1 denote two different clusters indexed by k and k + 1 respectively. This assignment is illustrated in <ref type="figure" target="#fig_0">Figure 2b</ref>, where pixels in red are assigned the same cluster.</p><p>Next, these horizontal clusters are assigned to existing lanes indexed by l using the vector field #» V pred (·, ·) corresponding to the VAF as follows:</p><formula xml:id="formula_3">c * vaf (l) = arg min C k d C k (l),<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">d * (l) = min C k d C k (l).<label>(5)</label></formula><p>Here d C k (l) denotes the error of associating cluster C k to an existing lane l:</p><formula xml:id="formula_5">d C k (l) = 1 |C k | |C k |−1 i=0 (x C k , y − 1) − (x l i , y) − #» V pred (x l i , y) · ||(x C k , y − 1) − (x l i , y) || .<label>(6)</label></formula><p>We illustrate this process in <ref type="figure" target="#fig_0">Figure 2d</ref>, where the cluster in red is assigned to the existing lane in green. By repeating the above steps row-by-row starting from the bottom and working to the top, we are able to assign every foreground pixel to their respective lanes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Losses</head><p>To train the proposed model, we use a separate loss at each prediction head. For our binary segmentation branch, we used weighted binary cross-entropy loss, a standard loss for imbalanced binary segmentation tasks. The raw logits produced by the model are first passed through a sigmoid activation for normalization. The loss is then calculated as:</p><formula xml:id="formula_6">L BCE = − 1 N i w·t i ·log(o i )+(1−t i )·log(1−o i ) ,<label>(7)</label></formula><p>where t i is the target value for the pixel i and o i is the sigmoid output. Since this is an unbalanced segmentation task, a weight w = 9.6 was used to increase penalization for foreground pixels. To further account for the imbalanced dataset, an additional intersection over union loss was used for the segmentation branch:</p><formula xml:id="formula_7">L IoU = 1 N i 1 − t i · o i t i + o i − t i · o i .<label>(8)</label></formula><p>For the affinity field branches of the model, a simple L1 regression loss was applied only to the foreground locations </p><formula xml:id="formula_8">L AF = 1 N f g i |t haf i − o haf i | + |t vaf i − o vaf i | . (9)</formula><p>The total loss applied to the model is a simple summation of the individual losses:</p><formula xml:id="formula_9">L total = L BCE + L IoU + L AF .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>To train and benchmark our proposed approach, we make use of the popular TuSimple and CULane <ref type="bibr" target="#b21">[20]</ref> datasets. TuSimple contains 3,626 annotated training video clips and 2,782 clips for testing. It features good and fair weather conditions in various daytime lighting and traffic conditions, employing highways with up to five lanes. Meanwhile, CU-Lane contains significantly more data with 88,880 annotated training video clips and 34,680 clips for testing. This dataset also divides test images into nine categories and contains more complex scenarios, including images with challenging lighting conditions. A summary of both datasets is compiled in <ref type="table" target="#tab_2">Table I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Our backbone architecture (DLA-34) is a fully convolutional network that does not retain the original resolution, but rather downsizes the outputs by a factor of 4; thus, we re-scaled the input images to 640 × 352 dynamically during run-time and also reshaped the ground truth affinity fields and segmentation masks to 160 × 88, which is one quarter of the input size. This has the added benefit of making our decoding process faster since we now process only one quarter of the original rows. We also make use of random rotations, crops, scales and horizontal flips during training.</p><p>We use ImageNet pre-trained weights for the DLA-34 backbone and normalize the inputs accordingly. Finally, the Adam optimizer is used as our solver in all experiments. Other training parameters are listed below:</p><formula xml:id="formula_10">• Epochs = 60 • Batch size = 2 • Learning rate = 1e − 4 • Weight decay = 1e − 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Metrics</head><p>We use the same evaluation metrics used in past literature to make a representative comparison between our approach and prior work. This consists of the official metric of the TuSimple dataset (accuracy), the false positive (FP) rate, and the false negative (FN) rate. The TuSimple accuracy is calculated as:</p><formula xml:id="formula_11">Accuracy = N pred N gt<label>(11)</label></formula><p>where N pred is the number of lane points that have been correctly predicted and N gt is the number of ground-truth lane points. Additionally, we report the F1 measure, which is based on the intersection over union (IoU) and is the only metric for CULane. This is calculated as in <ref type="bibr" target="#b21">[20]</ref>:</p><formula xml:id="formula_12">F 1 = 2 · precision · recall precision + recall<label>(12)</label></formula><p>where precision is defined as T P T P +F P , recall is defined as T P T P +F N , T P is the number of lane points that have been correctly predicted, F P is the number of false positives, and F N is the number of false negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Experiments</head><p>We conduct an ablation study to validate all our design choices. All ablation experiments were conducted on the TuSimple validation set and can be seen in <ref type="table" target="#tab_2">Table II</ref>. The first row contains the results of the standard LaneAF model, which we denote as the baseline model B. First, we train variants without the IoU loss (B w/o IoU) and the weighted binary cross-entropy loss (B w/o wBCE). Removing these losses decreased accuracy quite drastically while increasing the false positive and false negative rate. In fact, without the weighted binary cross-entropy loss, the F1 score in particular With regards to the down-sampling factor of the outputs, it is clear that the baseline model's factor of 4 achieved the best results; decreasing it to 2 (B (DS-2)) increased runtime and worsened accuracy and F1 slightly, while increasing it to 8 (B (DS-8)) had the most damaging effect on accuracy out of all modifications. We also trained a variant with 128 channels in the output head (B (HC-128)) compared to the original 256, and while this change had the smallest impact, it is evident that the baseline's 256 channels yields superior results. Finally, to validate the benefits of our clustering approach over standard multi-class segmentation, we trained a DLA-34 model to directly perform multi-class segmentation of all lanes (DLA-34 multi-class). This model obtained the worst F1 and accuracy scores out of all variants. The result clearly illustrates the effectiveness of using binary segmentation and affinity field-based clustering as in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results</head><p>Performance results from LaneAF on the TuSimple benchmark are shown in <ref type="table" target="#tab_2">Table III</ref>. It can be seen that our false positive rate sets a new standard (0.0298) among the current state-of-the-art. This demonstrates that our model does not incorrectly detect a lane pixel as often as other networks and that LaneAF's multi-branch approach leads to confident lane pixel predictions. While we obtain superior accuracy to other backbone architectures such as ResNet-18 and -34 <ref type="bibr" target="#b25">[23]</ref>, our approach falls slightly short of current state-of-the-art models such as PINet <ref type="bibr" target="#b16">[15]</ref>, ENet-SAD <ref type="bibr" target="#b12">[12]</ref>, and SCNN <ref type="bibr" target="#b21">[20]</ref>. However, our false negative rate is only marginally higher, signifying that the incorrectly classified lane pixels are most likely at the very ends of the lanes.</p><p>Table IV displays the state-of-the-art results of our model on the CULane benchmark. With this significantly larger and more complex dataset, we can see that LaneAF's performance improves greatly with respect to other models and demonstrates our network's ability to generalize. LaneAF  <ref type="figure" target="#fig_1">Figures 3a and 3b</ref>. The clustered outputs shown here were created using the affinity field decoder, outlined in Algorithm 2. In <ref type="figure" target="#fig_1">Figure 3a</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUDING REMARKS</head><p>In this paper, we proposed a novel approach to lane detection and instance segmentation through the use of binary segmentation masks and per-pixel affinity fields. The horizontal and vertical affinity fields, along with the predicted binary masks were demonstrated to successfully cluster lane pixels into unique lane instances in a post-processing step. This is accomplished using a simple row-by-row decoding process with little overhead, and enables LaneAF to detect a variable number of lanes of arbitrary width without assuming a fixed or maximum number of lanes. This form of clustering is also more interpretable in comparison to previous visual approaches since it can be analyzed to easily identify and correct sources of error. The ablation study conducted also validated the effectiveness of the approach over standard multi-class segmentation. Our proposed method achieves the lowest reported false positive rate (0.0298) on the TuSimple benchmark; on the larger and more comprehensive CULane dataset, LaneAF sets a new state-of-the-art result with a total F1 score of 77.25%, surpassing even much deeper and more complex models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>(a) HAF creation during training (b) HAF decoding during testing (c) VAF creation during training (d) VAF decoding during testing Illustrations of HAF and VAF creation and decoding processes during training and testing, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>(a) Predicted horizontal affinity field (HAF) (b) Predicted vertical affinity field (VAF) Example outputs produced by LaneAF with color coded affinity fields; each color represents a unique lane instance based on affinity field decoding. Of note is the successful discrimination of lane instances even as the lanes converge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, the HAF points towards the center of the lane line for each row of the output image. This is based on the valid lane pixels found in the binary segmentation output and represents the locations of potential lane instances with respect to all detected lane pixels. Lane clusters are still successfully separated despite their respective HAF being adjacent for numerous rows, demonstrated in yellow box of Figure 3a. Likewise, in Figure 3b, the VAF points along the lane towards the mean location of the next row's lane pixels, which is based on the HAF outputs. This is visualized in the yellow box of Figure 3b, where for each unique lane instance, the unit vector points towards the next row's mean lane pixel location. For both Figures 3a and 3b, the blue boxes clearly display how the HAF and VAF are implemented on a single detected lane instance. In Figure 4, additional qualitative results are shown with samples from the TuSimple dataset in row 1 and the CULane dataset in rows 2-4. The TuSimple examples demonstrate LaneAF's high performance on curved highways and on lanes that are merging and splitting due to entrances and exits, highlighting our model's flexibility to the number of lanes present on a given road. Also notable in the middle image of the first row is the false detection of a lane line due to an airplane contrail. The displayed results from CULane include challenging scenarios that further illustrate LaneAF's robustness on curved roads and in very poor lighting conditions. This diverse set of examples exhibit characteristics of the Dazzle, Shadow, Curve, and Night categories of the CULane dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>LaneAF qualitative results on TuSimple (row 1) and CULane (rows 2-4). The TuSimple examples demonstrate the model's high performance on curved highways and on merging and splitting lanes. The challenging CULane scenarios further illustrate LaneAF's robustness on curved roads and in a variety of poor lighting conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1</head><label>1</label><figDesc>Creating affinity fields from ground truth data Inputs: SEG(H × W ): ground truth segmentation lmax: maximum number of lanes HAF, V AF ← zeros(H, W, 2) initialize affinity fields for l ← 1 to L do go through each lane prev cols ← nonzero(SEG[H, :] == l) initialize /* row-by-row, from bottom to top */ for y ← H − 1 to 1 do cols ← f ind(SEG[row, :] == l)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>find lane pixels</cell></row><row><cell cols="3">/* horizontal affinity field */</cell></row><row><cell>for x in cols do HAF [y, x] ←</cell><cell cols="2">#» Hgt(x, y)</cell><cell>Eq. 1</cell></row><row><cell>end for</cell><cell></cell><cell></cell></row><row><cell cols="3">/* vertical affinity field */</cell></row><row><cell cols="2">for x in prev cols do HAF [y + 1, x] ←</cell><cell>#» V gt(x, y + 1)</cell><cell>Eq. 2</cell></row><row><cell>end for</cell><cell></cell><cell></cell></row><row><cell>prev cols ← cols</cell><cell></cell><cell></cell></row><row><cell>end for</cell><cell></cell><cell></cell></row><row><cell>end for</cell><cell></cell><cell></cell></row><row><cell>return HAF, V AF</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 2</head><label>2</label><figDesc>Decoding predicted affinity fields into lanes Inputs: BW (H × W ): binary segmentation mask HAF (H × W × 2): horizontal affinity field V AF (H × W × 2): vertical affinity field τ : clustering threshold SEG ← zeros(H, W ) initialize segmentation output lane end points ← [] keeps track of the latest points added to each lane L ← 0 initialize number of lanes to 0 /* row-by-row, from bottom to top */ for y ← H to 1 do cols ← f ind(BW [row, :] &gt; 0)</figDesc><table><row><cell></cell><cell cols="2">find foreground pixels</cell></row><row><cell>/* cluster horizontally */</cell><cell></cell></row><row><cell>clusters ← []</cell><cell></cell></row><row><cell>for x in cols do</cell><cell></cell></row><row><cell>clusters.update(c  *  haf (x, y))</cell><cell></cell><cell>Eq. 3</cell></row><row><cell>end for</cell><cell></cell></row><row><cell cols="2">/* assign clusters to existing lanes */</cell></row><row><cell>for l ← 1 to L do</cell><cell></cell></row><row><cell cols="2">if d  vaf (l)</cell><cell>Eq. 4,Eq. 6</cell></row><row><cell cols="3">update latest points added to lane</cell></row><row><cell>for x in c  *  vaf (l) do</cell><cell></cell></row><row><cell>SEG[y, x] ← l</cell><cell cols="2">assign cluster to lane l</cell></row><row><cell>end for</cell><cell></cell></row><row><cell>end if</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell>/end if</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell>return SEG</cell><cell></cell></row></table><note>* (l) &lt;= τ then error less than threshold (Eq. 5) lane end points[l] ← c** spawn new lanes with unassigned clusters */ for cluster in clusters do if cluster is not assigned then L ← L + 1 lane end points[L] ← cluster</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Attributes of Lane Detection Datasets</figDesc><table><row><cell>Dataset</cell><cell>TuSimple</cell><cell>CULane</cell></row><row><cell># Frames</cell><cell>6,408</cell><cell>133,325</cell></row><row><cell>Train</cell><cell>3,268</cell><cell>88,880</cell></row><row><cell>Validation</cell><cell>358</cell><cell>9,675</cell></row><row><cell>Test</cell><cell>2,782</cell><cell>34,680</cell></row><row><cell>Resolution</cell><cell>1280 × 720</cell><cell>1640 × 590</cell></row><row><cell>Road Type</cell><cell>highway</cell><cell>urban, rural, highway</cell></row><row><cell cols="3">of both the vertical and horizontal affinity fields:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>LaneAF Ablation Experiments on TuSimple for Validating Model Design</figDesc><table><row><cell>Model type</cell><cell>F1 (%)</cell><cell>Accuracy (%)</cell><cell>FP</cell><cell>FN</cell></row><row><cell>B 1</cell><cell>95.31</cell><cell>94.62</cell><cell>0.0435</cell><cell>0.0500</cell></row><row><cell>B w/o IoU 2</cell><cell>95.17</cell><cell>94.31</cell><cell cols="2">0.0456 0.0507</cell></row><row><cell>B w/o wBCE 3</cell><cell>93.75</cell><cell>94.19</cell><cell>0.0598</cell><cell>0.0649</cell></row><row><cell>B w/o RT 4</cell><cell>93.56</cell><cell>94.29</cell><cell cols="2">0.0614 0.0670</cell></row><row><cell>B (DS-2) 5</cell><cell>94.76</cell><cell>94.22</cell><cell>0.0484</cell><cell>0.0559</cell></row><row><cell>B (DS-8)</cell><cell>93.94</cell><cell>92.73</cell><cell cols="2">0.0549 0.0656</cell></row><row><cell>B (HC-128) 6</cell><cell>94.80</cell><cell>94.56</cell><cell>0.0503</cell><cell>0.0535</cell></row><row><cell cols="2">DLA-34 multi-class 7 88.86</cell><cell>92.59</cell><cell cols="2">0.1115 0.1114</cell></row><row><cell cols="5">1 B: baseline model with down-sampling factor 4 and 256 channels</cell></row><row><cell cols="5">in output head 2 IoU: IoU loss 3 wBCE: weighted BCE loss</cell></row><row><cell cols="3">4 RT: random transformations during training</cell><cell></cell><cell></cell></row><row><cell cols="3">5 DS-x: down-sampling factor for outputs</cell><cell></cell><cell></cell></row><row><cell cols="3">6 HC-x: number of channels in output head</cell><cell></cell><cell></cell></row><row><cell cols="5">7 DLA-34 multi-class: DLA-34 model trained for lane detection via</cell></row><row><cell cols="2">multi-class segmentation</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">dropped significantly. The same is observed for the baseline</cell></row><row><cell cols="5">model without random transformations during training (B</cell></row><row><cell cols="3">w/o RT), as depicted in the fourth row.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>LaneAF Results on the TuSimple Benchmark</figDesc><table><row><cell>Method</cell><cell>F1 (%)</cell><cell>Acc (%)</cell><cell>FP</cell><cell>FN</cell><cell>MACs (G)</cell></row><row><cell>SCNN [20]</cell><cell>96.53</cell><cell>96.53</cell><cell>0.0617</cell><cell>0.0180</cell><cell>-</cell></row><row><cell>ENet-SAD [12]</cell><cell>95.92</cell><cell>96.64</cell><cell cols="2">0.0602 0.0205</cell><cell>-</cell></row><row><cell>ResNet-18 [23]</cell><cell>87.87</cell><cell>92.69</cell><cell>0.0948</cell><cell>0.0822</cell><cell>-</cell></row><row><cell>ResNet-34 [23]</cell><cell>96.77</cell><cell>92.84</cell><cell cols="2">0.0918 0.0796</cell><cell>-</cell></row><row><cell>PINet [15]</cell><cell>97.20</cell><cell>96.75</cell><cell>0.0310</cell><cell>0.0250</cell><cell>-</cell></row><row><cell>LaneNet [13]</cell><cell>94.80</cell><cell>96.38</cell><cell cols="2">0.0780 0.0244</cell><cell>-</cell></row><row><cell>Cascaded-CNN [14]</cell><cell>90.82</cell><cell>95.24</cell><cell>0.1197</cell><cell>0.0620</cell><cell>-</cell></row><row><cell>PolyLaneNet [27]</cell><cell>90.62</cell><cell>93.36</cell><cell cols="2">0.0942 0.0933</cell><cell>1.7</cell></row><row><cell>LaneATT(ResNet-18) [21]</cell><cell>96.71</cell><cell>95.57</cell><cell>0.0356</cell><cell>0.0301</cell><cell>9.3</cell></row><row><cell>LaneATT (ResNet-34) [21]</cell><cell>96.77</cell><cell>95.63</cell><cell cols="2">0.0353 0.0292</cell><cell>18.0</cell></row><row><cell>LaneATT (ResNet-128) [21]</cell><cell>96.06</cell><cell>96.10</cell><cell>0.0564</cell><cell>0.0217</cell><cell>70.5</cell></row><row><cell>LaneAF (DLA-34)</cell><cell>96.42</cell><cell>95.64</cell><cell>0.0298</cell><cell>0.0413</cell><cell>17.5</cell></row><row><cell cols="6">outperforms the current state-of-the-art with an F1 score</cell></row><row><cell cols="6">of 77.25%, surpassing models of similar size and even</cell></row><row><cell cols="6">LaneATT with its largest backbone, ResNet-128. Moreover,</cell></row><row><cell cols="6">LaneAF sets a new benchmark in a majority of categories,</cell></row><row><cell cols="6">including difficult ones such as Dazzle, Shadow, Curve, and</cell></row><row><cell cols="6">Night, exhibiting our model's high adaptability to curving</cell></row><row><cell cols="4">roads and challenging lighting conditions.</cell><cell></cell><cell></cell></row><row><cell cols="3">Qualitative results can be seen in</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">LaneAF State-of-the-Art Results on the CULane Benchmark Method Total Normal Crowded Dazzle Shadow No line Arrow Curve Cross Night MACs (G)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Table Iv</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Medasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Behringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="18" to="23" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How would surround vehicles move? a unified framework for maneuver classification and motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Deo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Vehicles</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="140" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Looking at hands in autonomous vehicles: A convnet approach using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Vehicles</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="361" to="371" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Drive analysis using vehicle dynamics and vision-based lane semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Satzoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="18" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust lane detection from continuous driving scenes using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on vehicular technology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="54" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Elgan: Embedding loss driven generative adversarial networks for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fastdraw: Addressing the long tail of lane detection by adapting a sequential prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end lane detection through differentiable least-squares fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE intelligent vehicles symposium (IV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="286" to="291" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lane detection and classification using cascaded cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pizzati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Systems Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="95" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Key points estimation and point instance segmentation approach for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06604</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust lane detection using multiple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Sikchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Charkravarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1470" to="1475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d-lanenet: end-to-end 3d multiple lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pe&amp;apos;er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2921" to="2930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial-temproal based lane detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP International conference on artificial Intelligence applications and innovations</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On enhancing lane estimation using contextual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Satzoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1870" to="1881" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Keep your eyes on the lane: Attention-guided lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Paixão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olivera-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep multi-sensor lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mattyus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Lakshmikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="3102" to="3109" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Polylanenet: Lane estimation via deep polynomial regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tabelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Paixao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira-Santos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10924</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lane detection in lowlight conditions using an efficient data enhancement: Light conditions style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1394" to="1399" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Curvelane-nas: Unifying lane-sensitive architecture search and adaptive point blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
