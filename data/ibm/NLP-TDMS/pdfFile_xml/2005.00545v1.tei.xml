<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-Dimensional Hyperbolic Knowledge Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
							<email>chami@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adva</forename><surname>Wolf</surname></persName>
							<email>advaw@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
							<email>fredsala@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
							<email>sravi@sravi.org</email>
							<affiliation key="aff2">
								<orgName type="department">Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Low-Dimensional Hyperbolic Knowledge Graph Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph (KG) embeddings learn lowdimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs. In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns. Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns. Experimental results on standard KG benchmarks show that our method improves over previous Euclidean-and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that different geometric transformations capture different types of relations while attentionbased transformations generalize to multiple relations. In high dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR and 57.7% on YAGO3-10. * Work partially done during an internship at Google. † Work done while at Google AI. d ir e ct o r E.T fe a tu r e s married fe a tu r e s Movie Movie director Actor Singer Jurassic Park</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs), consisting of (head entity, relationship, tail entity) triples, are popular data structures for representing factual knowledge to be queried and used in downstream applications such as word sense disambiguation, question answering, and information extraction. Real-world KGs such as Yago <ref type="bibr" target="#b26">(Suchanek et al., 2007)</ref> or <ref type="bibr">Wordnet (Miller, 1995)</ref> are usually incomplete, so a common approach to predicting missing links in KGs is via embedding into vector spaces. Embedding methods learn representations of entities and relationships that preserve the information found in the graph, and have achieved promising results for many tasks.</p><p>Relations found in KGs have differing properties: for example, (Michelle Obama, married to, Barack Obama) is symmetric, whereas hypernym relations like (cat, specific type of, feline), are not ( <ref type="figure" target="#fig_0">Figure  1)</ref>. These distinctions present a challenge to embedding methods: preserving each type of behavior requires producing a different geometric pattern in the embedding space. One popular approach is to use extremely high-dimensional embeddings, which offer more flexibility for such patterns. However, given the large number of entities found in KGs, doing so yields very high memory costs.</p><p>For hierarchical data, hyperbolic geometry offers an exciting approach to learn low-dimensional embeddings while preserving latent hierarchies. Hyperbolic space can embed trees with arbitrarily low distortion in just two dimensions. Recent research has proposed embedding hierarchical graphs into these spaces instead of conventional Euclidean space <ref type="bibr" target="#b21">(Nickel and Kiela, 2017;</ref><ref type="bibr" target="#b23">Sala et al., 2018)</ref>. However, these works focus on embedding simpler graphs (e.g., weighted trees) and cannot express the diverse and complex relationships in KGs.</p><p>We propose a new hyperbolic embedding ap-proach that captures such patterns to achieve the best of both worlds. Our proposed approach produces the parsimonious representations offered by hyperbolic space, especially suitable for hierarchical relations, and is effective even with lowdimensional embeddings. It also uses rich transformations to encode logical patterns in KGs, previously only defined in Euclidean space. To accomplish this, we (1) train hyperbolic embeddings with relation-specific curvatures to preserve multiple hierarchies in KGs;</p><p>(2) parameterize hyperbolic isometries (distance-preserving operations) and leverage their geometric properties to capture relations' logical patterns, such as symmetry or anti-symmetry; (3) and use a notion of hyperbolic attention to combine geometric operators and capture multiple logical patterns.</p><p>We evaluate the performance of our approach, ATTH, on the KG link prediction task using the standard WN18RR <ref type="bibr" target="#b6">(Dettmers et al., 2018;</ref><ref type="bibr" target="#b4">Bordes et al., 2013)</ref>, FB15k-237 <ref type="bibr" target="#b29">(Toutanova and Chen, 2015)</ref> and <ref type="bibr">YAGO3-10 (Mahdisoltani et al., 2013)</ref> benchmarks. (1) In low (32) dimensions, we improve over Euclidean-based models by up to 6.1% in the mean reciprocical rank (MRR) metric. In particular, we find that hierarchical relationships, such as WordNet's hypernym and member meronym, significantly benefit from hyperbolic space; we observe a 16% to 24% relative improvement versus Euclidean baselines. (2) We find that geometric properties of hyperbolic isometries directly map to logical properties of relationships. We study symmetric and anti-symmetric patterns and find that reflections capture symmetric relations while rotations capture anti-symmetry. (3) We show that attention based-transformations have the ability to generalize to multiple logical patterns. For instance, we observe that ATTH recovers reflections for symmetric relations and rotations for the antisymmetric ones.</p><p>In high (500) dimensions, we find that both hyperbolic and Euclidean embeddings achieve similar performance, and our approach achieves new stateof-the-art results (SotA), obtaining 49.6% MRR on WN18RR and 57.7% YAGO3-10. Our experiments show that trainable curvature is critical to generalize hyperbolic embedding methods to highdimensions. Finally, we visualize embeddings learned in hyperbolic spaces and show that hyperbolic geometry effectively preserves hierarchies in KGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous methods for KG embeddings also rely on geometric properties. Improvements have been obtained by exploiting either more sophisticated spaces (e.g., going from Euclidean to complex or hyperbolic space) or more sophisticated operations (e.g., from translations to isometries, or to learning graph neural networks). In contrast, our approach takes a step forward in both directions.</p><p>Euclidean embeddings In the past decade, there has been a rich literature on Euclidean embeddings for KG representation learning. These include translation approaches <ref type="bibr" target="#b4">(Bordes et al., 2013;</ref><ref type="bibr" target="#b10">Ji et al., 2015;</ref><ref type="bibr" target="#b31">Wang et al., 2014;</ref><ref type="bibr" target="#b14">Lin et al., 2015)</ref> or tensor factorization methods such as RESCAL <ref type="bibr" target="#b20">(Nickel et al., 2011)</ref> or DistMult . While these methods are fairly simple and have few parameters, they fail to encode important logical properties (e.g., translations can't encode symmetry).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex embeddings</head><p>Recently, there has been interest in learning embeddings in complex space, as in the ComplEx <ref type="bibr" target="#b30">(Trouillon et al., 2016)</ref> and Ro-tatE <ref type="bibr" target="#b27">(Sun et al., 2019)</ref> models. RotatE learns rotations in complex space, which are very effective in capturing logical properties such as symmetry, anti-symmetry, composition or inversion. The recent QuatE model <ref type="bibr" target="#b34">(Zhang et al., 2019)</ref> learns KG embeddings using quaternions. However, a downside is that these embeddings require very highdimensional spaces, leading to high memory costs.</p><p>Deep neural networks Another family of methods uses neural networks to produce KG embeddings. For instance, R-GCN <ref type="bibr">(Schlichtkrull et al., 2018)</ref> extends graph neural networks to the multirelational setting by adding a relation-specific aggregation step. ConvE and ConvKB <ref type="bibr" target="#b6">(Dettmers et al., 2018;</ref><ref type="bibr" target="#b19">Nguyen et al., 2018)</ref> leverage the expressiveness of convolutional neural networks to learn entity embeddings and relation embeddings. More recently, the KBGAT <ref type="bibr" target="#b18">(Nathani et al., 2019)</ref> and A2N <ref type="bibr" target="#b2">(Bansal et al., 2019)</ref> models use graph attention networks for knowledge graph embeddings. A downside of these methods is that they are computationally expensive as they usually require pre-trained KG embeddings as input for the neural network.</p><p>Hyperbolic embeddings To the best of our knowledge, MuRP <ref type="bibr" target="#b0">(Balažević et al., 2019)</ref> is the only method that learns KG embeddings in hyperbolic space in order to target hierarchical data. MuRP minimizes hyperbolic distances between a re-scaled version of the head entity embedding and a translation of the tail entity embedding. It achieves promising results using hyperbolic embeddings with fewer dimensions than its Euclidean analogues. However, MuRP is a translation model and fails to encode some logical properties of relationships. Furthermore, embeddings are learned in a hyperbolic space with fixed curvature, potentially leading to insufficient precision, and training relies on cumbersome Riemannian optimization. Instead, our proposed method leverages expressive hyperbolic isometries to simultaneously capture logical patterns and hierarchies. Furthermore, embeddings are learned using tangent space (i.e., Euclidean) optimization methods and trainable hyperbolic curvatures per relationship, avoiding precision errors that might arise when using a fixed curvature, and providing flexibility to encode multiple hierarchies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation and Background</head><p>We describe the KG embedding problem setting and give some necessary background on hyperbolic geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Knowledge graph embeddings</head><p>In the KG embedding problem, we are given a set of triples (h, r, t) ∈ E ⊆ V × R × V, where V and R are entity and relationship sets, respectively. The goal is to map entities v ∈ V to embeddings e v ∈ U d V and relationships r ∈ R to embeddings r r ∈ U d R , for some choice of space U (traditionally R), such that the KG structure is preserved.</p><p>Concretely, the data is split into E T rain and E T est triples. Embeddings are learned by optimizing a scoring function s : V × R × V → R, which measures triples' likelihoods. s(·, ·, ·) is trained using triples in E T rain and the learned embeddings are then used to predict scores for triples in E T est . The goal is to learn embeddings such that the scores of triples in E T est are high compared to triples that are not present in E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyperbolic geometry</head><p>We briefly review key notions from hyperbolic geometry; a more in-depth treatment is available in standard texts (Robbin and Salamon). Hyperbolic geometry is a non-Euclidean geometry with constant negative curvature. In this work, we use the d- The tangent space T c x maps to B d,c via the exponential map ( <ref type="figure" target="#fig_1">Figure 2</ref>), and conversely, the logarithmic map maps B d,c to T c x . In particular, we have closed-form expressions for these maps at the origin:</p><formula xml:id="formula_0">exp c 0 (v) = tanh( √ c||v||) v √ c||v|| ,<label>(1)</label></formula><formula xml:id="formula_1">log c 0 (y) = arctanh( √ c||y||) y √ c||y|| .<label>(2)</label></formula><p>Vector addition is not well-defined in the hyperbolic space (adding two points in the Poincaré ball might result in a point outside the ball). Instead, Möbius addition ⊕ c <ref type="bibr" target="#b8">(Ganea et al., 2018)</ref> provides an analogue to Euclidean addition for hyperbolic space. We give its closed-form expression in Appendix A.1. Finally, the hyperbolic distance on B d,c has the explicit formula:</p><formula xml:id="formula_2">d c (x, y) = 2 √ c arctanh( √ c|| − x ⊕ c y||). (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>The goal of this work is to learn parsimonious hyperbolic embeddings that can encode complex logical patterns such as symmetry, anti-symmetry, or inversion while preserving latent hierarchies. Our model, ATTH, (1) learns KG embeddings in hyperbolic space in order to preserve hierarchies (Section 4.1), (2) uses a class of hyperbolic isometries parameterized by compositions of Givens transformations to encode logical patterns (Section 4.2), (3) combines these isometries with hyperbolic attention (Section 4.3). We describe the full model in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hierarchies in hyperbolic space</head><p>As described, hyperbolic embeddings enable us to represent hierarchies even when we limit ourselves to low-dimensional spaces. In fact, twodimensional hyperbolic space can represent any tree with arbitrarily small error <ref type="bibr" target="#b23">(Sala et al., 2018)</ref>. It is important to set the curvature of the hyperbolic space correctly. This parameter provides flexibility to the model, as it determines whether to embed relations into a more curved hyperbolic space (more "tree-like"), or into a flatter, more "Euclidean-like" geometry. For each relation, we learn a relation-specific absolute curvature c r , enabling us to represent a variety of hierarchies. As we show in Section 5.5, fixing, rather than learning curvatures can lead to significant performance degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperbolic isometries</head><p>Relationships often satisfy particular properties, such as symmetry: e.g., if (Michelle Obama, married to, Barack Obama) holds, then (Barack Obama, married to, Michelle Obama) does as well. These rules are not universal. For instance, (Barack Obama, born in, Hawaii) is not symmetric.</p><p>Creating and curating a set of deterministic rules is infeasible for large-scale KGs; instead, embedding methods represent relations as parameterized geometric operations that directly map to logical properties. We use two such operations in hyperbolic space: rotations, which effectively capture compositions or anti-symmetric patterns, and reflections, which naturally encode symmetric patterns.</p><p>Rotations Rotations have been successfully used to encode compositions in complex space with the RotatE model <ref type="bibr" target="#b27">(Sun et al., 2019)</ref>; we lift these to hyperbolic space. Compared to translations or tensor factorization approaches which can only infer some logical patterns, rotations can simultaneously model and infer inversion, composition, symmetric or anti-symmetric patterns.</p><p>Reflections These isometries reflect along a fixed subspace. While some rotations can represent symmetric relations (more specifically π−rotations), any reflection can naturally represent symmetric relations, since their second power is the identity. They provide a way to fill-in missing entries in symmetric triples, by applying the same operation to both the tail and the head entity. For instance, by modelling sibling of with a reflection, we can  <ref type="figure">Figure 3</ref>: Euclidean (left) and hyperbolic (right) isometries. In hyperbolic space, the distance between start and end points after applying rotations or reflections is much larger than the Euclidean distance; it approaches the sum of the distances between the points and the origin, giving more "room" to separate embeddings. This is similar to trees, where the shortest path between two points goes through their nearest common ancestor.</p><p>directly infer (Bob, sibling of, Alice) from (Alice, sibling of, Bob) and vice versa.</p><p>Parameterization Unlike RotatE which models rotations via unitary complex numbers, we learn relationship-specific isometries using Givens transformations, 2 × 2 matrices commonly used in numerical linear algebra. Let</p><formula xml:id="formula_3">Θ r := (θ r,i ) i∈{1,... d 2 } and Φ r := (φ r,i ) i∈{1,... d 2 } denote relation-specific parameters.</formula><p>Using an even number of dimensions d, our model parameterizes rotations and reflections with block-diagonal matrices of the form:</p><formula xml:id="formula_4">Rot(Θ r ) = diag(G + (θ r,1 ), . . . , G + (θ r, d 2 )), (4) Ref(Φ r ) = diag(G − (φ r,1 ), . . . , G − (φ r, n 2 )), (5) where G ± (θ) := cos(θ) ∓sin(θ) sin(θ) ±cos(θ) .<label>(6)</label></formula><p>Rotations and reflections of this form are hyperbolic isometries (distance-preserving). We can therefore directly apply them to hyperbolic embeddings while preserving the underlying geometry. Additionally, these transformations are computationally efficient and can be computed in linear time in the dimension. We illustrate two-dimensional isometries in both Euclidean and hyperbolic spaces in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyperbolic attention</head><p>Of our two classes of hyperbolic isometries, one or the other may better represent a particular relation.</p><p>To handle this, we use an attention mechanism to learn the right isometry. Thus we can represent symmetric, anti-symmetric or mixed-behaviour relations (i.e. neither symmetric nor anti-symmetric) as a combination of rotations and reflections. Let x H and y H be hyperbolic points (e.g., reflection and rotation embeddings), and a be an attention vector. Our approach maps hyperbolic representations to tangent space representations, x E = log c 0 (x H ) and y E = log c 0 (y H ), and computes attention scores:</p><formula xml:id="formula_5">(α x , α y ) = Softmax(a T x E , a T y E ).</formula><p>We then compute a weighted average using the recently proposed tangent space average <ref type="bibr" target="#b5">(Chami et al., 2019;</ref>:</p><formula xml:id="formula_6">Att(x H , y H ; a) := exp c 0 (α x x E + α y y E ). (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The ATTH model</head><p>We </p><formula xml:id="formula_7">q H Rot = Rot(Θ r )e H h , q H ref = Ref(Φ r )e H h . (8)</formula><p>ATTH then combines the two representations using hyperbolic attention <ref type="formula">(Equation 7</ref>) and applies a hyperbolic translation:</p><formula xml:id="formula_8">Q(h, r) = Att(q H Rot , q H Ref ; a r ) ⊕ cr r H r .<label>(9)</label></formula><p>Intuitively, rotations and reflections encode logical patterns while translations capture tree-like structures by moving between levels of the hierarchy. Finally, query embeddings are compared to target tail embeddings via the hyperbolic distance (Equation 3). The resulting scoring function is:</p><formula xml:id="formula_9">s(h, r, t) = −d cr (Q(h, r), e H t ) 2 + b h + b t ,<label>(10)</label></formula><p>where (b v ) v∈V are entity biases which act as margins in the scoring function <ref type="bibr" target="#b28">(Tifrea et al., 2019;</ref><ref type="bibr" target="#b0">Balažević et al., 2019)</ref>.</p><formula xml:id="formula_10">The model parameters are then {(Θ r , Φ r , r H r , a r , c r ) r∈R , (e H v , b v ) v∈V }.</formula><p>Note that the total number of parameters in ATTH is O(|V|d), similar to traditional models that do not use attention or geometric operations. The extra cost is proportional to the number of relations, which is usually much smaller than the number of entities.  <ref type="table">Table 1</ref>: Datasets statistics. The lower the metric ξ G is, the more tree-like the knowledge graph is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In low dimensions, we hypothesize (1) that hyperbolic embedding methods obtain better representations and allow for improved downstream performance for hierarchical data (Section 5.2). <ref type="formula" target="#formula_1">(2)</ref> We expect the performance of relation-specific geometric operations to vary based on the relation's logical patterns (Section 5.3).</p><p>(3) In cases where the relations are neither purely symmetric nor antisymmetric, we anticipate that hyperbolic attention outperforms the models which are based on solely reflections or rotations (Section 5.4). Finally, in high dimensions, we expect hyperbolic models with trainable curvature to learn the best geometry, and perform similarly to their Euclidean analogues (Section 5.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>Datasets We evaluate our approach on the link prediction task using three standard competition benchmarks, namely WN18RR <ref type="bibr" target="#b4">(Bordes et al., 2013;</ref><ref type="bibr" target="#b6">Dettmers et al., 2018)</ref>, <ref type="bibr">FB15k-237 (Bordes et al., 2013;</ref><ref type="bibr" target="#b29">Toutanova and Chen, 2015)</ref> and YAGO3-10 ( <ref type="bibr" target="#b16">Mahdisoltani et al., 2013)</ref>. WN18RR is a subset of WordNet containing 11 lexical relationships between 40,943 word senses, and has a natural hierarchical structure, e.g., (car, hypernym of, sedan). FB15k-237 is a subset of Freebase, a collaborative KB of general world knowledge. FB15k-237 has 14,541 entities and 237 relationships, some of which are non-hierarchical, such as born-in or nationality, while others have natural hierarchies, such as part-of (for organizations). YAGO3-10 is a subset of YAGO3, containing 123,182 entities and 37 relations, where most relations provide descriptions of people. Some relationships have a hierarchical structure such as playsFor or actedIn, while others induce logical patterns, like isMarriedTo.</p><p>For each KG, we follow the standard data augmentation protocol by adding inverse relations <ref type="bibr" target="#b13">(Lacroix et al., 2018)</ref> to the datasets. Additionally, we estimate the global graph curvature ξ G <ref type="bibr" target="#b9">(Gu et al., 2019)</ref>    which is a distance-based measure of how close a given graph is to being a tree. We summarize the datasets' statistics in <ref type="table">Table 1</ref>.</p><p>Baselines We compare our method to SotA models, including MurP <ref type="bibr" target="#b1">(Balazevic et al., 2019)</ref>, MurE (which is the Euclidean analogue or MurP), RotatE <ref type="bibr" target="#b27">(Sun et al., 2019)</ref>, ComplEx-N3 <ref type="bibr" target="#b13">(Lacroix et al., 2018)</ref> and TuckER <ref type="bibr" target="#b1">(Balazevic et al., 2019)</ref>. Baseline numbers in high dimensions <ref type="table" target="#tab_10">(Table 5)</ref> are taken from the original papers, while baseline numbers in the low-dimensional setting <ref type="table" target="#tab_3">(Table 2)</ref> are computed using open-source implementations of each model. In particular, we run hyper-parameter searches over the same parameters as the ones in the original papers to compute baseline numbers in the lowdimensional setting.</p><p>Ablations To analyze the benefits of hyperbolic geometry, we evaluate the performance of ATTE, which is equivalent to ATTH with curvatures set to zero. Additionally, to better understand the role of attention, we report scores for variants of ATTE/H using only rotations (ROTE/H) or reflections (REFE/H).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics</head><p>At test time, we use the scoring function in Equation 10 to rank the correct tail or head entity against all possible entities, and use in use inverse relations for head prediction <ref type="bibr" target="#b13">(Lacroix et al., 2018)</ref>. Similar to previous work, we compute two ranking-based metrics: (1) mean reciprocal rank (MRR), which measures the mean of inverse ranks assigned to correct entities, and (2) hits at K (H@K, K ∈ {1, 3, 10}), which measures the proportion of correct triples among the top K predicted triples. We follow the standard evaluation protocol in the filtered setting <ref type="bibr" target="#b4">(Bordes et al., 2013)</ref>: all true triples in the KG are filtered out during evaluation, since predicting a low rank for these triples should not be penalized.</p><p>Training procedure and implementation We train ATTH by minimizing the full cross-entropy loss with uniform negative sampling, where negative examples for a triple (h, r, t) are sampled uniformly from all possible triples obtained by perturbing the tail entity:</p><formula xml:id="formula_11">L = t ∼U (V) log(1+exp(y t s(h, r, t ))), (11) where y t = −1 if t = t 1 otherwise.</formula><p>Since optimization in hyperbolic space is practically challenging, we instead define all parameters in the tangent space at the origin, optimize embeddings using standard Euclidean techniques, and use the exponential map to recover the hyperbolic parameters <ref type="bibr" target="#b5">(Chami et al., 2019)</ref>. We provide more details on tangent space optimization in Appendix A.4. We conducted a grid search to select the learning rate, optimizer, negative sample size, and batch size, using the validation set to select the best hy-  perparameters. Our best model hyperparameters are detailed in Appendix A.3. We conducted all our experiments on NVIDIA Tesla P100 GPUs and make our implementation publicly available * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results in low dimensions</head><p>We first evaluate our approach in the lowdimensional setting for d = 32, which is approximately one order of magnitude smaller than SotA Euclidean methods. To understand the role of dimensionality, we also conduct experiments on WN18RR against SotA methods under varied low-dimensional settings <ref type="figure" target="#fig_3">(Figure 4)</ref>. We include error bars for our method with average MRR and standard deviation computed over 10 runs. Our approach consistently outperforms all baselines, suggesting that hyperbolic embeddings still attain high-accuracy across a broad range of dimensions.</p><p>Additionally, we measure performance per relation on WN18RR in <ref type="table" target="#tab_5">Table 3</ref> to understand the benefits of hyperbolic geometric on hierarchical relations. We report the Krackhardt hierarchy score  (Khs G ) <ref type="bibr" target="#b0">(Balažević et al., 2019)</ref> and estimated curvature per relation (see Appendix A.2 for more details). We consider a relation to be hierarchical when its corresponding graph is close to tree-like (low curvature, high Khs G ). We observe that hyperbolic embeddings offer much better performance on hierarchical relations such as hypernym or has part, while Euclidean and hyperbolic embeddings have similar performance on non-hierarchical relations such as verb group. We also plot the learned curvature per relation versus the embedding dimension in <ref type="figure" target="#fig_6">Figure 5b</ref>. We note that the learned curvature in low dimensions directly correlates with the estimated graph curvature ξ G in <ref type="table" target="#tab_5">Table 3</ref>, suggesting that the model with learned curvatures learns more "curved" embedding spaces for tree-like relations. Finally, we observe that MurP achieves lower performance than MurE on YAGO3-10, while ATTH improves over ATTE by 2.3% in MRR. This suggests that trainable curvature is critical to learn embeddings with the right amount of curvature, while fixed curvature might degrade performance. We elaborate further on this point in Section 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hyperbolic rotations and reflections</head><p>In our experiments, we find that rotations work well on WN18RR, which contains multiple hierarchical and anti-symmetric relations, while reflections work better for YAGO3-10 <ref type="table" target="#tab_10">(Table 5)</ref>. To better understand the mechanisms behind these observations, we analyze two specific patterns: relation symmetry and anti-symmetry. We report performance per-relation on a subset of YAGO3-10 relations in <ref type="table" target="#tab_8">Table 4</ref>. We categorize relations into symmetric, anti-symmetric, or neither symmetric nor anti-symmetric categories using data statistics. More concretely, we consider a relation to satisfy a logical pattern when the logical condition is satisfied by most of the triplets (e.g., a relation r is symmetric if for most KG triples (h, r, t), (t, r, h) is also in the KG). We observe that reflections encode   symmetric relations particularly well, while rotations are well suited for anti-symmetric relations. This confirms our intuition-and the motivation for our approach-that particular geometric properties capture different kinds of logical properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Attention-based transformations</head><p>One advantage of using relation-specific transformations is that each relation can learn the right geometric operators based on the logical properties it has to satisfy. In particular, we observe that in both low-and high-dimensional settings, attentionbased models can recover the performance of the best transformation on all datasets <ref type="table" target="#tab_3">(Tables 2 and 5)</ref>. Additionally, per-relationship results on YAGO3-10 in <ref type="table" target="#tab_8">Table 4</ref> suggest that ATTH indeed recovers the best geometric operation.</p><p>Furthermore, for relations that are neither symmetric nor anti-symmetric, we find that ATTH can outperform rotations and reflections, suggesting that combining multiple operators with attention can learn more expressive operators to model mixed logical patterns. In other words, attentionbased transformations alleviate the need to conduct experiments with multiple geometric transformations by simply allowing the model to choose which one is best for a given relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results in high dimensions</head><p>In high dimensions <ref type="table" target="#tab_10">(Table 5)</ref>, we compare against a variety of other models and achieve new SotA results on WN18RR and YAGO3-10, and thirdbest results on FB15k-237. As we expected, when the embedding dimension is large, Euclidean and hyperbolic embedding methods perform similarly across all datasets. We explain this behavior by noting that when the dimension is sufficiently large, both Euclidean and hyperbolic spaces have enough capacity to represent complex hierarchies in KGs. This is further supported by <ref type="figure" target="#fig_6">Figure 5b</ref>, which shows the learned absolute curvature versus the dimension. We observe that curvatures are close to zero in high dimensions, confirming our expectation that ROTH with trainable curvatures learns a roughly Euclidean geometry in this setting.</p><p>In contrast, fixed curvature degrades performance in high dimensions <ref type="figure" target="#fig_6">(Figure 5a</ref>), confirming the importance of trainable curvatures and its impact on precision and capacity (previously studied by <ref type="bibr" target="#b23">(Sala et al., 2018)</ref>). Additionally, we show the embeddings' norms distribution in the Appendix <ref type="figure" target="#fig_9">(Figure 7)</ref>. Fixed curvature results in embeddings being clustered near the boundary of the ball while trainable curvatures adjusts the embedding space to better distribute points throughout the ball. Precision issues that might arise with fixed curvature could also explain MurP's low performance in high dimensions. Trainable curvatures allow ROTH to perform as well or better than previous methods in both low and high dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Visualizations</head><p>In <ref type="figure" target="#fig_8">Figure 6</ref>, we visualize the embeddings learned by ROTE versus ROTH for a sub-tree of the organism entity in WN18RR. To better visualize the hierarchy, we apply k inverse rotations for all nodes at level k in the tree.</p><p>By contrast to ROTE, ROTH preserves the tree structure in the embedding space. Furthermore, we note that ROTE cannot simultaneously preserve the tree structure and make non-neighboring nodes far from each other. For instance, virus should be far from male, but preserving the tree structure (by going one level down in the tree) while making   <ref type="bibr" target="#b6">(Dettmers et al., 2018)</ref>. Best score in bold and best published underlined. ATTE and ATTH have similar performance in the high-dimensional setting, performing competitively with or better than state-of-the-art methods on WN18RR, FB15k-237 and YAGO3-10.  these two nodes far from each other is difficult in Euclidean space. In hyperbolic space, however, we observe that going one level down in the tree is achieved by translating embeddings towards the left. This pattern essentially illustrates the translation component in ROTH, allowing the model to simultaneously preserve hierarchies while making non-neighbouring nodes far from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce ATTH, a hyperbolic KG embedding model that leverages the expressiveness of hyperbolic space and attention-based geometric transformations to learn improved KG representations in low-dimensions. ATTH learns embeddings with trainable hyperbolic curvatures, allowing it to learn the right geometry for each relationship and generalize across multiple embedding dimensions. ATTH achieves new SotA on WN18RR and YAGO3-10, real-world KGs which exhibit hierar-chical structures. Future directions for this work include exploring other tasks that might benefit from hyperbolic geometry, such as hypernym detection. The proposed attention-based transformations can also be extended to other geometric operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>Below, we provide additional details. We start by providing the formula for the hyperbolic analogue of addition that we use, along with additional hyperbolic geometry background. Next, we provide more information about the metrics that are used to determine how hierarchical a dataset is. Afterwards, we give additional experimental details, including the table of hyperparameters and further details on tangent space optimization. Lastly, we include an additional comparison against the Dihedral model <ref type="bibr" target="#b32">(Xu and Li, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Möbius addition</head><p>The Möbius addition operation <ref type="bibr" target="#b8">(Ganea et al., 2018)</ref> has the closed-form expression:</p><p>x ⊕ c y = α xy x + β xy y 1 + 2cx T y + c 2 ||x|| 2 ||y|| 2 , where α xy = 1 + 2cx T y + c||y|| 2 , and β xy = 1 − c||x|| 2 .</p><p>In contrast to Euclidean addition, it is neither commutative nor associative. However, it provides an analogue through the lens of parallel transport: given two points x, y and a vector v in T c</p><p>x , there is a unique vector in T c y which creates the same angle as v with the direction of the geodesic (shortest path) connecting x to y. This map is the parallel transport P c x→y (·); Euclidean parallel transport is the standard Euclidean addition. Analogously, the Möbius addition satisfies <ref type="bibr" target="#b8">(Ganea et al., 2018)</ref>:</p><p>x ⊕ c y = exp c x (P c 0→x (log c 0 (y))).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hierarchy estimates</head><p>We use two metrics to estimate how hierarchical a relation is: the curvature estimate ξ G and the Krackhardt hierarchy score Khs G . While the curvature estimate captures global hierarchical behaviours (how much the graph is tree-like when zoomingout), the Krackhardt score captures a more local behaviour (how many small loops the graph has). See <ref type="figure">Figure 8</ref> for examples.</p><p>Curvature estimate To estimate the curvature of a relation r, we restrict to the undirected graph G r spanned by the edges labeled as r. Following <ref type="bibr" target="#b9">(Gu et al., 2019)</ref>, let ξ Gr (a, b, c) be the curvature estimate of a triangle in G r with vertices {a, b, c}, where m is the midpoint of the shortest path connecting b to c. This estimate is positive for triangles in circles, negative for triangles in trees, and zero for triangles in lines. Moreover, for a triangle in a Riemannian manifold M , ξ M (a, b, c) estimates the sectional curvature of the plane on which the triangle lies (see <ref type="bibr" target="#b9">(Gu et al., 2019)</ref> for more details). Let m r be the total number of connected components in G r . We sample 1000 w i,r triangles from each connected component c Krackhardt hierarchy score For the directed graph G r spanned by the relation r, we let R be the adjacency matrix (R i,j = 1 if there is an edge from node i to node j and 0 otherwise). Then:</p><formula xml:id="formula_12">Khs Gr = n i,j=1 R i,j (1 − R j,i ) n i,j=1 R i,j .</formula><p>See <ref type="bibr" target="#b12">(Krackhardt, 1994)</ref> for more details. We note that for fully observed symmetric relations (each edge is in a two-edge loop), Khs Gr = 0 while for anti-symmetric relations (no small loops), Khs Gr = 1.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A toy example showing how KGs can simultaneously exhibit hierarchies and logical patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of the exponential map exp x (v), which maps the tangent space T x M at the point x to the hyperbolic manifold M . dimensional Poincaré ball model with negative curvature −c (c &gt; 0): B d,c = {x ∈ R d : ||x|| 2 &lt; 1 c }, where || · || denotes the L 2 norm. For each point x ∈ B d,c , the tangent space T c x is a d-dimensional vector space containing all possible directions of paths in B d,c leaving from x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>WN18RR MRR dimension for d ∈ {10,16, 20, 32, 50, 200, 500}. Average and standard deviation computed over 10 runs for ROTH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>MRR for fixed and trainable curvatures on WN18RR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Curvatures learned by with ROTH on WN18RR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>(a): ROTH offers improved performance in low dimensions; in high dimensions, fixed curvature degrades performance, while trainable curvature approximately recovers Euclidean space. (b): As the dimension increases, the learned curvature of hierarchical relationships tends to zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Visualizations of the embeddings learned by ROTE and ROTH on a sub-tree of WN18RR for the hypernym relation. In contrast to ROTE, ROTH preserves hierarchies by learning tree-like embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Histogram of embeddings norm learned with fixed and trainable curvatures for the hypernym relation in WN18RR.which is given by:ξ Gr (a, b, c) = 1 2d Gr (a, m) d Gr (a, m) 2 + d Gr (b, c) 2 /4 − (d Gr (a, b) 2 + d Gr (a, c) 2 )/2 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>N i,r is the number of nodes in the component c i,r . ξ Gr is the mean of the estimated curvatures of the sampled triangles. For the full graph, we take the weighted average of the relation curvatures ξ Gr with respect to the weights</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>ξG &lt; 0 ,Figure 8 :</head><label>08</label><figDesc>KhsG = 1 ξG &lt; 0, KhsG = 0 ξG = 0, KhsG = 1 ξG = 0, KhsG = 0 ξG &gt; 0, KhsG = 1 ξG &gt; 0, KhsG = 0 ' The curvature estimate ξ G and the Krackhardt hierarchy score Khs G for several simple graphs. The top-left graph is the most hierarchical, while the bottom-right graph is the least hierarchical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Link prediction results for low-dimensional embeddings (d = 32) in the filtered setting. Best score in bold and best published underlined. Hyperbolic isometries significantly outperform Euclidean baselines on WN18RR and YAGO3-10, both of which exhibit hierarchical structures.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Mean Reciprocical Rank (MRR) vs. dimension</cell></row><row><cell></cell><cell>0.50</cell><cell></cell></row><row><cell></cell><cell>0.45</cell><cell></cell></row><row><cell>MRR</cell><cell>0.35 0.40</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>MurP</cell></row><row><cell></cell><cell>0.30</cell><cell></cell><cell>ComplEx − N3 RotH</cell></row><row><cell></cell><cell>0.25</cell><cell>10 1</cell><cell>10 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Embedding dimension</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of H@10 for WN18RR relations. Higher Khs G and lower ξ G means more hierarchical.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>compares the perfor-</cell></row><row><cell>mance of ATTH to that of other baselines, includ-</cell></row><row><cell>ing the recent hyperbolic (but not rotation-based)</cell></row><row><cell>MuRP model. In low dimensions, hyperbolic</cell></row><row><cell>embeddings offer much better representations for</cell></row><row><cell>hierarchical relations, confirming our hypothesis.</cell></row><row><cell>ATTH improves over previous Euclidean and hy-</cell></row><row><cell>perbolic methods by 0.7% and 6.1% points in MRR</cell></row><row><cell>on WN18RR and YAGO3-10 respectively. Both</cell></row><row><cell>datasets have multiple hierarchical relationships,</cell></row><row><cell>suggesting that the hierarchical structure imposed</cell></row><row><cell>by hyperbolic geometry leads to better embeddings.</cell></row><row><cell>On FB15k-237, ATTH and MurP achieve similar</cell></row><row><cell>performance, both improving over Euclidean base-</cell></row><row><cell>lines. We conjecture that translations are sufficient</cell></row><row><cell>to model relational patterns in FB15k-237.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Comparison of geometric transformations on a subset of YAGO3-10 relations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Link prediction results for high-dimensional embeddings (best for d ∈ {200, 400, 500}) in the filtered setting. DistMult, ConvE and ComplEx results are taken from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Comparison of Dihedral and ATTE in highdimensions.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* Code available at https://github.com/ tensorflow/neural-structured-learning/ tree/master/research/kg_hyp_emb</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Avner May for their helpful feedback and discussions. We gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, the HAI-AWS Cloud Credits for Research program, TOTAL, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Experimental details</head><p>For all our Euclidean and hyperbolic models, we conduct a hyperparameter search for the learning rate, optimizer <ref type="bibr">(Adam (Kingma and Ba, 2015)</ref> or Adagrad <ref type="bibr" target="#b7">(Duchi et al., 2011)</ref>), negative sample size and batch size. We train each model for 500 epochs and use early stopping after 100 epochs if the validation MRR stops increasing. We report the best hyperparameters for each dataset in <ref type="table">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Tangent space optimization</head><p>Optimization in hyperbolic space normally requires Riemannian Stochastic Gradient Descent (RSGD) <ref type="bibr" target="#b3">(Bonnabel, 2013)</ref>, as was used in MuRP. RSGD is challenging in practice. Instead, we use tangent space optimization <ref type="bibr" target="#b5">(Chami et al., 2019)</ref>. We define all the ATTH parameters in the tangent space at the origin (our parameter space), optimize embeddings using standard Euclidean techniques, and use the exponential map to recover the hyperbolic parameters.</p><p>Note that tangent space optimization is an exact procedure, which does not incur losses in representational power. This is the case in hyperbolic space specifically because of a completeness property: there is always a global bijection between the tangent space and the manifold.</p><p>Concretely, ATTH optimizes the entity and relationship embeddings (e E v ) v∈V and (r E r ) r∈R , which are mapped to the Poincaré ball with:  </p><p>which are all Euclidean parameters that can be learned using standard Euclidean optimization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Comparison to Dihedral</head><p>We compare the performance of Dihedral <ref type="bibr" target="#b32">(Xu and Li, 2019)</ref> versus that of ATTE in <ref type="table">Table 6</ref>. Both methods combine rotations and reflections, but our approach learns attention-based transformations, while Dihedral learns a single parameter to determine which transformation to use. ATTE significantly outperforms Dihedral on all datasets, suggesting that using attention-based representations is important in order to learn the right geometric transformation for each relation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-relational poincaré graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balažević</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4465" to="4475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tucker: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5188" to="5197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A2n: Attending to neighbors for knowledge graph inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4387" to="4392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvere</forename><surname>Bonnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2217" to="2229" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hyperbolic graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4869" to="4880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional 2D knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<title level="m">Hyperbolic neural networks. In Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning mixed-curvature representations in product spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph theoretical dimensions of informal organizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krackhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational organization theory</title>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="107" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hyperbolic graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8228" to="8239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Yago3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzaneh</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Poincaré embeddings for learning hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6338" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Joel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dietmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salamon</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Introduction to differential geometry</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Representation tradeoffs for hyperbolic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4457" to="4466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Poincaré GloVe: Hyperbolic word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Tifrea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relation embedding with dihedral group in knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Quaternion knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2731" to="2741" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
