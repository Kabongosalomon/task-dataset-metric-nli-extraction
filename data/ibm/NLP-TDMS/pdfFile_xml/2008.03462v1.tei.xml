<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PAN: Towards Fast Action Recognition via Learning Persistence of Appearance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Guang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Gan</surname></persName>
						</author>
						<title level="a" type="main">PAN: Towards Fast Action Recognition via Learning Persistence of Appearance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Fast Action Recognition</term>
					<term>Motion Representa- tion</term>
					<term>Persistent Appearance Network</term>
					<term>Persistence of Appearance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficiently modeling dynamic motion information in videos is crucial for action recognition task. Most state-of-the-art methods heavily rely on dense optical flow as motion representation. Although combining optical flow with RGB frames as input can achieve excellent recognition performance, the optical flow extraction is very time-consuming. This undoubtably will count against real-time action recognition. In this paper, we shed light on fast action recognition by lifting the reliance on optical flow. Our motivation lies in the observation that small displacements of motion boundaries are the most critical ingredients for distinguishing actions, so we design a novel motion cue called Persistence of Appearance (PA). In contrast to optical flow, our PA focuses more on distilling the motion information at boundaries. Also, it is more efficient by only accumulating pixel-wise differences in feature space, instead of using exhaustive patch-wise search of all the possible motion vectors. Our PA is over 1000× faster (8196fps vs. 8fps) than conventional optical flow in terms of motion modeling speed. To further aggregate the short-term dynamics in PA to long-term dynamics, we also devise a global temporal fusion strategy called Various-timescale Aggregation Pooling (VAP) that can adaptively model long-range temporal relationships across various timescales. We finally incorporate the proposed PA and VAP to form a unified framework called Persistent Appearance Network (PAN) with strong temporal modeling ability. Extensive experiments on six challenging action recognition benchmarks verify that our PAN outperforms recent state-of-the-art methods at low FLOPs. Codes and models are available at: https://github.com/zhang-can/PAN-PyTorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V IDEO action recognition has achieved impressive performance improvements in recent years, mainly due to the aid of deep models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and large datasets <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Although many prominent 2D CNNs <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> have been designed for image recognition task, these 2D CNNs cannot model effective dynamic motion by naively extending them to video domain. The inherent complexity of temporal evolution in videos makes motion modeling in action recognition still a very challenging task.</p><p>Optical flow can encode apparent motion of moving objects in visual scenes. When combining optical flow with RGB frames as input, two-stream CNNs variants <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> and 3D CNNs variants <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> largely outperform their counterparts using only RGB frames as input. Thus, optical flow has been commonly used as motion representation for video action recognition task. However, extract-C. <ref type="bibr">Zhang</ref>  ing optical flow is time-costly. Aiming at mitigating this inefficiency, several recent works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> introduce some fast and accurate optical flow estimation methods based on CNNs. But when applied to action recognition task, such solutions are still sub-optimal for two reasons: <ref type="bibr" target="#b0">(1)</ref> Computing optical flow in advance makes action recognition a two-stage task. This two-stage paradigm is time-consuming, storagedemanding and not end-to-end trainable. (2) Improving the estimation accuracy of optical flow is not well correlated with boosting the action recognition performance, which has been demonstrated by many works <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>To address those issues, we design an alternative motion representation to replace optical flow for action recognition task. Ideally, it should be: efficient to compute, effective in performance, flexible to implement and moreover, free of precomputation and storage. To this end, we need to investigate which parts of a moving object are most critical for distinguishing actions.</p><p>Generally, most existing action recognition methods follow a two-stage procedure: they first estimate optical flow using the EPE loss 1 , and then the estimated optical flow is fed into the subsequent action recognition module. The assumption of these methods is that more accurate optical flow (measured by EPE) will bring superior action recognition performance. However, this correlation is weak as demostrated by Sevilla-Lara et al. <ref type="bibr" target="#b20">[21]</ref> and Zhu et al. <ref type="bibr" target="#b12">[13]</ref>. They combined the optical flow estimation and action recognition modules to form a unified network. The optical flow estimation module is fine-tuned by optimizing the final recognition (CE) loss instead of the EPE loss. Compared with the common two-stage methods, this unified manner achieves better recognition performance.</p><p>To figure out what matters most that leads to the performance improvements, we analyze the visualization result comparisons of the estimated optical flow using the EPE loss, the new flow fine-tuned by the CE loss and their difference computed by the Euclidean distance, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The visualization results are from <ref type="bibr" target="#b20">[21]</ref>. From <ref type="figure" target="#fig_0">Fig. 1</ref>-D, we can clearly observe that the most salient parts are movement variations occurring at motion boundaries. According to the aforementioned analysis, we can conclude that small displacements of motion boundaries play a vital role in action recognition.</p><p>Inspired by this observation, we design a new motion cue which derives from optical flow yet focuses more on the small displacements of motion boundaries. From the perspective of human perception, a series of video frames give people a sense of motion when viewed in order at a certain speed. This phenomenon is termed as Persistence of Vision, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>-A blurred areas. Since we aim to extract motion information directly from RGB frames (a.k.a, the appearance information), we name the proposed motion cue as Persistence of Appearance (PA). Our PA enjoys high efficiency because we do not use exhaustive search of all the possible motion vectors like optical flow does. Instead, PA only contains pixelwise operations in feature space. Specifically, given two adjacent RGB frames, our PA first computes pixel-wise intensity variations in feature space and these variations are further accumulated to manifest the motion magnitude. With such a design, PA can model the small displacements at motion boundaries because: (1) small displacements are perceived since pixel-wise differences reflect the displacements of a small receptive field in input space; (2) motion boundaries are captured since general patterns, e.g., boundaries, texture, etc, can be encoded by the first few convolutional layers <ref type="bibr" target="#b22">[23]</ref>. So the differences among low-level feature maps can reflect the variations at boundaries.</p><p>In this way, our PA represents instantaneous motion information. However, most human actions last for a while, ranging from seconds to minutes or even longer, so long-term temporal modeling is of great importance for video action recognition. In order to aggregate the short-term dynamics contained in PA to long-term dynamics, we design a global temporal fusion strategy called Various-timescale Aggregation Pooling (VAP). It enables the network to model long-range temporal relationships across various timescales. We further incorporate the proposed motion representation PA and the global temporal fusion strategy VAP into a unified ConvNet called Persistent Appearance Network (PAN), which achieves fast action recognition with no upfront cost.</p><p>The preliminary work is published in ACM MM 2019 <ref type="bibr" target="#b23">[24]</ref> and we have extended it in several significant aspects:</p><p>• First, we add more exploration studies on various network depths and investigate two encoding schemes. These new analyses further demonstrate the efficient motion modeling ability of our PA.</p><p>• Second, we improve the previous Various-timescale Inference Pooling (VIP) <ref type="bibr" target="#b23">[24]</ref> to VAP. In the previous VIP, the weights of inference function for score fusion are fixed hyper-parameters. As for the enhanced version VAP, we design a more intelligent weight perception scheme that learns to adaptively aggregate the recalibrated features across different timescales. Substantial new analyses are also provided to the improved method VAP.</p><p>• Third, our experiments are extended from scene-dominant datasets (Kinetics400, UCF101 and HMDB51) to more challenging temporal-dominant datasets (Something-Something-V1 &amp; V2 and Jester). Since temporal modeling is more critical than RGB scene information for recognizing actions in temporal-dominant datasets, the motion modeling ability of our proposed approach can be better demonstrated, i.e., extracting motion information directly from RGB frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Network Architecture. Spatial appearance and temporal motion are two essential ingredients for action recognition. Modeling effective spatiotemporal information still remains a challenging task. Generally, from the architectural perspective, conventional CNN-based methods can be summarized into two categories: (1) Two-stream CNNs <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> that separately process RGB frames (spatial stream) and pre-computed optical flow (temporal stream) using two 2D CNNs and finally apply late fusion strategy to obtain spatiotemporal semantics; (2) 3D CNNs <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> that jointly learn spatiotemporal features from RGB frames using 3D convolutions. However, these two paradigms are both unsatisfactory in terms of efficiency. The two-stream CNNs heavily rely on optical flow as motion representation, while the extraction of optical flow is time-consuming and storage-demanding. And the methods based on 3D CNNs is too expensive to deploy because the 3D convolution kernels require heavy computational cost.</p><p>Our PAN follows the 2D two-stream paradigm, but the input modality is only raw RGB video frames. Aiming at fast action recognition, our PAN discards the pre-computed optical flow. Instead, the motion information is distilled by introducing an efficient motion cue PA.</p><p>Motion Representation. Motion representation serves as an important cue for video-based action recognition. Optical flow is considered as a useful representation of short-term motion, many works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref> have shown that adding optical flow as another input modality can significantly boost the recognition performance. However, conventional optical flow computation approaches <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> computing optical flow in advance and storing that into the disk are absolutely inefficient. In order to alleviate the inefficiency, several recent works speed up the optical flow estimation process by delicately designing some CNN models, such as FlowNet family <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, SpyNet <ref type="bibr" target="#b18">[19]</ref> and PWC-Net <ref type="bibr" target="#b19">[20]</ref>, etc. However, these models aim at accurately estimating optical flow in advance, which is separated from the ultimate action recognition task. Other works <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b12">[13]</ref> employ an encoder-decoder network to reconstruct optical flow and this network can be jointly trained with the subsequent action recognition network. But the encoder-decoder manner still requires expensive computational cost. Thus, it remains a challenging task to find an efficient and effective motion representation for action recognition. To this end, we decide to replace optical flow with an alternative motion cue for fast action recognition, rather than make optical flow estimation more accurate.</p><p>Another line of works make efforts to find auxiliary representations of motion in an end-to-end manner <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. In this way, various input modalities are carefully designed, such as RGBdiff <ref type="bibr" target="#b11">[12]</ref>, EMV <ref type="bibr" target="#b10">[11]</ref>, dynamic image <ref type="bibr" target="#b29">[30]</ref> and Displacement Map <ref type="bibr" target="#b30">[31]</ref>, which can be computed on-the-fly. These works still cannot perform on par with optical flow in terms of action recognition accuracy. Lee et al. <ref type="bibr" target="#b31">[32]</ref> proposed MFNet, which exploits five fixed directions searching strategy to encode temporal features in a unified manner. Recently, Sun et al. <ref type="bibr" target="#b32">[33]</ref> introduced Optical Flow Guided Feature (OFF), which obtains spatial and temporal features utilizing Sobel operator and element-wise subtraction respectively with ground-truth optical flow as supervision. It should be noted that our work does not require optical flow during both the training and testing phases.</p><p>In this paper, we distill the motion cue PA at the bottom of the network with concise pixel-wise computation. Our PA can be viewed as feature-level pixel-wise variation accumulation, where we encode the output motion as a single channel saliency map reflecting small displacement at movement boundaries, which is of the same spatial resolution as the input RGB frames.</p><p>Temporal Modeling. 3D CNNs are naturally expert in temporal modeling, as 3D convolutional operators are designed to fuse both spatial and temporal information within local receptive fields. Non-local Networks <ref type="bibr" target="#b2">[3]</ref> use self-attention mechanism to capture long-range temporal correlations. Slowfast Networks <ref type="bibr" target="#b33">[34]</ref> contain two pathways that capture categorical semantics and motion semantics at slow and fast frame rate respectively, and lateral connections are employed to fuse the two pathways. As for 2D CNNs, 2D convolutional operators only focus on local spatial regions within single frame, without exchanging information among neighboring frames. Such frame-level features are prone to cause partial observations, thus temporal modeling is necessary. Several works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> use 2D CNNs to process video frames independently and then obtain video-level features by late fusion strategies. Temporal Segment Networks (TSN) <ref type="bibr" target="#b11">[12]</ref> aggregate the frame-level features through average pooling consensus function to obtain video-level representations. But average operation can not infer the temporal order or more complicated temporal relationships. Temporal Relation Networks (TRN) <ref type="bibr" target="#b35">[36]</ref> further improve the TSN to utilize temporal relations in videos. Another noteworthy work is Temporal Shift Module (TSM) <ref type="bibr" target="#b36">[37]</ref>, it enables local temporal fusion among neighboring frames with temporal shift operation.</p><p>In this paper, we devise a global temporal fusion strategy VAP with negligible parameters for long-term temporal modeling at multiple timescales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PERSISTENCE OF APPEARANCE (PA)</head><p>To lift the reliance on optical flow, we devise a novel motion cue called Persistence of Appearance (PA). In this section, we first present the theoretical derivation of PA (Sec. III-A). Then we design an efficient PA module to speed up the motion modeling procedure (Sec. III-B) in action recognition task. Afterwards, we investigate the function of PA in motion modeling by exploring two meaningful encoding schemes (Sec. III-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Theoretical Derivation of PA</head><p>As discussed in Sec. I, the small displacements at motion boundaries matter most for action recognition. Thus, given an adjacent two-frame pair, we make efforts to obtain a saliency map that highlights such small motion variations at boundaries.</p><p>For traditional optical flow, the brightness constancy constraint is defined as follows:</p><formula xml:id="formula_0">I(x, y, t) ≈ I(x + ∆x, y + ∆y, t + ∆t)<label>(1)</label></formula><p>where I(x, y, t) denotes the pixel value at the location (x, y) of a video frame at time t. As time varies from t to (t + ∆t), the spatial displacements in horizontal and vertical axis are ∆x and ∆y respectively. This constraint formulation assumes that the brightness of a point remains unchanged if it moves from (x, y) at time t to (x + ∆x, y + ∆y) at time (t + ∆t).</p><p>The optical flow can be estimated by finding the optimal solution (∆x * , ∆y * ) through optimization methods, and additional constraints, e.g., local smoothness assumption, are also considered for estimating the actual flow. We extend Eq. 1 to the feature space by replacing the image I(x, y, t) with its i-th feature map F i (x, y, t) after a specific layer:</p><formula xml:id="formula_1">F i (x, y, t) ≈ F i (x + ∆x, y + ∆y, t + ∆t)<label>(2)</label></formula><p>The difference map D between i-th feature maps is given as:</p><formula xml:id="formula_2">D i (x, y, ∆t) = F i (x + ∆x, y + ∆y, t + ∆t) − F i (x, y, t) (3)</formula><p>If we apply the optical flow constraint in feature space, D tends to have lower absolute value. However, searching the neighboring areas to find the optimal solution (∆x * , ∆y * ) in each location is time-consuming, so we do not use such a complex searching strategy. In contrast to optical flow, we only capture the motion variation at a certain point in feature space without considering the direction of the movement, which perfectly aligns with our idea of modeling the small displacements at motion boundaries because: (1) small displacements are perceived since one pixel in low-level feature map contains information of a small receptive field in input space; (2) motion boundaries are captured since the first few convolutional layers tend to capture general patterns, e.g., boundaries, texture, etc <ref type="bibr" target="#b22">[23]</ref>. So the differences among lowlevel feature maps will pay more attention to the variations at boundaries. In summary, differences in low-level feature maps can reflect small displacements of motion boundaries due to convolutional operations. Therefore, we define the i-th PA component as follows:</p><formula xml:id="formula_3">P A i (p, ∆t) = F i (p, t + ∆t) − F i (p, t)<label>(4)</label></formula><p>where p = (x, y) and i = 1, . . . , C, and C is the channel number. Thus, we can conclude that our PA is highly correlated with optical flow. This definitely provides theoretical support for its effectiveness in modeling motion information.</p><p>All the computed PA i can be further accumulated to 1 channel to manifest the motion magnitude, which can reflect the motion variations at boundaries.</p><formula xml:id="formula_4">P A = C i=1 (P A i (p, ∆t)) 2<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PA Module Design</head><p>Since our PA operates in feature space, we need to search for the best depth choice of convolutional layers (conv-layers) to generate feature maps. We define the basic conv-layer as eight 7×7 convolutions with stride=1 and padding=3, so that the spatial resolutions of the obtained feature maps are not reduced. Assume that d basic conv-layers are sequentially stacked to form the d-depth network, we experiment with 5 networks having depth of d equal to 0, 1, 2, 4 and 8. The experimental results on UCF101 split 1 dataset are depicted in <ref type="figure" target="#fig_1">Fig. 2-B</ref>. The area of the circles indicates the computational cost (FLOPs). We find that directly applying the pixel-wise differences accumulation in input space (d = 0) does not perform best. The best performance is achieved when d = 1, i.e., only one basic conv-layer is adopted. As the network goes deeper, FLOPs significantly increases and the performance degrades. This is mainly because high-level features with large receptive fields have been highly abstracted and thus may not be able to reflect small motion variations in input images. The experimental results are consistent with our claim that differences in low-level feature maps can reflect small displacements of motion boundaries, which are the most critical ingredients for recognizing actions.</p><p>As d = 1 performs best, we design a light-weighted "PA module" which only contains single basic conv-layer (eight 7×7 convolutions) to obtain low-level features and several computing operations based on Eq. 4 and Eq. 5. This module performs low-level representations comparison pixel by pixel between two adjacent frames, and outputs one saliency map (PA) reflecting small displacements of motion boundaries for further processing. This module is located at the bottom of our network, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>-B&amp;3-C (detailed architectural information will be given in Sec. IV-A).</p><p>Formally, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>-A, given two adjacent frames ∈ R H×W ×3 with H, W and 3 being their height, width and channel number. First, low-level feature maps F 1 , F 2 ∈ R H×W ×C are obtained without spatial resolution reduction. Then, the pixel-wise value difference is computed between the two feature maps with the same index i (See in Eq. 4). Finally, all the computed PA i are accumulated to 1 channel based on Eq. 5, so the result PA ∈ R H×W is two-dimensional. Therefore, in "PA module", a mapping R H×W ×3 → R H×W is established from the appearance to the dynamic motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Encoding Schemes</head><p>As elaborated above, PA is a concise motion cue focusing on the small displacements of motion boundaries between two adjacent frames. We would also like to understand the practical function of PA in motion modeling. Intuitively, PA can serve as either auxiliary input modality or spatial attention map. So here we explore two meaningful encoding schemes: PA as motion modality vs. attention map. Given m adjacent frames set</p><formula xml:id="formula_5">{I (i) } m i=1 , the corresponding low-level feature maps in PA module are defined as {F (i) } m i=1</formula><p>, and each two adjacent frames are processed to obtain total (m−1) PA:</p><formula xml:id="formula_6">{P A (i) } m−1 i=1</formula><p>. Assuming that the input modality to the subsequent backbone network is O, so in this subsection, we will discuss two encoding schemes e 1 , e 2 that carry out the mapping procedure e i : P A → O, i.e., aggregating P A to O.</p><p>1) PA as motion modality. This is the most straightforward scheme to directly exploit motion information contained in PA. Generally, for action recognition methods, taking the stacked optical flow as input to capture motion information can significantly boost the performance. Since PA also has the capability of describing the pixel-level apparent motion information between two continuous RGB frames, we use stacked PA as input modality as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>-C. This scheme can be represented as:</p><formula xml:id="formula_7">O 1 = e 1 (P A) = m−1 Υ i=1 (P A (i) )<label>(6)</label></formula><p>Here, we define</p><formula xml:id="formula_8">m−1 Υ i=1</formula><p>(·) as the cumulative channel concatenation function that chronologically concatenate the input tensor along the channel dimension. Thus, if the input tensor P A (i) ∈ R H×W ×1 , then the output tensor O 1 ∈ R H×W ×(m−1) .</p><p>2) PA as attention. Human perception researches <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> suggest that instantaneous motion can attract attention. Recent video analysis works benefit a lot under the attention guidance of motion captured by optical flow, such as video salient object detection <ref type="bibr" target="#b39">[40]</ref>, video captioning <ref type="bibr" target="#b40">[41]</ref>, etc. Motivated by this, we attempt to exploit motion information in PA to emphasize some important regions in appearance feature maps, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>-D. This PA-guided spatial attention scheme is defined as follows, we employ PA with a sigmoid activation to attend the corresponding mean feature map:</p><formula xml:id="formula_9">O 2 = e 2 (P A) = m−1 Υ i=1 (σ(P A (i) ) µ(F (i) ))<label>(7)</label></formula><p>where σ(·) is a sigmoid function and µ(·) returns the mean value of the input feature maps along the channel dimension. denotes element-wise multiplication, so if the input tensor</p><formula xml:id="formula_10">P A (i) ∈ R H×W ×1 and F (i) ∈ R H×W ×C , then µ(F (i) ) ∈ R H×W ×1 and O 2 ∈ R H×W ×(m−1) .</formula><p>Which encoding scheme is better? We compare the performance of the PA module using these two encoding schemes in the aspect of their runtime efficiency and action recognition accuracy on UCF101 split 1 dataset. The results are shown in <ref type="table" target="#tab_1">Table I</ref>. To measure the efficiency, we consider computational cost (FLOPs), the number of parameters (#Param) and inference speed (Speed) of the PA module. To evaluate the performance of these two encoding schemes on action recognition task, we follow the TSN manner: firstly frames are sampled from evenly divided video segments, then these frames are fed into the PA module and backbone CNN (ResNet-50) sequentially, finally the output activations are averaged as the final prediction scores. More implementation details are in the supplementary material.</p><p>The results in <ref type="table" target="#tab_1">Table I</ref> clearly indicate that encoding scheme e 1 , directly exploiting the motion information contained in PA, performs better. It has not only fewer FLOPs but also higher runtime speed and superior recognition performance. The number of parameters of the two encoding schemes are the same, because the 1.184K parameters are completely from PA module, and the subsequent encoding procedure does not introduce any extra learnable parameters. The more FLOPs and lower speed of e 2 is mainly caused by the sigmoid function and element-wise multiplication. Notably, e 2 also degrades the accuracy by 1.5%. We hypothesize that when using appearance-dominant features (i.e., appearance feature maps) as input, the features must secure integral regions of appearance to represent the semantic information for the video category. However, for e 2 , attending appearance feature maps with PA will highlight the motion boundaries, leading to the imbalanced appearance responses both inside and at the boundaries of the moving objects, thus e 2 is limited in terms of such integrity. Encoding scheme e 1 , on the contrary, only utilizes motion-dominant features (i.e., PA), so there is no need to consider the appearance integrity. Therefore, based on the above observations, we employ e 1 (i.e., directly exploit PA as input motion modality) as the default encoding scheme in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PERSISTENT APPEARANCE NETWORK (PAN)</head><p>Our primary objective in this paper is to realize fast action recognition in real-time scenarios, so we propose the Persistent Appearance Network (PAN). In this section, we first give an architectural overview of our PAN framework (Sec. IV-A). Then we introduce our VAP method, a temporal feature aggregation strategy, applied within PAN framework. It assists learning long-term video-level representations by integrating information across various timescales (Sec. IV-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. PAN F ull and PAN Lite</head><p>The two network variants of our proposed Persistent Appearance Network (PAN) is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>-B&amp;3-C, namely PAN Full and PAN Lite respectively. They have identical sampling strategy but model the spatiotemporal features in different ways. Their sampling strategy is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>-A: the input video sequence V is firstly divided into N segments with equal length {S t } N t=1 . And m adjacent frames are randomly chosen from each segment as a "m-frame stack": {I t[m] } N t=1 , the first frames of each "m-frame stack" are denoted as {I t } N t=1 . The main difference between both variants are analyzed below.</p><p>1) PAN F ull : separate and accurate. As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>-B, this network is composed of dual branches: RGB branch and PA branch, capturing the spatial and temporal features separately. The RGB branch encodes the spatial appearance information. It takes the selected N frames {I t } N t=1 as input and processes them to obtain frame-level features through the backbone network H B (blue blocks). Then these obtained features are further aggregated as video-level features using VAP module H V AP . Mathematically, the output spatial features y s of RGB branch can be written as:</p><formula xml:id="formula_11">y s = H V AP (H B ({I t } N t=1 ))<label>(8)</label></formula><p>The other PA branch distills apparent motion information purely from adjacent RGB frames (i.e., appearance information). Firstly, N stacks of m adjacent frames {I t[m] } N t=1 are transformed to motion cue PA after the PA module H P A . Then the temporal features y t are obtained through the backbone network H B and VAP module H V AP :</p><formula xml:id="formula_12">y t = H V AP (H B (e(H P A ({I t[m] } N t=1 ))))<label>(9)</label></formula><p>where e(·) is the encoding scheme described in Sec. III-C.</p><p>Following the common practice <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, we merge the branch-level scores to obtain final prediction of the whole video through score fusion strategy, i.e., calculating the weighted average scores from the two branches.</p><p>2) PAN Lite : unified and light-weighted. As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>-C, our unified network PAN Lite stacks RGB and PA together and feeds them through the backbone network, allowing the network to decide itself how to extract the spatiotemporal information. Given N sampled m-frame stacks {I t[m] } N t=1 and their first frames {I t } N t=1 , the output y st can be written as:</p><formula xml:id="formula_13">y st = H V AP (H B (I t , e(H P A ({I t[m] } N t=1 ))))<label>(10)</label></formula><p>where (·, ·) is the channel concatenation operation and H P A , H B and H V AP indicate PA module, backbone network and VAP module, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Various-timescale Aggregation Pooling (VAP)</head><p>Long-term temporal modeling is of great importance for the video understanding task as discussed in Sec. II. In this paper, we devise a temporal fusion strategy called Various-timescale Aggregation Pooling (VAP), which adaptively emphasizes expressive features and suppresses less informative ones by observing global information across various timescales. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>-B&amp;3-C, VAP module is adopted at the top of each network. Overall, VAP contains two main steps as illustrated below.</p><p>(A) Specific-timescale Pooling. As shown in the top part of <ref type="figure" target="#fig_4">Fig. 4</ref>, assume that the original video is divided into N segments, and the sampled video frames are fed into the backbone 2D CNN to generate d-dim feature vectors f : {f 1 , f 2 , . . . , f N }, where f i ∈ R d . In order to temporally integrate these N features without overlapped scope, we adopt dilated max pooling over the time dimension, where the dilation rate controls the spacing between the kernel points. For brevity, this pooling function can be expressed . . . as maxpool (ks,st,dr) {activations}, where ks, st, dr refer to kernel size, stride and dilation rate respectively. Accordingly, the k-timescale pooling is defined as follows:</p><formula xml:id="formula_14">v k = maxpool ( N k ,1,k) {f 1 , f 2 , . . . , f N }<label>(11)</label></formula><p>where k is a positive integer s.t. N k ∈ Z. By convention, we use pyramidal timescale settings (i.e., k = 2 0 , 2 1 , . . . , 2 log2(N )−1 ). After Eq.11, the time span changes from N to k. So 2 0 + 2 1 + . . . + 2 log2(N )−1 = N − 1 timescale-level features are obtained in total (i.e., v ∈ R (N −1)×d ).</p><p>(B) Various-timescale Aggregation. Our basic idea is to fuse temporal information at each timescale by weighted timescale-wise aggregation. As shown in the bottom part of <ref type="figure" target="#fig_4">Fig. 4</ref>, given that v ∈ R T ×d represents the total T pooled features at different timescales, we first shrink global spatial semantics in each feature into a temporal descriptor reflecting the corresponding timescale-wise statistics. Thus, the output z ∈ R T ×1 can be expressed by:</p><formula xml:id="formula_15">z = F sh (v) = 1 d d i=1 v(i)<label>(12)</label></formula><p>To capture the cross-timescale interdependencies, we adopt a concise nonlinearity learning mechanism with a softmax activation for weight perception:</p><formula xml:id="formula_16">w = F wp (z, W ) = sof tmax(W 2 (δ(W 1 z)))<label>(13)</label></formula><p>where δ denotes the ReLU function, W 1 ∈ R αT ×T and W 2 ∈ R T ×αT (α is the expansion ratio) are the learnable parameters of two fully-connected (FC) layers. The output of F wp function is the weight vector w ∈ R T ×1 . The final global video-level representation f g ∈ R d of the proposed VAP is obtained by firstly rescaling v with the weight vector w and then aggregate the recalibrated features along T dimension.</p><formula xml:id="formula_17">f g = F agg (F scale (w, v)) = sum(wv)<label>(14)</label></formula><p>In particular, for action recognition task, the prediction scores s are obtained by:</p><formula xml:id="formula_18">s = F pred (f g , W ) = W 3 f g<label>(15)</label></formula><p>where W 3 ∈ R c×d , c is the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we first introduce the evaluation datasets and implementation details. Then in ablation studies, we investigate the importance of our proposed motion cue PA and various-timescale aggregation strategy VAP for real-time action recognition. During this investigation, we also explore some basic settings. Extensive results show the superior performance achieved by PAN compared with baselines and other state-of-the-art methods, on both temporal-dominant datasets and scene-dominant datasets. Finally, we visualize the proposed motion cue PA to qualitatively justify its superiority in motion modeling and discuss the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>Datasets. We evaluate our approach on six challenging benchmarks for action recognition. They can be grouped into two categories: (a) Temporal-Dominant Datasets, including Something-Something-V1 &amp; V2 <ref type="bibr" target="#b3">[4]</ref> and Jester <ref type="bibr" target="#b5">[6]</ref>. Recognizing actions in these datasets requires strong temporal modeling ability, as many action classes are symmetrical, e.g., "Moving something up" and "Moving something down". Something-Something datasets <ref type="bibr">(</ref>  <ref type="bibr" target="#b4">[5]</ref>, UCF101 <ref type="bibr" target="#b41">[42]</ref> and HMDB51 <ref type="bibr" target="#b42">[43]</ref>. RGB scene information in these datasets is more critical than temporal relations for action recognition. Kinetics400 is a large-scale action recognition benchmark including ∼300k videos with 400 human action classes. The performances are evaluated with the top-1 and top-5 accuracies. The UCF101 dataset includes 13,320 video clips with 101 action classes and the HMDB51 dataset contains 6,766 videos with 51 action categories. For these two datasets, the mean class accuracy over the three official splits is calculated as the final result.</p><p>Input &amp; Backbone. The input modality of our PAN is only raw RGB frames. We set N = 8 and m = 4 as default, thus 8 "4-frame stack" are sampled from a video. For PAN Full , the first frame in each "4-frame stack" will be selected and fed into RGB branch, meanwhile, all the frames will be fed into the PA branch. For PAN Lite , we take all the 8 sampled "4-frame stack" as input. For comparative purposes, we use ResNet-50 as the default backbone like other state-of-the-art methods <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Since TSM <ref type="bibr" target="#b36">[37]</ref> can exchange information between neighboring frames at zero cost, we opt to inject TSM into our backbone to enable local temporal fusion. Note that TSM module is orthogonal to our PA (efficient motion representation) and VAP (global temporal fusion strategy).</p><p>Training &amp; Testing. For relatively large-scale datasets (Something-Something-V1 &amp; V2, Jester and Kinetics400), the training parameters of PAN Lite and the PA branch of PAN Full are: initial learning rate 0.01 (total 80 epochs, divided by 10 after every 30 epochs). As for RGB branch of PAN Full , we also set initial learning rate as 0.01 (but total 50 epochs, decreases at epoch <ref type="bibr">20 &amp; 40)</ref>. We train our PAN using SGD algorithm, with weight decay 1e-4, mini-batch size 64. ImageNet pre-trained weights are employed for initialization. While for small-scale datasets (UCF101 and HMDB51) that are easy to over-fit, we use Kinetics400 for pre-training and scale the training epochs by half. During testing procedure, we report "view" (spatial crops × temporal clips) used in <ref type="bibr" target="#b33">[34]</ref>. For temporal-dominant datasets, we only use 1 view (1 center spatial '224×224' crop × 1 temporal clip) unless otherwise specified. While for scene-dominant datasets, we use 2 views (full resolution image with shorter side 256 pixels × 2 temporal clips), which is much more efficient than the common practice (30 views) in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Especially, for PAN Full , we average the class prediction scores of the RGB and PA branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Studies for PA</head><p>Following the common practice <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b45">[46]</ref>, all experiments in this ablation study are conducted on UCF101 split 1. In this subsection, we prove that our designed PA is a significant motion representation by performing in-depth studies to answer the following two questions:</p><p>Q1: Is PA efficient, effective and flexible enough? As mentioned in Sec. I, our main target in this paper is to design a motion representation alternative to optical flow with the merits of efficient, effective and flexible. We compare the PA with other mainstream optical flow extraction methods from the perspective of efficiency and effectiveness for action recognition task, including conventional optical flow <ref type="bibr" target="#b25">[26]</ref> and CNN-based estimated optical flow <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> methods. The comparison results are listed in <ref type="table" target="#tab_1">Table II</ref>. Note that all the network settings are the same and more details can be found in the supplementary material. Surprisingly, compared with conventional optical flow method TV-L1 <ref type="bibr" target="#b25">[26]</ref>, our proposed PA achieves over 1000× faster speed (8196fps vs. 8fps) as well as 1.3% higher action recognition accuracy. Furthermore, PA consistently outperforms all the seminal CNN-based optical flow estimation methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, suggesting that PA has fast (8196fps) and effective (89.5%) motion modeling ability.</p><p>To demonstrate the flexibility of our proposed PA, we deploy different backbone networks and compare their results. Here, we choose three widely adopted backbone networks for deep action recognition, including BN-Inception <ref type="bibr" target="#b46">[47]</ref> (2D CNN), ECO <ref type="bibr" target="#b47">[48]</ref> (mixed 2D-3D CNN) and 3D-ResNet <ref type="bibr" target="#b45">[46]</ref> (3D CNN). Experimental results on UCF101 split 1 are tabulated in <ref type="table" target="#tab_1">Table III</ref>. The results show that the selected backbone networks all significantly benefit from our PA with considerable accuracy improvements, demonstrating the flexibility of our proposed PA.</p><p>Therefore, compared with optical flow, our proposed PA is much faster to compute, superior in performance. And it is also flexible to implement.</p><p>Q2: Can PA represents apparent motion? A key intuition for designing PA is that it can capture apparent motion by focusing more on moving boundaries without the heavy precomputation of optical flow. In order to investigate whether our PA performs similarly to the optical flow, we compare the performance using three different input modalities: RGB, PA and optical flow. With each modality as input, we train the network first on UCF101 split 1, then we test the trained models to get the prediction scores for all the 101 classes. Finally, we plot the top 5 classes that show the largest differences in recognition accuracy between RGB and PA. For reference, the performance of optical flow in these classes is also depicted, as shown in the top part of <ref type="figure" target="#fig_5">Fig. 5</ref>. In this figure, we can clearly see that when PA outperforms RGB, the optical flow is also superior to RGB and vice versa. This indicates that PA can help the network learn patterns different from RGB but similar to optical flow.</p><p>After analyzing the recognition results, we find that most "Throw Discus" videos are misclassified as "Hammer Throw" when using just RGB modality. To demonstrate that our PA can represent apparent motion, we visualize the three modalities of these two classes in the bottom part of <ref type="figure" target="#fig_5">Fig. 5</ref>. In RGB images, it is unclear whether the athletes are holding the discus or the hammer due to the interference of background, illumination, etc, so distinguishing the two classes using only RGB modality may cause confusion. Encouragingly, our PA is able to highlight the moving objects (in this case human and the object in hands) as the optical flow does, which is crucial for differentiating between videos that look much alike.</p><p>Therefore, this result is in accordance with our idea that the PA, focusing on capturing small displacements at moving boundaries, can be used as apparent motion representation. See Sec. V-E for more visualization results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies for VAP</head><p>As introduced in Sec. IV-B, our proposed VAP is a temporal aggregation strategy that can learn to exploit global information across different timescales. In this subsection, we conduct ablation experiments to gain more insights about the effect of aggregating semantics across various timescales. "Typing" from UCF101  Q1: Is various-timescale aggregation effective? Following the common practice <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b45">[46]</ref>, we perform ablation studies on UCF101 split 1 dataset using different temporal aggregation strategies. N = 8 frames are sampled as input and ResNet-50 is used as backbone network. The results are shown in <ref type="table" target="#tab_1">Table IV</ref>. Avg Pooling here is referred to the conventional consensus function introduced in [12] that averages the output logits to get the final prediction. First, we explore the impact of the expansion ratio α ∈ {1, 2, 4, 8}. The accuracies between various expansion ratios exhibit stable performances, consistently outperforming the Avg Pooling baseline. This indicates that VAP is not sensitive to the hyperparameter setting α. As α = 4 achieves the best performance, we choose 4 as the default expansion ratio in VAP. Second, VAP is considered as an enhanced version of our previous work VIP <ref type="bibr" target="#b23">[24]</ref>. Different from VIP that exploits preset weights for inference, VAP learns the weights by adaptively using global information at different timescales. We also do comparison between these two variants. VAP achieves higher recognition accuracy than VIP (88.5% vs. 87.9%), showing the effectiveness of the enhancement in this paper. Furthermore, VAP and VIP utilizing various-timescale semantics consistently outperform baseline by 2.0% and 1.4% respectively, indicating the significance of long-term temporal modeling through various-timescale aggregation.</p><p>Q2: What does the VAP learn? We expect VAP module to let the network capture various-timescale interdependencies. To verify that this is indeed achieved, we output the weight results w learned from the weight perception Eq. 13. Here we consider both scene-dominant dataset (UCF101) and temporaldominant dataset (Something-Something-V1). N = 8 frames are sampled as input, thus k ∈ {1, 2, 3} and total 7 timescale-level features v are calculated according to Eq. 11. ResNet-50 is used as backbone and VAP module is adopted at the top of the network. We first train this network on the two datasets respectively, then we test the two trained models separately to obtain the corresponding weight values w of different timescales. <ref type="figure" target="#fig_6">Fig. 6</ref> shows two selected video sequences and the timescale-wise weights w with respect to the two action classes: "Typing" from UCF101 and "Moving something down" from Something-Something-V1 dataset. More visualization results can be found in the supplementary material. We can observe that the weight changes for different videos at various timescales, suggesting that VAP can adaptively guide the network to emphasize some expressive features while suppressing other less informative ones. Moreover, we can clearly see that VAP trained on UCF101 dataset tends to learn similar weights for all timescales, while the weights varies widely on Something-Something-V1. We speculate that this has to do with the fact that temporal relationships in Something-Something-V1 is more crucial than that in UCF101 for recognition <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b48">[49]</ref>, so VAP module trained on Something-Something-V1 prefers treating all timescales differently. This observation further proves that various-timescale aggregation is an important ingredient for video temporal semantics modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with the State-of-the-Arts</head><p>To validate the efficacy of our approach, we compare PAN with newly published state-of-the-art methods on six datasets. Since our proposed PAN focuses on temporal semantics modeling, we mainly analyze the results on temporal-dominant datasets (Something-Something-V1 &amp; V2 and Jester). We also evaluate PAN on various scene-dominant datasets (Kinet-ics400, UCF101 and HMDB51) to show its consistent strong performance.</p><p>1) Temporal-Dominant Datasets.</p><p>Something-Something-V1 &amp; V2 and Jester. <ref type="table" target="#tab_6">Table V</ref> shows the comparison results with the state-of-the-arts on Something-Something-V1 &amp; V2 and Jester datasets. We group the listed methods into two sections (separated by solid line) according to their backbone types: 3D CNN based methods <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b47">[48]</ref> and 2D CNN based methods <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. It is clear that with complex 3D convolutional operations, the FLOPs of I3D <ref type="bibr" target="#b43">[44]</ref> and ECO <ref type="bibr" target="#b47">[48]</ref> are much higher than the 2D CNN based methods. Our PAN surpasses all the 3D CNN Following <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b36">[37]</ref>, we also average the class prediction results of PAN Lite and PAN Full to form our ensemble version PAN En . Here, two views (1 full-resolution with 256 shorter size × 2 temporal clips) are used for inference. Compared with the optical flow based methods TRN RGB+Flow (42.0%), ECO En Lite RGB+Flow (49.5%) and TSM RGB+Flow (52.6%), our PAN En (53.4%) provides +11.4%, +3.9% and +0.8% top-1 accuracy improvements on Something-V1, respectively. As listed in <ref type="table" target="#tab_1">Table II,</ref>  Furthermore, when exploiting a deeper backbone ResNet-101, our PAN brings the current state-of-the-art results to a whole new level (55.3% on Something-V1, 66.5% on Something-V2, 97.4% on Jester). Notably, our method only takes RGB frames as input and thus is free of optical-flow pre-computation.</p><p>2) Scene-Dominant Datasets. Kinetics400, UCF101 and HMDB51. We also compare the performance of our PAN with other recent state-of-theart methods on three scene-dominant datasets: Kinetics400, UCF101 and HMDB51. The results are summarized in Table VI. Our method achieves very competitive performance on these datasets, where most actions can be classified by a single frame (e.g., "Typing" action shown in <ref type="figure" target="#fig_6">Fig. 6</ref>). Our PAN outperforms most of the heavy 3D CNN based architectures (first part in <ref type="table" target="#tab_1">Table VI</ref>) <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b15">[16]</ref> using 2D CNN as backbone, indicating that it can enhance the traditional 2D CNN with low cost. For 2D CNN based methods (second part), when pursuing high accuracy, they usually sample many views (spatial crops × temporal clips) per video for inference, thus leading to expensive computational cost. For example, TSM <ref type="bibr" target="#b36">[37]</ref> and TEA <ref type="bibr" target="#b44">[45]</ref> sample 30 views, their FLOPs are even higher than that of 3D CNN based architectures. In contrast, we only use 2 views for efficiency concerns (detailed in Sec. V-A) and achieve superior performance. For example, compared with baseline TSM <ref type="bibr" target="#b36">[37]</ref>, our PAN En achieves +1.2%, +1.3% and +3.8% top-1 accuracy improvements on Kinetics400, UCF101 and HMDB51 respectively with only  ∼20% computational cost (270G vs. 1290G). Notably, our PAN is even superior to the optical flow based methods (third part) except for I3D <ref type="bibr" target="#b15">[16]</ref>. Since I3D is based on heavy 3D convolutions and takes pre-computed optical flow as input, its computational cost is much more expensive than our PAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization and Discussion</head><p>In this section, we present the visualization results of two adjacent frames and their corresponding PA and optical flow in <ref type="figure" target="#fig_8">Fig. 7</ref>. PA can model motion information as optical flow does, but is more advanced for action recognition task with its special characteristic. Similar to optical flow, the computed PA highlights the moving objects and suppresses the stationary background, which visually demonstrates that our PA well characterizes the instantaneous motion. Differently, our PA focuses more on the motion boundaries instead of the whole moving areas. This aligns with our motivation that modeling small displacements of motion boundaries matters most for action recognition task. For more visualization results, please refer to our supplementary material.</p><p>Visually, our PA contains more noise than optical flow. We speculate that it is because the regularization term, penalizing high variations to obtain smooth displacement fields, is not applied for efficiency concerns, while it is used in conventional optical flow method <ref type="bibr" target="#b25">[26]</ref>. Although our PA has been proved superior to several optical flow methods in terms of motion modeling efficiency and action recognition accuracy, we have not fully exploited its potential because of the noise interference. So in the future, we plan to design the noise mitigation method to obtain more smooth PA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we shed light on fast action recognition by lifting the reliance on optical flow. We design a concise motion cue called Persistence of Appearance (PA) to capture motion information directly from RGB frames. In contrast to optical flow, our PA is more effective by focusing more on modeling the small displacements of motion boundaries, and it is more efficient by simply calculating the pixel-wise differences between two adjacent frames in feature space. Its efficiency, effectiveness and flexibility have been well elaborated by extensive theoretical support (Sec. III-A), experimental support (Sec. V-B) and visualization support (Sec. V-E). The motion modeling speed of our PA reaches 1000× faster than that of conventional optical flow method (8196fps vs 8fps). To further aggregate the short-term dynamics in PA to long-term dynamics, we also propose a temporal fusion strategy named Various-timescale Aggregation Pooling (VAP), which enables the network to capture long-range varioustimescale interdependencies. The proposed PA and VAP are finally incorporated to form a unified framework call Persistent Appearance Network (PAN). Extensive experiments on six challenging benchmarks demonstrate that our proposed PAN achieves the state-of-the-art recognition performance. Most importantly, it significantly accelerates the inference process of action recognition with the powerful motion cue PA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>−Supplementary Material− PAN: Towards Fast Action Recognition via</head><p>Learning Persistence of Appearance Can Zhang, Yuexian Zou*, Senior Member, IEEE, Guang Chen, and Lei Gan I. IMPLEMENTATION DETAILS <ref type="table" target="#tab_1">OF TABLE I AND TABLE II</ref> In this section, we describe the implementation details for the comparative experiments of two encoding schemes (see <ref type="table" target="#tab_1">Table  I</ref>) and the comparative experiments of PA with other mainstream optical flow computation methods (see <ref type="table" target="#tab_1">Table II</ref>). To have an apple-to-apple comparison, the implementation details of all the experiments are identical. We set N = 8 and m = 6, i.e., 8 sampled "6-frame stack" RGB frames are sampled as input.</p><p>To measure the efficiency of the motion modeling methods, we use three evaluation metrics including the computational cost (FLOPs), the number of parameters (#Param) and inference speed (Speed). All the efficiency metrics in these two tables are measured on a single NVIDIA TITAN X GPU with 1 mini batch size and 1 CPU thread. As the I/O is most relevant to hardware and operating system, the runtime speeds are reported without considering I/O.</p><p>To evaluate the performance of the motion representation (encoded PA or optical flow) on action recognition task, we obtain accuracy results on UCF101 split1 using the motion representation as the input modality. Following the TSN manner, we first sample frames from evenly divided video segments, then these frames are fed into the motion modeling module and backbone CNN (ResNet-50) sequentially. Finally, the output activations are averaged as the final prediction scores. Note that all the accuracy results in both tables are measured with the same network settings: initial learning rate: 0.001; total epochs: 80 (decreases lr by 10 after every 30 epochs); mini batchsize: 16; dropout ratio: 0.7; pretrain: ImageNet.</p><p>II. VISUALIZATION RESULTS OF PA "ShavingBeard" from UCF101 dataset.</p><p>"Pushing something so it spins" from Something-V1 dataset.</p><p>"Typing" from UCF101 dataset.</p><p>"Showing something to the camera" from Something-V1 dataset.</p><p>"CliffDiving" from UCF101 dataset.</p><p>"Moving something closer to something" from Something-V1 dataset.</p><p>"BaseballPitch" from UCF101 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"Plugging something into something"</head><p>from Something-V1 dataset.</p><p>"BenchPress" from UCF101 dataset.</p><p>"Bending something so that it deforms" from Something-V1 dataset.</p><p>"BlowDryHair" from UCF101 dataset.</p><p>"Spinning something that quickly stops spinning" from Something-V1 dataset.</p><p>"Drumming" from UCF101 dataset.</p><p>"Lifting something with something on it"</p><p>from Something-V1 dataset.</p><p>"ParallelBars" from UCF101 dataset.</p><p>"Spinning something that quickly stops spinning"</p><p>from Something-V1 dataset.</p><p>"PushUps" from UCF101 dataset.</p><p>"Turning the camera upwards while filming something" from Something-V1 dataset.</p><p>"CuttingInKitchen" from UCF101 dataset.</p><p>"Tipping something over" from Something-V1 dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Comparison of the estimated optical flow using conventional EPE loss (B) and the new flow fine-tuned by Cross-entropy loss (C). The part (D) is obtained by calculating the Euclidean distance at each pixel between (B) and (C). In (D), the optical flow vectors change more around human motion boundaries. Best viewed in color and zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(A) Illustrations of Persistence of Appearance (PA) design. (B) Depth of conv-layers in "PA Module" vs. accuracy. (C) Encoding scheme e 1 : PA as motion modality. (D) Encoding scheme e 2 : PA as attention.Here we only provide exemplars that processing two adjacent frames (i.e., m = 2) for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The overall architecture of Persistent Appearance Network (PAN). It has two network variants: (B) PAN Full -"divide and conquer", i.e., capturing the spatial and temporal semantics separately; (C) PAN Lite -"unified and efficient", i.e., extracting the spatial and temporal semantics simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Various-timescale Aggregation Pooling (VAP). Top part: (A) Specifictimescale Pooling. Bottom part: (B) Various-timescale Aggregation. The numbers in curly brackets indicate which f are involved in generating the timescale-wise features v (Eq. 11). Best viewed in color and zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Top part: The top 5 action classes that show the greatest difference in terms of accuracy between the input modality RGB and PA, and the performance of optical flow in these classes is also plotted for comparison. Bottom part: Input modality visualization of the two classes that are most easily misclassified by RGB. Best viewed in color and zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Visualization of two selected video sequences and their timescale-wise weights w calculated by Eq. 13. The sampled frames are processed by the backbone individually to obtain frame-level features f . The numbers in curly brackets indicate which f are involved in generating the timescale-wise features v (Eq. 11). Different colors (red, orange &amp; yellow) are employed to distinguish different timescales. Best viewed in color and zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>the mainstream optical flow extraction methods are computationally intensive, even the most lightweighted FlowNetS model needs 356G FLOPs and extra storage. Taking this cost into account, the optical flow based methods are expensive in both time and space. Surprisingly, the total cost of our PAN En model is only ∼270G. This observation confirms that our proposed PAN, with an efficient motion cue PA, can effectively accelerate the video representation learning process by lifting the reliance on optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Visualization of two adjacent frames and their corresponding PA and optical flow. Left: BodyWeightSquats. Middle: ApplyEyeMakeup. Right: PushUps. Best viewed in color and zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, Y. Zou, G. Chen and L. Gan are with the School of Electrical and Computer Engineering, Peking University, China (e-mail: {zhangcan, zouyx, guangchen, ganlei}@pku.edu.cn).</figDesc><table><row><cell>OF Module</cell><cell>AR Module</cell><cell>OF Module</cell><cell>AR Module</cell></row><row><cell>Image Overlay</cell><cell>Optical Flow</cell><cell>New Flow</cell><cell>Euclidean Distance</cell></row><row><cell>Persistence of Vision</cell><cell>using EPE loss</cell><cell>fine-tuned by CE loss</cell><cell>between (B) and (C)</cell></row><row><cell>(A)</cell><cell>(B)</cell><cell>(C)</cell><cell>(D)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PA</head><label>I</label><figDesc>AS MOTION CUE vs. PA AS ATTENTION. ACCURACIES ARE EVALUATED ON UCF101 SPLIT 1 WITH THE SAME NETWORK SETTINGS.</figDesc><table><row><cell>Encoding Schemes</cell><cell cols="3">Efficiency Metrics</cell><cell>Accuracy</cell></row><row><cell></cell><cell cols="2">FLOPs #Param</cell><cell>Speed</cell><cell></cell></row><row><cell>e 1 : PA as motion cue e 2 : PA as attention</cell><cell>2.868G 2.884G</cell><cell>1.184K 1.184K</cell><cell>8196fps 6752fps</cell><cell>89.5% 88.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II COMPARISON</head><label>II</label><figDesc>RESULTS OF PA WITH MAINSTREAM OPTICAL FLOW COMPUTATION METHODS. ACCURACIES ARE EVALUATED ON UCF101 SPLIT 1 WITH THE SAME NETWORK SETTINGS EXCEPT THE INPUT MODALITY.TABLE III FLEXIBILITY VALIDATION OF PA. OUR MOTION CUE PA CONSIDERABLY IMPROVES ALL THE BASELINES ON UCF101 SPLIT 1. ALL THE EXPERIMENTS ARE CONDUCTED USING THE SAME NETWORK SETTINGS.</figDesc><table><row><cell>Motion Rep. Method</cell><cell cols="3">Efficiency Metrics</cell><cell>Accuracy</cell></row><row><cell></cell><cell cols="2">FLOPs #Param</cell><cell>Speed</cell><cell></cell></row><row><cell>TV-L1 [26] FlowNetS [17] FlowNetC [17] FlowNet2.0 [18]</cell><cell>-356G 444G 2019G</cell><cell>-38.7M 39.2M 162.5M</cell><cell>8fps 204fps 151fps 25fps</cell><cell>88.2% 86.8% 87.3% 87.7%</cell></row><row><cell>PA (Ours)</cell><cell>2.868G</cell><cell>1.184K</cell><cell>8196fps</cell><cell>89.5%</cell></row><row><cell>Baseline</cell><cell>Pre-train</cell><cell>Modality</cell><cell>Acc.</cell><cell>∆Acc.</cell></row><row><cell>BN-Inception [47]</cell><cell>ImageNet</cell><cell>RGB RGB+PA</cell><cell>85.6% 91.3%</cell><cell>+5.7%</cell></row><row><cell>ECO [48]</cell><cell>Kinetics</cell><cell>RGB RGB+PA</cell><cell>90.4% 95.1%</cell><cell>+4.7%</cell></row><row><cell>3D ResNet-18 [46]</cell><cell>Kinetics</cell><cell>RGB RGB+PA</cell><cell>83.9% 86.8%</cell><cell>+2.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV EXPLORATION</head><label>IV</label><figDesc>OF DIFFERENT EXPANSION RATIO α AND COMPARISON AMONG DIFFERENT TEMPORAL AGGREGATION STRATEGIES.</figDesc><table><row><cell>Aggregation Strategy</cell><cell>Accuracy</cell></row><row><cell>Avg Pooling (baseline)</cell><cell>86.5%</cell></row><row><cell>VIP [24] (Our prev.)</cell><cell>87.9%</cell></row><row><cell>VAP(α = 1) VAP(α = 2)</cell><cell>88.4% 88.3%</cell></row><row><cell>VAP(α = 4) VAP(α = 8)</cell><cell>88.5% 88.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V COMPARISON</head><label>V</label><figDesc>RESULTS OF PAN WITH OTHER STATE-OF-THE-ART METHODS ON SOMETHING-SOMETHING V1 &amp; V2 AND JESTER DATASETS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Something-V1 Something-V2</cell><cell cols="2">Jester</cell></row><row><cell>Methods</cell><cell>Backbone</cell><cell>Flow?</cell><cell>#Frame</cell><cell>FLOPs×views</cell><cell>Val</cell><cell>Val</cell><cell>Val</cell><cell>Val</cell><cell>Val</cell><cell>Val</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Top1</cell><cell>Top5</cell><cell>Top1</cell><cell>Top5</cell><cell cols="2">Top1 Top5</cell></row><row><cell>I3D [44] NL I3D [44] NL I3D + GCN [44] ECO En Lite [48] ECO En Lite RGB+Flow [48]</cell><cell>3DResNet-50 3DResNet-50+GCN BNInception +3DResNet-18</cell><cell cols="2">32×2 92 ! 92+92×6  †</cell><cell>153G×2 168G×2 303G×2 267G×1 N/A</cell><cell>41.6 44.4 46.1 46.4 49.5</cell><cell>72.2 76.0 76.8 --</cell><cell>-----</cell><cell>-----</cell><cell>-----</cell><cell>-----</cell></row><row><cell>TSN 8F [12] TRN-Multiscale [36] TRN RGB+Flow [36] TSM 8F [37] TSM 16F [37] TSM RGB+Flow [37] TEA 8F [45] TEA 16F [45]</cell><cell>BNInception ResNet-50 BNInception ResNet-50 BNInception ResNet-50 ResNet-50</cell><cell cols="2">8 8 8 8 ! 8+8×6  † 8 16 ! 16+16×6  † 8 16</cell><cell>16G×1 33G×1 16G×1 33G×1 N/A 33G×1 65G×1 N/A 35G×30 70G×30</cell><cell>19.5 19.7 34.4 38.9 42.0 45.6 47.2 52.6 51.7 52.3</cell><cell>-46.6 -68.1 -74.2 77.1 81.9 80.5 81.9</cell><cell>-27.8 48.8 -55.5 59.1 63.4 66.0 --</cell><cell>--77.6 -83.1 -88.5 90.5 --</cell><cell>-81.0 95.3 --94.4 95.3 ---</cell><cell>-99.0 ---99.7 99.8 ---</cell></row><row><cell>PAN Lite (Ours) PAN Full (Ours) PAN En (Ours) PAN En (Ours)</cell><cell>ResNet-50+TSM ResNet-101+TSM</cell><cell></cell><cell cols="2">8+8×4  † 8+8×4  † (8+8×4)×2 (8+8×4)×2 (85.6G+166.1G)×2 35.7G×1 67.7G×1 (46.6G+88.4G)×2</cell><cell>48.0 50.5 53.4 55.3</cell><cell>76.1 79.2 81.1 82.8</cell><cell>60.8 63.8 66.2 66.5</cell><cell>86.7 88.6 90.1 90.6</cell><cell>96.2 96.6 97.2 97.4</cell><cell>99.8 99.8 99.9 99.9</cell></row><row><cell cols="10">† The flow streams of [48], [36], [37] take 10-channel (from "6-frame stack") optical flow as input, while our PAN only uses "4-frame stack".</cell><cell></cell></row><row><cell cols="4">based methods in both efficiency and accuracy aspects. For example, compared with NL I3D+GCN [44], our PAN Lite achieves 1.9% higher top-1 accuracy (48.0% vs. 46.1% on Something-V1) while with only ∼6% computational cost (35.7G vs. 303G×2). As for 2D CNN based methods, the performance of the baseline TSN [12] is relatively inferior than other methods, indicating the significance of temporal modeling for these datasets. Compared with the efficient (33G FLOPs) baselines TSN 8F [12] and TSM 8F [37], our PAN Lite achieves much higher top-1 accuracy (48.0% vs. 19.7%/45.6% on Something-V1, 60.8% vs. 27.8%/59.1% on Something-V2, 96.2% vs. 81.0%/94.4% on Jester) with only slight extra cost (0.08× FLOPs). With dual-path structure that separately processes RGB and PA modalities, PAN Full further bolsters the action recognition performance (50.5% on Something-V1, 63.8% on Something-V2 and 96.6% on Jester). The consistent improvements of our PAN over the other state-of-the-art methods strongly justify the superiority of our proposed PA and VAP for temporal modeling.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>RESULTS OF PAN WITH OTHER STATE-OF-THE-ART METHODS ON KINETICS400, UCF101 AND HMDB51 DATASETS.</figDesc><table><row><cell>Method</cell><cell>FLOPs ×views</cell><cell>Kinetics400 top1 (top5)</cell><cell cols="2">UCF101 HMDB51</cell></row><row><cell>STC [50] ECO En [48] I3D RGB [16] SlowFast-4×16 [34]</cell><cell>-368G×1 108G×-36.1G×30</cell><cell>68.7 (88.5) 70.0 (89.4) 71.1 (89.3) 75.6 (92.1)</cell><cell>93.7 94.8 95.1 -</cell><cell>66.8 72.4 74.3 -</cell></row><row><cell>ARTNet [51] Zhao et. al. [31] TSN RGB [12] OFF RGB [33] TSM [37] TEA [45]</cell><cell>23.5×250 -53G×10 -43G×30 35G×30</cell><cell>69.2 (88.3) 71.5 (89.9) 69.1 (88.7) -74.1 (91.2) 75.0 (91.8)</cell><cell>94.3 95.9 91.1 93.3 95.9 96.9</cell><cell>70.9 ---73.5 73.3</cell></row><row><cell>I3D RGB+Flow [16] TSN RGB+Flow [12] OFF RGB+Flow [33]</cell><cell>---</cell><cell>74.2 (91.3) 73.9 (91.1) -</cell><cell>98.0 97.0 96.0</cell><cell>80.7 -74.2</cell></row><row><cell>PAN Lite (Ours) PAN Full (Ours) PAN En (Ours)</cell><cell>47G×2 88G×2 135G×2</cell><cell>73.1 (91.1) 74.4 (91.6) 75.3 (92.4)</cell><cell>96.0 96.5 97.2</cell><cell>74.5 77.0 77.3</cell></row><row><cell cols="5">* For fair comparison, all the results on UCF101 and HMDB51 are obtained with Kinetics400 pre-train.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The evaluation metric of optical flow quality is end-point-error (EPE), the average Euclidean distance between the estimated and the ground-truth flow.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>"Typing" from UCF101 dataset.</p><p>"Dropping something onto something"</p><p>from Something-V1 dataset.</p><p>"ShavingBeard" from UCF101 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"Tearing something into two pieces"</head><p>from Something-V1 dataset.</p><p>"HulaHoop" from UCF101 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"Holding something behind something"</head><p>from Something-V1 dataset.</p><p>"PlayingPiano" from UCF101 dataset.</p><p>"Throwing something in the air and letting it fall" from Something-V1 dataset.</p><p>"ApplyLipstick" from UCF101 dataset.</p><p>"Unfolding something" from Something-V1 dataset.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The&quot; Something Something&quot; Video Database for Learning and Evaluating Visual Common Sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The jester dataset: A large-scale video dataset of human gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time action recognition with deeply transferred motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2326" to="2339" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hidden twostream convolutional networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00389</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On the integration of optical flow and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08416</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Actionflownet: Learning motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1616" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pan: Persistent appearance network with an efficient motion cue for fast action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint pattern recognition symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Secrets of optical flow estimation and their principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2432" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Endto-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6016" to="6025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward efficient action recognition: Principal backpropagation for training two-stream networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1773" to="1782" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action recognition with dynamic image networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2799" to="2813" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recognize Actions by Disentangling Components of Dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6566" to="6575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="387" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: a fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1390" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unexpected changes in direction of motion attract attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Holcombe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2087" to="2095" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bayesian surprise attracts human attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="547" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Motion guided attention for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7274" to="7283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Motion guided spatial attention for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8191" to="8198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatio-temporal channel correlation networks for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1430" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
