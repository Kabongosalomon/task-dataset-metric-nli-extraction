<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CondConv: Conditionally Parameterized Convolutions for Efficient Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
							<email>bcyang@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
							<email>gbender@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le Google Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
							<email>jngiam@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<title level="a" type="main">CondConv: Conditionally Parameterized Convolutions for Efficient Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional layers are one of the basic building blocks of modern deep neural networks. One fundamental assumption is that convolutional kernels should be shared for all examples in a dataset. We propose conditionally parameterized convolutions (CondConv), which learn specialized convolutional kernels for each example. Replacing normal convolutions with CondConv enables us to increase the size and capacity of a network, while maintaining efficient inference. We demonstrate that scaling networks with CondConv improves the performance and inference cost trade-off of several existing convolutional neural network architectures on both classification and detection tasks. On ImageNet classification, our CondConv approach applied to EfficientNet-B0 achieves state-ofthe-art performance of 78.3% accuracy with only 413M multiply-adds. Code and checkpoints for the CondConv Tensorflow layer and CondConv-EfficientNet models are available at: https://github.com/tensorflow/tpu/tree/master/ models/official/efficientnet/condconv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep convolutional neural networks (CNNs) have achieved state-of-the-art performance on many tasks in computer vision <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>. Improvements in performance have largely come from increasing model size and capacity to scale to larger and larger datasets <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref>. However, current approaches to increasing model capacity are computationally expensive. Deploying the best-performing models for inference can consume significant datacenter capacity <ref type="bibr" target="#b18">[19]</ref> and are often not feasible for applications with strict latency constraints.</p><p>One fundamental assumption in the design of convolutional layers is that the same convolutional kernels are applied to every example in a dataset. To increase the capacity of a model, model developers usually add more convolutional layers or increase the size of existing convolutions (kernel height/width, number of input/output channels). In either case, the computational cost of additional capacity increases proportionally to the size of the input to the convolution, which can be large.</p><p>Due to this assumption and focus on mobile deployment, current computationally efficient models have very few parameters <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>. However, there is a growing class of computer vision applications that are not constrained by parameter count, but have strict latency requirements at inference, such as real-time server-side video processing and perception for self-driving cars. In this paper, we aim to design models to better serve these applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: (a) Our CondConv layer architecture with n = 3 kernels vs. (b) a mixture of experts approach. By parameterizing the convolutional kernel conditionally on the input, CondConv is mathematically equivalent to the mixture of experts approach, but requires only 1 convolution.</p><p>We propose conditionally parameterized convolutions (CondConv), which challenge the paradigm of static convolutional kernels by computing convolutional kernels as a function of the input. In particular, we parameterize the convolutional kernels in a CondConv layer as a linear combination of n experts (α 1 W 1 + . . . + α n W n ) * x, where α 1 , . . . , α n are functions of the input learned through gradient descent. To efficiently increase the capacity of a CondConv layer, model developers can increase the number of experts. This is much more computationally efficient than increasing the size of the convolutional kernel itself, because the convolutional kernel is applied at many different positions within the input, while the experts are combined only once per input. This allows model developers to increase model capacity and performance while maintaining efficient inference.</p><p>CondConv can be used as a drop-in replacement for existing convolutional layers in CNN architectures. We demonstrate that replacing convolutional layers with CondConv improves model capacity and performance on several CNN architectures on ImageNet classification and COCO object detection, while maintaining efficient inference. In our analysis, we find that CondConv layers learn semantically meaningful relationships across examples to compute the conditional convolutional kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Conditional computation. Similar to CondConv, conditional computation aims to increase model capacity without a proportional increase in computation cost. In conditional computation models, this is achieved by activating only a portion of the entire network for each example <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2]</ref>. However, conditional computation models are often challenging to train, since they require learning discrete routing decisions from individual examples to different experts. Unlike these approaches, CondConv does not require discrete routing of examples, so can be easily optimized with gradient descent.</p><p>One approach to conditional computation uses reinforcement learning or evolutionary methods to learn discrete routing functions <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b8">9]</ref>. BlockDrop <ref type="bibr" target="#b46">[47]</ref> and SkipNet <ref type="bibr" target="#b44">[45]</ref> use reinforcement learning to learn the subset of blocks needed to process a given input. Another approach uses unsupervised clustering methods to partition examples into sub-networks. Gross et al. <ref type="bibr" target="#b11">[12]</ref> use a two stage training and clustering pipeline to train a hard mixture of experts model. Mullapudi et al. <ref type="bibr" target="#b30">[31]</ref> use clusters as labels to train a routing function between branches in a deep CNN model. Finally, Shazeer et al. <ref type="bibr" target="#b36">[37]</ref> proposed the sparsely-gated mixture-of-experts layer, which which achieves significant success on large language modeling using noisy top-k gating.</p><p>Prior work in computation demonstrates the potential of designing large models that process different sets of examples with different sub-networks. Our work on CondConv pushes the boundaries of this paradigm, by enabling each individual example to be processed with different weights.</p><p>Weight generating networks. Ha et al. <ref type="bibr" target="#b12">[13]</ref> propose the use of a small network to generate weights for a larger network. Unlike CondConv, for CNNs, these weights are the same for every example in the dataset. This enables greater weight-sharing, which achieves lower parameter count but worse performance than the original network. In neural machine translation, Platanios et al. <ref type="bibr" target="#b31">[32]</ref> generate weights to translate between different language pairs, but use the same weights for every example within each language pair.</p><p>Multi-branch convolutional networks. Multi-branch architectures like Inception <ref type="bibr" target="#b39">[40]</ref> and ResNext <ref type="bibr" target="#b47">[48]</ref> have shown success on a variety of computer vision tasks. In these architectures, a layer consists of multiple convolutional branches, which are aggregated to compute the final output. A CondConv layer is mathematically equivalent to a multi-branch convolutional layer where each branch is a single convolution and outputs are aggregated by a weighted sum, but only requires the computation of one convolution.</p><p>Example dependent activation scaling. Some recent work proposes to adapt the activations of neural networks conditionally on the input. Squeeze-and-Excitation networks <ref type="bibr" target="#b15">[16]</ref> learn to scale the activations of every layer output. GaterNet <ref type="bibr" target="#b3">[4]</ref> uses a separate network to select a binary mask for filters for a larger backbone network. Attention-based methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b43">44]</ref> scale previous layer inputs based on learned attention weights. Scaling activations has similar motivations as CondConv, but is restricted to modulating activations in the base network.</p><p>Input-dependent convolutional layers. In language modeling, Wu et al. <ref type="bibr" target="#b45">[46]</ref> use input-dependent convolutional kernels as a form of local attention. In vision, Brabandere et al. <ref type="bibr" target="#b17">[18]</ref> generate small input-dependent convolutional filters to transform images for next frame and stereo prediction. Rather than learning input-dependent weights, Dai et al. <ref type="bibr" target="#b6">[7]</ref> propose to learn different convolutional offsets for each example. Finally, in recent work, SplineNets <ref type="bibr" target="#b19">[20]</ref> apply input-dependent convolutional weights, modeled as 1-dimensional B-splines, to implement continuous neural decision graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conditionally Parameterized Convolutions</head><p>In a regular convolutional layer, the same convolutional kernel is used for all input examples. In a CondConv layer, the convolutional kernel is computed as a function of the input example <ref type="figure">(Fig 1a)</ref>. Specifically, we parameterize the convolutional kernels in CondConv by:</p><formula xml:id="formula_0">Output(x) = σ((α 1 · W 1 + . . . + α n · W n ) * x)</formula><p>where each α i = r i (x) is an example-dependent scalar weight computed using a routing function with learned parameters, n is the number of experts, and σ is an activation function. When we adapt a convolutional layer to use CondConv, each kernel W i has the same dimensions as the kernel in the original convolution.</p><p>We typically increase the capacity of a regular convolutional layer by increasing the kernel height/width or number of input/output channels. However, each additional parameter in a convolution requires additional multiply-adds proportional to the number of pixels in the input feature map, which can be large. In a CondConv layer, we compute a convolutional kernel for each example as a linear combination of n experts before applying the convolution. Crucially, each convolutional kernel only needs to be computed once but is applied at many different positions in the input image. This means that by increasing n, we can increase the capacity of the network with only a small increase in inference cost; each additional parameter requires only 1 additional multiply-add.</p><p>A CondConv layer is mathematically equivalent to a more expensive linear mixture of experts formulation, where each expert corresponds to a static convolution ( <ref type="figure">Fig 1b)</ref>:</p><formula xml:id="formula_1">σ((α 1 · W 1 + . . . + α n · W n ) * x) = σ(α 1 · (W 1 * x) + . . . + α n · (W n * x))</formula><p>Thus, CondConv has the same capacity as a linear mixture of experts formulation with n experts, but is computationally efficient since it requires computing only one expensive convolution. This formulation gives insight into the properties of CondConv and relates it to prior work on conditional computation and mixture of experts. The per-example routing function is crucial to CondConv performance: if the learned routing function is constant for all examples, a CondConv layer has the same capacity as a static convolutional layer.</p><p>We wish to design a per-example routing function that is computationally efficient, able to meaningfully differentiate between input examples, and is easily interpretable. We compute the exampledependent routing weights α i = r i (x) from the layer input in three steps: global average pooling, fully-connected layer, Sigmoid activation. where R is a matrix of learned routing weights mapping the pooled inputs to n expert weights. A normal convolution operation operates only over local receptive fields, so our routing function allows adaptation of local operations using global context.</p><formula xml:id="formula_2">r(x) = Sigmoid(GlobalAveragePool(x) R)</formula><p>The CondConv layer can be used in place of any convolutional layer in a network. The same approach can easily be extended to other linear functions like those in depth-wise convolutions and fully-connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate CondConv on ImageNet classification and COCO object detection by scaling up the MobileNetV1 <ref type="bibr" target="#b14">[15]</ref>, MobileNetV2 <ref type="bibr" target="#b35">[36]</ref>, ResNet-50 <ref type="bibr" target="#b13">[14]</ref>, MnasNet <ref type="bibr" target="#b40">[41]</ref>, and EfficientNet <ref type="bibr" target="#b41">[42]</ref> architectures. In practice, we have two options to train CondConv models, which are mathematically equivalent. We can either first compute the kernel for each example and apply convolutions with a batch size of one ( <ref type="figure">Fig. 1a</ref>), or we can use the linear mixture of experts formulation ( <ref type="figure">Fig. 1b</ref>) to perform batch convolutions on each branch and sum the outputs. Current accelerators are optimized to train on large batch convolutions, and it is difficult to fully utilize them for small batch sizes. Thus, with small numbers of experts (&lt;=4), we found it to be more efficient to train CondConv layers with the linear mixture of experts formulation and large batch convolutions, then use our efficient CondConv approach for inference. With larger numbers of experts (&gt;4), training CondConv layers directly with batch size one is more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ImageNet Classification</head><p>We evaluate our approach on the ImageNet 2012 classification dataset <ref type="bibr" target="#b34">[35]</ref>. The ImageNet dataset consists of 1.28 million training images and 50K validation images from 1000 classes. We train all models on the entire training set and compare the single-crop top-1 validation set accuracy with input image resolution 224x224. For MobileNetV1, MobileNetV2, and ResNet-50, we use the same training hyperparameters for all models on ImageNet, following <ref type="bibr" target="#b20">[21]</ref>, except we use BatchNorm momentum of 0.9 and disable exponential moving average on weights. For MnasNet <ref type="bibr" target="#b40">[41]</ref> and EfficientNet <ref type="bibr" target="#b41">[42]</ref>, we use the same training hyperparameters as the original papers, with the batch size, learning rate, and training steps scaled appropriately for our hardware configuration. For fair comparison, we retrain all of our baseline models with the same hyperparameters and regularization search space as the CondConv models. We used accuracy on the validation set to determine early stopping. We measure performance as ImageNet top-1 accuracy relative to computational cost in multiply-adds (MADDs). For each baseline architecture, we evaluate CondConv by replacing convolutional layers with Cond-Conv layers, and increasing the number of experts per layer. We share routing weights between layers in a block (a residual block, inverted bottleneck block, or separable convolution). Additionally, for some models, we replace the fully-connected classification layer with a 1x1 CondConv layer. For the exact architectural details, refer to Appendix A. Our ablation experiments in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table" target="#tab_3">Table 4</ref> suggest CondConv improves performance across a wide range of layer and routing architectures.</p><p>We use two general regularization techniques for models with large capacity. First, we use Dropout <ref type="bibr" target="#b38">[39]</ref> on the input to the fully-connected layer preceding the logits, with keep probability between 0.6 and 1.0. Second, we also add data augmentation using the AutoAugment <ref type="bibr" target="#b5">[6]</ref> ImageNet policy and Mixup <ref type="bibr" target="#b48">[49]</ref> with α = 0.2. To address overfitting in the large ResNet models, we additionally introduce a new data augmentation technique for CondConv based on Shake-Shake <ref type="bibr" target="#b9">[10]</ref> by randomly dropping out experts during training.</p><p>On MobileNetV1, we find that increasing the number of CondConv experts improves accuracy relative to inference cost compared to the performance frontier with static convolutional scaling techniques using the channel width and input size ( <ref type="figure" target="#fig_1">Figure 2</ref>). Moreover, we find that increasing the number of CondConv experts leads to monotonically increasing performance with sufficient regularization.</p><p>We further find that CondConv improves performance relative to inference cost on a wide range of architectures <ref type="table" target="#tab_0">(Table 1</ref>). This includes architectures that take advantage of architecture search <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b40">41]</ref>, Squeeze-and-Excitation <ref type="bibr" target="#b15">[16]</ref>, and large architectures with ordinary convolutions not optimized for inference time <ref type="bibr" target="#b13">[14]</ref>. For more in depth comparisons, see Appendix A.</p><p>Our CondConv-EfficientNet-B0 model achieves state-of-the-art performance of 78.3% accuracy with 413M multiply-adds, when compared to the MixNet frontier <ref type="bibr" target="#b42">[43]</ref>. To directly compare our CondConv scaling approach to the compound scaling coefficient proposed by Tan et al. <ref type="bibr" target="#b41">[42]</ref>, we additionally scale the CondConv-EfficientNet-B0 model with a depth multiplier of 1.1x, which we call CondConv-EfficientNet-B0-depth. Our CondConv-EfficientNet-B0-depth model achieves 79.5% accuracy with only 614M multiply-adds. When trained with the same hyperparameters and regularization search space, the EfficientNet-B1 model, which is scaled from the EfficientNet-B0 model using the compound coefficient, achieves 79.2% accuracy with 700M multiply-adds. In this regime, CondConv scaling outperforms static convolutional scaling with the compound coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COCO Object Detection</head><p>We next evaluate the effectiveness of CondConv on a different task and dataset with the COCO object detection dataset <ref type="bibr" target="#b23">[24]</ref>. Our experiments use the MobileNetV1 feature extractor and the Single Shot Detector <ref type="bibr" target="#b25">[26]</ref> with 300x300 input resolution (SSD300).</p><p>Following Howard et al. <ref type="bibr" target="#b14">[15]</ref>, we train on the combined COCO training and validation sets excluding 8,000 minival images, which we evaluate our networks on. We train our models using a batch size <ref type="bibr" target="#b1">2</ref> Our re-implementation of the baseline models and our CondConv models use the same hyperparameters and regularization search space for fair comparison. For reference, published results for baselines are: MobileNetV1 (1.0x): 70.6% <ref type="bibr" target="#b14">[15]</ref>. MobileNetV2 (1.0x): 72.0% <ref type="bibr" target="#b35">[36]</ref>. MnasNet-A1: 75.2% <ref type="bibr" target="#b40">[41]</ref>. ResNet-50: 76.4% <ref type="bibr" target="#b10">[11]</ref>. EfficientNet-B0: 76.3% <ref type="bibr" target="#b41">[42]</ref>.   of 1024 for 20,000 steps. For the learning rate, we use linear warmup from 0.3 to 0.9 over 1,000 steps, followed by cosine decay <ref type="bibr" target="#b26">[27]</ref> from 0.9. We use the data augmentation scheme proposed by Liu et al. <ref type="bibr" target="#b25">[26]</ref>. We use the same convolutional feature layer dimensions, SSD hyperparameters, and training hyperparameters across all models. We measure performance as COCO minival mean average precision (mAP) relative to computational cost in multiply-adds (MADDs).</p><p>We use our CondConv-MobileNetV1 models with depth multipliers {0.50, 0.75, 1.0} as the feature extractors for object detection. We further replace the additional convolutional feature extractor layers in SSD with CondConv layers.</p><p>CondConv with 8 experts improves object detection performance at all model sizes (  <ref type="bibr" target="#b3">4</ref> We choose this setup for ease of training and large effect of CondConv.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Routing function</head><p>We investigate different choices for the routing function in <ref type="table" target="#tab_2">Table 3</ref>. The baseline model computes new routing weights for each layer. Single computes the routing weights only once at CondConv 7 (the 7th separable convolutional block), and uses the same routing weights in all subsequent layers. Partially-shared shares the routing weights between every other layer. Both the baseline model and Partially-shared significantly outperform Single, which suggests that routing at multiple depths in the network improve quality. Partially-shared performs slightly outperforms the baseline, suggesting that sharing routing functions among nearby layers can improve quality.</p><p>We then experiment with more complex routing functions, by introducing a hidden layer with ReLU activation after the global average pooling step. We vary the hidden layer size to be input_dim/8 for Hidden (small), input_dim for Hidden (medium), and input_dim · 8 for Hidden (large). Adding a non-linear hidden layer of appropriate size can slightly improve performance. Large hidden layer sizes are prone to over-fitting, even with the same number of experts.</p><p>Next, we experiment with Hierarchical routing functions, by concatenating the routing weights of the previous layer to the output of the global average pooling layer in the routing function. This adds a dependency between CondConv routing weights, which we find is also prone to overfitting.</p><p>Finally, we experiment with the Softmax activation function to compute routing weights. The baseline's Sigmoid significantly outperforms Softmax, which suggests that multiple experts are often useful for a single example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">CondConv Layer Depth</head><p>We analyze the effect of CondConv layers at different depths in the CondConv-MobileNetV1 (0.25x) model <ref type="table" target="#tab_3">(Table 4</ref>). We use CondConv layers in the begin layer, and all subsequent layers. We further perform ablation studies specific to the final fully-connected classification layer. We find CondConv layers improve performance when applied at every layer in the network. Additionally, we find that additionally applying CondConv before layer 7 in the network has only small effects on performance.  For image classification with the CondConv-MobileNetV1 (0.25x) model, CondConv in the final classification layer accounts for a significant fraction of the additional inference cost. Using a normal final classification layer results in smaller performance gains, but is more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we aim to gain a better understanding of the learned kernels and routing functions in our CondConv-MobileNetV1 architecture. We study our CondConv-MobileNetV1 (0.50x) architecture with 32 experts per layer trained on ImageNet with Mixup and AutoAugment, which achieves 71.6% top-1 validation accuracy. We evaluate our CondConv-MobileNetV1 (0.5x) model on the 50,000</p><p>ImageNet validation examples, and compute the routing-weights at CondConv layers in the network.</p><p>We first study inter-class variation between the routing weights at different layers in the network. We visualize the average routing weight for four different classes (cliff, pug, goldfish, and plane, as suggested by Hu et al. <ref type="bibr" target="#b15">[16]</ref> for semantic and appearance diversity) at three different depths in the network (Layer 12, Layer 26, and the final fully-connected layer). The distribution of the routing weights is very similar across classes at early layers in the network, and become more and more class specific at later layers <ref type="figure" target="#fig_2">(Figure 3</ref>). This suggests an explanation for why replacing additional convolutional layers with CondConv layers near the input of the network does not significantly improve performance.</p><p>We next analyze the distribution of the routing weights of the final fully-connected layer in <ref type="figure" target="#fig_3">Figure  4</ref>. The routing weights follow a bi-modal distribution, with most experts receiving a routing weight close to 0 or 1. This suggests that the experts are sparsely activated, even without regularization, and further suggests the specialization of the experts.</p><p>We then study intra-class variation between the routing weights in the final CondConv layer ( <ref type="figure" target="#fig_4">Figure  5</ref>). Within one class, some kernels are activated with high weight and small variance for all examples. However, even within one class, there can be big variation in the routing weights between examples.</p><p>Finally, to better understand experts in the final CondConv layer, we visualize top 10 classes with highest mean routing weight for four difference experts on the ImageNet validation set ( <ref type="figure" target="#fig_5">Figure 6</ref>). We show the exemplar image with highest routing weight within each class. CondConv layers learn to specialize in semantically and visually meaningful ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed conditionally parameterized convolutions (CondConv  <ref type="bibr" target="#b35">[36]</ref>), but requires 585M multiply-adds.</p><p>CondConv-MnasNet-A1. We replace the convolutional layers in the final 3 block groups of the baseline MnasNet-A1 architecture with CondConv layers. We share routing weights between convolutional layers in each inverted bottleneck block within a block group. The baseline MnasNet-A1 model achieves 74.9% accuracy with 312M multiply-adds. Our CondConv-MnasNet-A1 model achieves 76.2% accuracy with 329M multiply-adds. A larger model from the same search space using static convolutional layers, MnasNet-A2, achieves 75.6% accuracy with 340M multiply-adds <ref type="bibr" target="#b40">[41]</ref>.</p><p>CondConv-ResNet-50. We replace the convolutional layers in the final 3 residual blocks and the final fully-connected classification layer of the baseline ResNet-50 architecture with CondConv layers. The baseline ResNet-50 model achieves 77.7% accuracy at 4096M multiply-adds. Our CondConv-ResNet-50 architecture achieves 78.6% accuracy at 4213 multiply-adds. With sufficient regularization, CondConv improves the performance of even large model architectures with ordinary convolutions that are not optimized for inference time.</p><p>CondConv-EfficientNet-B0. We replace the convolutional layers in the final 3 block groups of the baseline EfficientNet-B0 architecture with CondConv layers. We share routing weights between convolutional layers in each inverted bottleneck block within a block group. The baseline EfficientNet-B0 model achieves 77.2% accuracy with 391M multiply-adds. Our CondConv-EfficientNet-B0 model achieves 78.3% accuracy with 413M multiply-adds.</p><p>To directly compare our CondConv scaling approach to the compound scaling coefficient proposed by Tan et al. <ref type="bibr" target="#b41">[42]</ref>, we additionally scale the CondConv-EfficientNet-B0 model with a depth multiplier of 1.1x, which we call CondConv-EfficientNet-B0-depth. Our CondConv-EfficientNet-B0-depth model achieves 79.5% accuracy with only 614M multiply-adds. When trained with the same hyperparameters and regularization search space, the EfficientNet-B1 model, which is scaled from the EfficientNet-B0 model using the compound coefficient, achieves 79.2% accuracy with 700M multiply-adds. In this regime, CondConv scaling outperforms static convolutional scaling with the compound coefficient.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) CondConv: (α1W1 + . . . + αnWn) * x (b) Mixture of Experts: α1(W1 * x)+. . .+αn(Wn * x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>On ImageNet validation, increasing the number of experts per layer of our CondConv-MobileNetV1 models improves performance relative to inference cost compared to the MobileNetV1 frontier<ref type="bibr" target="#b37">[38]</ref> across a spectrum of model sizes. Models with more experts per layer achieve monotonically higher accuracy. We train CondConv models with {1, 2, 4, 8, 16, 32} experts at width multipliers {0.25, 0.50, 0.75, 1.0}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Mean routing weights for four classes averaged across the ImageNet validation set at three different depths in our CondConv-MobileNetV1 (0.5x) model. CondConv routing weights are more class-specific at greater depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Distribution of routing weights in the final CondConv layer of our CondConv-MobileNetV1 (0.5x) model when evaluated on all images in the ImageNet validation set. Routing weights follow a bi-modal distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Routing weights in the final CondConv layer in our CondConv-MobileNetV1 (0.5x) model for 2 classes averaged across the ImageNet validation set. Error bars indicate one standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Top 10 classes with highest mean routing weight for 4 different experts in the final CondConv layer in our CondConv-MobileNetV1 (0.5x) model, as measured across the ImageNet validation set. Expert 1 is most activated for wheeled vehicles; expert 2 is most activated for rectangular structures; expert 3 is most activated for cylindrical household objects; expert 4 is most activated for brown and black dog breeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>ImageNet validation accuracy and inference cost for our CondConv models on several baseline model architectures. All models use 8 experts per CondConv layer. CondConv improves the accuracy of all baseline architectures with small relative increase in inference cost (&lt;10%).</figDesc><table><row><cell></cell><cell>Baseline</cell><cell></cell><cell>CondConv</cell><cell></cell></row><row><cell></cell><cell cols="2">MADDs (×10 6 ) Top-1 (%)</cell><cell cols="2">MADDs (×10 6 ) Top-1 (%)</cell></row><row><cell>MobileNetV1 (1.0x)</cell><cell>567</cell><cell>71.9</cell><cell>600</cell><cell>73.7</cell></row><row><cell>MobileNetV2 (1.0x)</cell><cell>301</cell><cell>71.6</cell><cell>329</cell><cell>74.6</cell></row><row><cell>MnasNet-A1</cell><cell>312</cell><cell>74.9</cell><cell>325</cell><cell>76.2</cell></row><row><cell>ResNet-50</cell><cell>4093</cell><cell>77.7</cell><cell>4213</cell><cell>78.6</cell></row><row><cell>EfficientNet-B0</cell><cell>391</cell><cell>77.2</cell><cell>413</cell><cell>78.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>COCO object detection minival performance of our CondConv-MobileNetV1 SSD 300 architecture with 8 experts per layer. Mean average precision (mAP) reported with COCO primary challenge metric (AP at IoU=0.50:0.05:0.95). CondConv improves mAP at all model sizes with small relative increase in inference cost (&lt;5%).</figDesc><table><row><cell></cell><cell>Baseline</cell><cell></cell><cell>CondConv</cell><cell></cell></row><row><cell></cell><cell cols="2">MADDs (×10 6 ) mAP</cell><cell cols="2">MADDs (×10 6 ) mAP</cell></row><row><cell>MobileNetV1 (0.5x)</cell><cell>352</cell><cell>14.4</cell><cell>363</cell><cell>18.0</cell></row><row><cell>MobileNetV1 (0.75x)</cell><cell>730</cell><cell>18.2</cell><cell>755</cell><cell>21.0</cell></row><row><cell>MobileNetV1(1.0x)</cell><cell>1230</cell><cell>20.3</cell><cell>1280</cell><cell>22.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">Different routing architectures. Our</cell></row><row><cell cols="3">baseline CondConv(CC)-MobileNetV1 uses a</cell></row><row><cell cols="3">one-layer, fully-connected routing function with</cell></row><row><cell cols="3">Sigmoid activation for each CondConv block.</cell></row><row><cell>Routing Fn</cell><cell>MADDs</cell><cell>Valid</cell></row><row><cell></cell><cell cols="2">(×10 6 ) Top-1 (%)</cell></row><row><cell>CC-MobileNetV1 (0.25x)</cell><cell>55.7</cell><cell>62.0</cell></row><row><cell>Single</cell><cell>55.5</cell><cell>56.5</cell></row><row><cell>Partially-shared</cell><cell>55.6</cell><cell>62.5</cell></row><row><cell>Hidden (small)</cell><cell>55.6</cell><cell>57.7</cell></row><row><cell>Hidden (medium)</cell><cell>55.9</cell><cell>62.2</cell></row><row><cell>Hidden (large)</cell><cell>57.8</cell><cell>54.1</cell></row><row><cell>Hierarchical</cell><cell>55.7</cell><cell>60.3</cell></row><row><cell>Softmax</cell><cell>55.7</cell><cell>60.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">CondConv at different layers in our</cell></row><row><cell cols="3">CondConv(CC)-MobileNetV1 (0.25x) model.</cell></row><row><cell cols="3">FC refers to the final classification layer. Cond-</cell></row><row><cell cols="3">Conv improves performance at every layer.</cell></row><row><cell>CondConv Begin</cell><cell>MADDs</cell><cell>Valid</cell></row><row><cell>Layer</cell><cell cols="2">(×10 6 ) Top-1 (%)</cell></row><row><cell>CC-MobileNetV1 (0.25x)</cell><cell>55.7</cell><cell>62.0</cell></row><row><cell>MobileNetV1 (0.25x)</cell><cell>41.2</cell><cell>50.0</cell></row><row><cell>1</cell><cell>56.3</cell><cell>62.5</cell></row><row><cell>5</cell><cell>56.0</cell><cell>62.0</cell></row><row><cell>7</cell><cell>55.7</cell><cell>62.0</cell></row><row><cell>13</cell><cell>52.5</cell><cell>59.5</cell></row><row><cell>15 (FC Only)</cell><cell>49.3</cell><cell>54.2</cell></row><row><cell>7 (No FC)</cell><cell>47.6</cell><cell>60.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>We perform ablation experiments to better understand model design with the CondConv block. In all experiments, we compare against the same baseline CondConv-MobileNetV1 (0.25x) model with 32 experts per CondConv layer, trained with the same setup as Section 4 and no additional Dropout or data augmentation. The baseline model achieves 61.98% ImageNet Top-1 validation accuracy with 55.7M multiply-adds. The MobileNetV1(0.25x) architecture achieves 50.4% Top-1 accuracy with 41.2M multiply-adds.</figDesc><table><row><cell>). Our</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>). CondConv challenges the assumption that convolutional kernels should be shared across all input examples. This introduces a new direction for increasing model capacity while maintaining efficient inference: increase the size and complexity of the kernel-generating function. Since the kernel is computed only once, then convolved across the input, increasing the complexity of the kernel-generating function can be much more efficient than adding additional convolutions or expanding existing convolutions. CondConv also highlights an important research question in the trend towards larger datasets on how to best uncover, represent, and leverage the relationship between examples to improve model performance. In the future, we hope to further explore the design space and limitations of CondConv with larger datasets, more complex kernel-generating functions, and architecture search to design better base architectures.In this section, we provide detailed descriptions of the specific CondConv architectures we used for each baseline model, and elaborate on individual results. All CondConv results are reported with 8 experts per layer. For fair comparison, all results reported use the same training hyperparameters and regularization search space as the CondConv model they are compared against.CondConv-MobileNetV1. We replace the convolutional layers starting from the sixth separable convolutional block and the final fully-connected classification layer of the baseline MobileNetV1 model with CondConv layers. We share routing weights between depthwise and pointwise layers with a separable convolution block. Our CondConv-MobileNetV1 (0.5x) model with 32 experts per CondConv layer achieves 71.6% accuracy at 190M multiply-adds, comparable to the MobileNetV1 (1.0x) model at 71.7% at 571M multiply-adds. We replace the convolutional layers in the final 6 inverted residual blocks and the final fully-connected classification layer of the baseline MobileNetV2 architecture with CondConv layers. We share routing weights between convolutional layers in each inverted bottleneck block. Our CondConv-MobileNetV2 (1.0x) model achieves 74.6% accuracy at 329M multiply-adds. The MobileNetV2 (1.4x) architecture with static convolutions scaled by width multiplier achieves similar accuracy of 74.5% in our implementation (74.7% in</figDesc><table><row><cell>Appendices</cell></row><row><cell>A ImageNet Architectures</cell></row><row><cell>CondConv-MobileNetV2.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Our re-implementation of the baseline models and our CondConv models use the same hyperparameters for fair comparison. As published reference, Howard et al.<ref type="bibr" target="#b14">[15]</ref> report mAP of 19.3 for MobileNetV1 (1.0x).<ref type="bibr" target="#b3">4</ref> Our implementation. Howard et al.<ref type="bibr" target="#b14">[15]</ref> report a top-1 accuracy of 50.0% with different hyperparameters.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06297</idno>
		<title level="m">Conditional computation in neural networks for faster models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gaternet: Dynamic filter selection in convolutional neural network via a dedicated global gating network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11205</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.7362</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Low-rank approximations for conditional feedforward computation in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Arel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4461</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pathnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08734</idno>
		<title level="m">Evolution channels gradient descent in super neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<title level="m">Shake-shake regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hard mixtures of experts for large scale weakly supervised vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6865" to="6873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpipe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07233</idno>
		<title level="m">Efficient training of giant neural networks using pipeline parallelism</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Splinenets: Continuous neural decision graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a backpropagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic deep neural networks: Optimizing accuracy-efficiency trade-offs by selective execution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanlan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emperical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00932</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deciding how to decide: Dynamic routing in artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hydranets: Specialized dynamic architectures for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Ravi Teja Mullapudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayvon</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatahalian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contextual parameter generation for universal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Emmanouil Antonios Platanios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="425" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Routing networks: Adaptive selection of non-linear functions for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Riemer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Tensorflow-slim image classification model library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11626</idno>
		<title level="m">Platformaware neural architecture search for mobile</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09595</idno>
		<title level="m">Mixed depthwise convolutional kernels</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Skipnet: Learning dynamic routing in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Blockdrop: Dynamic inference paths in residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
