<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient RGB-D Semantic Segmentation for Indoor Scene Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seichter</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Köhler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lewandowski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Wengefeld</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst-Michael</forename><surname>Gross</surname></persName>
						</author>
						<title level="a" type="main">Efficient RGB-D Semantic Segmentation for Indoor Scene Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Analyzing scenes thoroughly is crucial for mobile robots acting in different environments. Semantic segmentation can enhance various subsequent tasks, such as (semantically assisted) person perception, (semantic) free space detection, (semantic) mapping, and (semantic) navigation. In this paper, we propose an efficient and robust RGB-D segmentation approach that can be optimized to a high degree using NVIDIA TensorRT and, thus, is well suited as a common initial processing step in a complex system for scene analysis on mobile robots. We show that RGB-D segmentation is superior to processing RGB images solely and that it can still be performed in real time if the network architecture is carefully designed. We evaluate our proposed Efficient Scene Analysis Network (ESANet) on the common indoor datasets NYUv2 and SUNRGB-D and show that we reach state-of-the-art performance while enabling faster inference. Furthermore, our evaluation on the outdoor dataset Cityscapes shows that our approach is suitable for other areas of application as well. Finally, instead of presenting benchmark results only, we also show qualitative results in one of our indoor application scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Semantic scene perception and understanding is essential for mobile robots acting in various environments. In our research projects, covering public environments from supermarkets <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> to hospitals <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and domestic applications <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, our robots need to perform several tasks in parallel such as obstacle avoidance, semantic mapping, navigation to semantic entities, and person perception. Most of the tasks require to be handled in real time given limited computing and battery capabilities. Hence, an efficient and shared initial processing step can facilitate subsequent tasks. Semantic segmentation is well suited for such an initial step, as it provides precise pixel-wise information that can be used for numerous subsequent tasks.</p><p>In this paper, we propose an efficient and robust encoderdecoder-based semantic segmentation approach that can be embedded in complex systems for semantic scene analysis such as shown in <ref type="figure">Fig. 1</ref>. The segmentation output enriches the robot's visual perception and facilitates subsequent processing steps by providing individual semantic masks. For our person perception <ref type="bibr" target="#b6">[7]</ref>, computations can be restricted to image regions segmented as person, instead of processing the entire image. Furthermore, the floor class indicates free space that can be used for inpainting invalid depth pixels as well as serves as additional information for avoiding even <ref type="figure">Fig. 1</ref>: Our proposed efficient RGB-D segmentation approach can serve as a common preprocessing step for subsequent tasks such as person perception, free space detection to avoid even small obstacles below the laser, or semantic mapping. small obstacles below the laser. For mapping <ref type="bibr" target="#b7">[8]</ref>, we can include semantics and ignore image regions segmented as dynamic classes such as person.</p><p>Our segmentation approach relies on both RGB and depth images as input. Especially in indoor environments, cluttered scenes may impede semantic segmentation. Incorporating depth images can alleviate this effect by providing complementary geometric information, as shown in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. In contrast to processing RGB images solely, this design decision comes with some additional computational cost. However, in this paper, we show that two carefully designed shallow encoder branches (one for RGB and one for depth data) can achieve better segmentation performance while still enabling faster inference (application of network) than a single deep encoder branch for RGB images solely. Moreover, our Efficient Scene Analysis Network (ESANet) enables much faster inference than most other RGB-D segmentation methods, while performing on par or even better as shown by our experiments.</p><p>We evaluate our ESANet on the commonly used indoor datasets NYUv2 <ref type="bibr" target="#b11">[12]</ref> and SUNRGB-D <ref type="bibr" target="#b12">[13]</ref> and further present qualitative results in our indoor application. Instead of only focusing on mean intersection over union (mIoU) as evaluation metric for benchmarking, we also strive for fast inference on embedded hardware. However, rather than reporting the inference time on high-end GPUs, we measure inference time on our robot's NVIDIA Jetson AGX Xavier. We designed our network architecture such that it arXiv:2011.06961v3 [cs.CV] 7 Apr 2021 can be executed as a single optimized graph using NVIDIA TensorRT. Moreover, by evaluating on the outdoor dataset Cityscapes <ref type="bibr" target="#b13">[14]</ref>, we show that our approach can also be applied to other areas of application. The main contributions of this paper are • an efficient RGB-D segmentation approach, which can serve as initial processing step to facilitate subsequent scene analysis tasks and is characterized by: a carefully designed architecture that can be optimized to a high degree using NVIDIA TensorRT and, thus, enables fast inference an efficient ResNet-based encoder that utilizes a modified basic block that is computationally less expensive while achieving higher accuracy a decoder that utilizes a novel learned upsampling • a detailed ablation study to the fundamental parts of our approach and their impact on segmentation performance and inference time • qualitative results in a complex system for robotic scene analysis proving the applicability and robustness. Our code as well as the trained networks are publicly available at: https://github.com/TUI-NICR/ESANet II. RELATED WORK Common network architectures for semantic segmentation follow an encoder-decoder design. The encoder extracts semantically rich features from the input and performs downsampling to reduce computational effort. The decoder restores the input resolution and, finally, assigns a semantic class to each input pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RGB-D Semantic Segmentation</head><p>Depth images provide complementary geometric information to RGB images and, thus, improve segmentation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. However, incorporating depth information into RGB segmentation architectures is challenging as depth introduces deviating statistics and characteristics from another modality.</p><p>In <ref type="bibr" target="#b14">[15]</ref>, depth information is used to project the RGB images into a 3D space. However, processing the resulting 3D data, leads to significantly higher computational complexity. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> design specifically tailored convolutions, taking into account depth information. Nevertheless, these modified convolutions often lack optimized implementations and, thus, are slow and not applicable for real-time segmentation on embedded hardware.</p><p>The majority of approaches for RGB-D segmentation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> simply use two branches, one for RGB and one for depth data and fuse the feature representations later in the network. This way, each branch can focus on extracting modality-specific features, such as color and texture from RGB images and geometric, illumination-independent features from depth images. Fusing these modality-specific features leads to stronger feature representations. Instead of fusing only low-level or highlevel features, <ref type="bibr" target="#b8">[9]</ref> shows that the segmentation performance increases if the features are fused at multiple stages. Typically, the features are fused once at each resolution stage with the last fusion at the end of both encoders. Using only one decoder for the combined features reduces the computational effort. FuseNet <ref type="bibr" target="#b8">[9]</ref> and RedNet <ref type="bibr" target="#b9">[10]</ref> fuse the depth features into the RGB encoder, which follows the intuition that the semantically richer RGB features can be further enhanced using complementary depth information. SA-Gate <ref type="bibr" target="#b21">[22]</ref> combines RGB and depth features and fuses the recalibrated features back into both encoders. In order to make the two encoders independent of each other, ACNet <ref type="bibr" target="#b10">[11]</ref> uses an additional, virtual, third encoder that obtains modalityspecific features from the two encoders and processes the combined features. Instead of fusing in the encoder, the modality-specific features can also be used to refine the features in the common decoder via skip connections as in RDFNet <ref type="bibr" target="#b22">[23]</ref>, SSMA <ref type="bibr" target="#b23">[24]</ref> and MMAF-Net <ref type="bibr" target="#b24">[25]</ref>.</p><p>However, none of the aforementioned methods focus on efficient RGB-D segmentation for embedded hardware. Using deep encoders such as ResNets with 50, 101 or even 152 layers results in high inference times and, therefore, makes them inappropriate for deploying to mobile robots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Efficient Semantic Segmentation</head><p>In contrast to RGB-D segmentation, recent RGB approaches <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> also address reducing computational complexity to enable real-time segmentation. Most efficient segmentation approaches propose specifically tailored network architectures, which reduce both the number of operations and parameters to enable faster inference while still retaining good segmentation performance. Approaches, such as ERFNet <ref type="bibr" target="#b25">[26]</ref>, LEDNet <ref type="bibr" target="#b26">[27]</ref>, or DABNet <ref type="bibr" target="#b27">[28]</ref> introduce efficient encoder blocks by replacing expensive 3×3 convolutions with more light-weight variants such as factorized, grouped, or depth-wise separable convolutions. Nevertheless, although requiring more operations and memory, SwiftNet <ref type="bibr" target="#b29">[30]</ref> and BiSeNet <ref type="bibr" target="#b30">[31]</ref> are still faster than many other methods, while achieving higher segmentation performance by simply using a pretrained ResNet18 as encoder. This can be deduced to utilizing early and high downsampling in the encoder, a light-weight decoder, and using standard 3×3 convolutions, which are currently implemented more efficiently than grouped or depth-wise convolutions and have large representational power.</p><p>Following SwiftNet and BiSeNet, our approach also uses a ResNet-based encoder. However, in order to further reduce inference time, we exchange the basic block in all ResNet layers with a more efficient block, based on factorized convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EFFICIENT RGB-D SEGMENTATION</head><p>The architecture of our Efficient Scene Analysis Network (ESANet) for RGB-D semantic segmentation is depicted in <ref type="figure" target="#fig_0">Fig. 2 (top)</ref>. It is inspired by the RGB segmentation approach SwiftNet <ref type="bibr" target="#b29">[30]</ref>, i.e., a shallow encoder with a pretrained ResNet18 backbone and large downsampling, a context module similar to the one in PSPNet <ref type="bibr" target="#b32">[33]</ref>, a shallow decoder with skip connections from the encoder, and final upsampling by a factor of 4. However, SwiftNet does not  incorporate depth information at all. Therefore, our ESANet uses an additional encoder for depth data. This depth encoder extracts complementary geometric information that is fused into the RGB encoder at several stages using an attention mechanism. Furthermore, both encoders use a revised architecture enabling faster inference. The decoder is comprised of multiple modules, each is upsampling the resulting feature maps by a factor of 2 and is refining the features using convolutions as well as by incorporating encoder features. Finally, the decoder maps the features to the classes and rescales the class mapping to the input resolution. Our entire network features simple components implemented in PyTorch <ref type="bibr" target="#b33">[34]</ref>. We do not use complex structures or specifically tailored operations as these are often incompatible for converting to ONNX <ref type="bibr" target="#b34">[35]</ref> or NVIDIA TensorRT and, thus, result in slower inference time.</p><formula xml:id="formula_0">Dec Mod Decoder Module RGB-D Fusion RGB-D Fusion RGB-D Fusion RGB-D Fusion RGB-D</formula><p>In the following, we explain each part of our network design in detail as well as its motivation. <ref type="figure" target="#fig_0">Fig. 2</ref> (bottom) depicts the exact structure of our network modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Encoder</head><p>The RGB and depth encoder both use a ResNet architecture <ref type="bibr" target="#b35">[36]</ref> as backbone. For efficiency reasons, we do not replace strided convolutions by dilated convolutions as in PSPNet <ref type="bibr" target="#b32">[33]</ref> or DeepLabv3 <ref type="bibr" target="#b36">[37]</ref>. Thus, the resulting feature maps at the end of the encoder are 32 times smaller than the input image. For a trade-off between speed and accuracy, we use ResNet34 but also show results for ResNet18 and ResNet50. We replace the basic block in each layer of ResNet18 and ResNet34 with a spatially factorized version. More precisely, each 3×3 convolution is replaced by a 3×1 and a 1×3 convolution with a ReLU in-between. The so-called Non-Bottleneck-1D-Block (NBt1D) is depicted in <ref type="figure" target="#fig_0">Fig. 2</ref> (violet) and was initially proposed in ERFNet <ref type="bibr" target="#b25">[26]</ref> for another network architecture. In our experiments, we show that this block can also be used in ResNet and simultaneously reduces inference time and increases segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RGB-D Fusion</head><p>At each of the five resolution stages in the encoders (see <ref type="figure" target="#fig_0">Fig. 2</ref>), depth features are fused into the RGB encoder. The features from both modalities are first reweighted with a Squeeze and Excitation (SE) module <ref type="bibr" target="#b37">[38]</ref> and then summed element-wisely, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref> (light green). Using this channel attention mechanism, the model can learn which features of which modality to focus on and which to suppress, depending on the given input. In our experiments, we show that this fusion mechanism notably improves segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Context Module</head><p>Due to the limited receptive field of ResNet <ref type="bibr" target="#b32">[33]</ref>, we additionally incorporate context information by aggregating features at different scales using several branches in a context module similar to the Pyramid Pooling Module in PSP-Net <ref type="bibr" target="#b32">[33]</ref> (see <ref type="figure" target="#fig_0">Fig. 2</ref> orange). Since NVIDIA TensorRT only supports pooling with fixed sizes, we carefully designed the context module such that the pooling sizes are always a factor of the input resolution of the context module and no adaptive pooling is required. Note that, depending on the image resolution of the respective dataset, the number of existing factors and, thus, the branches b and pooling sizes p bw ×p b h differ. Our experiments show that this additional context module improves segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Decoder</head><p>As shown in <ref type="figure" target="#fig_0">Fig 2,</ref> our decoder is comprised of three decoder modules (depicted in red in <ref type="figure" target="#fig_0">Fig 2)</ref>. Our decoder module extends the one of SwiftNet <ref type="bibr" target="#b29">[30]</ref>, which is comprised of a 3×3 convolution with a fixed number of 128 channels and a subsequent bilinear upsampling. However, our experiments show that for indoor RGB-D segmentation a more complex decoder is required. Therefore, we use 512 channels in the first decoder module and decrease the number of channels in each 3×3 convolution as the resolution increases. Moreover, we incorporate three additional Non-Bottleneck-1D-blocks to further increase segmentation performance.</p><p>Finally, we upsample the feature maps by a factor of 2. We do not use transposed convolutions for upsampling as they are computationally expensive and often introduce undesired gridding artifacts to the final segmentation, as shown in <ref type="figure">Fig. 3 (right)</ref>. Moreover, instead of using bilinear interpolation, we propose a novel light-weight learned upsampling method (see <ref type="figure" target="#fig_0">Fig. 2</ref> dark green), which achieves better segmentation results: In particular, we first use nearest neighbor upsampling to enlarge the resolution. Afterwards, a 3×3 depthwise convolution is applied to combine adjacent features. We initialize the kernels such that the whole learned upsampling initially mimics bilinear interpolation. However, our network is able to adapt the weights during training and, thus, can learn how to combine adjacent features in a more useful manner, which improves segmentation performance.</p><p>Although being upscaled, the resulting feature maps still lack fine-grained details that were lost during downsampling in the encoders. Therefore, we design skip connections from encoder to decoder stages of the same resolution. To be precise, we take the fused RGB-D encoder feature maps, project them with a 1×1 convolution to the same number of channels used in the decoder, and add them to the decoder feature maps. Incorporating these skip connections results in more detailed semantic segmentations.</p><p>Similar to <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b38">[39]</ref>, we only process feature maps in the decoder until they are 4× smaller than the input images and use a 3×3 convolution to map the features to the classes of the respective dataset. Two final learned upsampling modules restore the resolution of the input image.</p><p>Instead of calculating the training loss only at the final output scale, we add supervision to each decoder module. At each scale, a 1×1 convolution computes a segmentation of a smaller scale, which is supervised by the down-scaled ground truth segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate our approach on two commonly used RGB-D indoor datasets, namely SUNRGB-D <ref type="bibr" target="#b12">[13]</ref> and NYUv2 <ref type="bibr" target="#b11">[12]</ref> and present an ablation study to essential parts of our network. In order to demonstrate that our approach is suitable for other areas of application as well, we also show results on the Cityscapes <ref type="bibr" target="#b13">[14]</ref> dataset, the most widely used outdoor dataset for semantic segmentation. Finally, instead of reporting benchmark results only, we present qualitative results when using our approach in a robotic indoor application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details &amp; Datasets</head><p>We trained our networks using PyTorch <ref type="bibr" target="#b33">[34]</ref> for 500 epochs with batches of size 8. For optimization, we used both SGD with momentum of 0.9 and Adam <ref type="bibr" target="#b39">[40]</ref> with learning rates of {0.00125, 0.0025, 0.005, 0.01, 0.02, 0.04} and {0.0001, 0.0004}, respectively, and a small weight decay of 0.0001. We adapted the learning rate using PyTorch's onecycle learning rate scheduler. To further increase the number of training samples, we augmented the images using random scaling, cropping, and flipping. For RGB images, we also applied slight color jittering in HSV space.</p><p>The best models were chosen based on the mean intersection over union (mIoU). We used bilinear upsampling to rescale the resulting class mapping to the size of the ground truth segmentation before computing the argmax for the final segmentation mask.</p><p>NYUv2 &amp; SUNRGB-D: NYUv2 contains 1,449 indoor RGB-D images, of which 795 are used for training and 654 for testing. We used the common 40-class label setting. SUNRGB-D has 37 classes and consists of 10,335 indoor RGB-D images, including all images of NYUv2. There are 5,285 training and 5,050 testing images. Our ablation study is based on NYUv2 as it is smaller and, thus, leads to faster trainings. However, according to <ref type="bibr" target="#b40">[41]</ref>, training on a subset is sufficient for a reliable model selection. For both datasets, we used a network input resolution of 640×480 and applied median frequency class balancing <ref type="bibr" target="#b41">[42]</ref>. As the input to the context module has a resolution of 20×15 due to the downsampling of 32, we used b = 2 branches, one with global average pooling and one with a pooling size of 4×3.</p><p>Cityscapes: This dataset contains 5,000 images with finegrained annotation for 19 classes. The images have a high resolution of 2048×1024. There are 2,975 images for training, 500 for validation, and 1,525 for testing. Cityscapes also provides 20k coarsely annotated images, which we did not use for training. We computed corresponding depth images from the disparity images. Since we set the network input <ref type="bibr">RGB</ref>   <ref type="figure">Fig. 3</ref>: Qualitative comparison of upsampling methods on NYUv2 test set (same colors as in <ref type="figure">Fig. 1 and Fig. 6</ref>). resolution to 1024×512, the input to our context module has a resolution of 32×16, which allows b = 4 branches in the context module, one with global average pooling and the others with pooling sizes of 16×8, 8×4, and 4×2.</p><p>For further details and other hyperparameters, we refer to our implementation available on GitHub. <ref type="figure" target="#fig_1">Fig. 4</ref> compares our RGB-D approach on NYUv2 to single-modality baselines for RGB and depth (single encoder) as well as evaluates different encoder backbones. As expected, neither processing depth data nor RGB data alone reach the segmentation performance of our proposed RGB-D network. Remarkably, the shallow ResNet18-based RGB-D network performs better than the much deeper ResNet50-based RGB network while still being faster. Moreover, replacing ResNet's basic block with Non-Bottleneck-1D (NBt1D) block can further improve both segmentation and inference time. Note that ResNet50 incorporates bottleneck blocks, which cannot be replaced the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on NYUv2 &amp; SUNRGB-D</head><p>Tab. I lists the results of our RGB-D approach for both indoor datasets. For the larger SUNRGB-D dataset, a similar trend can be observed. Compared to the state of the art, our smaller ESANet achieves similar segmentation results as the often much deeper networks. Besides focusing on segmentation performance alone, we also strive for low inference time on the embedded hardware of our robots. Therefore, we measured the inference time for all available approaches on a NVIDIA Jetson AGX Xavier using NVIDIA TensorRT. For our carefully designed ESANet, NVIDIA TensorRT enables up to 5× faster inference compared to PyTorch. As shown in Tab. I (last column), our approach enables much faster inference while performing on par or even better than other approaches. For our application, we choose ESANet with ResNet34 backbone and Non-Bottleneck-1D (NBt1D) block (printed in bold in Tab. I) as it offers the best tradeoff between inference time and performance. The last row in Tab. I further indicates that additional pretraining on synthetic data, such as SceneNet <ref type="bibr" target="#b42">[43]</ref>, should be preferred to deeper backbones, especially if the target dataset is small. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study on NYUv2</head><p>Fig <ref type="figure" target="#fig_2">. 5</ref> shows the ablation study for fundamental parts of our network architecture and justifies our design choices. Furthermore, it indicates the impact of each part when it is necessary to adapt our selected network to deviating realtime requirements.</p><p>As shown in purple, a shallow decoder similar to Swift-Net <ref type="bibr" target="#b29">[30]</ref> is not as good as more complex decoders. Therefore, we gradually increased the number of additional NBt1D blocks in the decoder module. Apparently, a fixed number of three blocks in each decoder module performs better than a different number or a reversed layout of the encoder's design.</p><p>In dark green, different upsampling methods in the decoder are displayed. Although increasing inference time, the learned upsampling improves mIoU by 0.9. Moreover, as shown in <ref type="figure">Fig. 3</ref>, the obtained segmentation contains more fine-grained details compared to using bilinear interpolation. It further prevents gridding artifacts introduced by transposed convolutions as used in ACNet <ref type="bibr" target="#b10">[11]</ref> or RedNet <ref type="bibr" target="#b9">[10]</ref>.</p><p>As shown in gray in <ref type="figure" target="#fig_2">Fig. 5</ref>, a context module, encoderdecoder skip connections, as well as reweighting modalityspecific features with Squeeze-and-Excitation before fusion, independently improve segmentation performance. Incorporating all three network parts leads to the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results on Cityscapes</head><p>To demonstrate that our approach is applicable to other areas such as outdoor environments as well, in Tab. II, we further present an evaluation on the Cityscapes dataset.</p><p>We first focus on the smaller resolution of 1024×512 as it is commonly used for efficient segmentation. Moreover, since most approaches rely on RGB as input solely, we start by comparing a single-modality RGB version of our approach. Efficient approaches with custom architectures such as ERFNet <ref type="bibr" target="#b25">[26]</ref>, LEDNet <ref type="bibr" target="#b26">[27]</ref>, and ESPNetv2 <ref type="bibr" target="#b31">[32]</ref>   We assume that this can be deduced to the fact that the disparity images of Cityscapes are not as precise as the indoor depth images of NYUv2 and SUNRGB-D. Compared to the RGB-D approach LDFNet <ref type="bibr" target="#b43">[44]</ref> with similar inference time, we achieve notably higher mIoU. For completeness, we also evaluated our networks on the full resolution of 2048×1024. Compared to other methods, our ESANet lies in between mobile (SwiftNet, BiSeNet) and non-mobile approaches for both mIoU and inference time.</p><p>However, compared to SwiftNet (RGB, 2048×1024), our ESANet-R34-NBt1D achieves similar segmentation performance and slightly faster inference while processing RGB-D inputs with the smaller input resolution of 1024×512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Application on our Robots</head><p>Instead of evaluating on benchmark datasets only, we further present qualitative results with a Kinect2 sensor <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> in one of our indoor applications. We deployed our proposed ESANet-R34-NBt1D to our robot in order to  accomplish the complex system for semantic scene analysis shown in <ref type="figure">Fig. 1</ref>. The obtained segmentation masks enrich the robot's visual perception enabling stronger person perception and robust semantic mapping including a refined floor representation which indicates free space. <ref type="figure">Fig. 6</ref> provides an insight into the entire system. For further qualitative results and a comparison to non-semantic scene perception, we refer to the attached video or our repository on GitHub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have presented an efficient RGB-D segmentation approach, called ESANet, which is characterized by two enhanced ResNet-based encoders utilizing the Non-Bottleneck-1D block, an attention-based fusion for incorporating depth information, and a decoder utilizing a novel learned upsampling. On the indoor datasets NYUv2 and SUNRGB-D, our ESANet performs on par or even better while enabling much faster inference compared to other state-of-the-art methods. Thus, it is well suited for embedding in a complex system for scene analysis on mobile robots given limited hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wall Floor Ceiling Lamp</head><p>Chair <ref type="table">Table  Sofa</ref> Picture Box TV Bag Pillow <ref type="figure">Fig. 6</ref>: Application in our robotic scene analysis system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>-1D (NBt1D) Legend: kw×kh, C : Convolution with kernel size kw×kh and C output channels, S2: Stride 2, BN: Batch Normalization, Up.: Upsampling, DW: Depthwise, : Concatenation Overview of our proposed ESANet for efficient RGB-D segmentation (top) and specific network parts (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Comparison of RGB-D to RGB and depth networks (single encoder) and different backbones on NYUv2 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Ablation Study on NYUv2 test. Each color indicates modifying one aspect: purple: number of NBt1D blocks in decoder module, dark green: upsampling method, and gray: usage of specific network parts with CM : no context module, Skip: no encoder-decoder skip connections, and SE: no Squeeze-and-Excitation before fusing RGB and depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>are quite</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>NYUv2</cell><cell>SUN-RGB-D</cell><cell>FPS</cell></row><row><cell>FuseNet [9]</cell><cell>2× VGG16</cell><cell>-</cell><cell>37.29</cell><cell>†</cell></row><row><cell>RedNet [10]</cell><cell>2× R34</cell><cell>-</cell><cell>46.8</cell><cell>26.0</cell></row><row><cell>SSMA [24]</cell><cell>2× mod. R50</cell><cell>-</cell><cell>44.43</cell><cell>12.4</cell></row><row><cell>MMAF-Net [25]</cell><cell>2× R50</cell><cell>-</cell><cell>45.5</cell><cell>N/A</cell></row><row><cell>RedNet [10]</cell><cell>2× R50</cell><cell>-</cell><cell>47.8</cell><cell>22.1</cell></row><row><cell>RDFNet [23]</cell><cell>2× R50</cell><cell>47.7*</cell><cell>-</cell><cell>7.2</cell></row><row><cell>ACNet [11]</cell><cell>3× R50</cell><cell>48.3</cell><cell>48.1</cell><cell>16.5</cell></row><row><cell>SA-Gate [22]</cell><cell>2× R50</cell><cell>50.4</cell><cell>49.4*</cell><cell>11.9</cell></row><row><cell>SGNet [19]</cell><cell>R101</cell><cell>49.0</cell><cell>47.1</cell><cell>N/A</cell></row><row><cell>Idempotent [21]</cell><cell>2× R101</cell><cell>49.9</cell><cell>47.6</cell><cell>N/A</cell></row><row><cell>2.5D Conv [16]</cell><cell>R101</cell><cell>48.5</cell><cell>48.2</cell><cell>N/A</cell></row><row><cell>MMAF-Net [25]</cell><cell>2× R152</cell><cell>44.8</cell><cell>47.0</cell><cell>N/A</cell></row><row><cell>RDFNet [23]</cell><cell>2× R152</cell><cell>50.1*</cell><cell>47.7*</cell><cell>5.8</cell></row><row><cell>ESANet-R18</cell><cell>2× R18</cell><cell>47.32</cell><cell>46.24</cell><cell>34.7</cell></row><row><cell>ESANet-R18-NBt1D</cell><cell cols="2">2× R18 NBt1D 48.17</cell><cell>46.85</cell><cell>36.3</cell></row><row><cell>ESANet-R34</cell><cell>2× R34</cell><cell>48.81</cell><cell>47.08</cell><cell>27.5</cell></row><row><cell>ESANet-R34-NBt1D</cell><cell cols="2">2× R34 NBt1D 50.30</cell><cell>48.17</cell><cell>29.7</cell></row><row><cell>ESANet-R50</cell><cell>2× R50</cell><cell>50.53</cell><cell>48.31</cell><cell>22.6</cell></row><row><cell cols="3">ESANet (pre. SceneNet) 2× R34 NBt1D 51.58</cell><cell>48.04</cell><cell>29.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell>: Mean intersection over union of our ESANet com-</cell></row><row><cell>pared to state-of-the-art methods on NYUv2 and SUNRGB-</cell></row><row><cell>D test set ordered by SUNRGB-D performance and backbone</cell></row><row><cell>complexity. FPS is reported for NVIDIA Jetson AGX Xavier</cell></row><row><cell>(Jetpack 4.4, TensorRT 7.1, Float16). Legend: R: ResNet,</cell></row><row><cell>*: additional test-time augmentation, i.e., flipping or multi-</cell></row><row><cell>scale (not timed), N/A: no implementation available,  †: in-</cell></row><row><cell>cludes operations, which are not supported by TensorRT, and</cell></row><row><cell>: expected to be slower due to complex backbone.</cell></row><row><cell>fast but also perform notably worse than our ESANet. Com-</cell></row><row><cell>pared to ERFNet, LEDNet, and ESPNetv2, SwiftNet [30]</cell></row><row><cell>is both faster and achieves higher mIoU. Nevertheless, with</cell></row><row><cell>an input resolution of 1024×512, our ESANet-R34-NBt1D</cell></row><row><cell>still exceed 30 FPS while outperforming all other efficient</cell></row><row><cell>approaches by at least 2.2 mIoU. Incorporating depth further</cell></row><row><cell>increases segmentation performance. However, the perfor-</cell></row><row><cell>mance gain is not as high as for the indoor dataset NYUv2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II :</head><label>II</label><figDesc>Mean intersection over union of our ESANet on Cityscapes for both common input resolutions compared to state-of-the-art methods. FPS is reported for NVIDIA Jetson AGX Xavier (Jetpack 4.4, TensorRT 7.1, Float16). Legend:: test server result, *: trained with additional coarse data.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TOOMAS: Interactie shopping guide robots in everyday use -final implementation and experiences from long-term field trials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2005" to="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Socially compliant human-robot interaction for autonomous scanning tasks in supermarket environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lewandowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symp. on Robot and Human Interactive Communication</title>
		<meeting><address><addrLine>RO-MAN</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mobile robot companion for walking training of stroke patients in clinical post-stroke rehabilitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation (ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1028" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autonomous mobile gait training robot for orthopedic rehabilitation in a clinical environment*</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Trinh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robot and Human Interactive Communication</title>
		<meeting><address><addrLine>RO-MAN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robot companion for domestic health assistance: Implementation, test and case study under everyday conditions in private apartments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5992" to="5999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Living with a mobile companion robot in your own apartment -final implementation and results of a 20-weeks field study with 20 seniors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation (ICRA)</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2253" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-task deep learning for depth-based person perception in mobile robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seichter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generic 2D/3D SLAM with NDT maps for lifelong application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Einhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europ. Conf. on Mobile Robots (ECMR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FuseNet: Incorporating Depth into Semantic Segmentation via Fusion-based CNN Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">RedNet: Residual Encoder-Decoder Network for indoor RGB-D Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01054</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ACNet: Attention Based Network to Exploit Complementary Features for RGBD Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Indoor Segmentation and Support Inference from RGBD Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europ. Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3D Geometry-Aware Semantic Labeling of Outdoor Street Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2343" to="2349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">2.5D Convolution for RGB-D Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1410" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Malleable 2.5D Convolution: Learning Receptive Fields along the Depth-axis for RGB-D Scene Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europ. Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depth-Aware CNN for RGB-D Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europ. Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="144" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Spatial Information Guided Convolution for Real-Time RGBD Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04534</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3D Neighborhood Convolution: Learning Depth-Aware Features for RGB-D and RGB Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Coupling Two-Stream RGB-D Semantic Segmentation Network by Idempotent Mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1850" to="1854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bi-directional Cross-Modality Feature Propagation with Separation-and-Aggregation Gate for RGB-D Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europ. Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="561" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RDFNet: RGB-D Multi-level Residual Feature Fusion for Indoor Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-supervised model adaptation for multimodal semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multi-Modal Attention-based Fusion Model for Semantic Segmentation of RGB-Depth Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fooladgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11691</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Intelligent Transportation Systems (ITS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">LEDnet: A Lightweight Encoder-Decoder Network for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1860" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DABNet: Depth-wise Asymmetric Bottleneck for Realtime Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient dense modules of asymmetric convolution for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. on Multimedia in Asia</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">In Defense of Pre-trained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oršić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BiSeNet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europ. Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9190" to="9200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Onnx: Open neural network exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<ptr target="https://github.com/onnx/onnx" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Rethinking Atrous Convolution for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europ. Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learning Representation (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Small Data, Big Decisions: Model Selection in the Small-Data Regime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bornschein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mccormac</surname></persName>
		</author>
		<title level="m">SceneNet RGB-D: Can 5M Synthetic Images Beat Generic ImageNet Pre-training on Indoor Segmentation?&quot; Int. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2697" to="2706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Incorporating Luminance, Depth and Color Information by a Fusion-Based Network for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2374" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Libfreenect2: Release 0.2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingzhu</forename><surname>Xiang</surname></persName>
		</author>
		<ptr target="https://zenodo.org/record/50641" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient multi-frequency phase unwrapping using kernel density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europ. Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="170" to="185" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
