<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Proxy Anchor Loss for Deep Metric Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyeon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<email>mscho@postech.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>POSTECH</roleName><forename type="first">Suha</forename><surname>Kwak</surname></persName>
							<email>suha.kwak@postech.ac.kr</email>
						</author>
						<title level="a" type="main">Proxy Anchor Loss for Deep Metric Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing metric learning losses can be categorized into two classes: pair-based and proxy-based losses. The former class can leverage fine-grained semantic relations between data points, but slows convergence in general due to its high training complexity. In contrast, the latter class enables fast and reliable convergence, but cannot consider the rich datato-data relations. This paper presents a new proxy-based loss that takes advantages of both pair-and proxy-based methods and overcomes their limitations. Thanks to the use of proxies, our loss boosts the speed of convergence and is robust against noisy labels and outliers. At the same time, it allows embedding vectors of data to interact with each other through its gradients to exploit data-to-data relations. Our method is evaluated on four public benchmarks, where a standard network trained with our loss achieves state-ofthe-art performance and most quickly converges.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning a semantic distance metric has been a crucial step for many applications such as content-based image retrieval <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>, face verification <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>, person re-identification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38]</ref>, few-shot learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>, and representation learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41</ref>]. Following their great success in visual recognition, deep neural networks have been employed recently for metric learning. The networks are trained to project data onto an embedding space in which semantically similar data (e.g., images of the same class) are closely grouped together. Such a quality of the embedding space is given mainly by loss functions used for training the networks, and most of the losses are categorized into two classes: pair-based and proxy-based.</p><p>The pair-based losses are built upon pairwise distances between data in the embedding space. A seminal example is Contrastive loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>, which aims to minimize the distance between a pair of data if their class labels are identical and to separate them otherwise. Recent pair-based losses consider a group of pairwise distances to handle relations between more than two data <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proxy-Anchor (Ours)</head><p>MS <ref type="bibr" target="#b33">[34]</ref> Proxy-NCA <ref type="bibr" target="#b20">[21]</ref> Semi-Hard Triplet <ref type="bibr" target="#b24">[25]</ref> N-Pair <ref type="bibr" target="#b26">[27]</ref> Time Per Epoch  <ref type="bibr" target="#b16">[17]</ref> dataset. Note that all methods were trained with batch size of 150 on a single Titan Xp GPU. Our loss enables to achieve the highest accuracy, and converge faster than the baselines in terms of both the number of epochs and the actual training time.</p><p>These losses provide rich supervisory signals for training embedding networks by comparing data to data and examining fine-grained relations between them, i.e., data-to-data relations. However, since they take a tuple of data as a unit input, the losses cause prohibitively high training complexity <ref type="bibr" target="#b0">1</ref> </p><formula xml:id="formula_0">, O(M 2 ) or O(M 3 )</formula><p>where M is the number of training data, thus slow convergence. Furthermore, some tuples do not contribute to training or even degrade the quality of the learned embedding space. To resolve these issues, learning with the pair-based losses often entails tuple sampling techniques <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref>, which however have to be tuned by hand and may increase the risk of overfitting. The proxy-based losses resolve the above complexity issue by introducing proxies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. A proxy is a representative of a subset of training data and learned as a part of the network parameters. Existing losses in this category consider each data point as an anchor, associate it with proxies instead of other images, and encourage the anchor to be close to proxies of the same class and far apart from those of different classes. Proxy-based losses reduce the training  . Comparison between popular metric learning losses and ours. Small nodes are embedding vectors of data in a batch, and black ones indicate proxies; their different shapes represent distinct classes. The associations defined by the losses are expressed by edges, and thicker edges get larger gradients. Also, embedding vectors associated with the anchor are colored in red if they are of the same class of the anchor (i.e., positive) and in blue otherwise (i.e., negative). (a) Triplet loss <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref> associates each anchor with a positive and a negative data point without considering their hardness. (b) N -pair loss <ref type="bibr" target="#b26">[27]</ref> and (c) Lifted Structure loss <ref type="bibr" target="#b28">[29]</ref> reflect hardness of data, but do not utilize all data in the batch. (d) Proxy-NCA loss <ref type="bibr" target="#b20">[21]</ref> cannot exploit data-to-data relations since it associates each data point only with proxies. (e) Our loss handles entire data in the batch, and associates them with each proxy with consideration of their relative hardness determined by data-to-data relations. See the text for more details. complexity and enable faster convergence since the number of proxies is substantially smaller than that of training data in general. Further, these losses tend to be more robust against label noises and outliers. However, since they associate each data point only with proxies, proxy-based losses can leverage only data-to-proxy relations, which are impoverished compared to the rich data-to-data relations available for pair-based losses.</p><p>In this paper, we propose a novel proxy-based loss called Proxy-Anchor loss, which takes good points of both proxybased and pair-based losses while correcting their defects. Unlike the existing proxy-based losses, the proposed loss utilizes each proxy as an anchor and associates it with all data in a batch. Specifically, for each proxy, the loss aims to pull data of the same class close to the proxy and to push others away in the embedding space. Due to the use of proxies, our loss boosts the speed of convergence with no hyperparameter for tuple sampling, and is robust against noisy labels and outliers. At the same time, it can take data-to-data relations into account like pair-based losses; this property is given by associating all data in a batch with each proxy so that the gradients with respect to a data point are weighted by its relative proximity to the proxy (i.e., relative hardness) affected by the other data in the batch. Thanks to the above advantages, a standard embedding network trained with our loss achieves state-of-the-art accuracy and most quickly converges as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The contribution of this paper is three-fold:</p><p>• We propose a novel metric learning loss that takes advantages of both pair-based and proxy-based methods; it leverages rich data-to-data relations and enables fast and reliable convergence. • A standard embedding network trained with our loss achieves state-of-the-art performance on the four public benchmarks for metric learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>. • Our loss speeds up convergence greatly without careful data sampling; its convergence is even faster than those of Proxy-NCA <ref type="bibr" target="#b20">[21]</ref> and Multi-Similarity loss <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we categorize metric learning losses into two classes, pair-based and proxy-based losses, then review relevant methods for each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pair-based Losses</head><p>Contrastive loss <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9]</ref> and Triplet loss <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref> are seminal examples of loss functions for deep metric learning. Contrastive loss takes a pair of embedding vectors as input, and aims to pull them together if they are of the same class and push them apart otherwise. Triplet loss considers a data point as an anchor, associates it with a positive and a negative data point, and constrains the distance of the anchorpositive pair to be smaller than that of the anchor-negative pair in the embedding space as illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>(a).</p><p>Recent pair-based losses aim to leverage higher order relations between data and reflect their hardness for further enhancement. As generalizations of Triplet loss, N -pair loss <ref type="bibr" target="#b26">[27]</ref> and Lifted Structure loss <ref type="bibr" target="#b28">[29]</ref> associate an anchor with a single positive and multiple negative data points, and pull the positive to the anchor and push the negatives away from the anchor while considering their hardness. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>(b) and 2(c), however, these losses do not utilize entire data in a batch since they sample the same number of data per negative class, thus may drop informative examples during training. In contrast, Ranked List loss <ref type="bibr" target="#b34">[35]</ref> takes into account all positive and negative data in a batch and aims to separate the positive and negative sets. Multi-Similarity loss <ref type="bibr" target="#b33">[34]</ref> also considers every pair of data in a batch, and assigns a weight to each pair according to three complementary types of similarity to focus more on useful pairs for improving performance and convergence speed.</p><p>Pair-based losses enjoy rich and fine-grained data-todata relations as they examine tuples (i.e., data pairs or their combinations) during training. However, since the number of tuples increases polynomially with the number of training data, their training complexity is prohibitively high and convergence is slow. In addition, a large amount of tuples are not effective and sometimes even degrade the quality of the learned embedding space <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37]</ref>. To address this issue, most pair-based losses entail tuple sampling techniques <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref> to select and utilize tuples that will contribute to training. However, these techniques involve hyperparameters that have to be tuned carefully, and may increase the risk of overfitting since they rely mostly on local pairwise relations within a batch. Another way to alleviating the complexity issue is to assign larger weights to more useful pairs during training as in <ref type="bibr" target="#b33">[34]</ref>, which however also incorporates a sampling technique.</p><p>Our loss resolves this complexity issue by adopting proxies, which enables faster and more reliable convergence compared to pair-based losses. Furthermore, it demands no additional hyperparameter for tuple sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Proxy-based Losses</head><p>Proxy-based metric learning is a relatively new approach that can address the complexity issue of the pair-based losses. A proxy means a representative of a subset of training data and is estimated as a part of the embedding network parameters. The common idea of the methods in this category is to infer a small set of proxies that capture the global structure of an embedding space and relate each data point with the proxies instead of the other data points during training. Since the number of proxies is significantly smaller than that of training data, the training complexity can be reduced substantially.</p><p>The first proxy-based loss is Proxy-NCA <ref type="bibr" target="#b20">[21]</ref>, which is an approximation of Neighborhood Component Analysis (NCA) <ref type="bibr" target="#b7">[8]</ref> using proxies. In its standard setting, Proxy-NCA loss assigns a single proxy for each class, associates a data point with proxies, and encourages the positive pair to be close and negative pairs to be far apart, as illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>(d). SoftTriple loss <ref type="bibr" target="#b22">[23]</ref>, an extension of SoftMax loss for classification, is similar to Proxy-NCA yet assigns multiple proxies to each class to reflect intra-class variance. Manifold Proxy loss <ref type="bibr" target="#b0">[1]</ref> is an extension of N -pair loss using proxies, and improves the performance by adopting a manifold-aware distance instead of Euclidean distance to measure the semantic distance in the embedding space.</p><p>Using proxies in these losses helps improve training convergence greatly, but has an inherent limitation as a side effect: Since each data point is associated only with proxies, the rich data-to-data relations that are available for the pair-based methods are not accessible anymore. Our loss can overcome this limitation since its gradients reflect rela-tive hardness of data and allow their embedding vectors to interact with each other during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>We propose a new metric learning loss called Proxy-Anchor loss to overcome the inherent limitations of the previous methods. The loss employs proxies that enable fast and reliable convergence as in proxy-based losses. Also, although it is built upon data-proxy relations, our loss can utilize data-to-data relations during training like pair-based losses since it enables embedding vectors of data points to be affected by each other through its gradients. This property of our loss improves the quality of the learned embedding space substantially.</p><p>In this section, we first review Proxy-NCA loss <ref type="bibr" target="#b20">[21]</ref>, a representative proxy-based loss, for comparison to our Proxy-Anchor loss. We then describe our Proxy-Anchor loss in detail and analyze its training complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Review of Proxy-NCA Loss</head><p>In the standard setting, Proxy-NCA loss <ref type="bibr" target="#b20">[21]</ref> assigns a proxy to each class so that the number of proxies is the same with that of class labels. Given an input data point as an anchor, the proxy of the same class of the input is regarded as positive and the other proxies are negative. Let x denote the embedding vector of the input, p + be the positive proxy, and p − be a negative proxy. The loss is then given by</p><formula xml:id="formula_1">(X) = x∈X − log e s(x,p + ) p − ∈P − e s(x,p − ) (1) = x∈X − s(x, p + ) + LSE p − ∈P − s(x, p − ) ,<label>(2)</label></formula><p>where X is a batch of embedding vectors, P − is the set of negative proxies, and s(·, ·) denotes the cosine similarity between two vectors. In addition, LSE in Eq. (2) means the Log-Sum-Exp function, a smooth approximation to the max function. The gradient of Proxy-NCA loss with respect to s(x, p) is given by</p><formula xml:id="formula_2">∂ (X) ∂s(x, p) =          −1, if p = p + , e s(x,p) p − ∈P − e s(x,p − ) , otherwise.<label>(3)</label></formula><p>Eq.</p><p>(3) shows that minimizing the loss encourages x and p + to be close to each other, and x and p − to be far away.</p><p>In particular, x and p + are pulled together by the constant power, while x and p − closer to each other (i.e., harder negative) are more strongly pushed away. Proxy-NCA loss enables fast convergence thanks to its low training complexity, O(M C) where M is the number of training data and C is that of classes, which is substantially lower than O(M 2 ) or O(M 3 ) of pair-based losses since C M ; refer to Section 3.3 for details. Also, proxies are robust against outliers and noisy labels since they are trained to represent groups of data. However, since the loss associates each embedding vector only with proxies, it cannot exploit fine-grained data-to-data relations. This drawback limits the capability of embedding networks trained with Proxy-NCA loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proxy-Anchor Loss</head><p>Our Proxy-Anchor loss is designed to overcome the limitation of Proxy-NCA while keeping the low training complexity. The main idea is to take each proxy as an anchor and associate it with entire data in a batch, as illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>(e), so that the data interact with each other through the proxy anchor during training. Our loss assigns a proxy for each class following the standard proxy assignment setting of Proxy-NCA, and is formulated as</p><formula xml:id="formula_3">(X) = 1 |P + | p∈P + log 1 + x∈X + p e −α(s(x,p)−δ) + 1 |P | p∈P log 1 + x∈X − p e α(s(x,p)+δ) ,<label>(4)</label></formula><p>where δ &gt; 0 is a margin, α &gt; 0 is a scaling factor, P indicates the set of all proxies, and P + denotes the set of positive proxies of data in the batch. Also, for each proxy p, a batch of embedding vectors X is divided into two sets: X + p , the set of positive embedding vectors of p, and X − p = X − X + p . The proposed loss can be rewritten in an easierto-interpret form as</p><formula xml:id="formula_4">(X) = 1 |P + | p∈P + Softplus LSE x∈X + p −α(s(x, p) − δ) + 1 |P | p∈P Softplus LSE x∈X − p α(s(x, p) + δ) ,<label>(5)</label></formula><p>where Softplus(z) = log (1 + e z ), ∀z ∈ R, and is a smooth approximation of ReLU.</p><p>How it works: Regarding Log-Sum-Exp as the max function, it is easy to notice that the loss aims to pull p and its most dissimilar positive example (i.e., hardest positive example) together, and to push p and its most similar negative example (i.e., hardest negative example) apart. Due to the nature of Log-Sum-Exp, the loss in practice pulls and pushes all embedding vectors in the batch, but with different degrees of strength that are determined by their relative hardness. This characteristic is demonstrated by the gradi-ent of our loss with respect to s(x, p), which is given by</p><formula xml:id="formula_5">∂ (X) ∂s(x, p) =                    1 |P + | −α h + p (x) 1 + x ∈X + p h + p (x ) , ∀x ∈ X + p , 1 |P | α h − p (x) 1 + x ∈X − p h − p (x ) , ∀x ∈ X − p ,<label>(6)</label></formula><p>where h + p (x) = e −α(s(x,p)−δ) and h − p (x) = e α(s(x,p)+δ) are positive and negative hardness metrics for embedding vector x given proxy p, respectively; h + p (x) is large when the positive embedding vector x is far from p, and h − p (x) is large when the negative embedding vector x is close to p. The scaling parameter α and margin δ control the relative hardness of data points, and in consequence, determine how strongly pull or push their embedding vectors.</p><p>As shown in the above equations, the gradient for s(x, p) is affected by not only x but also other embedding vectors in the batch; the gradient becomes larger when x is harder than the others. In this way, our loss enables embedding vectors in the batch to interact with each other and reflects their relative hardness through the gradients, which helps enhance the quality of the learned embedding space.</p><p>Comparison to Proxy-NCA: The key difference and advantage of Proxy-Anchor over Proxy-NCA is the active consideration of relative hardness based on data-to-data relations. This property enables Proxy-Anchor loss to provide richer supervisory signals to embedding networks during training. The gradients of the two losses demonstrate this clearly. In Proxy-NCA loss, the scale of the gradient is constant for every positive example and that of a negative example is calculated by taking only few proxies into account as shown in Eq. (3). In particular, the constant gradient scale for positive examples damages the flexibility and generalizability of embedding networks <ref type="bibr" target="#b36">[37]</ref>. In contrast, Proxy-Anchor loss determines the scale of the gradient by taking relative hardness into consideration for both positive and negative examples as shown in Eq. (6). This feature of our loss allows the embedding network to consider data-to-data relations that are ignored in Proxy-NCA and observe much larger area of the embedding space during training than Proxy-NCA. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates these differences between the two losses in terms of handling the relative hardness of embedding vectors. In addition, unlike Proxy-Anchor loss, the margin imposed in our loss leads to intra-class compactness and inter-class separability, resulting in a more discriminative embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Complexity Analysis</head><p>Let M , C, B, and U denote the numbers of training samples, classes, batches per epoch, and proxies held by each class, respectively. U is 1 thus ignored in most of proxybased losses including ours, but is nontrivial for those managing multiple proxies per class such as SoftTriple loss <ref type="bibr" target="#b22">[23]</ref>. <ref type="table" target="#tab_0">Table 1</ref>  The complexity of Proxy-NCA <ref type="bibr" target="#b20">[21]</ref> is also O(M C) since each data point is associated with one positive proxy and C−1 negative proxies as can be seen in Eq. (2). On the other hand, SoftTriple loss <ref type="bibr" target="#b22">[23]</ref>, a modification of SoftMax using multiple proxies per class, associates each data point with U positive proxies and U (C −1) negative proxies. The total training complexity of this loss is thus O(M CU 2 ). In conclusion, the complexity of our loss is the same with or even lower than that of other proxy-based losses.</p><p>The training complexity of pair-based losses is higher than that of proxy-based ones. Since Contrastive loss <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9]</ref> takes a pair of data as input, its training complexity is O(M 2 ). On the other hand, Triplet loss that examines triplets of data has complexity O(M 3 ), which can be reduced by triplet mining strategies. For example, semi-hard mining <ref type="bibr" target="#b24">[25]</ref> reduces the complexity to O(M 3 /B 2 ) by selecting negative pairs that are located within a neighborhood of anchor but sufficiently far from it. Similarly, Smart mining <ref type="bibr" target="#b9">[10]</ref>  O(M C) SoftTriple <ref type="bibr" target="#b22">[23]</ref> O(M CU 2 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pair</head><p>Contrastive <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9]</ref> O(M 2 ) Triplet (Semi-Hard) <ref type="bibr" target="#b24">[25]</ref> O(M 3 /B 2 ) Triplet (Smart) <ref type="bibr" target="#b9">[10]</ref> O(M 2 ) N -pair <ref type="bibr" target="#b26">[27]</ref> O(M 3 ) Lifted Structure <ref type="bibr" target="#b28">[29]</ref> O(M 3 ) hard triplets using an approximated nearest neighbor index. However, even with these techniques, the training complexity of Triplet loss is still high. Like Triplet loss, N -pair loss <ref type="bibr" target="#b26">[27]</ref> and Lifted Structure loss <ref type="bibr" target="#b28">[29]</ref> that compare each positive pair of data to multiple negative pairs also have complexity O(M 3 ). The training complexity of these losses becomes prohibitively high as the number of training data M increases, which slows down the speed of convergence as demonstrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, our method is evaluated and compared to current state-of-the-art on the four benchmark datasets for deep metric learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>. We also investigate the effect of hyperparameters and embedding dimensionality of our loss to demonstrate its robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall@K</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB-200-2011</head><p>Cars </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We employ CUB-200-2011 <ref type="bibr" target="#b35">[36]</ref>, Cars-196 <ref type="bibr" target="#b16">[17]</ref>, Stanford Online Product (SOP) <ref type="bibr" target="#b28">[29]</ref> and In-shop Clothes Retrieval (In-Shop) <ref type="bibr" target="#b18">[19]</ref> datasets for evaluation. For CUB-200-2011, we use 5,864 images of its first 100 classes for training and 5,924 images of the other classes for testing. For Cars-196, 8,054 images of its first 98 classes are used for training and 8,131 images of the other classes are kept for testing. For SOP, we follow the standard dataset split in <ref type="bibr" target="#b28">[29]</ref> using 59,551 images of 11,318 classes for training and 60,502 images of the rest classes for testing. Also for In-Shop, we follow the setting in <ref type="bibr" target="#b18">[19]</ref> using 25,882 images of the first 3,997 classes for training and 28,760 images of the other classes for testing; the test set is further partitioned into a query set with 14,218 images of 3,985 classes and a gallery set with 12,612 images of 3,985 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Embedding network: For a fair comparison to previous work, the Inception network with batch normalization <ref type="bibr" target="#b11">[12]</ref> pre-trained for ImageNet classification <ref type="bibr" target="#b4">[5]</ref> is adopted as our embedding network. We change the size of its last fully connected layer according to the dimensionality of embedding vectors, and L 2 -normalize the final output. Training: In every experiment, we employ AdamW optimizer <ref type="bibr" target="#b19">[20]</ref>, which has the same update step of Adam <ref type="bibr" target="#b15">[16]</ref> yet decays the weight separately. Our model is trained for 40 epochs with initial learning rate 10 −4 on the CUB-200-2011 and Cars-196, and for 60 epochs with initial learning rate 6 · 10 −4 on the SOP and In-shop. The learning rate for proxies is scaled up 100 times for faster convergence. Input batches are randomly sampled during training. Proxy setting: We assign a single proxy for each semantic class following Proxy-NCA <ref type="bibr" target="#b20">[21]</ref>. The proxies are initialized using a normal distribution to ensure that they are uniformly distributed on the unit hypersphere. Image setting: Input images are augmented by random cropping and horizontal flipping during training while they are center-cropped in testing. The default size of cropped images is 224×224 as in most of previous work, but for comparison to HORDE <ref type="bibr" target="#b12">[13]</ref>, we also implement models trained and tested with 256×256 cropped images. Hyperparameter setting: α and δ in Eq. (4) is set to 32 and 10 −1 , respectively, for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to Other Methods</head><p>We demonstrate the superiority of our Proxy-Anchor loss quantitatively by evaluating its image retrieval performance on the four benchmark datasets. For a fair comparison to previous work, the accuracy of our model is measured in three different settings: 64/128 embedding dimension with the default image size (224×224), 512 embedding dimension with the default image size, and 512 embedding dimension with the larger image size (256×256).</p><p>Results on the CUB-200-2011 and Cars-196 datasets are summarized in <ref type="table">Table 2</ref>. Our model outperforms all the previous arts including ensemble methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref> in all the three settings. In particular, on the challenging CUB-200-2011 dataset, it improves the previous best score by a large margin, 2.7% in Recall@1. As reported in <ref type="table" target="#tab_0">Table 3,   Recall@K   1</ref> 10 100 1000 Clustering 64 <ref type="bibr" target="#b27">[28]</ref> 67.0 83.7 93.2 -Proxy-NCA 64 <ref type="bibr" target="#b20">[21]</ref> 73.7 ---MS 64 <ref type="bibr" target="#b33">[34]</ref> 74.  our model also achieves state-of-the-art performance on the SOP dataset. It outperforms previous models in all the cases except for Recall@10 and Recall@100 with 64 dimensional embedding, but even in these cases it achieves the second best. Finally, on the In-Shop dataset, it attains the best scores in all the three settings as shown in <ref type="table">Table 4</ref>. For all the datasets, our model with the larger crop size and 512 dimensional embedding achieves the state-of-theart performance. Also note that our model with the low embedding dimension often outperforms existing models with the high embedding dimension, which suggests that our loss allows to learn a more compact yet effective embedding space. Last, but not least, our loss boosts the convergence speed greatly as summarized in <ref type="figure" target="#fig_0">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>To further demonstrate the superiority of our loss, we present qualitative retrieval results of our model on the four  datasets. As can be seen in <ref type="figure" target="#fig_8">Figure 4</ref>, intra-class appearance variation is significantly large in these datasets in particular by pose variation and background clutter in the CUB200-2011, distinct object colors in the Cars-196, and view-point changes in the SOP and In-Shop datasets. Even with these challenges, the embedding network trained with our loss performs retrieval robustly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Impact of Hyperparameters</head><p>Batch size: To investigate the effect of batch size on the performance of our loss, we examine Recall@1 of our loss while varying batch size on the four benchmark datasets. The result of the analysis is summarized in <ref type="table">Table 5</ref> and 6, where one can observe that larger batch sizes improve performance since our loss can consider a larger number of examples and their relations within each batch. On the other hand, performance is slightly reduced when the batch size is small since it is difficult to determine the relative hardness in this setting. On the datasets with a large number of images and classes, i.e., SOP and In-shop, our loss needs to utilize more examples to fully leverage the relations be-  tween data points. Our loss achieves the best performance when the batch size is equal to or larger than 300.</p><p>Embedding dimension: The dimension of embedding vectors is a crucial factor that controls the trade-off between speed and accuracy in image retrieval systems. We thus investigate the effect of embedding dimensions on the retrieval accuracy in our Proxy-Anchor loss. We test our loss with embedding dimensions varying from 64 to 1,024 following the experiment in <ref type="bibr" target="#b33">[34]</ref>, and further examine that with 32 embedding dimension. The result of analysis is quantified in <ref type="figure" target="#fig_9">Figure 5</ref>, in which the retrieval performance of our loss is compared with that of MS loss <ref type="bibr" target="#b33">[34]</ref>. The performance of our loss is fairly stable when the dimension is equal to or larger than 128. Moreover, our loss outperforms MS loss in all embedding dimensions, and more importantly, its accuracy does not degrade even with the very high dimensional embedding unlike MS loss.  suggest that when α is greater than 16, the accuracy of our model is high and stable, thus insensitive to the hyperparameter setting. Our loss outperforms current state-of-theart with any α greater than 16. In addition, increasing δ improves performance although its effect is relatively small when α is large. Note that our hyperparameter setting reported in Section 4.2 is not the best, although it outperforms all existing methods on the dataset, as we did not tune the hyperparameters to optimize the test accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a novel metric learning loss that takes advantages of both proxy-and pair-based losses. Like proxy-based losses, it enables fast and reliable convergence, and like pair-based losses, it can leverage rich data-todata relations during training. As a result, our model has achieved state-of-the-art performance on the four public benchmark datasets, and at the same time, converged most quickly with no careful data sampling technique. In the future, we will explore extensions of our loss for deep hashing networks to improve its computational efficiency in testing as well as that in training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>This appendix presents additional experimental results omitted from the main paper due to the space limit. Section A.1 analyzes the impact of the backbone networks and the size of input images in our framework. Finally, Section A.2 provides t-SNE visualization of the learned embedding space and more qualitative results of image retrieval on the four benchmark datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Impact of Backbone Network &amp; Image Size</head><p>Existing methods in deep metric learning have adopted various kinds of backbone networks. In this section, we compare the performance of our loss using popular network architectures as backbone networks on the CUB-200-2011 <ref type="bibr" target="#b35">[36]</ref> and Cars-196 <ref type="bibr" target="#b16">[17]</ref>. For all experiments, we use 512 dimensional embedding and fix hyperparameters α and δ to 32 and 10 −1 , respectively. In addition to the Inception with batch normalization (Inception-BN) used in the main paper, we adopt GoogleNet, ResNet-50, and ResNet-101 as embedding networks trained with our loss. The results are summarized in <ref type="table" target="#tab_6">Table 7</ref>, where a more powerful architecture achieves a better score in general. Note that our method with GoogleNet backbone outperforms existing models based on the same backbone, except for ABE <ref type="bibr" target="#b14">[15]</ref>, an ensemble model, on the Cars-196 dataset. Furthermore, when using ResNet-50 and ResNet-101 as backbone networks, our model outperforms all the previous methods by large margins.</p><p>The main paper showed that the large image size contributed significantly to performance improvement, and we further investigate the performance at larger image size settings. We evaluate our method with Inception-BN backbone while varying the sizes of input images: {224 × 224, 256 × 256, 324 × 324, 448 × 448}. <ref type="table" target="#tab_6">Table 7</ref> also shows that the accuracy improves consistently as the sizes of the input images increase. Even our model with images size of 448 × 448 has 8.9% improvement over the default images size (224 × 224) in Recall@1. Increasing the image size decreases the allowable batch size, but with enough GPU memory, using a larger image size is the most effective way to improve performance than using a powerful architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Additional Qualitative Results</head><p>More qualitative examples for image retrieval on the CUB-200-2011 and Cars-196 are shown in <ref type="figure" target="#fig_11">Figure 7</ref> and <ref type="figure">Figure 8</ref>, respectively. The results of our model are compared with those of model trained with Proxy-NCA loss <ref type="bibr" target="#b20">[21]</ref> using the same backbone network. The overall results indicate that our model learned a higher quality embedding space than the baseline. In the examples in the 3rd and 4th rows of <ref type="figure" target="#fig_11">Figure 7</ref>, both models retrieved birds with a similar appearance to the query, but only our model produces accurate results. Also, the example in the first row of <ref type="figure">Figure 8</ref> shows successful retrievals despite different view-point changes and colors. <ref type="figure" target="#fig_0">Figures 9 and 10</ref> compare the qualitative results of the SOP and In-shop datasets. As shown in the 2nd, 4th, and 5th rows of <ref type="figure">Figure 9</ref>, our model successfully retrieved the same object even with extreme view-point changes. Also, in the 4th row of <ref type="figure" target="#fig_0">Figure 10</ref>, the baseline is confused with a short dress of similar pattern, whereas our model retrieves the long dress exactly.</p><p>Finally, <ref type="figure" target="#fig_0">Figures 11, 12, 13 and 14</ref> show t-SNE visualizations of the embedding spaces learned by our loss on the test splits of the four benchmark datasets. These 2D visualizations are generated by mapping each image onto a location of a square grid using Jonker-Volgenant algorithm. The results demonstrate that all data points in the embedding space have relevant nearest neighbors, which suggest that our model learns a semantic similarity that can be generalized even in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Proxy-NCA Proxy-Anchor (Ours) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Proxy-Anchor (Ours) Proxy-NCA      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Accuracy in Recall@1 versus training time on the Cars-196</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>Figure 2. Comparison between popular metric learning losses and ours. Small nodes are embedding vectors of data in a batch, and black ones indicate proxies; their different shapes represent distinct classes. The associations defined by the losses are expressed by edges, and thicker edges get larger gradients. Also, embedding vectors associated with the anchor are colored in red if they are of the same class of the anchor (i.e., positive) and in blue otherwise (i.e., negative). (a) Triplet loss [25, 32] associates each anchor with a positive and a negative data point without considering their hardness. (b) N -pair loss [27] and (c) Lifted Structure loss [29] reflect hardness of data, but do not utilize all data in the batch. (d) Proxy-NCA loss [21] cannot exploit data-to-data relations since it associates each data point only with proxies. (e) Our loss handles entire data in the batch, and associates them with each proxy with consideration of their relative hardness determined by data-to-data relations. See the text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Differences between Proxy-NCA and Proxy-Anchor in handling proxies and embedding vectors during training. Each proxy is colored in black and three different colors indicate distinct classes. The associations defined by the losses are expressed by edges, and thicker edges get larger gradients. (a) Gradients of Proxy-NCA loss with respect to positive examples have the same scale regardless of their hardness. (b) Proxy-Anchor loss dynamically determines gradient scales regarding relative hardness of all positive examples so as to pull harder positives more strongly. (c) In Proxy-NCA, each negative example is pushed only by a small number of proxies without considering the distribution of embedding vectors in fine details. (d) Proxy-Anchor loss considers the distribution of embedding vectors in more details as it has all negative examples affect each other in their gradients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>compares the training complexity of our loss with those of popular pair-and proxy-based losses. The complexity of our loss is O(M C) since it compares every proxy with all positive or all negative examples in a batch. More specifically, in Eq. (4), the complexity of the first summation is O(M C) and that of the second summation is also O(M C), hence the total training complexity is O(M C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>lowers the complexity to O(M 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results on the CUB-200-2011 (a), Cars-196 (b), SOP (c) and In-shop (d). For each query image (leftmost), top-4 retrievals are presented. The results with red boundary are failure cases, which are however substantially similar to their query images in terms of appearance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>Accuracy in Recall@1 versus embedding dimension on the Cars-196.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Accuracy in Recall@1 versus δ and α on the Cars-196. α and δ of our loss: We also investigate the effect of the two hyperparameters α and δ of our loss on the Cars-196 dataset. The results of our analysis are summarized in Figure 6, in which we examine Recall@1 of Proxy-Anchor by varying the values of the hyperparameters α ∈ {4, 8, 16, 32, 64} and δ ∈ {0, 0.1, 0.2, 0.3, 0.4}. The results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results on the CUB-200-2011 comparing with Proxy-NCA loss. For each query image (leftmost), top-4 retrievals are presented. The result with red boundary is a failure case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Qualitative results on the Cars-196 comparing with Proxy-NCA loss. For each query image (leftmost), top-4 retrievals are presented. The result with red boundary is a failure case. Qualitative results on the SOP comparing with Proxy-NCA loss. For each query image (leftmost), top-4 retrievals are presented. The result with red boundary is a failure case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 .</head><label>10</label><figDesc>Qualitative results on the In-shop comparing with Proxy-NCA loss. For each query image (leftmost), top-4 retrievals are presented. The result with red boundary is a failure case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 .</head><label>11</label><figDesc>t-SNE visualization of our embedding space learned on the test split of CUB-200-2011 dataset in a grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 .</head><label>12</label><figDesc>t-SNE visualization of our embedding space learned on the test split of Cars-196 dataset in a grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 13 .</head><label>13</label><figDesc>t-SNE visualization of our embedding space learned on the test split of SOP dataset in a grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 14 .</head><label>14</label><figDesc>t-SNE visualization of our embedding space learned on the test split of In-shop dataset in a grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of training complexities.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Recall@K (%) on the SOP. Superscripts denote embedding sizes and † indicates models using larger input images.</figDesc><table><row><cell>Recall@K</cell><cell>1</cell><cell>10</cell><cell>20</cell><cell>40</cell></row><row><cell>HDC 384 [40]</cell><cell cols="4">62.1 84.9 89.0 92.3</cell></row><row><cell>HTL 128 [7]</cell><cell cols="4">80.9 94.3 95.8 97.4</cell></row><row><cell>MS 128 [34]</cell><cell cols="4">88.0 97.2 98.1 98.7</cell></row><row><cell>Proxy-Anchor 128</cell><cell cols="4">90.8 97.9 98.5 99.0</cell></row><row><cell>FashionNet 4096 [19]</cell><cell cols="4">53.0 73.0 76.0 79.0</cell></row><row><cell>A-BIER 512 [22]</cell><cell cols="4">83.1 95.1 96.9 97.8</cell></row><row><cell>ABE 512 [15]</cell><cell cols="4">87.3 96.7 97.9 98.5</cell></row><row><cell>MS 512 [34]</cell><cell cols="4">89.7 97.9 98.5 99.1</cell></row><row><cell>Proxy-Anchor 512</cell><cell cols="4">91.5 98.1 98.8 99.1</cell></row><row><cell cols="5">† Contra+HORDE 512 [13] 90.4 97.8 98.4 98.9</cell></row><row><cell>† Proxy-Anchor 512</cell><cell cols="4">92.6 98.3 98.9 99.3</cell></row></table><note>Table 4. Recall@K (%) on the In-Shop. Superscripts denote embedding sizes and † indicates models using larger input images.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Comparison both different backbone networks and different sizes of images on the CUB-200-2011 and Cars-196 datasets.</figDesc><table><row><cell></cell><cell></cell><cell>R@1</cell><cell>R@2</cell><cell>R@4</cell><cell>R@8</cell><cell>R@1</cell><cell>R@2</cell><cell>R@4</cell><cell>R@8</cell></row><row><cell>GoogleNet</cell><cell></cell><cell>63.8</cell><cell>74.4</cell><cell>83.6</cell><cell>90.4</cell><cell>84.3</cell><cell>90.4</cell><cell>94.1</cell><cell>96.7</cell></row><row><cell>Inception-BN</cell><cell></cell><cell>68.4</cell><cell>79.2</cell><cell>86.8</cell><cell>91.6</cell><cell>86.1</cell><cell>91.7</cell><cell>95.0</cell><cell>97.3</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>69.7</cell><cell>80.0</cell><cell>87.0</cell><cell>92.4</cell><cell>87.7</cell><cell>92.9</cell><cell>95.8</cell><cell>97.9</cell></row><row><cell>ResNet-101</cell><cell></cell><cell>70.8</cell><cell>81.0</cell><cell>88.1</cell><cell>93.0</cell><cell>87.9</cell><cell>93.0</cell><cell>96.1</cell><cell>97.9</cell></row><row><cell></cell><cell>256 × 256</cell><cell>71.1</cell><cell>80.4</cell><cell>87.4</cell><cell>92.5</cell><cell>88.3</cell><cell>93.1</cell><cell>95.7</cell><cell>97.5</cell></row><row><cell>Inception-BN</cell><cell>324 × 324</cell><cell>74.0</cell><cell>82.9</cell><cell>88.9</cell><cell>93.2</cell><cell>91.1</cell><cell>94.9</cell><cell>96.9</cell><cell>98.3</cell></row><row><cell></cell><cell>448 × 448</cell><cell>77.3</cell><cell>85.6</cell><cell>91.1</cell><cell>94.2</cell><cell>92.9</cell><cell>96.1</cell><cell>97.7</cell><cell>98.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The training complexity indicates the amount of computation required to address the entire training dataset<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ensemble deep manifold similarity learning using hard proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Aziere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Sckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: A deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A theoretically sound upper bound on the triplet loss for improving the efficiency of deep distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Metric learning with horde: High-order regularizer for deep embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aymeric</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep metric learning beyond binary supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention-based ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunjoo</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep metric learning with bier: Boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Softtriple loss: Deep metric learning without triplet sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limeng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning finegrained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ranked list loss for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Garnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil M</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep metric learning with tuplet margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Cars-196</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
