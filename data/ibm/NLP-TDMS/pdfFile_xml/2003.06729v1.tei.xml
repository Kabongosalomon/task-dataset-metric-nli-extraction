<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NoiseRank: Unsupervised Label Noise Reduction with Dependence Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karishma</forename><surname>Sharma</surname></persName>
							<email>krsharma@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Donmez</surname></persName>
							<email>pinared@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enming</forename><surname>Luo</surname></persName>
							<email>eluo@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zeki Yalniz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NoiseRank: Unsupervised Label Noise Reduction with Dependence Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Label noise</term>
					<term>Unsupervised learning</term>
					<term>Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Label noise is increasingly prevalent in datasets acquired from noisy channels. Existing approaches that detect and remove label noise generally rely on some form of supervision, which is not scalable and error-prone. In this paper, we propose NoiseRank, for unsupervised label noise reduction using Markov Random Fields (MRF). We construct a dependence model to estimate the posterior probability of an instance being incorrectly labeled given the dataset, and rank instances based on their estimated probabilities. Our method 1) Does not require supervision from ground-truth labels, or priors on label or noise distribution.</p><p>2) It is interpretable by design, enabling transparency in label noise removal. 3) It is agnostic to classifier architecture/optimization framework and content modality. These advantages enable wide applicability in real noise settings, unlike prior works constrained by one or more conditions. NoiseRank improves state-of-the-art classification on Food101-N (∼20% noise), and is effective on high noise Clothing-1M (∼40% noise).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning has become an indispensable component of most applications across numerous domains, ranging from vision, language and speech to graphs and other relational data <ref type="bibr" target="#b21">[22]</ref>. It has also led to an increase in the amount of training data required to effectively solve target problems. Labeled datasets, typically obtained through manual efforts, are prone to labeling errors arising from annotator biases, incompetence, lack of attention, or ill-formed and insufficient labeling guidelines. The likelihood of human errors increases in domains with high ambiguity <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref>. Additionally, there is an increasing dependence on automated data collection such as employing web-scraping, crowd-sourcing and machine-generated labeling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref>. However, the cheap but noisy channels have made it imperative to deal with incorrectly labeled samples.</p><p>Existing literature either focus on training noise-robust classifiers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>, or attempt to reduce or correct label noise in the dataset generally with some form of supervision <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. However, attention is shifting towards unsupervised label noise reduction due to obvious practical benefits. Most earlier methods use some form of supervision, either from verified or clean labels, or priors on the label/noise distribution, in order to guide the detection of mislabeled examples. In this work, we propose a fully unsupervised approach for label noise detection using Markov Random Fields (MRF), also known as dependence models, which provide a generic framework for modeling the joint distribution of a large set of random variables. We formulate a dependence model to estimate the posterior probability of an instance being incorrectly labeled, given the dataset, and rank instances based on the estimated posterior. We provide an iterative framework for label noise reduction using our dependence model for noise ranking, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The iterative framework is used to first learn instance representations from the noisy dataset, and detect label noise, then fine-tune on denoised (cleaned) subsets in order to improve classification and learned representations, which iteratively improve label noise detection. Our approach addresses several shortcomings of existing methods. First, our proposed method "NoiseRank" removes dependence on supervision for label noise detection. This allows wider practical applicability of our method to real domains. In contrast, most supervised approaches dealing with label noise are error-prone and hardly scalable. Second, our proposed framework for label noise ranking and improving classification is agnostic to both the classifier architecture and its training procedure. The implication of this is that we can train classifiers on any domain (image, text, multi-modal, etc.) within the same framework, using any standard classification architecture and optimization framework. In comparison, methods such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11]</ref> require careful network initialization and regularization of the loss function for optimization. Lastly, NoiseRank's un-derlying algorithm and its output are human interpretable by design. Our main contributions are summarized as follows:</p><p>-A fully unsupervised label noise detection approach which is a probabilistic dependence model estimating the likelihood of being mislabeled. It does not require ground-truth labels, or priors on label or noise distribution. -The proposed framework is generic, i.e., independent of application domain and content modality, and applicable with any standard classifier model architecture and optimization framework, unlike many recent unsupervised approaches <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25]</ref>. -Its underlying algorithm and output are human interpretable. Again many unsupervised methods do not incorporate interpretability <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10]</ref>, which reduces their transparency in label noise detection and ranking.</p><p>-Experiments on real noise benchmark datasets, Food101-N (∼20% noise) and Clothing-1M (∼40% noise) for label noise detection and classification tasks, which improved state-of-the-art classification on Food101-N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Robust and noise-tolerant classifiers: Methods that focus on training noise tolerant or robust classifiers attempt to directly modify the training framework for learning in the presence of label noise. <ref type="bibr" target="#b10">[11]</ref> introduces a non-linear noise modeling layer in a text classifier architecture to encode the distribution of label noise. <ref type="bibr" target="#b0">[1]</ref> fits a beta mixture model on the training loss distribution to estimate the likelihood of label noise and uses that to guide the classifier training with a carefully selected loss function based on bootstrapping <ref type="bibr" target="#b22">[23]</ref> and mix-up data augmentation <ref type="bibr" target="#b39">[39]</ref>. Approaches based on meta-learning and curriculum learning are also studied for modifying the training procedure, where training samples are either ordered based on learning difficulty or mixed with synthetic noise distributions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b9">10]</ref>. However, these methods limit the choice of classifier architectures, and furthermore are known to work only with careful initialization <ref type="bibr" target="#b10">[11]</ref> and regularization <ref type="bibr" target="#b0">[1]</ref> needed for convergence. Label noise reduction/correction: Other methods, including ours, are based on label noise reduction, that attempt to detect, and remove or correct label noise. Prominent approaches utilize supervision to guide label noise detection. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref> require clean (ground-truth) labels for a subset of the data to learn a mapping from noisy to clean labels. <ref type="bibr" target="#b14">[15]</ref> requires binary verification labels instead, which indicate whether the given label is correct or noisy, in order to train an attention mechanism that can select reference images as class prototypes, and learn to predict if a given label is noisy. Similar to <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b7">[8]</ref> uses prototypes (more than one per class) to generate corrected labels which are then employed to iteratively train a network. However, <ref type="bibr" target="#b7">[8]</ref> does not rely on any supervision or assumptions on the label distribution. As compared to <ref type="bibr" target="#b7">[8]</ref>, our method uses standard cross-entropy loss, whereas their framework is based on self-supervised learning, limiting flexibility on classifier optimization framework.</p><p>Another iterative approach is of <ref type="bibr" target="#b38">[38]</ref>, which updates both network parameters and label distributions to iteratively correct the noisy labels. <ref type="bibr" target="#b19">[20]</ref> relies on the availability of a noise transition matrix for loss correction when training a classifier, which specifies the noise distribution in terms of the probability of one class being mislabeled as another. <ref type="bibr" target="#b31">[32]</ref> employs a deep learning based risk consistent estimator to fine-tune a noise transition matrix. One type of unsupervised approach is outlier removal <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21]</ref>. However, outliers are not necessarily mislabeled and removing them presents a challenge <ref type="bibr" target="#b4">[5]</ref>. There are also several methods addressing instance selection for kNN classification, which retain a subset of instances that allow correct classification of the remaining instances <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref>, or remove instances whose labels are different from the majority labels of their nearest neighbors <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14]</ref>. However, the proposed heuristics have been criticized for removing too many instances or keeping mislabeled instances <ref type="bibr" target="#b4">[5]</ref>. Our approach is related to these methods but focuses on leveraging both label (in)consistencies to globally rank noisy candidates and more effectively detect mislabeled instances even without any supervision. Weakly-supervised methods based on classification filtering such as <ref type="bibr" target="#b26">[27]</ref> remove samples misclassified by SVM trained on the noisy data. However, it could amount to removing non-noisy hard samples or not removing noisy samples that the classifier mistakenly fits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unsupervised Label Noise Reduction and Model Training Framework</head><p>Our ultimate goal is to learn an effective classifier from the noisy labeled dataset without any form of human supervision (i.e., label verification), prior knowledge on the target domain, or label/noise distribution. In this section, we elaborate our multi-step model training framework. As is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, we first describe vector representation, which is necessary for similarity measure and label prediction in our framework. Next, our proposed probabilistic dependence model "NoiseRank" is elaborated for ranking dataset examples based on their likelihood of being noisy. Finally, we discuss the iterative model training steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vector Representation</head><p>The vector space representation (i.e., embeddings) is a core component of our design because we rely on the vector representations to determine content similarity between examples in the given noisy dataset. Our framework is agnostic to any modality as well as the solution for learning representations. However, a high-quality representation improves the similarity measure and thus our unsupervised method for label noise detection. In Sec. 3.3, we will discuss how we improve the representation through iterative training.</p><p>With the vector representation, we could determine the content similarity. More formally, let the noisy labeled dataset be denoted as</p><formula xml:id="formula_0">D = {(x i , y i )} N i=1</formula><p>where x i ∈ R m is the vector representation for example i, and y i ∈ {1, 2 . . . C} is the given label (potentially incorrect) with C ≥ 2 being the total number of classes in the dataset. In this work, we define instance similarity in terms of Euclidean distance between x i and x j as d(x i , x j ) = x i − x j 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Label Noise Detection</head><p>In this section, we first describe our process for generating class prototypes, that are representative instances selected for each of the C classes in the dataset; followed by our non-parametric approach to generate label predictions, y i , for each prototype i. Let Y = {y i } P i=1 denote the predictions. Next, we elaborate the proposed dependence model, named NoiseRank, to globally rank the dataset examples based on their likelihood of having incorrect labels given their vector representations, labels and predictions.</p><p>Generating class prototypes Each of the C classes in the dataset can be represented by a set of class prototypes, i.e. a representative subset of instances in that class. We select the prototypes using K-means clustering on the vector space representations of instances in each class, given by the noisy labels. As a rule of thumb <ref type="bibr" target="#b12">[13]</ref>, we select ρ/2 cluster centroids per class, where ρ is the average number of instances per class in the dataset. Selecting class prototypes is beneficial towards improving scalability when the number of dataset instances grows. We find that it is also important for robustness in high noise datasets, and K-means based selection is effective compared to randomly selected prototypes.</p><p>Generating label predictions For each prototype instance i represented by vector x i , we generate the predicted label y i by a weighted k nearest neighbor classifier, as specified in Eq. 1.</p><formula xml:id="formula_1">y i = arg max v∈{1,2...C} xj ∈N (xi) κ(x i , x j )1{y j = v}<label>(1)</label></formula><p>where 1 is the indicator function, and the distance kernel function κ(</p><formula xml:id="formula_2">x i , x j )</formula><p>is used to weigh the contribution of each neighbor x j in the neighborhood N comprising the k nearest neighbors of x i :</p><formula xml:id="formula_3">κ(x i , x j ) = 1 b + d(x i , x j ) e<label>(2)</label></formula><p>where d(x i , x j ) is the distance function discussed in Sec. 3.1, and b &gt; 0 and e &gt; 0 are parameters for the bias and weight exponent, respectively. The kernel function is negatively correlated to the distance function. For example, when e = 2, the kernel will be inversely proportional to the squared distance between the instances. Since 0 ≤ d(x i , x j ) ≤ ∞, by setting a positive bias b, we can prevent κ(x i , x j ) from being undefined when d = 0.</p><p>Dependence Model Formulation Our formulation is to estimate the posterior probability P (x i , y i |D, Y ) that indicates the likelihood of label noise for all examples (x i , y i ) in the dataset D and rank them based on this estimate. For this purpose, we use Markov Random Fields (also known as MRFs or "dependence models" <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref>) which provide a generic framework for modeling the joint distribution of a large set of random variables. In dependence models, conditional dependencies are defined only for certain groups of random variables called "cliques", and are represented with edges in an undirected graph. We represent the graph with G and the cliques in the graph as C(G) in our formulations. For each type of clique c ∈ C(G), we define a nonnegative potential function φ(c; Λ) parameterized by Λ. The joint probabilities are estimated based on the Markov assumption as follows:</p><formula xml:id="formula_4">P (x i , y i , D, Y ) = 1 Z c∈C(G) φ(c; Λ) (3) where Z = xi,yi,D,Y c∈C(G) φ(c; Λ)</formula><p>is a normalization term. Computing Z is very expensive due to the large number of summands. Since our aim is to rank examples in the dataset based on their posterior probabilities P (x i , y i |D, Y ) and ignoring Z in this formulation does not change the ranking result, the posterior probability is estimated as follows:</p><formula xml:id="formula_5">P (x i , y i |D, Y ) = P (x i , y i , D, Y ) P (D, Y ) (4) rank = log P (x i , y i , D, Y ) − log P (D, Y ) rank = c∈C(G) log φ(c; Λ)</formula><p>where rank = indicates rank equivalence. The formulation is a sum of logarithm of potential functions over all cliques. For simplification purposes, the potential function is assumed to be φ(c; Λ) = exp(λ c f (c)), where f (c) is the feature function over the clique c and λ c is the weight for the feature function. The final ranking function is computationally tractable and linear over feature functions:</p><formula xml:id="formula_6">P (x i , y i |D, Y ) rank = c∈C(G) λ c f (c) (5)</formula><p>Depending on the choice of the feature functions and their corresponding weights, the final ranking score in 5 can be negative. In the next subsection, we elaborate our dependence model for the task of label noise detection by explicitly defining each clique, its feature function f (c) and the corresponding weight λ c .  The dependence graph illustrated for a given example i in a database containing five examples. Clique types and weights are determined based on the given y i and y j and predicted labels y j . Edge lengths indicate distance in the vector representation space dependence graph. Each example is associated with its given label y i and each prototype is associated with its given label y j and predicted label y j , which are used for determining the clique weights as shown in <ref type="figure" target="#fig_2">Fig. 2</ref> and explained below. All cliques are assumed to share the same feature function f (c) = κ(x i , x j ) as defined by the kernel function in Eq. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependence Graph Construction</head><p>We differentiate cliques into four types based on the values of the given and predicted labels of the examples. The first clique type, denoted by c 11 , is for all pairs of examples (i, j) that share the same given label. If the examples share the same label (i.e., y i = y j ), we assign a negative "blame" score (i.e., reward) weighted by f (c) to example i so that it ranks lower in the final rank list of examples sorted by their overall MRF scores. For this clique type, we set the clique weight parameter as λ c = −1.</p><p>The second clique type is denoted by c 10 . It is defined for all pairs of examples (i, j) with different labels (i.e., y i = y j ) where y j = y j . In this case, example j blames example i for providing an incorrect prediction vote, even though the false vote did not change the prediction output y j which is consistent with example j's original label y j . For this type, we set λ c = 1 − α, where α is a hyper-parameter defined in the range [0.5, 1] to control the impact of incorrect vote (i.e., y j = y i ) on the blame score.</p><p>The third clique type, denoted by c 01 , is for all pairs of examples (i, j) with different labels (i.e., y i = y j ) where y j = y j and y j = y i . In other words, the prediction output y j is different from both its own label y j and example i's label y i . While example i did not directly influence the mispredicted label, it did not contribute towards the correct prediction. By setting λ c = α, example j assigns a scaled blame score to example i.</p><p>The fourth clique type, denoted by c 00 , is for all pairs of examples (i, j) with different labels (i.e., y i = y j ) where y j = y j and y j = y i . Example j blames example i strongly for supporting a prediction different from its own label y j which is the same as its prediction y j . For this type, we set λ c = α × b f where b f in [1, inf ) is the "blame factor" and controls the strength of the blame.</p><p>NoiseRank Score Function The final ranking function in Eq. 6 is the sum of all blame and reward scores accumulated for example i according to Eq. 5.</p><formula xml:id="formula_7">P (x i , y i |D, Y ) rank = (xi,xj )∈c11 −κ(x i , x j ) + (xi,xj )∈c10 (1 − α)κ(x i , x j ) + (xi,xj )∈c01 ακ(x i , x j ) + (xi,xj )∈c00 (α × b f )κ(x i , x j )<label>(6)</label></formula><p>The aggregate score P (x i , y i |D, Y ) is the basis for ranking instances in the dataset. The rank reflects the relative likelihood of being mislabeled and the impact on mispredictions, accounted for in the penalty function. Since the score function is unbounded, detecting label noise given the ranked list requires threshold δ ≥ 0 to determine if an instance with detected label noise should be retained (w = 1) or removed (w = 0) from the dataset.</p><formula xml:id="formula_8">w(x i ) = 0, if P (x i , y i |D, Y ) &gt; δ 1, otherwise<label>(7)</label></formula><p>It should be noted that as the dataset size increases, computing the ranking function over all O(N |P |) pairs is less efficient. Moreover, the value of the feature function f (c) approaches zero as distances between pairs increase. We therefore limit the cliques to the k closest neighbors of example i for assigning blame and reward scores as defined above. This approximation is quite effective especially because the blame and reward scores diminish rapidly and approach zero as distances get larger. Another note is that we use the same k value in the score function and in the kernel function (Eq. 2) for both computing the aggregate rank score function and generating label predictions y j for simplification purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Iterative Training</head><p>We provide a generic iterative framework to learn classifiers with label noise reduction. As described earlier, the vector space representations of examples in the dataset are used in determining content similarity for noise ranking. Initially, representations can be learned with the available (potentially noisy) labels. In order to improve the learned representations, we can iterate over representation learning, noise ranking and reduction, model training, in order.</p><p>The framework is agnostic to the model used for representation learning and classification, depending on the content modality. At a first step, we train the classifier model (eg. standard CNN with simple cross-entropy loss) with the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We report experiments on three public datasets: Food-101N, Clothing1M and YFCC100m on both label noise detection and classification.</p><p>Food-101N <ref type="bibr" target="#b14">[15]</ref> and Clothing-1M <ref type="bibr" target="#b33">[34]</ref>: These are real noise public datasets collected from noisy channels; which are used to study methods for learning in the presence of label noise. These datasets also contain additional verification/clean labels used for noise detection training and validation by supervised label noise reduction methods. Note that for NoiseRank, we do not use these additional verified/clean labels in training or validation. We only use the verified validation labels for evaluation of our method to report results on label noise detection recall and accuracy. These datasets also provide a clean test set with 25K and 10K examples respectively, used for evaluation of the classification task top-1 accuracy. YFCC100m <ref type="bibr" target="#b25">[26]</ref>: This is a large-scale dataset with 99.2M images used in the semi-supervised learning setting in <ref type="bibr" target="#b35">[36]</ref> and we combine it with Nois-eRank for detecting label noise in machine-generated labels originated by the semi-supervised learning setup. We use NoiseRank to detect and remove mislabeled examples; and the rest are then leveraged to improve target ImageNet-1k classification. Dataset statistics are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment setup and hyper-parameters</head><p>For representation learning, we introduced a 256-dimensional bottleneck layer to the ResNet-50 model pre-trained on ImageNet1k. First, the pre-trained ResNet-50 is fully fine-tuned with the entire noisy dataset using learning rate 0.002 for <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b9">10]</ref> epochs and learning rate decay rate 0.1. The output of the bottleneck layer is L2 normalized and used for representing image content. We report results for NoiseRank which conducts label noise detection and removal only once; and iterative NoiseRank wherein after one round of noise removal we fine-tune the ResNet and repeat noise removal. For efficient nearest neighbor search, we use open-source library FAISS <ref type="bibr" target="#b11">[12]</ref> which takes less than 10 minutes on one GPU for dataset of size 1M with the 256d vector representations.</p><p>Unsupervised hyper-parameter selection: The improvement in data quality can be directly measured (without supervision from verified labels) by the improvement in learnability of the classifier. We measure the training loss at epoch 10 on denoised subsets and select NoiseRank hyper-parameter setting that results in the least training loss. To reduce the parameter search, we first select and fix the best k (number of nearest neighbors) by grid search in {5, 10, 20, 50, 100, 250} and then search α ∈ {0.5, 0.6, 0.8} and b f ∈ {1.0, 1.5, 2.0}, and b = e = 1 in the distance kernel. The ranking cut-off δ = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Label Noise Detection Experiments</head><p>We report the effectiveness of our proposed method on detecting label noise in <ref type="table" target="#tab_1">Table 2</ref>, in terms of i) averaged detection error rate over all classes in Food101-N and Clothing-1M, and ii) in terms of label noise recall and F1. <ref type="table" target="#tab_1">Table 2</ref> details the average error rate of label noise detection on the verified validation set compared against a wide range of baselines, as reported in <ref type="bibr" target="#b14">[15]</ref>. The naive baseline predicts all samples as correctly labeled, and therefore its error rate approximates the true noise distribution assuming a random selection of the ground truth set. Clothing-1M has a significant amount of noise estimated at 38.46%. In this significantly noisy dataset, iterative NoiseRank even as an unsupervised method, strongly outperforms unsupervised outlier removal method DRAE <ref type="bibr" target="#b32">[33]</ref> by a large margin of <ref type="bibr" target="#b15">16</ref>.15% (which is 40% error reduction) and weakly supervised Average Baseline (Avg. Base.) <ref type="bibr" target="#b14">[15]</ref> by 7.75% (which is 25% error reduction) on avg error rate. This is state-of-the-art noise detection error rate among unsupervised alternatives on this dataset. Avg. Base. computes the cosine similarity between an instance representation and the averaged representation of a class; and although it does not use verified labels in training, it uses them to select the threshold on cosine similarity for label noise detection. On Food101-N the estimated noise is 19.66% and the avg error rate of iterative NoiseRank and DRAE are comparable. However, since noise vs. clean instance distribution is imbalanced, we further measure recall, F1 and macro-F1 scores for label noise detection in <ref type="table" target="#tab_1">Table 2</ref>. NoiseRank has state-of-the-art recall of 85.61% on Food-101N and 74.18% on Clothing-1M. NoiseRank F1/MacroF1 is competitive with the best supervised method in noise detection CleanNet <ref type="bibr" target="#b14">[15]</ref> which requires verified labels in training and validation, and thus has a significant advantage compared to unsupervised and weakly-supervised methods. It should be noted that effective noise recall directly impacts classification, and is therefore an important evaluation metric for label noise detection and removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Classification Experiments</head><p>We conducted experiments to study the impact of data quality on the classification task using the ResNet-50 classifier pretrained on ImageNet and initially fine-tuned on noisy dataset and later on denoised subset with NoiseRank. In results table 3 and 4, in each row, the model is fine-tuned with the mentioned training examples on the specified pre-trained model (eg. "noisy train # 1" refers to the model # 1 referenced in the table that was trained using noisy training samples on ImageNet pre-trained Resnet-50). Similarly, in <ref type="table" target="#tab_3">Table 4</ref>, "# 4" in the pre-training column, refers to model # 4 indicated in the table.</p><p>In <ref type="table" target="#tab_2">Table 3</ref> Food101-N, NoiseRank achieves state of the art 85.78% in top-1 accuracy compared to unsupervised <ref type="bibr" target="#b7">[8]</ref>'s 85.11%, and 11% error reduction over supervised noise reduction method CleanNet. This can be attributed to the high noise recall on Food-101N as examined earlier. In <ref type="table" target="#tab_3">Table 4</ref> Clothing-1M, NoiseRank used to reduce label noise in noisy train (∼40% estimated noise) is effective in improving classification from 68.94% to 73.82% (16% error reduction), even without supervision from clean set in high noise regime, and performs comparable to recent unsupervised <ref type="bibr" target="#b7">[8]</ref> and marginally outperforms unsupervised PEN-CIL <ref type="bibr" target="#b38">[38]</ref>. In contrast to <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b38">[38]</ref>, NoiseRank framework allows for flexible choice of classifier and optimization/loss function, and yet achieves comparable improvement due to noise reduction, using standard cross-entropy loss and standard training framework. This underlines the benefits of the proposed framework without compromising on classification improvements. Supervised baselines CleanNet and Loss Correction respectively utilize additional verified labels, and yet the performance gain from noise removal for ours is highly competitive, even in this high noise regime. Lastly, we also reported results of fine-tuning each method with an additional clean 50k set, as per the setting followed in <ref type="bibr" target="#b19">[20]</ref>. <ref type="bibr" target="#b7">[8]</ref> achieves best result of 81.16% with clean 50k sample set. We note that even without noise correction, the inclusion of the clean set boosts accuracy from 68.94% to 79.43% and may shadow the benefit of noise removal; with CleanNet <ref type="bibr" target="#b14">[15]</ref> at 79.90% and ours at 79.57% being comparable in this setting.</p><p>In <ref type="table">Table 5</ref>, we report semi-supervised learning results on large YFCC100m dataset, with and without label denoising. <ref type="bibr" target="#b35">[36]</ref> is used to train a ResNet-101 to label images in YFCC100m into 1K ImageNet classes. 16K images from each class that have the most confident machine label predictions are retained. However, even after filtering, these labels contain noise as shown by examples detected using NoiseRank (right: <ref type="table">Table 5</ref>). We run NoiseRank to remove label noise from the 16M machine labeled images. The denoised images are used to pre-train ResNet-50, then fine-tuned with ImageNet-1K train set and evaluated on benchmark ImageNet-1K test set. The top-1 accuracy without noise removal is 79.06% <ref type="table">Table 5</ref>: Left: ImageNet benchmark top-1 accuracy (%). NoiseRank with removal of top x% ranked instances in noisy machine generated labels, against random removal. Right: Examples of noisy machine generated labels detected by NoiseRank (M: mislabeled instances, C: correctly labeled instances mistakenly identified by NoiseRank)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top-1 Accuracy None <ref type="bibr" target="#b35">[36]</ref> 79  <ref type="figure">Fig. 3</ref>: Interpretability analysis of NoiseRank predictions on Food101-N and with noise removal is 79.34%, in comparison to removing the same number of random instances (78.96%) averaged over three runs. Note that in this setting, noise removal is not applied directly to the target classification task, but rather to the dataset used to pre-train the model, later trained on the target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Interpretability Analysis</head><p>Interpretability is useful in providing explanations about the predictions made by machine learning algorithms. NoiseRank is a transparent framework that can be easily used to provide human level analysis of why a given example was predicted as mislabeled or not mislabeled. In <ref type="figure">Fig. 3a</ref>, we show sample images correctly identified by NoiseRank as label noise in the Food-101N dataset, along with their nearest neighbors. The nearest neighbors provide supporting visual evidence towards understanding the pre- diction made by NoiseRank. Interpretability is also useful for identifying hard instances; to support building better datasets and models. In <ref type="figure">Fig. 3b</ref> (top row) we show sample images incorrectly identified as label noise in Food-101N. As seen, these are tough examples with contradictory labels to their nearest neighbors, which provides insights into when and which instances might be confused with others. The bottom row shows sample images identified as label noise by NoiseRank but verified (seemingly incorrectly) by humans. Such samples further justify our belief that human label verification can also be prone to errors.</p><p>In <ref type="figure" target="#fig_3">Fig. 4</ref>, we provide class-wise analysis on the 14 Clothing-1M dataset class types. <ref type="figure" target="#fig_3">Fig. 4a</ref> reports noise detection accuracy per class (NoiseRank), compared to the original noise ratio in the dataset (Naive). <ref type="figure" target="#fig_3">Fig 4b provides</ref> the estimated probability of flipping a clean label of one class to another (noise transition matrix) estimated on the validation set. In <ref type="figure" target="#fig_3">Fig. 4c</ref>, we visualize how NoiseRank scores each example based on its neighboring prototypes. Each cell in 4c maps a given noisy label class and its prototype neighbors' class aggregated over the pairs used in the scoring function (Eq. 6), with weight equal to its contribution in the score function; and the matrix is then column-normalized for the distribution. It implicitly encodes the noise transition probability between class types without any knowledge of the clean labels, as seen from its similarity to 4b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed an unsupervised label noise ranking algorithm based on Markov Random Fields. The dependence model is used to estimate and rank instances by the posterior probability of being incorrectly labeled in the dataset. We evaluated the framework on real noise datasets and showed its effectiveness on improving image classification, even compared to supervised alternatives. The proposed iterative framework has advantages over recent unsupervised methods in that it provides interpretable label noise detection and ranking, and is agnostic to classifier architecture and optimization framework (works for any content modality with standard widely used deep learning models without constraining the choice of model, loss function, or optimizer).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of "NoiseRank" framework for unsupervised label noise reduction and iterative model training (interpretable, and agnostic to classification model architecture and optimization framework)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>In our formulation, we define cliques between all pairs of examples (i, j) where i = j and i ∈ D, j ∈ P for dataset D with size N and set of all prototypes P ⊆ D. There are O(N |P |) cliques defined in the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The dependence graph illustrated for a given example i in a database containing five examples. Clique types and weights are determined based on the given y i and y j and predicted labels y j . Edge lengths indicate distance in the vector representation space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>(a) Accuracy per class for label noise detection(b) Noise transition matrix between classes, estimated on validation set (c) NoiseRank score function matrix between class types Interpretability analysis of NoiseRank predictions on Clothing-1M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset Statistics. We use only the noisy (train) labels in NoiseRank. Verified labels (train/validation) are used in other supervised/weakly-supervised methods</figDesc><table><row><cell cols="5">Dataset # Classes # Train # Verified (tr/va) # Test</cell></row><row><cell>Food-101N</cell><cell>101</cell><cell>310K</cell><cell>55k/5k</cell><cell>25k</cell></row><row><cell>Clothing1M</cell><cell>14</cell><cell>1M</cell><cell>25k/7k</cell><cell>10k</cell></row><row><cell>YFCC100m</cell><cell>1000</cell><cell>99.2M</cell><cell>-/-</cell><cell>50k</cell></row><row><cell cols="5">available noisy dataset D. The classifier can be used to extract representations</cell></row><row><cell cols="5">for examples in the dataset, which are used to run label noise detection with</cell></row><row><cell cols="5">NoiseRank and remove the examples that are ranked as noisy (i.e., w(x i ) = 0).</cell></row><row><cell cols="5">Finally, we fine-tune the trained model with the denoised subset of the dataset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Label noise detection accuracy. Left: average error rate over all the classes (%) Right: Label noise recall, F1 and macro-F1 (%). NoiseRank(I) is iterative NoiseRank</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Average error rate</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Food-101N Clothing-1M Supervised</cell><cell>Method</cell><cell cols="2">Type Recall F1 MacroF1</cell></row><row><cell>MLP</cell><cell></cell><cell>10.42</cell><cell>16.09</cell><cell cols="3">Food-101N (19.66% estimated noise)</cell></row><row><cell>Label Prop [35]</cell><cell></cell><cell>13.24</cell><cell>17.81</cell><cell>CleanNet</cell><cell>sup.</cell><cell>71.06 74.01 84.04</cell></row><row><cell cols="2">Label Spread [40]</cell><cell>12.03</cell><cell>17.71</cell><cell>Avg. Base.</cell><cell cols="2">weakly 47.70 59.57 76.08</cell></row><row><cell>CleanNet [15]</cell><cell></cell><cell>6.99</cell><cell>15.77</cell><cell>unsup-kNN</cell><cell cols="2">unsup. 22.02 24.23 54.03</cell></row><row><cell cols="3">Weakly-Supervised</cell><cell></cell><cell cols="3">NoiseRank (I) unsup. 85.61 64.42 76.06</cell></row><row><cell>Cls. Filt.</cell><cell></cell><cell>16.60</cell><cell>23.55</cell><cell cols="3">Clothing-1M (38.46% estimated noise)</cell></row><row><cell>Avg. Base. [15]</cell><cell></cell><cell>16.20</cell><cell>30.56</cell><cell>CleanNet</cell><cell>sup.</cell><cell>69.40 73.99 79.65</cell></row><row><cell cols="3">Unsupervised</cell><cell></cell><cell>Avg. Base.</cell><cell cols="2">weakly 43.92 55.14 67.65</cell></row><row><cell>DRAE [33]</cell><cell></cell><cell>18.70</cell><cell>38.95</cell><cell>unsup-kNN</cell><cell cols="2">unsup. 10.85 16.60 44.26</cell></row><row><cell>unsup-kNN</cell><cell></cell><cell>26.63</cell><cell>43.31</cell><cell cols="3">NoiseRank (I) unsup. 74.18 71.74 76.52</cell></row><row><cell>NoiseRank</cell><cell></cell><cell>24.02</cell><cell>23.54</cell><cell></cell><cell></cell></row><row><cell cols="2">NoiseRank (I)</cell><cell>18.43</cell><cell>22.81</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Image classification on Food-101N results in terms of top-1 accuracy (%). Train data (310k) and test data (25k). CleanNet is trained with an additional 55k/5k (tr/va) verification labels to provide the required supervision on noise detection</figDesc><table><row><cell>#</cell><cell>Method</cell><cell>Training</cell><cell cols="2">Pre-training Top-1</cell></row><row><cell>1</cell><cell>None [15]</cell><cell>noisy train</cell><cell>ImageNet</cell><cell>81.44</cell></row><row><cell cols="4">2 CleanNet [15] noisy(+verified) ImageNet</cell><cell>83.95</cell></row><row><cell cols="2">3 DeepSelf [8]</cell><cell>noisy train</cell><cell>ImageNet</cell><cell>85.11</cell></row><row><cell cols="2">4 NoiseRank</cell><cell>cleaned train</cell><cell>ImageNet</cell><cell>85.20</cell></row><row><cell>5</cell><cell></cell><cell cols="3">cleaned train noisy train #1 85.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Image classification on Clothing-1M results in terms of top-1 accuracy (%). Train data (1M) and test data (10k). CleanNet and Loss Correction are trained with an additional 25k/7k (train/validation) verification labels to provide required supervision on noise detection/correction</figDesc><table><row><cell>#</cell><cell>Method</cell><cell>Training</cell><cell cols="2">Pre-training Top-1</cell></row><row><cell>1</cell><cell>None [20]</cell><cell>clean50k</cell><cell>ImageNet</cell><cell>75.19</cell></row><row><cell>2</cell><cell>None [20]</cell><cell>noisy train</cell><cell>ImageNet</cell><cell>68.94</cell></row><row><cell>3</cell><cell></cell><cell>clean50k</cell><cell cols="2">noisy train # 2 79.43</cell></row><row><cell cols="3">4 loss cor.[20] noisy(+verified)</cell><cell>ImageNet</cell><cell>69.84</cell></row><row><cell>5</cell><cell></cell><cell>clean50k</cell><cell># 4</cell><cell>80.38</cell></row><row><cell cols="3">6 Joint opt. [25] noisy train</cell><cell>ImageNet</cell><cell>72.16</cell></row><row><cell cols="2">7 PENCIL [38]</cell><cell>noisy train</cell><cell>ImageNet</cell><cell>73.49</cell></row><row><cell cols="3">8 CleanNet [15] noisy(+verified)</cell><cell>ImageNet</cell><cell>74.69</cell></row><row><cell>9</cell><cell></cell><cell>clean50k</cell><cell># 8</cell><cell>79.90</cell></row><row><cell cols="2">10 DeepSelf [8]</cell><cell>noisy train</cell><cell>ImageNet</cell><cell>74.45</cell></row><row><cell>11</cell><cell></cell><cell>clean50k</cell><cell># 10</cell><cell>81.16</cell></row><row><cell>12</cell><cell></cell><cell>cleaned train</cell><cell>ImageNet</cell><cell>73.77</cell></row><row><cell cols="2">13 NoiseRank</cell><cell cols="3">cleaned train noisy train #2 73.82</cell></row><row><cell>14</cell><cell></cell><cell>clean50k</cell><cell># 13</cell><cell>79.57</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oconnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Leveraging weakly annotated data for fashion image retrieval and label prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Corbiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ollion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2268" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An analysis of case-base editing in a spam filtering system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Delany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Case-Based Reasoning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="128" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data pre-processing through rewardpunishment editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maltoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nanni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="367" to="381" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frénay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="845" to="869" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The reduced nearest neighbor rule (corresp.)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="433" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep self-learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE international conference on computer vision</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The condensed nearest neighbor rule (corresp.)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="515" to="516" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2309" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An effective label noise model for dnn text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pressel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nokleby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3246" to="3256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with gpus</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Review on determining number of cluster in k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Kodinariya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Makwana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="90" to="95" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving classification by removing or relabeling mislabeled instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lallich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Muhlenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Zighed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Methodologies for Intelligent Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="5" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A markov random field model for term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/1076034.1076115</idno>
		<ptr target="http://doi.acm.org/10.1145/1076034.1076115" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;05</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identifying and handling mislabelled instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Muhlenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lallich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Zighed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="109" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reduced reward-punishment editing for building ensembles of classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Franco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2395" to="2400" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<title level="m">Estimating the support of a high-dimensional distribution</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey on deep learning: Algorithms, techniques, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pouyanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sadiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">92</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Measuring the reliability of hate speech annotations: The case of the european refugee crisis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kurowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wojatzki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08118</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01817</idno>
		<title level="m">Yfcc100m: The new data in multimedia research</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Support vector machine for outlier detection in breast cancer survivability prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thongkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Asia-Pacific Web Conference</title>
		<imprint>
			<biblScope unit="page" from="99" to="109" />
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficiently scaling up crowdsourced video annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="204" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Waseem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first workshop on NLP and computational social science</title>
		<meeting>the first workshop on NLP and computational social science</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="138" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Asymptotic properties of nearest neighbor rules using edited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="408" to="421" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<title level="m">Are anchor points really indispensable in label-noise learning? In: NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning discriminative reconstructions for unsupervised outlier removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1511" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zoubin</surname></persName>
		</author>
		<idno>CMU-CALD-02-107</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Billion-scale semisupervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Z</forename><surname>Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dependence models for searching text in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Z</forename><surname>Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="63" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TPAMI.2017.2780108</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2017.2780108" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
