<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training Recurrent Answering Units with Joint Loss Minimization for VQA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<settlement>Pohang</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
							<email>bhhan@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<settlement>Pohang</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Training Recurrent Answering Units with Joint Loss Minimization for VQA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel algorithm for visual question answering based on a recurrent deep neural network, where every module in the network corresponds to a complete answering unit with attention mechanism by itself. The network is optimized by minimizing loss aggregated from all the units, which share model parameters while receiving different information to compute attention probability. For training, our model attends to a region within image feature map, updates its memory based on the question and attended image feature, and answers the question based on its memory state. This procedure is performed to compute loss in each step. The motivation of this approach is our observation that multi-step inferences are often required to answer questions while each problem may have a unique desirable number of steps, which is difficult to identify in practice. Hence, we always make the first unit in the network solve problems, but allow it to learn the knowledge from the rest of units by backpropagation unless it degrades the model. To implement this idea, we early-stop training each unit as soon as it starts to overfit. Note that, since more complex models tend to overfit on easier questions quickly, the last answering unit in the unfolded recurrent neural network is typically killed first while the first one remains last. We make a single-step prediction for a new question using the shared model. This strategy works better than the other options within our framework since the selected model is trained effectively from all units without overfitting. The proposed algorithm outperforms other multi-step attention based approaches using a single step prediction in VQA dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Visual Question Answering (VQA) <ref type="bibr" target="#b2">(Antol et al. 2015)</ref> is the problem to answer questions related to an input image. Contrary to traditional visual recognition tasks, which concentrate on a predefined problem such as object detection, scene classification, activity recognition, etc., VQA involves various recognition tasks at the same time and provides a unified approach to solve the problems defined by questions. Due to these reasons, VQA requires substantial amount of learning to capture information from images and understand questions.</p><p>Recently, deep learning based approaches using multiple reasoning steps with attention <ref type="bibr" target="#b15">Xiong, Merity, and Socher 2016)</ref> have been proposed to improve the performance of VQA systems. After extract-ing features from an image and a question using Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), respectively, these methods iteratively generate answers by combining question feature with image feature of local area given by attention. This approach is natural as many questions in VQA conceptually require multiple steps for reasoning. For example, to answer the question like "what is in front of the giraffe?" the VQA model should be able to solve the subtasks in sequel as "finding a giraffe," "identifying the region in front of the giraffe" and "classifying object in the specified region."</p><p>One of the critical limitations of the existing methods is that the number of steps for reasoning is fixed to a predefined number. Even though the exact number of steps required to answer a question is difficult to figure out, it is reasonable to assume that different questions need the different numbers of steps to reach solutions. For example, a question "what color is the apple on the table?" requires more steps than another question "what color is the apple?" If the number of inference steps is not adaptive to questions, we may need to solve multiple subtasks in a single step or a single subtask in multiple steps, which are likely to degrade system performance by causing overfitting or underfitting of trained models.</p><p>Instead of presetting the number of steps, we make the model learn the number of steps for inference of each question implicitly by simply minimizing joint loss from multiple steps of our recurrent deep neural network without extra supervision. We believe that learning to predict with a proper number of steps not only enhance the overall accuracy but also help each answering unit to generalize better. However, identifying the optimal number of steps is an extremely difficult task, and we propose an indirect but simple solution to learn a unified model for inference. By analyzing the answers from each step, we find out that predictions with more steps tend to overfit to easier questions even if later steps could solve more complex questions better than the earlier ones. Based on this observation, we progressively stop backpropagating the loss from the overfitting units. Training is terminated when the accuracy of the first answering unit is saturated. With this training strategy, we empirically show that the single-step prediction based on the first unit, whose model is trained with joint loss from multiple steps, outperforms other training-testing variations, and the accuracy is further improved by integrating early-stopping training strategy. The main contribution of the proposed algorithm is summarized below:</p><p>• We propose a novel architecture based on a deep recurrent neural network for VQA, which is composed of multiple answering units with shared parameters and optimized by minimizing joint loss from multiple units.</p><p>• We show that the VQA model involving multiple reasoning steps tends to overfit to easier questions quickly, and develop a unique training strategy to early-stop overfitting units progressively until the accuracy of the first unit is saturated.</p><p>• The proposed algorithm outperforms other multi-step attention based approaches using a single step prediction in VQA dataset.</p><p>The rest of the paper is organized as follows. We first review related work in Section and discuss our main idea with motivation in Section . Section describes the architecture to implement our idea and the training and testing methods of the proposed algorithm. We present experimental results of our algorithm in the standard public benchmark dataset in Section .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>The VQA problem is first addressed by (Malinowski and Fritz 2014), which proposes a Bayesian framework to combine symbolic reasoning and uncertain visual information. <ref type="bibr" target="#b18">(Zhou et al. 2015)</ref> has proposed a shallow neural network for VQA, which accepts the CNN features of images and the Bagof-Words (BoW) representations of questions as its inputs. Recently, VQA algorithms are often formulated with deep neural networks since the techniques based on deep learning have straightforward end-to-end training procedure by backpropagation and present competitive performance in terms of accuracy. These approaches typically pose VQA problems as simple classification tasks based on the joint features from images and questions, where CNNs and LSTMs (Hochreiter and Schmidhuber 1997) are employed to encode images and questions, respectively <ref type="bibr" target="#b13">(Ren, Kiros, and Zemel 2015;</ref><ref type="bibr" target="#b12">Malinowski, Rohrbach, and Fritz 2015)</ref> while only CNNs are used for both encoding and classification in <ref type="bibr" target="#b12">(Ma, Lu, and Li 2016)</ref>.</p><p>Several VQA systems <ref type="bibr" target="#b15">Xiong, Merity, and Socher 2016;</ref><ref type="bibr" target="#b14">Shih, Singh, and Hoiem 2016)</ref> attempt to identify relevant regions in the input image to answer questions, which is often performed by the soft attention mechanism <ref type="bibr" target="#b16">(Xu et al. 2015)</ref>. Specifically, <ref type="bibr" target="#b14">(Shih, Singh, and Hoiem 2016)</ref> utilizes a single step attention to an object proposal for VQA while <ref type="bibr" target="#b15">(Xiong, Merity, and Socher 2016)</ref> also employs multi-step attention based on the dynamic attention network <ref type="bibr" target="#b8">(Kumar et al. 2016)</ref>. Stacked attention network ) is motivated by memory networks <ref type="bibr" target="#b8">Kumar et al. 2016)</ref>, and constructs two succesive attention modules for multi-step inferences.</p><p>Several approaches employ the network architectures adaptive to input questions. Dynamic parameter prediction network (Noh, Seo, and Han 2016) determines the parameters of a fully connected layer in CNN given a question, and constructs a unified model to handle various tasks related to input images. With similar motivation, neural module networks combine multiple module networks to construct a single deep network <ref type="bibr" target="#b0">(Andreas et al. 2016a;</ref><ref type="bibr" target="#b1">Andreas et al. 2016b)</ref>, where the module combinations are given by syntactic parsing of questions.</p><p>Deep neural networks are sometimes trained with multiple supervisions to facilitate training procedure, where losses are backpropagated from multiple branches. <ref type="bibr">GoogLeNet (Szegedy et al. 2015)</ref> attaches auxiliary classifiers to a few intermediate layers of the network to provide additional supervision for training. Deeply supervised nets  has a companion objective function to avoid the vanishing gradient problem. Deeply recursive convolutional network  provides supervision to every recurrent convolutional layer and makes inference based on an ensemble of individual predictions. Our approach has something common with  in that supervision is given to each recurrent unit, but is differentiated since we analyze role of each supervision and propose a novel training strategy of early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>This section describes the general formulation of VQA and discusses our approach based on recurrent deep neural network with its motivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Formulation</head><p>We formulate VQA problem as a classification task. For a given image I and a question q, a VQA model predicts the best answer a * , which is formally given by</p><formula xml:id="formula_0">a * = argmax a∈Ω p(a|I, q; θ)<label>(1)</label></formula><p>where Ω denotes a set of all possible answers and θ is a set of model parameters in the network. There exist a lot of different methods to implement this probabilistic framework, but we focus on the models based on deep neural networks, which are frequently employed for VQA problems in recent years.</p><p>The models based on deep neural networks are typically composed of three main components: image encoder, question encoder, and answering module. Image and question encoders extract image feature f I from an input image I and question feature f q from a question sentence q, respectively. Answering module takes extracted image and question features, and generates an answer by combining information from the image and the question. Therefore, the module should be able to perform various tasks defined by questions. In this work, we propose a deep neural network architecture and investigate a training strategy for the answering module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Idea</head><p>The motivation behind this work is our observation that solving a task defined by a question often requires the capability to solve a sequence of atomic subtasks. However, the kinds and numbers of the subtasks in the sequence vary in individual questions, and, in addition, the same subtasks may appear in any places within the sequence depending on the question. Due to these reasons, it is extremely difficult to develop a unified algorithm to handle all the variations. Instead, to overcome the challenges indirectly, we design a novel neural network architecture for VQA and propose a training strategy with early-stopping based on a simple joint loss minimization.</p><p>The overall architecture of the proposed algorithm is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. The main component of the proposed network is an answering unit. Each answering unit is capable of solving the full task; based on the features extracted from image and question, it predicts an answer and updates its hidden state. By concatenating multiple answering units sequentially, we infer a series of answers, which are predicted by the model integrating subtasks progressively and solve more and more composite tasks. This procedure is implemented as a recurrent neural network whose recurrent unit corresponds to each answering unit.</p><p>We need to train the proposed network so that it can answer a given question by solving a series of subtasks one by one using the answering units. The main challenge to this objective is that it is difficult to know the optimal number of steps to solve each problem since there are various question types with different complexity. To circumvent this problem, we always make the first unit in the network solve problems, but allow it to learn the knowledge from the rest of units by backpropagation unless it degrades the model. For the purpose, we simply provide the same supervision to every step in the unfolded recurrent neural network and optimize all the answering units with shared parameters jointly using the standard backpropagation technique for RNNs. We also propose a unique training technique with early-stopping strategy, which is useful to avoid overfitting in complex models among multiple answering units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>This section discusses the proposed answering module and the procedure of training and testing. We provide more detailed description about the answering modules in the supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answering Module</head><p>Answering module is a recurrent neural network, which is composed of multiple answering units with shared model parameters as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. Using a given image feature map f I ∈ R P ×L (with P channels and L locations), a question feature f q ∈ R Q and an initial memory state h 0 mem ∈ R R , answering module provides a sequence of answer probability vectors a k ∈ R C (k = 1, . . . , K), where K is the number of answering units. Formally, the k th answering unit predicts an answer probability as</p><formula xml:id="formula_1">a k = Answer k (f I , f q , h k−1 mem ),<label>(2)</label></formula><p>where Answer k (·) is the answering operation in the k th answering unit. The function Answer k (·) requires several internal operations to predict answer probability as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. The first operation is to compute a subtask feature, which is extracted based on the question and the history of performed subtasks in the previous steps as follows:</p><formula xml:id="formula_2">f k sub = Subtask k (f q , h k−1 mem ; θ sub ),<label>(3)</label></formula><p>where θ sub is parameter for the subtask module. This subtask feature is used to perform an attention operation, which finds a relevant location in an image feature map with its corresponding feature vector based on the key f k sub . We employ soft attention mechanism <ref type="bibr" target="#b16">(Xu et al. 2015)</ref> and the attention operation is formally given by</p><formula xml:id="formula_3">f k loc , f k att = Attend k (f I , f k sub , h k−1 mem ; θ att ),<label>(4)</label></formula><p>where θ att is the parameters for soft attention, and f k loc and f k att denote attention probability map and attended feature, respectively. The last operation is Predict k (·) function. It receives subtask feature, attention information, and the memory state in the previous step to produce the final answer and updates hidden state. This operation involves LSTM to update hidden state in memory and the answer probability is given by applying a softmax function. The prediction operation is formally defined as</p><formula xml:id="formula_4">a k , h k mem = Predict k (f k sub , f k loc , f k att , h k−1 mem ),<label>(5)</label></formula><p>and the two outputs are specifically given by <ref type="formula">(7)</ref> where θ LSTM is the parameters for LSTM and θ soft is the parameters for softmax classifier.</p><formula xml:id="formula_5">a k = softmax(f k sub , f k loc , f k att , h k mem ; θ soft ) (6) h k mem = LSTM({f k sub , f k loc , f k att }, h k−1 mem ; θ LSTM ),</formula><p>To implement the Answering module, we need to learn the parameters for the all internal operations, and it is performed by the standard backpropagation technique. The detailed training procedure is described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>We minimize joint loss from multiple steps by providing the ground-truth answer y ∈ {0, 1} C to every step k as a supervision. Loss from a single step is computed by a cross entropy between the answer probability a k and the groundtruth answer y. Loss from each step is simply aggregated to compute the overall loss. Given image I n , question q n , and ground-truth label y n of the n th training example, the joint loss function is formally given by</p><formula xml:id="formula_6">L = 1 N N n=1 K k=1 −y T n log a k n ,<label>(8)</label></formula><p>where a k n = Answer k f I (I n ; θ I ), f q (q n ; θ q ), h k−1 mem ; θ ans . (9) Note that θ I and θ q denote parameters for image and question encoder, respectively. The model parameter for the answering module is given by θ ans = {θ sub , θ att , θ soft , θ LSTM }.</p><p>We train the model end-to-end by backpropagation, where the objective function based on cross entropy in Eq. (8) is minimzed. Given an image I n and a question q n , image encoder and question encoder extract features, f I and f q , respectively. The extracted features are given to the individual unfolded answering units. As the answering module itself is a recurrent neural network, we compute the gradients of all the parameters in θ ans by backpropagation through time (BPTT). Note that the backpropagated loss to the inputs of answering module, f I and f q , is used to learn the parameters of the image and question encoders, θ I and θ q .</p><p>When the network is trained with the joint loss from multiple steps as in Eq. (8), we observe that models with multiple steps generally overfit to training data easily and show lower validation/testing accuracy than the model in the first step as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Note that this observation also coincides with , which states that using three or more attention does not improve the performance any further. We believe that the answering units in later steps have more model capacity and can fit to training data better while it loses the generality of models. Such overfitting tendency may ruin the models in the earlier steps since the losses in the later steps are propagated backwards until the first answering unit. To circumvent this issue, we stop backpropagating losses from the overfitted answering units progressively as soon as we identify their overfitting. In practice, we validate the model in every epoch and early-stop training from an overfitted answering unit if the validation accuracy of the answering unit drops more than a predefined threshold from its maximum value. According to our observation, the answering units in the later steps typically are terminated first and the first answering unit always manages to stay alive. If validation dataset is not available, we early-stop training based on the formula for convenience, which is empirically determined as</p><formula xml:id="formula_7">t k stop = round (t max − t min ) e λ(K−k) − 1 e λ(K−1) − 1 + t min ,<label>(10)</label></formula><p>where t min is the number of epochs before the first early stopping, t max is the total number of epochs for training and t k stop is the number of epochs before training the k th answering unit is terminated. The early stopping is scheduled by controlling the configuration parameter λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing</head><p>In testing time, we predict answers only from the first answering unit in our model since it has better generalization accuracy in our experiments. Although we believe that some complex tasks can be solved better in the later steps, it is very difficult to know which step is optimal to find the correct solution. Hence, selecting one of the answers from multiple answering units is not feasible in practice. Also, the ensemble of the answers from multiple units is not particularly better than the solution from the first unit. Note that, since the losses from the later steps are always backpropagated until the first answering module, it gradually learns how to solve more complex problems and does not tend to overfit by employing early-stopping strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to Memory Network based Approaches</head><p>At first glance, the proposed method resembles the VQA methods such as <ref type="bibr" target="#b15">Xiong, Merity, and Socher 2016)</ref>, which are based on memory network . However, we introduce a novel training strategy with early-stopping and a simple testing method with a singlestep inference. This decision is based on our observations in VQA problems, but it is contradictory to <ref type="bibr" target="#b15">Xiong, Merity, and Socher 2016)</ref>. The previous works <ref type="bibr" target="#b15">Xiong, Merity, and Socher 2016)</ref> state that multistep training and testing without parameter sharing is advantageous, but our results suggest that multi-step training with parameter sharing and a single-step testing may be better in practice. This is partly supported by <ref type="figure" target="#fig_2">Figure 3</ref> although the experiment setting is not exactly identical. Also, our experiment supports the claim that benefit from multi-step training with weight sharing is larger than that from multi-step training and testing without sharing weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Dataset and Evaluation Metric</head><p>We train and test the proposed network in VQA dataset <ref type="bibr" target="#b2">(Antol et al. 2015)</ref>, which borrows images from MSCOCO dataset <ref type="bibr" target="#b10">(Lin et al. 2014)</ref> and collects questions and answers via Amazon Mechanical Turk. The dataset consists of 248,349 questions for training, 121,512 questions for validation, and 244,302 questions for testing. For each image, 3 questions are asked and 10 independent answers are given to each question. There are two test datasets; we typically use test-dev split for the control experiments and test-standard for comparison with external algorithms.</p><p>Two tasks are defined on VQA dataset: open-ended task and multple-choice task. The model has to predict an answer for an open-ended question without knowing predefined candidate answers, but select one of 18 candidate answers in multiple-choice task In both cases, the answers given by a model are evaluated by the following metric reflecting human </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>consensus:</head><p>Acc VQA (ans) = min #humans that said ans 3 , 1</p><p>where the score for a question is proportional to the number of matches with ground-truth answers, and the tested model receives full credit for each question if at least 3 people agree to the predicted answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We use VGG-16 net (Simonyan and Zisserman 2015) and ResNet-101 ) as image encoders. After rescaling input images to 448 × 448, we extract the feature maps from the last pooling layer in VGG or the layer below global average pooling layer in ResNet. The question features are given by a 2-layer LSTM 1 ; the final hidden and cell states in both layers are concatenated to be used as a question feature. The dimensionality of the attended image feature and the subtask feature are both 512. For better generalization, we apply dropout with rate of 0.5 to the features for input images and questions in each answering unit independently. The vocabulary size of our model is 14, 772 and the rest of words are converted to UNK tokens. The set of possible answers (Ω in Eq. (1)) contains 1, 000 answers with the highest frequencies among all answers in the training dataset. We set the number of steps K to 8 for multi-step training, and determine the parameters in Eq. (10), t max , t min , and λ, using validation set. We use Adam (Kingma and Ba 2015) for optimization. Learning rate for question encoder and answering modules are set to 3 × 10 −3 and 3 × 10 −4 , respectively, and the image encoder is not fine-tuned. Both learning rates are decayed in every epoch by the factor of 0.9. Additionally, we inject random noises sampled from Gaussian distribution to the gradient as suggested in <ref type="bibr">(Neelakantan et al. 2016)</ref>, where η is the number of training iterations. To alleviate the exploding gradient problem, we limit the magnitude of gradient vector to 0.1 by normalization. To facilitate reproduction, we make our code publicly available 2 . 1: Single model performance on the VQA test-dev dataset of all compared algorithms including the variations of our algorithm. Asterisk (*) denotes the concurrent submission with this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open-Ended</head><p>Multiple-Choice All Y/N Num Others All Y/N Num Others BoW Q <ref type="bibr" target="#b2">(Antol et al. 2015)</ref> 48.1 75.7 36.7 27.1 53.7 75.7 37.1 38.6 I <ref type="bibr" target="#b2">(Antol et al. 2015)</ref> 28.1 64.0 00.4 03.8 30.5 69.9 00.5 03.8 Bow Q+I <ref type="bibr" target="#b2">(Antol et al. 2015)</ref> 52.6 75.6 33.7 37.4 59.0 75.6 34.4 50.3 LSTM Q <ref type="bibr" target="#b2">(Antol et al. 2015)</ref> 48  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Analysis</head><p>To ensure the effectiveness of the multi-step training and early-stopping strategy, we perform several control experiments. The baseline is the proposed model without both components, which is referred to as Ours SS. We also test the model trained with multi-step training but without earlystopping strategy, Ours MS, and our full model denoted by  <ref type="table">Table 1</ref> illustrates the single model performance of various approaches trained without data augmentation. The proposed algorithm, Ours FULL outperforms the models based on multi-step attention <ref type="bibr" target="#b8">(Kumar et al. 2016;</ref><ref type="bibr" target="#b11">Lu et al. 2016)</ref> in the VQA dataset using the same image encoder. The best result is achieved by the work based on the multimodal compact bilinear pooling <ref type="bibr" target="#b3">(Fukui et al. 2016)</ref>. We believe that the multimodal compact bilinear pooling can be integrated within the proposed model, but we leave it as a future work.</p><p>Note that the performance gain from Ours SS to Ours FULL is about 2.3, which are significant in VQA context. Both training schemes turn out to be useful for performance improvement. These results are interesting because multi-step training is effective even for a single-step prediction and early-stopping strategy improves accuracy as long as all active steps are free from overfitting. Since we schedule early-stopping based on the formula from simple empirical study, the accuracy may be further improved with more sophisticated validation. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates training and validation accuracy of Ours FULL and Ours SS. The proposed training strategy denoted by Ours FULL outperforms Ours SS consistently even though they have the exactly same architecture for a single step prediction during testing. <ref type="figure">Figure 6</ref> presents predicted answers with attended regions. Visualization of attention shows that Ours FULL tends to focus on critical regions while Ours SS are often distracted by irrelevant objects. We believe that our training strategy helps the model learn to answer questions in appropriate steps in training while it enables the answering unit to implicitly solve a series of subtasks given a question in testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Stacked Attention Network</head><p>The architecture of our single answering unit is similar to the stacked attention network (SAN) . The difference lies in how to handle questions that require multi-ple steps to answer. As the number of required steps varies across questions and it is difficult to figure out the desirable number, SAN uses the same fixed number of steps for training and testing, which might result in overfitting observed in <ref type="figure" target="#fig_2">Figure 3</ref>. Instead, we provide multiple supervision in our network, but hope a single answering unit (the first one, in practice) to learn the best model for evaluation in testing. The comparison between the SAN and the proposed method in <ref type="table">Table 1</ref> clearly shows the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results with ResNet</head><p>We can improve the accuracy of a VQA system by using a better image encoder. Hence, we train the model with image features extracted from ResNet-101 , and this model is denoted by Ours ResNet. The model is trained with same hyper-parameters with Ours FULL in <ref type="table">Table 1</ref>, and evaluated both in VQA test-dev and test-standard. The results from Ours ResNet is presented in <ref type="table">Table 1</ref> and 2, where we observe that ResNet improves performance substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We proposed a VQA algorithm based on a recurrent deep neural network, which is trained by minimizing the joint loss from all answering units. The answering units share model parameters while the outputs from the units in the later steps depend on the results from the ones in the earlier steps. To maximize performance, we introduce an early stopping training strategy, where individual answering units are disregarded in training as soon as they start to overfit to the training set. Only the first answering unit is employed for inference since it is the model learned from all the multiple answering units without overfitting. The proposed architecture illustrates the outstanding performance in the standard VQA dataset without data augmentation. We believe that our algorithm has great potential to be used as a general framework for VQA problems by replacing our answering unit with any other networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The illustration of the answering unit which comprises subtask embedding, attention and predict operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Overall architecture of the proposed network. The proposed network is a recurrent deep neural network, where each recurrent unit corresponds to a complete module for visual question answering. For training, we unfold the network to predict answer and give supervision for every steps. For testing, we use a single answering unit to answer a question about an image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Training and validation accuracy curve for varying k. The prediction from the later step shows higher training accuracy but lower validation accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Training and validation accuracy of Ours SS and Ours Full.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative comparisons of attention betweenOurs Full and Ours SS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of single model performance in the VQA test-standard. Asterisk (*) denotes the concurrent submission with this paper.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Open-Ended</cell><cell></cell><cell cols="3">Multiple-Choice</cell></row><row><cell></cell><cell cols="8">All Y/N Num Others All Y/N Num Others</cell></row><row><cell>Human (Antol et al. 2015)</cell><cell cols="4">83.3 95.8 83.4 72.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>(Antol et al. 2015)</cell><cell cols="8">58.2 80.6 36.5 43.7 63.1 80.6 37.7 53.6</cell></row><row><cell>(Andreas et al. 2016a)</cell><cell cols="2">55.1 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>(Zhou et al. 2015)</cell><cell cols="8">55.9 76.8 35.0 42.6 62.0 76.9 37.3 54.6</cell></row><row><cell>(Shih, Singh, and Hoiem 2016)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">62.4 77.2 33.5 56.1</cell></row><row><cell>(Noh, Seo, and Han 2016)</cell><cell cols="8">57.4 80.3 36.9 42.2 62.7 80.4 38.8 52.8</cell></row><row><cell>(Andreas et al. 2016b)</cell><cell cols="2">58.0 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>(Yang et al. 2016)</cell><cell cols="2">58.9 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>(Wu et al. 2016)</cell><cell cols="4">59.4 81.1 37.1 45.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">(Xiong, Merity, and Socher 2016) 60.4 80.4 36.8 48.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>(Kim et al. 2016) ResNet*</cell><cell cols="8">61.8 82.4 38.2 49.4 66.3 82.4 39.6 58.4</cell></row><row><cell>(Lu et al. 2016) ResNet*</cell><cell cols="2">62.1 -</cell><cell>-</cell><cell>-</cell><cell cols="2">66.1 -</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours ResNet</cell><cell cols="8">63.2 81.7 38.2 52.8 67.3 81.7 40.0 61.0</cell></row><row><cell cols="9">Ours FULL employ both strategies for training. We use the</cell></row><row><cell cols="9">image features extracted from VGG-16 net (Simonyan and</cell></row><row><cell cols="6">Zisserman 2015) for all the control experiments.</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/VT-vision-lab/VQA_ LSTM_CNN 2 http://cvlab.postech.ac.kr/research/rau_ vqa/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Recurrent Answering Units with Joint Loss Minimization for VQA Supplementary Document</head><p>This document provides our implementation details and additional results that could not be accommodated in the main paper due to space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>In this section, we provide implementation details for each components of answering units described in Eq. (2) to (7) of the main paper. Without loss of generality, we describe the architecture of the k th answering unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtask Module</head><p>Subtask module in Eq. (3) of the main paper generates subtask feature f sub ∈ R S from a question feature f q ∈ R Q and the previous memory state h k−1 mem ∈ R M . This operation is given by</p><p>where W q ∈ R S×Q , W h ∈ R S×M and b sub ∈ R S are weight parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Module</head><p>Attention module in Eq. (4) of the main paper generates the attention probability map f k loc ∈ R L and attended feature f k att ∈ R S based on a soft attention mechanism. The computation of f k loc and f k att requires embedding of input image feature map f I ∈ R P ×L (with P channels and L locations), and the embedded feature map g I is given by</p><p>where W I ∈ R S×P and b I ∈ R S are weight parameters.</p><p>The procedure to compute attention probability map f k loc ∈ R L is composed of the following three steps. First, we compute pre-attention score α k ∈ R L based on the embeded image feature map g I ∈ R S×L and the subtask feature f k sub by the following equation:</p><p>are weight parameters, and 1 ∈ R L is one vector. Next, we compute attention score β k ∈ R L by adding another pre-attention score extracted from the previous memory h k−1 mem , which is given by</p><p>where W β ∈ R L×M and b β ∈ R L are weight parameters. Last, attention probability map f k loc ∈ R L is given by applying softmax function to the attention score β k ∈ R L as described below:</p><p>.</p><p>Once f k loc is obtained, attended feature f k att ∈ R S is computed using embedded image feature g I and attention probability map f k loc as follws:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction Module</head><p>Prediction module in Eq. (5) of the main paper has two submodule: LSTM and softmax classification. As specified in the main paper, we use publicly available LSTM implementation. Single layer LSTM is constructed and the dimensionality of LSTM hidden state is equal to that of memory state h k−1 mem ∈ R S . Input to the LSTM, denoted by f k join , is computed by</p><p>where W join ∈ R S×L and b join ∈ R S are weight parameters. Answer probability a k ∈ R C is obtained by applying a softmax function, which is given by</p><p>where W s ∈ R C×S and b s ∈ R C are weight parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Parameters</head><p>Network parameters used for the experiments are as follows: S = 512, Q = 1024, M = 512, P = 512, L = 196, A = 256, and C = 1000.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep compositional question answering with neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Fukui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<title level="m">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01455</idno>
		<title level="m">Multimodal residual learning for visual qa</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee ;</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00061</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li ;</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohrbach</forename><surname>Fritz ; Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">; L</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<editor>AAAI. [Malinowski and Fritz</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiros</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ask me anything: Free-form visual question answering based on knowledge from external sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Singh</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Merity</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Simple baseline for visual question answering</title>
		<idno>arXiv 1512:02167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
