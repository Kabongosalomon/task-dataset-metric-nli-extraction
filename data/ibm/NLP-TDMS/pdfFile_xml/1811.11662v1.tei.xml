<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Face Detection via Learning Small Faces on Hard Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
							<email>zhshuai.zhang@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
							<email>joe.siyuan.qiao@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
							<email>alan.l.yuille@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Face Detection via Learning Small Faces on Hard Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent anchor-based deep face detectors have achieved promising performance, but they are still struggling to detect hard faces, such as small, blurred and partially occluded faces. A reason is that they treat all images and faces equally, without putting more effort on hard ones; however, many training images only contain easy faces, which are less helpful to achieve better performance on hard images. In this paper, we propose that the robustness of a face detector against hard faces can be improved by learning small faces on hard images. Our intuitions are (1) hard images are the images which contain at least one hard face, thus they facilitate training robust face detectors; (2) most hard faces are small faces and other types of hard faces can be easily converted to small faces by shrinking. We build an anchor-based deep face detector, which only output a single feature map with small anchors, to specifically learn small faces and train it by a novel hard image mining strategy. Extensive experiments have been conducted on WIDER FACE, FDDB, Pascal Faces, and AFW datasets to show the effectiveness of our method. Our method achieves APs of 95.7, 94.9 and 89.7 on easy, medium and hard WIDER FACE val dataset respectively, which surpass the previous state-of-the-arts, especially on the hard subset. Code and model are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face detection is a fundamental and important computer vision problem, which is critical for many face-related tasks, such as face alignment <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref>, tracking <ref type="bibr" target="#b8">[9]</ref> and recognition <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref>. Stem from the recent successful development of deep neural networks, massive CNN-based face detection approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref> have been proposed and achieved the state-of-the-art performance. However, face detection remains a challenging task due to occlusion, illumination, makeup, as well as pose and scale variance, as shown in the benchmark dataset WIDER FACE <ref type="bibr" target="#b40">[41]</ref>.</p><p>Current state-of-the-art CNN-based face detectors at- tempt to address these challenges by employing more powerful backbone models <ref type="bibr" target="#b0">[1]</ref>, exploiting feature pyramid-style architectures to combine features from multiple detection feature maps <ref type="bibr" target="#b29">[30]</ref>, designing denser anchors <ref type="bibr" target="#b46">[47]</ref> and utilizing larger contextual information <ref type="bibr" target="#b29">[30]</ref>. These methods and techniques have been shown to be successful to build a robust face detector, and improve the performance towards human-level for most images. In spite of their success for most images, an evident performance gap still exists especially for those hard images which contain small, blurred and partially occluded faces. We realize that these hard images have become the main barriers for face detectors to achieve human-level detection performance. In <ref type="figure" target="#fig_0">Figure 1</ref>, we show that, even on the train set of WIDER FACE, the official pre-trained SSH 1 still fails on some of the images with extremely hard faces. We show two such hard training images in the upper right corner in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>On the other hand, most training images with easy faces can be almost perfectly detected (see the illustration in the right lower corner of <ref type="figure" target="#fig_0">Figure 1</ref>). As shown in left part of <ref type="figure" target="#fig_0">Figure 1</ref>, over two thirds of the training images already obtained perfect detection accuracy, which indicates that those easy images are less useful towards training a robust face detector. To address this issue, in this paper, we propose a robust face detector by putting more training focus on those hard images.</p><p>This issue is most related to anchor-level hard example mining discussed in OHEM <ref type="bibr" target="#b25">[26]</ref>. However, due to the sparsity of ground-truth faces and positive anchors, traditional anchor-level hard example mining mainly focuses on mining hard negative anchors, and mining hard anchors on well-detected images exhibits less effectiveness since there is no useful information that can be further exploited in these easy images. To address this issue, we propose to mine hard examples at image level in parallel with anchor level. More specifically, we propose to dynamically assign difficulty scores to training images during the learning process, which can determine whether an image is already well-detected or still useful for further training. This allows us to fully utilize the images which were not perfectly detected to better facilitate the following learning process. We show this strategy can make our detector more robust towards hard faces, without involving more complex network architecture and computation overhead.</p><p>Apart from mining the hard images, we also propose to improve the detection quality by exclusively exploiting small faces. Small faces are typically hard and have attracted extensive research attention <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b46">47]</ref>. Existing methods aim at building a scale-invariant face detector to learn and infer on both small and big faces, with multiple levels of detection features and anchors of different sizes. Compared with these methods, our detector is more efficient since it is specially designed to aggressively leveraging the small faces during training. More specifically, large faces are automatically ignored during training due to our anchor design, so that the model can fully focus on the small hard faces. Additionally, experiments demonstrate that this design effectively achieves improvements on detecting all faces in spite of its simple and shallow architecture.</p><p>To conclude, in this paper, we propose a novel face detector with the following contributions:</p><p>• We propose a hard image mining strategy, to improve the robustness of our detector to those extremely hard faces. This is done without any extra modules, parameters or computation overhead added on the existing detector.</p><p>• We design a single shot detector with only one detection feature map, which focuses on small faces with a specific range of sizes. This allows our model to be simple and focus on difficult small faces without struggling with scale variance.</p><p>• Our face detector establishes state-of-the-art performance on all popular face detection datasets, including WIDER FACE, FDDB, Pascal Faces, and AFW. We achieve 95.7, 94.9 and 89.7 on easy, medium and hard WIDER FACE val dataset. Our method also achieves APs of 99.00 and 99.60 on Pascal Faces and AFW respectively, as well as a TPR of 98.7 on FDDB.</p><p>The remainder of this paper is organized as follows. In Section 2, we discuss some studies have been done which are related to our paper. In Section 3, we dive into details of our proposed method, and we discuss experiment results and ablation experiments in Section 4. Finally, conclusions are drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Face detection has received extensive research attention <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>. With the emergence of modern CNN <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27]</ref> and object detector <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b45">46]</ref>, there are many face detectors proposed to achieve promising performances <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b44">45]</ref>, by adapting general object detection framework into face detection domain. We briefly review hard example mining, face detection architecture, and anchor design &amp; matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Hard example mining</head><p>Hard example mining is an important strategy to improve model quality, and has been studied extensively in image classification <ref type="bibr" target="#b14">[15]</ref> and general object detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>. The main idea is to find some hard positive and hard negative examples at each step, and put more effort into training on those hard examples <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref>. Recently, with modern detection frameworks proposed to boost the performance, OHEM <ref type="bibr" target="#b25">[26]</ref> and Focal loss <ref type="bibr" target="#b12">[13]</ref> have been proposed to select hard examples. OHEM computed the gradients of the networks by selecting the proposals with highest losses in every minibatch; while Focal loss aimed at naturally putting more focus on hard and misclassified examples by adding a factor to the standard cross entropy criterion. However, these algorithms mainly focused on anchor-level or proposal-level mining. It cannot handle the imbalance of easy and hard images in the dataset. In our paper, we propose to exploit hard example mining on image level, i.e. hard image mining, to improve the quality of face detector on extremely hard faces. More specifically, we assign difficulty scores to training images while training with an SGD mechanism, and re-sample the training images to build a new training subset at the next epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Face Detection Architecture</head><p>Recent state-of-the-art face detectors are generally built based on Faster-RCNN <ref type="bibr" target="#b21">[22]</ref>, R-FCN <ref type="bibr" target="#b3">[4]</ref> or SSD <ref type="bibr" target="#b13">[14]</ref>. SSH <ref type="bibr" target="#b16">[17]</ref> exploited the RPN (Region Proposal Network) from Faster-RCNN to detect faces, by building three detection feature maps and designing six anchors with different sizes attached to the detection feature maps. S 3 FD <ref type="bibr" target="#b44">[45]</ref> and PyramidBox <ref type="bibr" target="#b29">[30]</ref>, on the other hand, adopted SSD as their detection architecture with six different detection feature  <ref type="figure">Figure 2</ref>: The framework of our face detector. We take VGG16 as our backbone CNN, and we fuse two layers (conv4 3 and conv5 3) after dimension reduction and bilinear upsampling, to generate the final detection feature map. Based on that, we add a detection head for classification and bounding-box regression.</p><p>maps. Different from S 3 FD, PyramidBox exploited a feature pyramid-style structure to combine features from different detection feature maps. Our proposed method, on the other hand, only builds single level detection feature map, based on VGG16, for classification and bounding-box regression, which is both simple and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Anchor design and matching</head><p>Usually, anchors are designed to have different sizes to detect objects with different scales, in order to build a scaleinvariant detector. SSD as well as its follow-up detectors S 3 FD and PyramidBox, had six sets of anchors with different sizes, ranging from (16 × 16) to (512 × 512), and their network architectures had six levels of detection feature maps, with resolutions ranging from 1 4 to 1 128 , respectively. Similarly, SSH had the same anchor setting, and those anchors were attached to three levels of detection feature maps with resolutions ranging from 1 8 to 1 32 . The difference between SSH and S 3 DF is that in SSH, anchors with two neighboring sizes shared the same detection feature map, while in S 3 DF, anchors with different sizes are attached to different detection feature maps.</p><p>SNIP <ref type="bibr" target="#b27">[28]</ref> discussed an alternative approach to handle scales. It showed that CNNs are not robust to changes in scale, so training and testing on the same scales of an image pyramid can be a more optimal strategy. In our paper, we exploit this idea by limiting the anchor sizes to be (16×16), (32 × 32) and (64 × 64). Then those faces with either too small or too big sizes will not be matched to any of the anchors, thus will be ignored during the training and testing. By removing those large anchors with sizes larger than (64 × 64), our network focuses more on small faces which are potentially more difficult. To deal with large faces, we use multiscale training and testing to resize them to match our anchors. Experiments show this design performs well on both small and big faces, although it has fewer detection feature maps and anchor sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>In this section, we introduce our proposed method for effective face detection. We first discuss the architecture of our detector in Section 3.1, then we elaborate our hard image mining strategy in Section 3.2, as well as some other useful training techniques in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Single-level small face detection framework</head><p>The framework of our face detector is illustrated in Figure 2. We use VGG16 network as our backbone CNN, and combine conv4 3 and conv5 3 features, to build the detection feature map with both low-level and high-level semantic information. Similar to SSH <ref type="bibr" target="#b16">[17]</ref>, we apply 1×1 convolution layers after conv4 3 and conv5 3 to reduce dimension, and then apply a 3×3 convolution layer on the concatenation of these two dimension reduced features. The output feature of the 3×3 convolution layer is the final detection feature map, which will be fed into the detection head for classification and bounding-box regression.</p><p>The detection feature map has a resolution of 1 8 of the original image (of size H × W ). We attach three anchors at each point in the grid as default face detection boxes. Then we do classification and bounding-box regression on those 3 × H 8 × W 8 anchors. Unlike many other face detectors which build multiple feature maps to detect face with a variant range of scales, inspired by SNIP <ref type="bibr" target="#b27">[28]</ref>, faces are trained and inferred with roughly the same scales. We only have one detection feature map, with three sets of anchors attached to it. The anchors have sizes of (16 × 16), <ref type="bibr">(32 × 32)</ref>   <ref type="figure">Figure 3</ref>: The framework of our dilated detection head for classification and regression. Based on the detection feature from the backbone CNN, we first perform a dimension reduction to reduce the number of channels from 512 to 128. Then we put three convolution layers with the shared weight, and different dilation rates, to generate final detection and classification features.</p><p>and (64 × 64), and the aspect ratio is set to be 1. By making this configuration, our network only trains and infers on small and medium size of faces; and we propose to handle large faces by shrinking the images in the test phase. We argue that there is no speed or accuracy degradation for large faces, since inferring on a tiny image (with short side containing 100 or 300 pixels) is very fast, and the shrinked large face will still have enough information to be recognized.</p><p>To handle the difference of anchor sizes attached to the same detection feature map, we propose a detection head which uses different dilation rates for anchors with different sizes, as shown in <ref type="figure">Figure 3</ref>. The intuition is that in order to detect faces with different sizes, different effective receptive fields are required. This naturally requires the backbone feature map to be invariant to scales. To this end, we adopt different dilation rates for anchors with different sizes. For anchors with size (16 × 16), (32 × 32) and (64 × 64), we use a convolution with kernel size of 3 and dilation rate of 1, 2 and 4 to gather context features at different scales. These three convolution layers share weights to reduce the model size. With this design, the input of the 3 × 3 convolution, will be aligned to the same location of faces, regardless of the size of faces and anchors. Ablation experiments show the effectiveness of this multi-dilation design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hard image mining</head><p>Different from OHEM discussed in Section 3.3, which selects proposals or anchors with the highest losses, we propose a novel hard image mining strategy at image level. The intuition is that most images in the dataset are very easy, and we can achieve a very high AP even on the hard subset of the WIDER FACE val dataset with our baseline model. We believe not all training images should be treated equally, and well-recognized images will not help towards training a more robust face detector. To put more attention on training hard images instead of easy ones, we use a subset D of all training images D, to contain hard ones for training. At the beginning of each epoch, we build D based on the difficulty scores obtained in the previous epoch.</p><p>We initially use all training images to train our model (i.e. D = D). This is due to the fact that our initial Im-ageNet pre-trained model will only give random guess towards face detection. In this case, there is no easy image. In other words, every image is considered as hard image and fed to the network for training at the first epoch. During the training procedure, we dynamically assign different difficulty scores to training images, which is defined by the metric where A(I) + is the set of positive anchors for image I, with IoU over 0.5 against ground-truth boxes, l is the classification logit and l(I; Θ) a,1 , l(I; Θ) a,0 are the logits of anchor a for image I to be foreground face and background. All images are initially marked as hard, and any image with WPAS greater than a threshold th will be marked as easy image.</p><p>At the beginning of each epoch, we first randomly shuffle the training dataset to generate the complete training list D = [I i1 , I i2 , · · · , I in ] for the following epoch of training. Then given an image marked as easy, we remove it from D with a probability of p. The remaining training list D = [I ij i , I ij 2 , · · · , I ij k ], which focuses more on hard images, will be used for training at this epoch. Note that for multi-GPU training, each GPU will maintain its training list D independently. In our experiments, we set the probability p to be 0.7, and the threshold th to be 0.85.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training strategy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-scale training and anchor matching</head><p>Since we only have anchors covering a limited range of face scales, we train our model by varying the sizes of training images. During the training phase, we resize the training images so that the short side of the image contains s pixels, where s is randomly selected from {400, 800, 1200}. We also set an upper bound of 2000 pixels to the long side of the image considering the GPU memory limitation.</p><p>For each anchor, we assign a label {+1, 0, −1} based on how well it matches with any ground-truth face bounding box. If an anchor has an IoU (Intersection over Union) over 0.5 against a ground-truth face bounding box, we assign +1 to that anchor. On the other hand, if the IoU against any ground-truth face bounding box is lower than 0.3, we assign 0 to that anchor. All other anchors will be given −1 as the label, and thus will be ignored in the classification loss. By doing so, we only train on faces with designated scales. Those faces with no anchor matching will be simply ignored, since we do not assign the anchor with largest IoU to it (thus assign the corresponding anchor label +1) as Faster-RCNN does. This anchor matching strategy will ignore the large faces, and our model can put more capacity on learning different face patterns on hard small faces instead of memorizing the change in scales.</p><p>For the regression loss, all anchors with IoU greater than 0.3 against ground-truth faces will be taken into account and contribute to the smooth 1 loss. We use a smaller threshold (i.e. 0.3) because (1) this will allow imperfectly matched anchors to be able to localize the face, which may be useful during the testing and (2) the regression task has less supervision since unlike classification, there are no negative anchors for computing loss and the positive anchors are usually sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anchor-level hard example mining</head><p>OHEM has been proven to be useful for object detection and face detection in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>. During our training, in parallel with our newly proposed hard image mining, we also exploit the traditional hard anchor mining method to focus more on the hard and misclassificed anchors. Given a training image with size H × W , there are 3 × H 8 × W 8 anchors at the detection head, and we only select 256 of them to be involved in computing the classification loss. For all positive anchors with IoU greater than 0.5 against ground-truth boxes, we select the top 64 of them with lowest confidences to be recognized as face. After selecting positive anchors, (256 − #pos anchor) negative anchors with highest face confidence are selected to compute the classification loss as the hard negative anchors. Note that we only perform OHEM for classification loss, and we keep all anchors with IoU greater than 0.3 for computing regression loss, without selecting a subset based on either classification loss or bounding-box regression loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data augmentation</head><p>Data augmentation is extremely useful to make the model robust to light, scale changes and small shifts <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref>. In our proposed method, we exploit cropping and photometric distortion as data augmentation. Given a training image after resizing, we crop a patch of it with a probability of 0.5. The patch has a height of H and a width of W which are independently drawn from U(0.6H, H) and U(0.6W, W ), where U is the uniform distribution and H, W are the height and width of the resized training image. All ground-truth boxes whose centers are located inside the patch are kept. After the random cropping, we apply photometric distortion following SSD by randomly modifying the brightness, contrast, saturation and hue of the cropped image randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To verify the effectiveness of our model and proposed method, we conduct extensive experiments on popular face detection datasets, including WIDER FACE <ref type="bibr" target="#b40">[41]</ref>, FDDB <ref type="bibr" target="#b7">[8]</ref>, Pascal Faces <ref type="bibr" target="#b37">[38]</ref> and AFW <ref type="bibr" target="#b48">[49]</ref>. It is worth noting that the training is only performed on the train set of WIDER FACE, and we use the same model for evaluation on all these datasets without further fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental settings</head><p>We train our model on the train set of WIDER FACE, which has 12880 images with 159k faces annotated. We flip all images horizontally, to double the size of our training dataset to 25760. For each training image, we first randomly resize it, and then we use the cropping and photometric distortion data augmentation methods discussed in Section 3.3 to pre-process the resized image. We use an ImageNet pretrained VGG16 <ref type="bibr" target="#b9">[10]</ref> model to initialize our network backbone, and our newly introduced layers are randomly initialized with Gaussian initialization. We train the model with the itersize to be 2, for 46k iterations, with a learning rate of 0.004, and then for another 14k iterations with a smaller learning rate of 0.0004. During training, we use 4 GPUs to simultaneously to compute the gradient and update the weight by synchronized SGD with Momentum <ref type="bibr" target="#b19">[20]</ref>. The first two blocks of VGG16 are frozen during the training, and the rest layers of VGG16 are set to have a double learning rate.</p><p>Since our model is designed and trained on only small faces, we use a multiscale image pyramid for testing to deal with faces larger than our anchors. Specifically, we resize the testing image so that the short side contains 100, 300, 600, 1000 and 1400 pixels for evaluation on WIDER FACE dataset. We also follow the testing strategies used in Pyra-midBox <ref type="bibr" target="#b29">[30]</ref> 2 such as horizontal flip and bounding-box voting <ref type="bibr" target="#b4">[5]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment results</head><p>WIDER FACE dataset includes 3226 images and 39708 faces labelled in the val dataset, with three subsetseasy, medium and hard. In <ref type="figure">Figure 4</ref>, we show the precision-recall (PR) curve and average precision (AP) for our model compared with many other state-of-the-arts <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref> on these three subsets. As we can see, our method achieves the best performance on the hard subset, and outperforms the current state-of-the-art by a large margin. Since the hard set is a super set of small and medium, which contains all faces taller than 10 pixels, the performance on hard set can represent the performance on the full testing dataset more accurately. Our performance on the medium subset is comparable to the most recent state-of-the-art and the performance on the easy subset is a bit worse since our method focuses on learning hard faces, and the architecture of our model is simpler compared with other state-of-thearts.</p><p>There is also a WIDER FACE test dataset with no annotations provided publicly. It contains 16097 images, and is evaluated by WIDER FACE author team. We report the performance of our method at <ref type="figure">Figure 5</ref> for the hard subset.</p><p>FDDB dataset includes 5171 faces on a set of 2845 images, and we use our model trained on WIDER FACE train set to infer on the FDDB dataset. We use the raw boundingbox result without fitting it into ellipse to compute ROC. We show the discontinuous ROC curve at <ref type="figure" target="#fig_4">Figure 6a</ref> compared with <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49]</ref>, and our method achieves the state-of-the-art performance of TPR=98.7% given 1000 false positives.</p><p>Pascal Faces dataset includes 1335 labeled faces on a set of 851 images extracted for the Pascal VOC dataset. We show the PR curve at <ref type="figure" target="#fig_4">Figure 6b</ref> compared with <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b44">45]</ref>, and our method achieves a new the state-of-the-art performance of AP=99.0.</p><p>AFW dataset includes 473 faces labelled in a set of 205 images. As shown in <ref type="figure" target="#fig_4">Figure 6c</ref> compared with <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49]</ref>, our method achieves state-of-the-art and almost perfect performance, with an AP of 99.60.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study and diagnosis Ablation experiments</head><p>In order to verify the performance of our single level face detector, as well as the effectiveness of our proposed hard image mining, the dilated-head classification and regression structure, we conduct various ablation experiments on the WIDER FACE val dataset. All results are summarized in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>From <ref type="table" target="#tab_1">Table 1</ref>, we can see that our single level baseline model can achieve performance comparable to the current  Baseline-Single is our proposed detector with single detection feature map shown in <ref type="figure">Figure 2</ref>. HIM and DH represents hard image mining (Subsection 3.2) and dilated head architecture <ref type="figure">(Figure 3</ref>).</p><p>state-of-the-art face detector, especially on the hard subset.</p><p>Our model with single detection feature map performs better than the one with three detection feature maps, despite its shallower structure, fewer parameters and anchors. This confirms the effectiveness of our simple face detector with single detection feature map focusing on small faces. We also separately verify our newly proposed hard image mining (HIM) and dilated head architecture (DH) described in Subsection 3.2 and <ref type="figure">Figure 3</ref> respectively. HIM can improve the performance on hard subset significantly without involving more complex network architecture nor computation overhead. DH itself can also boost the performance, which shows the effectiveness of designing larger convolution for larger anchors. Combining HIM and DH together can improve further towards the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagnosis of hard image mining</head><p>We investigate the effects of our hard image mining mechanism. We show the ratio of |D | and |D − D | (i.e. the ratio of the number of selected training images to the number of ignored training images) in <ref type="figure" target="#fig_5">Figure 7</ref> for each epoch. We can see that at the first epoch, all training images are used to train the model. Meanwhile, as the training process continues, more and more training images will be ignored. At the last epoch, over a half images will be ignored and thus will not be included in D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagnosis of data augmentation</head><p>We investigate the effectiveness of the photometric distortion as well as the cropping mechanisms as discussed in Subsection 3.3. The ablation results evaluated on WIDER FACE val dataset are shown in <ref type="table" target="#tab_3">Table 2</ref>. Both photometric distortion and cropping can contribute to a more robust face detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagnosis of multi-scale testing</head><p>Our face detector with one detection feature map is design for small face detection, and our anchors are only capable of capturing faces with sizes ranging from <ref type="bibr">(</ref>   <ref type="table">Table 3</ref>: Diagnosis of multi-scale testing. <ref type="table">Table 3</ref>, the extra small scales are crucial to detect easy faces. Without resizing the short side to contain 100 and 300 pixels, the performance on easy subset is only 78.2, which is even lower than the performance on medium and hard which contain much harder faces. We will show in the next subsection that these extra small scales (100 and 300) lead to negligible computation overhead, due to the lower resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagnosis of accuracy/speed trade-off</head><p>We evaluate the speed of our method as well as some other popular face detectors in <ref type="table" target="#tab_5">Table 4</ref>. For fair comparison, we run all methods on the same machine, with one Titan X (Maxwell) GPU, and Intel Core i7-4770K 3.50GHz. All methods except for PyramidBox are based on Caffe1 implementation, which is compiled with CUDA 9.0 and CUDNN 7. For PyramidBox, we follow the official fluid code and the default configurations 3 . We use the officially built Pad-dlePaddle with CUDA 9.0 and CUDNN 7 <ref type="bibr" target="#b3">4</ref> .</p><p>For SSH, S 3 FD and Pyramid, we use the official inference code and configurations. For SSH, we use multi-scale  testing with the short side containing 500, 800, 1200 and 1600 pixels, and for S 3 FD, we execute the official evaluation code with both multi-scale testing and horizontal flip. PyramidBox takes a similar testing configuration as S 3 FD. As shown in <ref type="table" target="#tab_5">Table 4</ref>, our detector can outperform SSH, S 3 FD and PyramidBox significantly with a smaller inference time. Based on that, using horizontal flip can further improve the performance slightly. In terms of GPU memory usage, our method uses only a half of what PyramidBox occupies, while achieving better performance. Ours * in <ref type="table" target="#tab_5">Table 4</ref> indicates our method without extra small scales in inference, i.e., evaluated with scales [600, 1000, 1400]. It is only 6.5% faster than evaluation with [100, 300, 600, 1000, 1400] (1.59 compared with 1.70). This proves that although our face detector is only trained on small faces, it can perform well on large faces, by simply shrinking the testing image with negligible computation overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>To conclude, we propose a novel face detector to focus on learning small faces on hard images, which achieves the state-of-the-art performance on all popular face detection datasets. We propose a hard image mining strategy by dynamically assigning difficulty scores to training images, and re-sampling subsets with hard images for training before each epoch. We also design a single shot face detector with only one detection feature map, to train and test on small faces. With these designs, our model can put more attention on learning small hard faces instead of memorizing change of scales. Extensive experiments and ablations have been done to show the effectiveness of our method, and our face detector achieves the state-of-the-art performance on all popular face detection datasets, including WIDER FACE, FDDB, Pascal Faces and AFW. Our face detector also enjoys faster multi-scale inference speed and less GPU memory usage. Our proposed method are flexible and can be applied to other backbones and tasks, which we remain as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: AP of each training image computed based on official SSH model, the x-axis the is index of the training image, the y-axis is the AP for the corresponding image. Upper right: hard training images. Lower right: easy training images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Worst Positive Anchor Score (WPAS): WPAS(I; Θ) = min a∈A(I) + exp(l(I; Θ) a,1 ) exp(l(I; Θ) a,1 ) + exp(l(I; Θ) a,0 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Precision-recall curve on WIDER FACE val dataset. Precision-recall curve on the hard subset of WIDER FACE test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Performance compared with state-of-the-arts on other face datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Ratio of ignored images. X-axis is the epoch index and y-axis is the proportion of ignored training images by hard image mining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>low-dim feature Sharing weight detection feature</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>3x3 conv</cell><cell></cell></row><row><cell></cell><cell>N=128</cell><cell></cell></row><row><cell>3x3 conv N=128 D=1</cell><cell>3x3 conv N=128 D=2</cell><cell>3x3 conv N=128 D=4</cell></row><row><cell>feature</cell><cell>feature</cell><cell>feature</cell></row><row><cell>A1</cell><cell>A2</cell><cell>A4</cell></row><row><cell>Anchor = 16x16</cell><cell>Anchor = 32x32</cell><cell>Anchor = 64x64</cell></row><row><cell>Classification</cell><cell>Classification</cell><cell>Classification</cell></row><row><cell>Regression</cell><cell>Regression</cell><cell>Regression</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation experiments. Baseline-Three is a face detector similar to SSH with three detection feature maps.</figDesc><table><row><cell>Method</cell><cell cols="3">easy medium hard</cell></row><row><cell cols="2">Baseline-Three 95.0</cell><cell>93.8</cell><cell>88.5</cell></row><row><cell cols="2">Baseline-Single 95.1</cell><cell>94.2</cell><cell>89.1</cell></row><row><cell>+ HIM</cell><cell>95.4</cell><cell>94.8</cell><cell>89.6</cell></row><row><cell>+ DH</cell><cell>95.4</cell><cell>94.5</cell><cell>89.3</cell></row><row><cell>+ DH + HIM</cell><cell>95.7</cell><cell>94.9</cell><cell>89.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Diagnosis of data augmentation. PD indicates photometric distortion. All entries are based on our Baseline-SingleLevel configuration without HIM and DH.</figDesc><table><row><cell cols="4">(64 × 64). As a result, it is critical to adopt multi-scale test-</cell></row><row><cell cols="4">ing to deal with large faces. Different from SSH, S 3 FD and</cell></row><row><cell cols="4">PyramidBox, our testing pyramid includes some extreme</cell></row><row><cell cols="4">small scales (i.e. short side contains only 100 or 300 pix-</cell></row><row><cell cols="4">els). In Table 3, we show the effectiveness of these extreme</cell></row><row><cell cols="4">small scales to deal with easy and large images. Our full</cell></row><row><cell cols="4">evaluation resizes the image so that the short side contains</cell></row><row><cell cols="4">100, 300, 600, 1000 and 1400 pixels respectively, to build</cell></row><row><cell cols="4">an image pyramid. We diagnose the impact of the extra</cell></row><row><cell cols="4">small scales (i.e. 100 and 300) by removing them from the</cell></row><row><cell>image pyramid.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Testing Scales</cell><cell cols="3">easy medium hard</cell></row><row><cell>[600, 1000, 1400]</cell><cell>78.2</cell><cell>85.7</cell><cell>86.1</cell></row><row><cell>[300, 600, 1000, 1400]</cell><cell>91.3</cell><cell>92.6</cell><cell>88.8</cell></row><row><cell cols="2">[100, 300, 600, 1000, 1400] 95.7</cell><cell>94.9</cell><cell>89.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Diagnosis of inference speed. MS and HF indicate multi-scale testing and horizontal flip; Time is the inference time (in second) for a single image; G-Mem is the GPU memory usage in gigabyte; AP-h is the average precision on the hard subset of WIDER FACE val set. Ours * indicates our detector without extra small scales. All entries are evaluated with a single nVIDIA Titan X (Maxwell).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/PaddlePaddle/models/blob/develop/fluid/PaddleCV/ face detection/widerface eval.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/PaddlePaddle/models/blob/develop/fluid/PaddleCV/ face detection/widerface eval.py 4 pip install paddlepaddle-gpu</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finding tiny faces in the wild with generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Face alignment by explicit shape regression. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1522" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face tracking and recognition with visual constraints in real-world videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient boosted exemplar-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1843" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning surf cascade for fast and accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3468" to="3475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Online batch selection for faster training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06343</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SSH: Single stage headless face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4885" to="4894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">To boost or not to boost? on the limits of boosted trees for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="41" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Neural networkbased face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>PAMI</publisher>
			<biblScope unit="page" from="23" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detecting and aligning faces by image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3460" to="3467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection-snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3059" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyramidbox: A context-assisted single shot face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="812" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01061</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Face r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Face attention network: An effective face detector for the occluded faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07246</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Detecting faces using region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05256</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Detecting faces using region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05256</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Face detection by structural models. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aggregate channel features for multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCB</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3676" to="3684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02863</idno>
		<title level="m">Face detection through scale-friendly deep convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Face detection using improved faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02142</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">S3 fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Single-shot object detection with enriched semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5813" to="5821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Seeing small faces from robust anchors perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5127" to="5136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cms-rcnn: contextual multi-scale region-based cnn for unconstrained face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning for Biometrics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="57" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
