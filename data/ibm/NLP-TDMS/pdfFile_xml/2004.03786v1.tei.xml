<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Downstream Model Design of Pre-trained Language Model for Relation Extraction Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
							<email>licheng81@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">AI Application Research Center</orgName>
								<orgName type="institution" key="instit2">Huawei Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">AI Application Research Center</orgName>
								<orgName type="institution" key="instit2">Huawei Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Downstream Model Design of Pre-trained Language Model for Relation Extraction Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised relation extraction methods based on deep neural network play an important role in the recent information extraction field. However, at present, their performance still fails to reach a good level due to the existence of complicated relations. On the other hand, recently proposed pre-trained language models (PLMs) have achieved great success in multiple tasks of natural language processing through fine-tuning when combined with the model of downstream tasks. However, original standard tasks of PLM do not include the relation extraction task yet. We believe that PLMs can also be used to solve the relation extraction problem, but it is necessary to establish a specially designed downstream task model or even loss function for dealing with complicated relations. In this paper, a new network architecture with a special loss function is designed to serve as a downstream model of PLMs for supervised relation extraction. Experiments have shown that our method significantly exceeded the current optimal baseline models across multiple public datasets of relation extraction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>However, the current methods do not perform very well on public datasets given the existence of complicated relations, such as long-distance relation, single sentence with multiple relations, and overlapped relations on entity-pairs.</p><p>Recently, the emergence of pre-trained language models has provided new ideas for solving relation extraction problems. Pre-trained language models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22]</ref> are a kind of super-large-scale neural network model based on the deep Transformer <ref type="bibr" target="#b25">[26]</ref> structure. Their initial parameters are learned through super-large self-supervised training, and then combined with multiple downstream models to fix special tasks by fine-tuning. Experiments show that the downstream tasks' performances of this kind of models are usually far superior to those of conventional models <ref type="bibr" target="#b15">[16]</ref>. Unfortunately, the relation extraction task is not in the original downstream tasks list directly supported by the pre-trained language model. Our current work attempts to leverage the power of the PLMs to establish an effective downstream model that is competent for the relation extraction tasks. In particular, we implement three important improvements in the main steps, as described above:</p><p>1. Use a pre-trained language model (this article uses BERT <ref type="bibr" target="#b0">[1]</ref>) instead of the traditional encoder, and obtain a variety of token embeddings from the model. We extract the embeddings from two different layers to represent the head and tail entities separately, because it may help to learn reversible relations. On the other hand, we also add the context information into the entity to deal with long-distance relations.</p><p>2. After then, we calculate a parameterized asymmetric kernel inner product matrix between all the head and tail embeddings of each token in a sequence. Since kernels are different between each relation, we believe such a product is helpful for distinguishing multiple relations between same entity pairs. Thus the matrix can be treated as the tendency score to indicate where a certain relation exists.</p><p>3. Use the Sigmoid classifier instead of the Softmax classifier, and use the average probability of token pairs corresponding to each entity pair as the final probability that the entity pair has a certain relation. Thus for each type of relation, and each entity pair in the input text, we can independently calculate the probability of whether a relation exists. This mechanism allows the model to predict multiple relations even overlapped on same entities.</p><p>We also notice that it is very easy to integrate Named Entity Recognition (NER) task into our model to deal with joint extraction task. For instance, adding Bi-LSTM and CRF <ref type="bibr" target="#b7">[8]</ref> after the BERT encoding layer makes it easy to build an efficient NER task processor. By simply adding the NER loss on original loss, we can build a joint extraction model to avoid the pipeline errors. However, in this paper we will focus on relation extraction and will not pay attention to this method.</p><p>Our experiments mainly verify two conclusions. 1. Pre-trained language model can perform well in relation extraction task after our re-designing. We evaluate the method on 3 public datasets: SemEval 2010 Task 8 <ref type="bibr" target="#b6">[7]</ref>, NYT <ref type="bibr" target="#b22">[23]</ref>, WebNLG <ref type="bibr" target="#b2">[3]</ref>. The experimental results show that our model achieves a new state-of-the-art.</p><p>2. A test for our performance in different overlapped relation situations and multiple relation situations shows that our model is robust when faced with complex relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In recent years, numerous models have been proposed to process the supervised relation extraction task with deep neural networks. The general paradigm is to create a deep neural network model by learning from texts labeled with entities information and the relation-type-label between them. The model then can predict the possible relation-type-label for the specified entities in a new text. We introduce three kinds of common neural network structures here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CNN-based methods</head><p>CNN based methods apply Convolutional Neural Network <ref type="bibr" target="#b12">[13]</ref> to capture text information and reconstruct embeddings <ref type="bibr" target="#b14">[15]</ref>. Besides simple CNN, a series of improved models have been proposed <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9]</ref>. However, the CNN approach limits the model's ability to handle remote relations. The information that is fused through the CNN network is often local, so it is difficult to deal with distant relations. Therefore, these methods are currently limited to a large extent and are not able to achieve a good level of application. Since more efficient methods are proposed recently, we will not compare our approach with this early work in this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GNN-based methods</head><p>GNN based methods use Graph Neural Network <ref type="bibr" target="#b23">[24]</ref>, mainly Graph Convolutional Network <ref type="bibr" target="#b11">[12]</ref> to deal with entities' relations. GNN is a neural network that can capture the topological characteristics of the graph type data, so it is necessary to define a prior graph structure. The GNN-based relation extraction methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2]</ref> usually use the text dependency tree as an input prior graph structure, thereby obtaining richer information representation than CNN. However, this kind of methods relies on the dependency parser thus pipeline errors exist. In addition, grammar-level dependency tree is still a shallow representation which fails to efficiently express relations between words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Methods with Pre-trained Language Model</head><p>Some recent approaches consider relation extraction as a downstream task for PLMs. These methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27]</ref> have made some success, but we believe that they have not yet fully utilized the language model. The main reason is the lack of a valid representation of the relation -those methods tend to express the relation as a one-dimensional vector. We believe that since the relation is determined by the vectors' correlations of head and tail entities, it should naturally be represented as a matrix rather than one-dimensional vector. In this way, more information like the order of the entities and their positions in the text will be used while predicting their relations. <ref type="figure">Figure 1</ref>: Our method's network architecture. F is an asymmetric kernel inner product function of embeddings at each position. Thus we get a product matrix of l × l (l = 7 in this figure) for each relation type. We further use Sigmoid activation function to scale each element and get probability matrix P . For each relation type, the average value of the elements that correspond to the entity-pair can be treated as the final predicted score for the possible triplet relation?head : tail .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We introduce the method from two aspects: network structure and loss function. <ref type="figure">Figure 1</ref> shows the overall architecture of this method.</p><p>From the perspective of the network structure, our model has two major parts. The first part is an encoder that utilizes pre-trained language model like BERT. We obtain three embeddings for a given input text from the BERT model: the embedding E w for each token in the text , the embeddings E p obtained by passing E w through a self-attention Transformer, and the embedding E a of the entire text (that is, the CLS embedding provided by BERT). The second part is the relation computing layer. In this layer, we assume that the E p represents the tail entity encoded with some available predicate information, while E a combined with E w represents the head entity with context information. By performing a correlation calculation F on those embeddings, the tendency scores matrix S i of relation i in all entity pairs can be obtained.</p><p>From the perspective of loss function, we first use the Sigmoid activation function to compute probabilites P i of relation i by using S i . We use locations of entities to construct a mask matrix M and use it to preserve information in P i which represents existing entity-pairs in a sentence (See details in Section 3.3 and <ref type="figure">Figure 1</ref>). Based on the labels indicating whether each entity-pair is an instance of the i-th relation type or not, we use the average values in each area of P i to compute a Binary Cross Entropy (BCE) loss of this specific relation. Eventually, the final loss sums all values from all relations. This formulation allows the model to predict multiple relations in a single sentence or even multiple relations for a single entity-pair. Details and formulas are described in subsections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder</head><p>The current pre-training language models are basically based on the Transformer structure <ref type="bibr" target="#b25">[26]</ref> . They have a common feature: each layer of the Transformer can output a hidden vector corresponding to the input text T . T is an array of tokens with length l. Taking BERT as an example, some simple analyses <ref type="bibr" target="#b0">[1]</ref> show that the hidden vector of each layer can be used as word embeddings of T , with modest difference in precisions. Generally speaking, the deepest hidden vector representation of the Transformer network tends to work best for a downstream finetuning task thanks to the information integration performed by a deeper network. However, here we select the penultimate layer output vector as the initial embedding E w (E w ∈ R l×h , where h is the number of hidden dimensions), for the text representation with entity information.</p><p>To get E p , we use the last Transformer layer of BERT which is actually a multi-head self-attention with a fully connected FFN <ref type="bibr" target="#b25">[26]</ref> to deal with our initial embeddings:</p><formula xml:id="formula_0">E p = T ransf ormer(E w ),<label>(1)</label></formula><p>where E p is the last output vector of BERT, E p ∈ R l×h . Such an operation is applied so that the embedding of every token in E p will, in addition to E w , fuse some information from tokens in other positions. In this way, Although dataset annotations usually do not carry explicit predicate information, the Transformer structure of the BERT model allows E p to selectively blend contextual information that is helpful for the final task. We expect that after well fine-tuning training, words with higher attention association scores correspond to a predicate of a certain relation to some extent. E w and E p can be respectively used as basic entity representations and entity representations that incorporate predicate information. In order to better capture the overall context information, the BERT's CLS embedding E a (E a ∈ R h ) is also added to each token's embedding to improve the basic entity representation:</p><formula xml:id="formula_1">E b = E w + E a .<label>(2)</label></formula><p>Note E a is actually broadcasting to all tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation Computing Layer</head><p>We apply an asymmetric kernel inner product method to calculate the similarity between E b and E p :</p><formula xml:id="formula_2">S i = F i (E b , E p ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">F i (X, Y ) = XW hi · (Y W ti ) T .<label>(4)</label></formula><p>Here</p><formula xml:id="formula_4">S i ∈ R l×l ; W hi , W ti ∈ R h×h .</formula><p>Actually, W hi and W ti are respectively the transformation matrices of headentity and tail-entity embeddings in i-th relation. They are the parameters learned during the training process for each relation.</p><p>If there are N tokens in one input text, we find that S i is actually a square matrix with N rows and columns. Thus it can be treated as unnormalized probability scores for i-th relation between all the tokens. That is to say, S i mn , an element of position (m, n), represents the existence possibility of i-th relation between tokens at these two locations. Finally we use Sigmoid functions to normalize S i to range (0, 1):</p><formula xml:id="formula_5">P i = 1 1 + e −S i ,<label>(5)</label></formula><p>where P i is the normalized probability matrix of i-th relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Calculation</head><p>A problem of P i is that it describes relations between tokens, not entities. Therefore, we use entity-mask matrix to fix this problem. For each entity pair, the location information of the entities is known. Suppose that all entities from input text T constitute a set of entity pairs in the form:</p><formula xml:id="formula_6">S = {(x, y)}. Suppose (B x , E x )</formula><p>is the beginning and end of the position index of an entity x in the token array. Therefore, we construct a mask matrix M (M ∈ R l×l ) to satisfy</p><formula xml:id="formula_7">∀(x, y) ∈ S, M mn = 1, B x ≤ m ≤ E x ∧ B y ≤ n ≤ E y 0, otherwise<label>(6)</label></formula><p>where m, n is the subscript of the matrix element. Similarly, we can construct a label matrix Y i (Y i ∈ R l×l ) for the i-th relation:</p><formula xml:id="formula_8">∀(x, y) ∈ Y i , Y imn = 1, B x ≤ m ≤ E x ∧ B y ≤ n ≤ E y 0, otherwise<label>(7)</label></formula><p>where Y i is the labeled i-th relation set of entity pairs from the input text T . We use this mask matrix to reserve the predicted probabilities of every entity pair from  P i , and then use the average Binary Cross Entropy to calculate the target loss L i of relation i:</p><formula xml:id="formula_9">L i = BCE avg (P i * M , Y i )<label>(8)</label></formula><p>where * is Hadamard product and</p><formula xml:id="formula_10">BCE avg (X, Y ) = mn,∀Y mn =1 log(X mn ) + mn,∀Y mn =0 log(1 − X mn ) mn,∀M mn =1 Y mn<label>(9)</label></formula><p>Thus the final loss L r of relation predication is</p><formula xml:id="formula_11">L r = i L i<label>(10)</label></formula><p>where i is the index of each relation. While predicting, we use the average value of elements in P i , whose location accords with a certain entity-pair (x, y), as the probability of the possible triplet i?x : y consisting of i-th relation and entity-pair (x, y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section describes the experimental process and best results while testing our methods on multiple public datasets. We performed overall comparison experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>SemEval NYT WebNLG C-AGGCN <ref type="bibr" target="#b3">[4]</ref> 85.7 --GraphRel2p <ref type="bibr" target="#b1">[2]</ref> -61.9 42.9 BERT EM -MTB <ref type="bibr" target="#b24">[25]</ref> 89.   <ref type="table">Table 4</ref>: Precision, Recall and Micro-F1 scores of our model tested on different types of relations in multiple datasets. Note for some relation types the score is not available (denoted as "-") because there is no such type in the dataset (see <ref type="table" target="#tab_1">Table 2</ref>).</p><p>with the baseline methods and completed more fine-grained analysis and comparison in different types of complex relations. Codes and more details can be found in Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We use Nvidia Tesla V100 32GB for training. The BERT model we use is [BERT-Base, Uncased]. Hyper-parameters are shown in <ref type="table" target="#tab_0">Table 1</ref>. The optimizer is Adam <ref type="bibr" target="#b10">[11]</ref>. Based on our problem formulation as described in Section 3, our model actually fits a binary classifier to predict whether a triplet exists or not. Therefore, it actually gives a probability for each possible triplet. We still need a threshold to divide the positive and negative classes, and we set it as 0.5 for balance. More details are shown in Supplementary Materials. Our codes are developed on OpenNRE <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline and Evaluation Metrics</head><p>As described above, Pre-trained Language Model (PLM) is so powerful that it may lead to unfairness in the comparison between our method and some old methods. Therefore, we chose some recent work (C-AGGCN <ref type="bibr" target="#b3">[4]</ref>, GraphRel2p <ref type="bibr" target="#b1">[2]</ref>, BERT EM -MTB <ref type="bibr" target="#b24">[25]</ref>, HBT <ref type="bibr" target="#b26">[27]</ref>) published after the appearance of PLMs, especially BERT, as our baseline. Such a selection is useful for measuring whether we have better exploited the potential of PLM in relational extraction tasks, rather than only relied on its power. As usual, we used the micro-F1 score as evaluation criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Datasets</head><p>We performed our experiments on three commonly used public datasets (SemEval 2010 Task 8 <ref type="bibr" target="#b6">[7]</ref>, NYT <ref type="bibr" target="#b22">[23]</ref>, WebNLG <ref type="bibr" target="#b2">[3]</ref>) and compared the performance of our method to the baseline methods mentioned above. We followed splits and special process for the datasets conducted by the previous baseline models. More details are in Supplementary Materials. We found that the complexity of the samples in these data sets varied widely. Similar to the approach of Copy re <ref type="bibr" target="#b29">[30]</ref>, we measured the complexity of the relations in the three datasets from two dimensions, i.e., the number of samples with overlapping relations and the number of samples with multiple relations.</p><p>For overlapping relations, we followed the method proposed in Copy re <ref type="bibr" target="#b29">[30]</ref> to divide samples into three types: "Normal" as all relations in the sample is normal; "EPO" as there are at least two relations overlapped in the same entity-pair in the sample; "SEO" as there are at least two relations sharing a single entity in the sample. These three types can reflect the complexity of relations. For multiple relations, we also divide samples into three types: "Single" as only one relation appears in the sample, while "Double" as two and "Multiple" as no less than three. These three types can reflect the complexity of samples. <ref type="table" target="#tab_1">Table 2</ref> shows the complexity analysis of each dataset. <ref type="table" target="#tab_3">Table 3</ref> shows the performance of our method on all test data sets and the comparisons with the corresponding baseline methods. Given different test settings, we find that our model generally outperformed the baseline models, with a margin over the optimal baseline ranging from 1% to 8%. To explain the difference in performance margin, we design a detailed experiment to evaluate how our model performs in each relation type as shown in <ref type="table">Table 4</ref>. On the other hand, we also conducted the comparison regarding each relation type with 3 baseline methods (GraphRel 1p <ref type="bibr" target="#b1">[2]</ref>, GraphRel 2p <ref type="bibr" target="#b1">[2]</ref>, HBT <ref type="bibr" target="#b26">[27]</ref>) in <ref type="figure" target="#fig_0">Figure 2</ref> and <ref type="figure" target="#fig_1">Figure 3</ref>. We only compared F1 scores on NYT and WebNLG, since nearly no complex relations exist in SemEval. Detailed analyses on each dataset are listed as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Analysis</head><p>SemEval. SemEval only has around 20 samples with two relations ("Others" class excluded), with only 6 of them in test dataset. Thus in Double-relation type, our model's performance crashes down about 8% because of large variance.</p><p>NYT. On NYT our method has experienced large fluctuations. The reason is that NYT is the only dataset constructed by distant supervision <ref type="bibr" target="#b17">[18]</ref>, so the data quality is low. Distant supervision methods, which automatically label data from knowledge graphs, bring more errors in complex relations than simple relations. Therefore, although it seems that the amount of complex relations is sufficiently large, the performance of our model on complex relations still lags behind that on simple relations (around 10% lower in F1 score, around 17% lower in Recall). Nonetheless, comparison results still show that our metric scores while dealing with simple relations are higher than both baselines (around 7% higher than HBT and 25% higher than GraphRel). Even for complex relations, we are still significantly better (around 30%) than GraphRel, but a little bit lower (around 3%) than HBT.</p><p>WebNLG. It is easy to find our performances (Precision, Recall and Micro-F1 score) keep stable no matter how complicated the type is, except for EPO. It is because there are only around 100 samples in EPO thus the model's performances are of high variance. Interestingly, the Micro-F1 scores of complex relations are even higher than simple relations around 2%, which is further discussed below.</p><p>Comparison results show our model is far better than baselines (around 8% higher than HBT and 50% higher than GraphRel).</p><p>Given the results from all the datasets, our method shows consistent high performance on simple relation extraction tasks. Furthermore, it generally demonstrates stable performance when faced with more challenging settings including overlapped relation and multiple relation extraction.</p><p>Another interesting phenomenon is, on WebNLG, our model does better (around 1.5% higher on F1 score, 3% higher on Recall) while dealing with complicated relations than simple relations. Our guess is that since our model can predict multiple relations at the same time, it may combine semantic correlations between multiple relations to find more annotated relations by preventing some semantic drift. On the other hand, on NYT, where semantic correlations between multiple relations generated by distant supervision are very likely to be fake, our model tends to neglect those meaningless semantic correlations. Therefore, it filters out potential falsely labeled relations and generate lower Recall.</p><p>To support the above reasoning, believe this network will not conflict with many other methods, thus it can be combined with them (e.g., use other special PLMs like ERNIE <ref type="bibr" target="#b31">[32]</ref>, BERT EM -MTB <ref type="bibr" target="#b24">[25]</ref>) and performs better.</p><p>In addition, we believe that the current architecture has the potential to be improved for dealing with many other relation problems, including applications in long-tail relation extraction, open relation extraction, and joint extraction and so on.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Micro-F1 scores of Normal, Entity Pair Overlapped (EPO) and Single Entity Overlapped (SEO). Our methods are tested on NYT and WebNLG, with comparison of GraphRel and HBT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Micro-F1 scores of x-relations in one sample sentence. (Here x = 1, 2, 3, 4 or x ≥ 5.) Our methods are tested on NYT and WebNLG, with comparison of GraphRel and HBT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Hyper-parameters used for training on each dataset.</figDesc><table><row><cell>Hyper-parameters</cell><cell cols="2">SemEval</cell><cell>NYT</cell><cell>WebNLG</cell></row><row><cell>Batch size</cell><cell></cell><cell>64</cell><cell>20</cell><cell>20</cell></row><row><cell>Learning Rate</cell><cell cols="4">3 × 10 −5 5 × 10 −5 3 × 10 −5</cell></row><row><cell cols="2">Maximum Training Epochs</cell><cell>50</cell><cell>10</cell><cell>30</cell></row><row><cell cols="2">Maximum Sequence Length</cell><cell>512</cell><cell>100</cell><cell>512</cell></row><row><cell cols="4">Situations SemEval NYT WebNLG</cell></row><row><cell>Normal</cell><cell>10695</cell><cell>33566</cell><cell>12391</cell></row><row><cell>EPO</cell><cell>0</cell><cell>30775</cell><cell>121</cell></row><row><cell>SEO</cell><cell>0</cell><cell>13927</cell><cell>19059</cell></row><row><cell>Single</cell><cell>10673</cell><cell>33287</cell><cell>12237</cell></row><row><cell>Double</cell><cell>22</cell><cell>24174</cell><cell>9502</cell></row><row><cell>Multiple</cell><cell>0</cell><cell>12249</cell><cell>9772</cell></row><row><cell>All</cell><cell>10695</cell><cell>69710</cell><cell>31511</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Statistics of different types of sample sentences (No repetition) in multiple datasets. Note a sample sentence may belong to both EPO and SEO.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Micro-F1 scores of our method tested on multiple datasets, compared with other four baseline methods. The first two methods are GNN-based while the last two are PLM-based. Their performances come from their original papers, as quoted above.</figDesc><table><row><cell>Situations</cell><cell>P</cell><cell>SemEval R</cell><cell>F1</cell><cell>P</cell><cell>NYT R</cell><cell>F1</cell><cell>P</cell><cell>WebNLG R</cell><cell>F1</cell></row><row><cell>Normal</cell><cell cols="9">94.2 88.0 91.0 95.1 94.0 94.5 96.2 92.9 94.5</cell></row><row><cell>EPO</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">96.3 73.2 83.2 100.0 90.3 94.9</cell></row><row><cell>SEO</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">92.2 78.9 85.0 97.2 96.3 96.8</cell></row><row><cell>Single</cell><cell cols="9">94.2 88.0 91.0 95.1 94.0 94.6 96.1 92.7 94.4</cell></row><row><cell>Double</cell><cell cols="9">83.3 83.3 83.3 89.6 80.2 84.7 96.9 96.5 96.7</cell></row><row><cell>Multiple</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="6">95.8 75.3 84.3 97.3 96.3 96.8</cell></row><row><cell>All</cell><cell cols="9">94.2 88.0 91.0 94.2 85.7 89.8 97.0 95.7 96.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>illustrates some real examples from WebNLG and NYT dataset. On WebNLG, our examples demonstrate the beneficial effects from properly labeled complex relations on our model. In the simple sample,</figDesc><table><row><cell></cell><cell></cell><cell>NYT</cell><cell>WebNLG</cell></row><row><cell></cell><cell>Complex</cell><cell>Simple</cell><cell>Complex</cell><cell>Simple</cell></row><row><cell>Text</cell><cell>Ernst Haefliger...died on Saturday in Davos, Switzerland, where he maintained a second home.</cell><cell>Georgia Powers...said Louisville was finally ready to welcome Muhammad Ali home.</cell><cell>The 1 Decembrie 1918 University is located in Alba Iulia, Romania. The capital of the country is Bucharest...</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>our model made a mistake to consider Romania as an ethnic group in Alba Iulia, while in the complex sample from WebNLG, the model made the correct prediction that Romania is the country of Alba Iulia, by successfully identifying Bucharest as the capital of Romania. In comparison, for the simple sample from NYT, the model predicted "place of birth" correctly, while failed to predict it in the complex sample, since this relation is not real and also has no semantic correlations with the correct relation "place of death".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper introduces a downstream network architecture of pre-trained language model to process supervised relation extraction. The network calculates the relation score matrix of all entity pairs on all relation types by extracting the different head and tail entities' embeddings from the pre-trained language model. Experiments have shown that it has achieved significant improvements across multiple public datasets when compared to current best practices. Moreover, further experiments demonstrate the ability of this method to deal with complex relations. Also, we</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graphrel: Modeling text as relational graphs for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Hsuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1409" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Creating training corpora for nlg micro-planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Attention guided graph convolutional networks for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07510</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Opennre: An open and extensible toolkit for neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13078</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical relation extraction with coarse-to-fine grained attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2236" to="2245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relation extraction with multi-instance multi-label convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1471" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<title level="m">Spanbert: Improving pre-training by representing and predicting spans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolution neural network for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Data Mining and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="231" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation extraction: Perspective from convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03158</idno>
		<title level="m">Matching the blanks: Distributional similarity for relation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A novel hierarchical binary tagging framework for joint extraction of entities and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03227</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10185</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07129</idno>
		<title level="m">Ernie: Enhanced language representation with informative entities</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
