<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-View Dynamic Facial Action Unit Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Romero</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad de los Andes</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>León</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad de los Andes</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
							<email>pa.arbelaez@uniandes.edu.co</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad de los Andes</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-View Dynamic Facial Action Unit Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel convolutional neural network approach to address the fine-grained recognition problem of multi-view dynamic facial action unit detection. We leverage recent gains in large-scale object recognition by formulating the task of predicting the presence or absence of a specific action unit in a still image of a human face as holistic classification. We then explore the design space of our approach by considering both shared and independent representations for separate action units, and also different CNN architectures for combining color and motion information. We then move to the novel setup of the FERA 2017 Challenge, in which we propose a multi-view extension of our approach that operates by first predicting the viewpoint from which the video was taken, and then evaluating an ensemble of action unit detectors that were trained for that specific viewpoint. Our approach is holistic, efficient, and modular, since new action units can be easily included in the overall system. Our approach significantly outperforms the baseline of the FERA 2017 Challenge, with an absolute improvement of 14% on the F1-metric. Additionally, it compares favorably against the winner of the FERA 2017 challenge. Code source is available at https: // github. com/ BCV-Uniandes/ AUNets .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The field of human facial expression interpretation has benefited from seminal contributions by renowned psychologists such as P. Ekman, who characterized and studied the manifestation of prototypical emotions through changes in facial features <ref type="bibr" target="#b12">[12]</ref>. From the computer vision perspective, the problem of automated facial expression analysis is a cornerstone towards highlevel human computer interaction, and its study has a long tradition within the community. Initially, the problem was approached by focusing on its most basic version, and classifying static images or short sequences <ref type="figure">Figure 1</ref>: Results of our approach. Given a video of a human face that was taken from an unknown viewpoint, our system predicts the presence or absence of multiple action units in each frame. of faces into a handful of prototypical emotions (e.g., happiness, sadness, fear, etc.). Recent methods <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b18">18]</ref> have achieved significant progress in the study of this task, leading to the near saturation of standard benchmarks such as the CK+ dataset <ref type="bibr" target="#b36">[36]</ref>.</p><p>In order to systematize the study of facial expressions, Ekman and his collaborators designed the Facial Action Coding System (FACS) <ref type="bibr" target="#b13">[13]</ref>. FACS relies on identifying visible local appearance variations in the human face, called Action Units (AUs), that are produced by the individual activation of facial muscles. Thus, a raised eyebrow is coded as the activation of the outer brow raiser, and noted AU02. Since any facial expression can be represented as a combination of action units, they constitute a natural physiological basis for face analysis. The existence of such a basis is a rare boon for a computer vision domain, as it allows focusing on the essential atoms of the problem and, by virtue of their exponentially large possible combinations, opens the door for studying a wide range of applications beyond prototypical emotion classifica- <ref type="figure">Figure 2</ref>: Overview of AUNets. Our system takes as input a video of a human head and computes its optical flow field. It predicts the viewpoint from which the video was taken, and uses this information to select and evaluate an ensemble of holistic action unit detectors that were trained for that specific view. Final AUNets predictions are then temporally smoothed.</p><p>tion. Consequently, in the last years, the main focus of the community has shifted towards the detection of action units, and recent datasets such as BP4D <ref type="bibr" target="#b54">[54]</ref> and the FERA 2017 challenge <ref type="bibr" target="#b46">[46]</ref> include annotations by trained psychologists for multiple action units in individual video frames.</p><p>When compared to global emotion classification, action unit detection is a much challenging and finegrained recognition task, as shown by the local and delicate appearance variations in <ref type="figure">Fig. 1</ref>. The expression of action units is typically brief and unconscious, and their detection requires analyzing subtle appearance changes in the human face. Furthermore, action units do not appear in isolation, but as elemental units of facial expressions, and hence some AUs co-occur frequently while others are mutually exclusive. In response to these challenges, the literature has converged to a dominant approach for the study of action unit detection <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b34">34]</ref> that uses the localization of facial landmarks as starting point. Recent methods implement this paradigm in two different ways, either using facial landmarks as anchors and analyzing the appearance of patches centered in those keypoints as well as their combinations, or using landmarks to perform face alignment prior to AU detection. Focusing on fixed facial regions has the advantage of constraining appearance variations. However, facial landmark localization in the wild is still an open problem, its study requires also very expensive annotations, and its solution is as challenging as the AU detection itself. Furthermore, while such an approach is suitable for a near frontal face setup, as is the case in the BP4D dataset, it finds its limitations in a multi-view setting such as FERA 2017, in which large variations in head pose imply occlusion of multiple facial landmarks and significant appearance changes.</p><p>In this paper, we depart from the mainstream approach for AU detection and propose a system that directly analyzes the information on the whole human face in order to predict the presence of specific action units, bypassing thus landmark localization on the standard benchmarks for this task. For this purpose, we leverage recent insights on designing Convolutional Neural Networks (CNNs) for recognition applications. It is important to mention that <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b34">34]</ref> can avoid keypoints, however, they are required to use a different intermediate step in order to align faces.</p><p>First, we observe that large-capacity CNN architectures <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b20">20]</ref> that were originally designed for object recognition on datasets such as Imagenet <ref type="bibr" target="#b10">[10]</ref> analyze a whole raw image and are capable of making subtle category distinctions (e.g. between dog breeds). Moreover, they can be successfully specialized for other fine-grained recognition problems <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b5">5]</ref>. Therefore, we formulate the problem of predicting the presence or absence of an specific AU in a single face image as holistic binary classification. We explore the design space of our approach and, in particular, the trade-off between efficiency and accuracy when considering shared or independent convolutional representations for different action units.</p><p>Second, since action units appear dynamically within the spatio-temporal continuum of human facial expressions, we model explicitly temporal information in two different ways. At frame level, we compute an optical flow field and use it as an additional cue for improving AU detection.Then, we enforce short-term consistency across the sequence by a smoothing opera-tor that improves our predictions over a small temporal window.</p><p>Third, in order to address multi-view detection, the main focus of the recent FERA 2017 challenge, we take inspiration from the work of Dai et al . <ref type="bibr" target="#b9">[9]</ref>, who showed that training CNNs for a hard recognition task such as object instance segmentation on MS-COCO <ref type="bibr" target="#b30">[30]</ref> benefits from decomposing learning into a cascade of different sub-tasks of increasing complexity. Thus, our multi-view system starts by predicting the overall view of an input sequence before proceeding to the finer grained task of action unit detection. Furthermore, as the experiments will show, our system benefits from a gradual domain adaptation to the final multi-view setup. <ref type="figure">Fig. 2</ref> presents an overview of our approach, which we call AUNets.</p><p>We perform an extensive empirical validation of our approach. We develop our system on the BP4D dataset, the largest and most varied available benchmark for frontal action unit detection, where we report an absolute improvement of 7% over the previous state-of-the-art by Li et al . <ref type="bibr" target="#b29">[29]</ref>. We then turn to the FERA2017 challenge to evaluate our multi-view system, and report an absolute improvement of 14% on the F1-metric over the challenge baseline of Valstar et al . <ref type="bibr" target="#b46">[46]</ref>, while comparing favorably against state-ofthe-art method by Tang et al . <ref type="bibr" target="#b44">[44]</ref>. In order to ensure reproducibility of our results and to promote future research on action unit detection, all the resources of this project -source code, benchmarks and results-will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Before deep learning techniques became mainstream within the field of computer vision <ref type="bibr" target="#b28">[28]</ref>, most methods relied on the classical two-stage approach of designing fixed handcrafted features such as SIFT <ref type="bibr" target="#b35">[35]</ref> or LBP <ref type="bibr" target="#b39">[39]</ref>, and then training unrelated classifiers for recognition <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b26">26]</ref>. However, similarly to AUNets, the best performing techniques currently available <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b29">29]</ref> rely on the power of deep convolutional neural networks for joint representation and classification.</p><p>Most recent methods <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b26">26]</ref> follow the paradigm of first detecting facial landmarks using external approaches such as Active Appearance Models <ref type="bibr" target="#b8">[8]</ref>, either to treat these keypoints as anchors for extracting rectangular regions for further analysis, to perform face alignment, or both. The recent method of Li et al . <ref type="bibr" target="#b29">[29]</ref> does not require robust facial keypoint alignment as it is trained taking this issue into account; however, facial alignment is recommended and the method is developed and tested only in a frontal face setup. In contrast, AUNets operate on the whole face and do not require any particular alignment in existing AU detection multi-view benchmarks. Pioneering methods for this task used the whole face image as input <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33]</ref>. However, the trend reversed towards analyzing patches in more local approaches such as <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b23">23]</ref>. State-of-the-art techniques <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b29">29]</ref> join AUNets in returning to a holistic face analysis, in particular Li et al . <ref type="bibr" target="#b29">[29]</ref> shares ideas with Zhao et al . <ref type="bibr" target="#b57">[57]</ref> in forcing a CNN-based approach to specifically focusing on specific regions of the face by using a map saliency. Our method is far from these approaches since we train specific networks for each AU which avoids the need of building attention maps.</p><p>Given that groups of action units can co-occur or be mutually exclusive in the human face, several methods <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b29">29]</ref> have approached the task as multilabel learning. However, as AUs databases are becoming larger and better annotated <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b37">37]</ref>, these methods need to be completely retrained to integrate new action units. In contrast, AUNets are modular by design, and can naturally evolve towards more general datasets and new action units. Furthermore, by analyzing the whole face, AUNets learn naturally the relevant local face regions for predicting each action unit.</p><p>While most methods operate on single RGB images both at training and testing <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b31">31]</ref>, some techniques focus on exploiting the temporal information for improved AU detection <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b32">32]</ref>. In particular, Jaiswal et al . <ref type="bibr" target="#b23">[23]</ref> and Chu et al . <ref type="bibr" target="#b6">[6]</ref> use CNN's and Bidirectional Long Short-Term Memory to model time dependencies. Similarly, Liu et al . <ref type="bibr" target="#b34">[34]</ref> tackle the temporal information by taking advantage of the Optical Flow <ref type="bibr" target="#b21">[21]</ref>; for this purpose, starting from the OF they extract hand-crafted features as histogram oriented <ref type="bibr" target="#b4">[4]</ref> in order to train a classifier. However, our method takes advantage of the full resolution of the OF by feeding it into a CNN. Moreover, on behalf of the Optical Flow, we perform simple statistical operations to smooth the temporal flow as a post-processing step.</p><p>The FERA17 Challenge <ref type="bibr" target="#b46">[46]</ref> introduces a new experimental framework for the facial expression recognition problem. It does not only consider frontal images, but also multi-view rendered facial images <ref type="bibr" target="#b46">[46]</ref>. Our insight comes from realizing that each view should be detected first and then treated independently as well as each AU. Instead of automatically frontalizing each view by using facial alignment <ref type="bibr" target="#b46">[46]</ref>, AUNets, similar to other FERA17 participants <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b19">19]</ref>, learn independent AUs regardless the multi-label setup, and avoiding intermediate steps such as keypoint detection. However, AUNets core lies on the simplicity of using in-dependent AUs for each view; thus, by taking advantage of the temporal information we compare favorably against state-of-the-art methods <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b19">19]</ref> in this challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Facial Expression Representation</head><p>We formulate individual action unit detection as holistic binary classification in order to build on recent high-capacity CNN architectures <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b20">20]</ref> that were originally designed for object classification in large-scale datasets such as Imagenet <ref type="bibr" target="#b10">[10]</ref>. Recent works have shown that these networks can learn useful representations for different object-level applications such as contour detection <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b49">49]</ref>, semantic segmentation <ref type="bibr" target="#b38">[38]</ref>, instance segmentation <ref type="bibr" target="#b9">[9]</ref>, and action recognition <ref type="bibr" target="#b2">[3]</ref>.</p><p>However, relative to ImageNet, the domain adaptation we require for action unit detection is much larger than the one for the above mentioned problems, which are typically studied in VOC PASCAL <ref type="bibr" target="#b15">[15]</ref>. Therefore, we start by learning a high-level representation on a related but simpler facial expression analysis task. We achieve this goal by converting the last fully connected layers of a popular CNN architecture such (e.g., VGG <ref type="bibr" target="#b42">[42]</ref>) into 22 different outputs instead of the 1000 of ImageNet, and then re-train it for the task of emotion classification. In order to generalize across ethnicity, gender and age, we train the encoder on a nontraditional dataset <ref type="bibr" target="#b11">[11]</ref> labeled beyond the prototypical 8 basic emotions, instead it uses 22 different emotions (e.g., happily surprised, sadly fearful, angrily disgusted, etc) with different characteristics, see <ref type="figure">Figure 3</ref> for details about this dataset. After "EmoNet"(ENet) learning is completed, we remove the output layer of the network and use the learned weights as convolutional encoder for facial expressions in all subsequent experiments. Our results show that this initial domain adaptation is beneficial for good performance, indicating that the convolutional encoder is indeed learning features that are specific to faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Single-Image Action Unit Detection</head><p>Starting from the learned convolutional encoder, training a specialized detector for a given action unit in single RGB images is straightforward. We extend the encoder with a binary output softmax layer predicting the presence or absence of the AU in the image, we initialize it with random weights, and then resume training in a database that provides AU annotations, such as BP4D. However, when considering multiple action units, two architectural options emerge, depending <ref type="figure">Figure 3</ref>: Dataset for domain adaptation. This dataset contains images from 230 different subjects (Caucasian, Asian, African American and Hispanic) and 21 different compound emotions (22 including neutral) for a total of 5060 images (See <ref type="bibr" target="#b11">[11]</ref> for more details about the acquisition). Once we extract the face, we perform data augmentation by randomly cropping from the center and tuning the saturation, brightness, contrast and hue. on whether or not we share features at the lower layers of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">HydraNet</head><p>The first option is to preserve a shared representation and to train individual modules for different AUs. This strategy is implemented by freezing the lower layers of the convolutional encoder and learning separate weights at the upper layers sequentially for each action unit. We call this network "Hydra-Net" in reference to the fabulous Lernaean Hydra from the Greek mythology, a dragon with a single body and multiple heads that would multiply when severed. Hydra-Net is efficient at test time and modular by design, as it can be easily extended to new action units by "growing" additional heads. Furthermore, as the experiments will show, this baseline system already outperforms the state-of-the-art on BP4D for single-image AU detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">AUNets</head><p>The elegance of a shared representation in Hydra-Net comes at the price of a reduction in absolute perfor-mance, because only a fraction of the original encoder weights are being specialized for each action unit. In order to quantify this trade-off, we train a second variant of our system in which we relearn all the weights of the encoder for each action unit. This strategy results in a battery of independent AU detectors, which we call "AU-Nets". When compared to Hydra-Net, the larger-capacity ensemble of AU-Nets requires more data at training time and is less efficient at test time. However, both approaches are modular in the presence of new action units and, as the experiments will show, the increased flexibility of AU-Nets allows them to focus better on specific regions of the face associated to different action units, leveraging thus local information for improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic Action Unit Detection</head><p>Although trained human observers can annotate action units on single images, AUs are continuous and precisely localized events that occur within the spatiotemporal flow of facial expressions, and therefore an explicit analysis of motion information should facilitate AU detection when video data is available. We model motion by computing a dense optical flow field and embedding it into a three dimensional space in which the first two dimensions are normalized x and y pixel motion and the third dimension is the optical flow magnitude. This embedding is common in video analysis applications <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b41">41]</ref> and has the advantage of providing a motion signal that has the same scale and dimensions as the original RGB frame. We considered three architectures for combining color and motion information, as illustrated in <ref type="figure" target="#fig_0">Fig. 4</ref>, b, c, and d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Additional input Channels</head><p>A first option is to consider the optical flow embedding as three additional channels of the input RGB image, ending up with and input of 224x224x6. This architecture is economical, as it adds only additional weights for the first layer of the network, which are initialized as copies of the RGB weights. This first layer is then learning to combine all channels for each spatial location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Concatenation</head><p>A larger architecture is obtained by concatenating color and motion images in one of the two spatial dimensions. In this case, the network analyzes the two signals with the same first-layer filters, but its first fully convolutional architecture is expanded accordingly in order to combine their information (replicating the pre-trained weights); resulting also in larger capacity models. The concatenation is performed in horizontal position, resulting therefore in an input of 224x448x3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Two streams</head><p>A third option is to consider two different streams to represent color and motion information, and to merge them in higher layers. In this case, we extract the weights of the RGB encoder and copy them to initialize the motion encoder. We are thus adapting the RGB representation to the motion signal, while simultaneously learning its optimal combination with the color representation in the deeper layers. This strategy is also known as 'π Network' and it is carried out whether after the last convolutional layer (conv5 3 of VGG), after the first FC layer (fc6 of VGG), or after the second fully connected layer (fc7 of vgg), also known as π/conv, π/fc6 and π/fc7 respectively.</p><p>The experimental section will show that explicit modeling of motion between successive frames is beneficial for AU detection. Nevertheless, we note that our dynamic AU detectors above still operate independently in each frame, and hence cannot capture the temporal smoothness of action unit activation. In order to enforce such consistency, we perform a simple post-processing by applying a temporal sliding median filter over predictions of each action unit across the video. We found this simple smoothing to be empirically beneficial for all the variants of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multi-View System</head><p>Our final system is evaluated on the FERA 2017 challenge, which focuses on multi-view AU detection. Starting from a video of a face that was taken from an unknown viewpoint among nine options, the task consists in predicting labels for twelve common action units on all its frames. We extend our system in two ways to address this multi-view version of the problem. First, we train a view classifier, which takes the video as input and predicts its viewpoint. Once the viewpoint has been estimated, our cascaded approach proceeds to evaluate an ensemble of temporally consistent dynamic AU-Nets that were trained for that specific view. In order to perform domain adaptation, we first retrain our system developed in BP4D in the frontal view of FERA 2017, and then use those weights to initialize learning for the other views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Validation</head><p>In this section, we provide the empirical evidence to support our approach. We first conduct extensive experiments in the BP4D dataset analyzing different aspects of our method for frontal faces. We then turn to the FERA 2017 dataset in order to extend our system to a challenging multi-view setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on BP4D-Spontaneous</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Experimental Setup</head><p>The BP4D-Spontaneous database <ref type="bibr" target="#b54">[54]</ref> is currently one of the largest and most challenging benchmarks available for the task of action unit detection <ref type="bibr" target="#b54">[54]</ref>. BP4D contains 2D and 3D videos of spontaneous facial expressions in young people. All of them were recorded in non-acted scenarios, as the emotions were always elicited by experts. This dataset contains around 150.000 individual frames from 41 subjects annotated by trained psychologists. The BP4D dataset has a significant class imbalance for most AUs; for instance, there are 6 times more negatives samples than positive samples for AU23, while AU12 is almost balanced. We follow the common practice in the literature <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b29">29]</ref>, and consider the 12 most frequent AUs <ref type="bibr" target="#b54">[54]</ref> for the BP4D dataset. In order to compare our approach directly with the state-of-theart, we report results using the standard performance metric from the literature, the image-based F1-score (f1-frame) <ref type="bibr" target="#b45">[45]</ref>, also known as F-measure and defined by the harmonic mean of precision and recall, which considers a threshold of 0.5 in the operating regime of the detector. Since state-of-the-art weights are not publicly available, for all the BP4D experiments, we strictly follow the experimental framework of Zhao et al . <ref type="bibr" target="#b57">[57]</ref> and Chu et al . <ref type="bibr" target="#b6">[6]</ref> and perform three-fold cross validation (so called three fold testing) using their same splits, in order to ensure that our results are directly comparable.</p><p>It is important to mention that we split the training set into training and validation. We randomly extract all the images from one single subject of the full training set to validation, for the three folds. All hyperparameters and training decisions were tuned based on the performance of the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Implementation</head><p>All models are trained using Adam <ref type="bibr" target="#b27">[27]</ref> optimizer with β1=0.5, β2=0.999. We train with an initial learning rate of 0.0001 linearly decaying to 0 over the next 12 epochs, unless early stop due to validation convergence i.e., stop only if F1-val reaches plateau region after 3 consecutive epochs. All experiments were performed on a single GPU TITAN X. Training details regarding time consumption are discussed in §5.</p><p>To overcome the BP4D dataset's AU skew, we augment the positives training samples using the well known strategy of image jittering <ref type="bibr" target="#b28">[28]</ref>, that is shifting the bounding box to the right, left, up, down and any possible combination of those until we reach an approximate balance with the negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Domain Adaptation</head><p>We first quantify the importance of learning a highlevel representation for faces prior to action unit detection. We select three popular CNN architectures that were originally designed and trained on ImageNet, and that have been applied to multiple vision problems: AlexNet <ref type="bibr" target="#b28">[28]</ref>, VGG-16 <ref type="bibr" target="#b42">[42]</ref>, and GoogLeNet <ref type="bibr" target="#b43">[43]</ref>. In order to expose the network to a large variety of faces, we first train an Facial Expression Classification network using a non-traditional dataset created by Du et al . <ref type="bibr" target="#b11">[11]</ref> for facial expressions recognition. We chose this dataset because of its subject variability and not being grounded to the basic emotions (happiness, sadness, etc). Thus, we modify the output layer for 22-way emotion classification and train following §4.1.2. At the end, we use the best model trained on EmoNet and it is called ENet. To this point, we have three different models (AlexNet, GoogLeNet, and VGG) trained on facial expression classification (ENet) and three models originally trained on ImageNet (INet). We then use these six convolutional encoders as different initialization to the Hydra-Net architecture in BP4D and train them until validation convergence.</p><p>Results are presented in the left panel of <ref type="table">Table 1</ref>. We first observe that all three architectures produce reasonable AU detection results when initialized with Im-ageNet weights, and that deeper architectures produce improved results, in accordance to their original performance in ImageNet. This behavior contrasts with random initialization, which causes the training process to diverge for the three CNNs (results not included). Furthermore, we observe a significant improvement in AU detection for all base CNN architectures when initializing with our face expression convolutional encoder instead of the original ImageNet weights. Among the three CNNs considered, VGG-16 generalizes better to our application domain, obtaining a +6% absolute improvement in performance over ImageNet weights. These results highlight the relevance of domain adaptation for facial expression analysis. We use the convolutional encoder based on the VGG-16 architecture for all subsequent experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Shared vs. Independent Representation</head><p>We next explore the trade-off between efficiency and accuracy when considering shared or independent representations for different action units. For this purpose, we start from our shared representation Hydra-Net and train with the above mentioned parameters a new fully independent detector for each AU. Learning is carried out by unfreezing all layers in the convolutional encoder. The right column of <ref type="table">Table 1</ref> presents the results for independent detectors, which we call in the sequel AUNets. In comparison to HydraNet, under an independent representation 11 out of the 12 action units show improvements. Moreover, we observe an overall better performance under this strategy as, on average, the F1 measure increases by 6 over the whole set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Optical Flow</head><p>To incorporate short term motion patterns into our experimental setup, we estimate the optical flow (OF) with the variational model proposed by Brox et al . <ref type="bibr" target="#b1">[2]</ref>. Given the large scale of the BP4D and FERA17 datasets, we resort to a GPU implementation of this algorithm <ref type="bibr" target="#b22">[22]</ref> as it provides a fair trade-off between accuracy and computation time. In practice, we calculate the OF for the whole training set of FERA17 in under 60 hours with a single Titan X GPU.</p><p>To adapt the resulting dense vector map into the AU-Nets, it is transformed into an RGB image as proposed by Gkioxari et al . <ref type="bibr" target="#b16">[16]</ref> and Peng et al . <ref type="bibr" target="#b41">[41]</ref>, by independently normalizing the l2 norm of the x and y dimension over the OF field, extracting the face from the RGB bounding box, and then re-scaling to 224 to conform the images R and G channels, whereas the B channel is calculated as the norm of the original vector.</p><p>Initially, we consider motion information in isolation and train our AUNets ensemble under the same experimental set up as the original RGB frames. Column 'Alone' in <ref type="table" target="#tab_1">Table 2</ref> summarizes the results for this experiment. Even without any color information, the motion signal alone achieves a performance of 50.4%, which is competitive with the state-of-the-art in BP4D.</p><p>We then explore the strategies outlined in Section 3.3 to fuse information from the OF and RGB domains.   <ref type="figure" target="#fig_0">(Figure 4a</ref>). Channel corresponds to the channel embedding <ref type="figure" target="#fig_0">(Figure 4b)</ref>. Horizontal means horizontal concatenation in the input <ref type="figure" target="#fig_0">(Figure 4c</ref>). π/conv, π/fc6 and π/fc7 are the different two CNN streams that fuses before the FC6, FC7, FC8 layer of vgg, respectively <ref type="figure" target="#fig_0">(Figure 4d</ref>). See more details at §3.3.3.</p><p>Two Stream strategy. We observe that the motion signal is complementary to the color information, as the inclusion of OF always brings an improvement over the RGB results. The Two Stream and the Concatenation strategies are both superior to the Additional Channels, and provide comparable performance. We conclude that, overall, the best approach is to concatenate the OF and RGB information at the very input stage; it has the very same performance as a π network joint at the FC7 layer, yet, the number of parameters is 12% smaller, this means that further domain adaptation will be faster, and overfit will be less likely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.6">Temporal Smoothing</head><p>Since action units appear in smooth, short-term duration intervals, we add a post-processing step which aims at removing instantaneous AU activation and possible misclassification inside a continuous sequence. For this goal we use a median operator in a sliding window fashion over the AU detection the sequences. This hyper-parameter was tuned based on the validation set.</p><p>Our approach uses small window sizes as we want to avoid the suppression of properly detected but short temporal series of AU activation. Therefore, we test with window sizes: {3,5,7,9,10,11}, these remain fixed for all AUs over the entire train set. Empirically we conclude that the best window sizes are {3,5,7} with 7 being the optimal, longer temporal windows do not improve the base performance. <ref type="table" target="#tab_4">Table 4</ref> shows the result (+median) of performing this temporal smoothing to the best OF arrangement (Horizontal concatenation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.7">Comparison against the state-of-the-art</head><p>We now compare our results against the state-of-theart approaches <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b56">56]</ref>. <ref type="table">Table 5</ref> summarizes the average of three-fold cross validation for each action unit. We observe that our method consistently outperforms all methods for 10 out of 12 action units, with an average improvement over the F1 measure of 7%, over EAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on the FERA 2017 Challenge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Experimental Setup</head><p>The new version of the FERA17 challenge <ref type="bibr" target="#b46">[46]</ref> introduces a novel way to approach the problem of facial expression analysis. This dataset contains a 3D video database rendered mainly from BP4D <ref type="bibr" target="#b54">[54]</ref>. Moreover, for the test set it includes subjects from a different dataset <ref type="bibr" target="#b55">[55]</ref>, which do not overlap with BP4D subjects. The FERA17 dataset includes 9 different camera angles for non-symmetric facial views. It contains videos from 41 subjects and around 1.500.000 frames for the whole training split. An additional set of videos from 20 different subjects with about 750.000 frames integrate the validation split. A final set of 1080 videos are included in the withheld test split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Viewpoint Classification</head><p>Our AU-Net ensemble is view specific, and thus relies on a proper view selection for optimal results. We approach this problem by performing a view recognition sub-task, prior to the AU-Net ensemble. For this sub-task, we build a view classifier on top of another deep convolutional encoder. Since we require</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Params HydraNets</head><p>AUNets <ref type="table" target="#tab_1">AUNets+OF  Channel  Horizontal  pi/conv  pi/fc6  pi/fc7  Total  134m  134m  134m  237m  251m  268m  268m  Learnable  119m  134m  134m  237m  237m  151m  134m  12 models  1443m  1608m  1608m  2844m  3012m</ref> 3216m 3216m   <ref type="table">Table 5</ref>: 3-fold cross validation for each Action Unit over BP4D dataset using F1-metric.</p><p>Comparisons against the state-of-the-art methods: JPML <ref type="bibr" target="#b56">[56]</ref>, DRML <ref type="bibr" target="#b57">[57]</ref>, MSTC <ref type="bibr" target="#b6">[6]</ref>, and EAC <ref type="bibr" target="#b29">[29]</ref>.</p><p>a simple architecture to train, we start from the Caffe-GoogLeNet <ref type="bibr" target="#b24">[24]</ref> reference model, and proceed to learn a suitable representation for the view classification problem using the full training set. To avoid overfitting, we initially freeze the first 6 Inception modules and optimize over the final 3 modules (4f,5b,5c), we also drop the two deep supervision branches as they remain connected to frozen segments of the encoder. We use the default ImageNet weights for the viewpoint network, but learn from scratch the weights for the final fully connected layer. After this modifications the encoder produces a 9-dimensional output, its soft-max normalized output approximates the probability for each view. The viewpoint-network is trained for two epochs, with an initial learning rate of 5 −5 , gamma parameter of 0.1, and a weight decay of 5 −5 , using Stochastic Gradient Descend, we reduce the learning rate after the first epoch.</p><p>As final step we get a single prediction for any video as max v (p), where p is the per frame prediction and v is the full set of frames for a video. Evaluation results in <ref type="table" target="#tab_6">Table 6</ref> suggest that the viewpoint classifier is almost perfect in the FERA 2017 setup. This result indicates that AU detection will not be affected in any significant way by prior view classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Multi-View System</head><p>For our first approach to the FERA17 Challenge, we want to assess the generalization capability of our system, so we proceed to evaluate our system trained in BP4D over the entire validation partition of FERA17 without any retraining. The evaluation procedure results in an performance of 46.5%, which shows that our system trained on BP4D with no previous training on FERA17 already surpasses the baseline approach for validation partition in FERA17 Challenge <ref type="bibr" target="#b46">[46]</ref> that is 41.6%. We observe that, when the view is close to the frontal one, our model improves performance, which is 51.9% over the frontal view, and 41.4% over an upperleft view.</p><p>We use the BP4D models as starting-point to train on the frontal view of FERA17 dataset. We follow the same strategy for each view, in order to learn the entire 9 views for 10 AUs, resulting in a total of 90 models. For each view, we learn our models with the same setup as in BP4D, as outlined in Section 3.4. We train each view and each AU until validation convergence with learning rate of 10 −4 and weight decay of 5 −3 , the whole system is trained for a total of 50h in a single Titan X GPU. AUNets+OF trained on FERA17 and evaluated in the validation set obtain 64.4% on average, which widely outperforms the 40.4% of the baseline <ref type="bibr" target="#b46">[46]</ref>, and the FERA17-winner <ref type="bibr" target="#b44">[44]</ref>, which reports 58.0%.   <ref type="table">Table 7</ref>: Results over FERA17 Validation and Test, using F1-score and Accuracy (ACC). Comparison with the baseline approach <ref type="bibr" target="#b46">[46]</ref> and the challenge winner <ref type="bibr" target="#b44">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Evaluation on Test Set</head><p>It is important to mention that the test set of FERA17 Challenge <ref type="bibr" target="#b46">[46]</ref> includes subjects from BP4D+ dataset <ref type="bibr" target="#b55">[55]</ref>, which are different subjects that the BP4D dataset <ref type="bibr" target="#b54">[54]</ref>. We include the results obtained during the FERA17 Challenge, where the test server was made available from February 8 to March 1 2017 with a limit of 5 submissions per group. The challenge rules required participants to upload their source code to the FERA17 server, and the organizers ran the models on an undisclosed test set. Following the challenge setup, we report F1-score and ACC in <ref type="table">Table 7</ref>, 8 and 9, for Val and Test sets. Our final submitted system is represented in <ref type="figure">Fig. 2</ref>. As can be observed, our model sets a new benchmark in the problem of multi-view action unit detection by improving with an 14% in F1-score over the baseline approach in the Test set, and it is onpar with the challenge winner <ref type="bibr" target="#b44">[44]</ref>, under that metric. Furthermore, on the ACC metric, we outperform both the baseline and the challenge winner with an absolute improvement of 15% and 4% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Efficiency</head><p>We measure the average execution time for the final multi-view system as follows: For a video with 500 frames, it takes on average 12 seconds to transform it into frames. The Optical Flow takes about 2 minutes producing the whole set of OF frames. Once the RGB and OF frames are complete, the view detector takes around 2 seconds predicting the corresponding view of the video from a frame sub-sampling. For the final step, it takes roughly 8 more minutes to forward every frame of the video through the 10 AUs detectors. In summary, it takes less than 1 second to process each frame once the OF is computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Results</head><p>Similar to Zhao et al . <ref type="bibr" target="#b57">[57]</ref> and Chu et al . <ref type="bibr" target="#b6">[6]</ref>, we present qualitative results over BP4D dataset using the visualization techniques of Zeiler and Fergus <ref type="bibr" target="#b52">[52]</ref>, and Yosinski <ref type="bibr" target="#b51">[51]</ref>.</p><p>One of Zeiler's insights is to occlude parts of the input image in order to look for the classification probability this region is supplying to the full image. <ref type="figure">Fig. 5</ref> show this approach over several images for different AUs. We can observe that our approach emphasizes very specific regions of the face (third row) using thus   <ref type="table">Table 9</ref>: Results over FERA17 Test set, using F1-score and Accuracy . local information for each action unit despite analyzing the face hollistically and not using facial alignment.</p><p>Additionally, Yosinski's method creates synthetic images as input in order to maximize the output of one specific neuron; for instance, we maximize the output of the binary one. Similar to Zeiler's, <ref type="figure">Fig. 6</ref> shows that AU1 and AU2 models are focused on the eyes, thus AU12 and AU15 representations are looking for patterns over the mouth region, confirming thus the specificity of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations</head><p>Despite our approach radically pushes forward the state-of-the-art in this problem, it has two main shortcomings regarding the huge number of parameters (12 models for each fold in BP4D, and 90 different models for FERA17) and the time required to train all of them. <ref type="table" target="#tab_3">Table 3</ref> summarizes the number of parameters. On the other hand, in order to train 12 models in 3 fold cross validation over a 12GB GPU Titan X it takes around one week per variant architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have presented HydraNets and AUNets for recognizing 12 different facial action units. Our method takes advantage of the power of CNNs for large-scale classification problems, and is capable of detecting multiple action units simultaneously. Our method uses the temporal information OF to enhance performance. There is a trade-off between efficiency and performance with HydraNets and AUNets, yet both approaches obtain competitive results when compared against stateof-the-art methods, and our final multi-view system compares favorably in performance against state-ofthe-arts approaches in a challenging benchmark. At the core of our approach lies a flexible and modular architecture that can easily incorporate new action units. In order to promote further research in action unit detection, our source code, models, and results can be found at https://github.com/BCV-Uniandes/ AUNets 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inner Brow Raiser Outer Brow Raiser Upper Lip Raiser</head><p>Lip Corner Puller</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lip Corner Depressor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AU01</head><p>AU02 AU10 AU12 AU15 AU01 AU02 AU10 AU12 AU15 AU01 AU02 AU10 AU12 AU15 <ref type="figure">Figure 5</ref>: Zeiler's method <ref type="bibr" target="#b52">[52]</ref> for network visualization. The top row presents 5 different Action Units <ref type="bibr" target="#b7">[7]</ref>. The heat maps highlight the most important regions in the human face for each specific Action Unit (blue: less important, red: more important).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AU01</head><p>AU02 AU10 AU12 AU15 <ref type="figure">Figure 6</ref>: Yosinski's method <ref type="bibr" target="#b51">[51]</ref> for network visualization. This approach shows synthetic hallucinations that maximize the output of the network for AU 1,2,10,12,15 i.e., generate the best image that entirely maximize the output neuron.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>We consider three architectures for combining color and motion information: (a) RGB/OF alone architecture, (b) additional input channels, (c) an extended image (horizontal concatenation in the input), and (d) two separate CNN streams (independent networks that fuse in the fully connected layer). Layers whose weights are altered by the input size are displayed with a red star. Yellow circle depicts the binary output. See text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>INet ENet INet ENet ENet 1 19.8 40.3 27.0 34.1 24.6 39.9 48.4 2 16.4 23.3 17.2 28.0 23.6 35.0 42.3 4 31.4 43.3 44.9 46.0 40.9 48.9 55.6 6 49.4 65.3 71.6 76.3 72.8 75.2</figDesc><table><row><cell></cell><cell>HydraNets</cell><cell></cell><cell>AUNets</cell></row><row><cell>AU</cell><cell>AlexNet GoogLeNet</cell><cell>VGG</cell><cell>VGG</cell></row><row><cell></cell><cell cols="3">INet ENet 77.8</cell></row><row><cell cols="3">7 51.3 71.3 64.8 70.5 72.0 73.6</cell><cell>78.9</cell></row><row><cell cols="3">10 47.2 83.7 70.4 75.0 81.0 83.8</cell><cell>81.6</cell></row><row><cell cols="3">12 53.7 79.5 75.1 84.1 81.7 86.2</cell><cell>88.4</cell></row><row><cell cols="3">14 23.4 49.1 51.1 58.4 60.0 64.8</cell><cell>65.7</cell></row><row><cell cols="3">15 22.7 40.0 31.3 32.3 19.1 35.8</cell><cell>50.6</cell></row><row><cell cols="3">17 21.4 27.3 51.2 54.9 54.3 58.0</cell><cell>60.4</cell></row><row><cell cols="3">23 20.4 34.2 28.1 29.6 28.6 30.6</cell><cell>42.9</cell></row><row><cell cols="3">24 21.0 43.1 37.5 39.7 35.3 38.8</cell><cell>47.4</cell></row><row><cell cols="3">Av. 31.5 49.3 47.5 52.2 49.5 55.9</cell><cell>61.6</cell></row><row><cell cols="4">Table 1: Control experiments for convolutional</cell></row><row><cell cols="4">encoder. Results are reported over BP4D by using</cell></row><row><cell cols="4">3-fold cross validation and the F1-metric. It compares</cell></row><row><cell cols="4">several base networks as starting point for training Hy-</cell></row><row><cell cols="4">draNets and AUNets, using either Imagenet (INet) or</cell></row><row><cell cols="3">EmoNet (ENet) pre-trained weights.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 -</head><label>2</label><figDesc>right shows the results for the five proposed architectures, including three options for the</figDesc><table><row><cell>AU</cell><cell>AUNets ENet</cell><cell>Alone</cell><cell>Horizontal</cell><cell>AUNets+OF Channels</cell><cell>π/conv</cell><cell>π/fc6</cell><cell>π/fc7</cell></row><row><cell>1</cell><cell>48.4</cell><cell>35.0</cell><cell>52.8</cell><cell>52.0</cell><cell>46.8</cell><cell>48.0</cell><cell>53.4</cell></row><row><cell>2</cell><cell>42.3</cell><cell>26.5</cell><cell>44.0</cell><cell>44.5</cell><cell>42.1</cell><cell>41.2</cell><cell>49.8</cell></row><row><cell>4</cell><cell>55.6</cell><cell>37.6</cell><cell>55.3</cell><cell>54.6</cell><cell>55.9</cell><cell>56.8</cell><cell>56.0</cell></row><row><cell>6</cell><cell>77.8</cell><cell>67.0</cell><cell>78.9</cell><cell>77.4</cell><cell>77.6</cell><cell>78.2</cell><cell>77.4</cell></row><row><cell>7</cell><cell>78.9</cell><cell>69.3</cell><cell>77.5</cell><cell>78.5</cell><cell>78.0</cell><cell>79.3</cell><cell>78.4</cell></row><row><cell>10</cell><cell>81.6</cell><cell>74.0</cell><cell>82.8</cell><cell>83.2</cell><cell>83.3</cell><cell>83.2</cell><cell>83.2</cell></row><row><cell>12</cell><cell>88.4</cell><cell>77.6</cell><cell>88.0</cell><cell>87.8</cell><cell>88.7</cell><cell>88.8</cell><cell>88.0</cell></row><row><cell>14</cell><cell>65.7</cell><cell>61.0</cell><cell>66.2</cell><cell>66.4</cell><cell>65.9</cell><cell>65.8</cell><cell>67.3</cell></row><row><cell>15</cell><cell>50.6</cell><cell>29.2</cell><cell>47.7</cell><cell>47.9</cell><cell>50.1</cell><cell>50.8</cell><cell>44.2</cell></row><row><cell>17</cell><cell>60.4</cell><cell>54.2</cell><cell>61.6</cell><cell>61.2</cell><cell>60.1</cell><cell>60.8</cell><cell>61.5</cell></row><row><cell>23</cell><cell>42.9</cell><cell>36.8</cell><cell>46.9</cell><cell>41.9</cell><cell>44.0</cell><cell>45.8</cell><cell>41.7</cell></row><row><cell>24</cell><cell>47.4</cell><cell>37.0</cell><cell>49.4</cell><cell>45.5</cell><cell>50.3</cell><cell>49.8</cell><cell>49.5</cell></row><row><cell>Av.</cell><cell>61.6</cell><cell>50.4</cell><cell>62.6</cell><cell>61.7</cell><cell>61.9</cell><cell>62.4</cell><cell>62.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Control experiments for Optical Flow. It presents experiments on AUNets and different combination of Optical Flow and Color. Alone means solely training with the OF as input</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Number of parameters per architecture per AU. Total number of parameters and total learnable neurons for HydraNet, AUNets and AUNets with Optical Flow with additional input channels, horizontal concatenation, streaming fusion after last convolutional layer, after FC6 layer and after FC7 layer, respectively</figDesc><table><row><cell>AU</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>7</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>15</cell><cell>17</cell><cell>23</cell><cell>24</cell><cell>Av.</cell></row><row><cell>F1</cell><cell>53.4</cell><cell>44.7</cell><cell>55.8</cell><cell>79.2</cell><cell>78.1</cell><cell>83.1</cell><cell>88.4</cell><cell>66.6</cell><cell>47.5</cell><cell>62.0</cell><cell>47.3</cell><cell>49.7</cell><cell>63.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results of our best approach (RGB+OF Horizontal concatenation).</figDesc><table><row><cell>These results are reported</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Per Frame Precision 0.96 0.96 1.0 0.96 0.97 0.99 0.98 0.91 0.94 0.96 Per Frame Recall 0.99 0.95 0.95 1.0 0.96 0.98 0.95 0.93 0.96 0.96 Per Video Precision 0.97 0.96 1.0 0.96 0.97 0.99 0.98 0.92 0.94 0.97 Per Video Recall 0.99 0.96 0.95 1.0 0.96 0.98 0.95 0.94 0.96 0.97</figDesc><table><row><cell>V2</cell><cell>V3</cell><cell>V4</cell><cell>V5</cell><cell>V6</cell><cell>V7</cell><cell>V8</cell><cell>V9</cell><cell>Av.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of Multi-view Classifier, Per frame and per video results for the view recognition sub-task in the validation set of FERA17. 86.2 85.7 20.7 47.5 49.6 43.2 75.4 74.3 34.2 51.9 48.6 Av. 54.9 79.7 83.4 41.6 58.0 64.4 56.1 77.8 81.8 45.2 57.4 57.7</figDesc><table><row><cell></cell><cell></cell><cell>Validation</cell><cell></cell><cell>Test</cell></row><row><cell>AU</cell><cell></cell><cell>ACC</cell><cell>F1</cell><cell>ACC</cell><cell>F1</cell></row><row><cell></cell><cell>[46]</cell><cell>[44] Ours [46]</cell><cell>[44] Ours [46]</cell><cell>[44] Ours [46]</cell><cell>[44] Ours</cell></row><row><cell>1</cell><cell cols="5">57.0 78.2 93.3 15.4 30.4 48.7 53.0 76.5 89.8 14.7 26.3 30.9</cell></row><row><cell>4</cell><cell cols="5">52.0 80.8 92.7 17.2 36.2 55.6 55.7 85.7 93.5 4.40 11.8 16.6</cell></row><row><cell>6</cell><cell cols="5">67.6 79.9 83.2 56.4 71.2 76.1 66.2 79.4 81.0 63.0 77.6 79.9</cell></row><row><cell>7</cell><cell cols="5">64.2 73.7 75.5 72.7 77.9 81.8 66.4 76.3 77.7 75.5 80.8 83.6</cell></row><row><cell>10</cell><cell cols="5">63.8 82.9 82.3 69.2 83.6 83.6 67.1 83.2 81.1 75.8 86.5 84.6</cell></row><row><cell>12</cell><cell cols="5">66.0 86.0 85.9 64.7 84.0 85.3 65.1 82.9 82.4 68.7 84.3 82.7</cell></row><row><cell>14</cell><cell cols="5">62.2 66.7 64.8 62.2 69.7 73.4 61.5 68.3 69.2 66.8 75.7 78.2</cell></row><row><cell>15</cell><cell cols="5">30.7 80.6 88.1 14.6 35.3 43.1 31.0 73.6 83.6 22.0 36.2 28.0</cell></row><row><cell>17</cell><cell cols="5">48.5 82.2 82.2 22.4 44.2 46.6 52.2 76.3 79.0 27.4 42.4 44.3</cell></row><row><cell>23</cell><cell>37.3</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>47.3 45.2 54.5 49.5 54.8 45.0 49.6 47.1 48.7 4 53.4 57.7 46.0 53.3 81.5 55.3 59.1 51.1 43.1 55.6 6 75.4 73.6 77.6 77.2 77.7 79.0 77.3 72.3 74.4 76.1 7 81.6 81.9 81.4 81.9 83.3 82.6 80.7 81.7 80.8 81.8 10 82.4 83.9 82.6 83.1 84.5 84.8 85.0 83.7 82.0 83.6 12 86.2 86.6 84.6 86.7 85.3 85.2 85.2 84.9 82.9 85.3 14 72.3 73.6 71.7 73.8 77.0 76.4 72.5 74.9 68.2 73.4 15 40.4 45.5 40.2 53.0 47.4 47.7 45.0 39.9 28.6 43.1 17 52.0 52.4 51.0 45.2 45.7 48.4 43.7 44.8 36.5 46.6 23 52.0 54.2 53.6 49.4 53.9 54.2 49.4 44.4 34.9 49.6 Av. 64.1 65.7 63.4 65.8 68.6 66.8 64.3 62.7 57.9 64.4</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">F1-score</cell><cell></cell><cell></cell></row><row><cell>AU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell></row><row><cell></cell><cell>v1</cell><cell>v2</cell><cell>v3</cell><cell>v4</cell><cell>v5</cell><cell>v6</cell><cell>v7</cell><cell>v8</cell><cell>v9 Global</cell></row><row><cell cols="2">1 45.1 AU v1</cell><cell>v2</cell><cell>v3</cell><cell>v4</cell><cell>v5</cell><cell>ACC v6</cell><cell>v7</cell><cell>v8</cell><cell>v9 Global</cell></row><row><cell>1</cell><cell cols="8">91.2 92.9 92.2 94.6 93.4 95.2 92.3 93.7</cell><cell>94</cell><cell>93.3</cell></row><row><cell>4</cell><cell cols="9">92.1 94.4 90.0 93.2 97.3 94.0 92.6 92.7 88.1</cell><cell>92.7</cell></row><row><cell>6</cell><cell cols="9">81.4 81.6 84.5 84.5 84.1 85.9 84.2 80.1 82.7</cell><cell>83.2</cell></row><row><cell>7</cell><cell cols="9">75.3 75.9 73.9 76.3 79.0 77.6 73.8 74.9 72.4</cell><cell>75.5</cell></row><row><cell cols="10">10 80.1 83.1 81.4 81.9 83.4 83.6 83.5 82.8 80.6</cell><cell>82.3</cell></row><row><cell cols="10">12 87.1 87.8 86.0 87.5 86.2 86.2 83.9 85.3 83.4</cell><cell>85.9</cell></row><row><cell cols="10">14 62.5 64.6 62.4 65.0 71.1 72.7 63.4 66.6 54.9</cell><cell>64.8</cell></row><row><cell cols="10">15 86.8 90.4 84.7 92.4 91.0 91.4 90.6 89.8 75.9</cell><cell>88.1</cell></row><row><cell cols="10">17 83.4 86.5 85.0 76.1 85.8 84.9 79.8 82.0 76.4</cell><cell>82.2</cell></row><row><cell cols="10">23 87.5 88.5 87.3 87.3 87.8 87.9 86.4 82.3 76.7</cell><cell>85.7</cell></row><row><cell cols="10">Av. 82.7 84.6 82.7 83.9 85.9 85.9 83.1 83.0 78.5 83.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Results over FERA17 Validation, using F1-score and Accuracy (ACC). 85.2 85.0 83.3 83.7 83.5 83.2 82.8 83.3 83.6 10 80.7 83.8 81.4 85.9 87.8 85.8 86.1 84.7 84.3 84.6 12 82.3 81.8 83.7 84.0 84.5 84.1 79.3 82.1 81.9 82.7 14 77.4 78.9 78.0 78.1 79.1 80.3 76.8 77.8 77.3 78.2 15 23.2 21.1 38.8 26.6 28.0 24.1 26.4 30.2 29.1 28.0 17 50.1 48.4 52.5 43.2 43.5 48.7 42.3 37.9 37.8 44.3 23 46.5 51.4 51.7 50.6 54.3 51.3 42.4 44.3 44.7 48.6 Av. 57.8 59.1 59.4 58.5 58.7 56.9 56.5 56.7 56.6 57.7 80.5 80.3 77.1 78.4 78.5 75.9 75.8 76.6 77.7 10 76.8 81.5 79.2 81.9 84.3 81.9 82.4 81.4 80.9 81.1 12 81.8 82.6 83.6 82.7 83.5 83.1 79.0 82.5 82.4 82.4 14 66.5 70.4 68.7 68.7 72.6 73.2 67.7 68.2 67.1 69.2 15 82.4 86.1 84.2 84.6 86.7 85.1 82.9 81.2 79.7 83.6 17 83.0 84.3 85.1 75.4 82.5 82.3 74.0 73.1 72.1 79.0 23 72.2 77.0 75.9 75.5 79.1 72.6 72.9 71.4 72.1 74.3 Av. 81.0 83.3 82.3 81.1 82.2 82.3 79.8 79.5 79.2 81.2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">F1-score</cell><cell></cell><cell></cell></row><row><cell>AU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell></row><row><cell></cell><cell>v1</cell><cell>v2</cell><cell>v3</cell><cell>v4</cell><cell>v5</cell><cell>v6</cell><cell>v7</cell><cell>v8</cell><cell>v9 Global</cell></row><row><cell>1</cell><cell cols="9">33.7 30.5 28.8 35.0 32.3 28.1 34.0 27.5 28.7</cell><cell>30.9</cell></row><row><cell>4</cell><cell cols="9">22.6 29.2 13.2 18.9 14.9 2.90 14.7 19.3 19.0</cell><cell>16.6</cell></row><row><cell>6</cell><cell cols="9">79.8 80.3 80.4 79.8 79.2 80.1 79.7 80.4 79.7</cell><cell>79.9</cell></row><row><cell cols="2">7 82.0 AU v1</cell><cell>v2</cell><cell>v3</cell><cell>v4</cell><cell>v5</cell><cell>ACC v6</cell><cell>v7</cell><cell>v8</cell><cell>v9 Global</cell></row><row><cell>1</cell><cell cols="9">92.4 91.3 90.6 90.3 87.7 89.9 89.2 88.0 88.7</cell><cell>89.8</cell></row><row><cell>4</cell><cell cols="9">97.3 97.6 93.8 94.7 87.9 94.9 92.7 91.5 91.2</cell><cell>93.5</cell></row><row><cell>6</cell><cell cols="9">81.5 81.9 81.5 80.1 79.0 81.6 80.9 81.5 80.8</cell><cell>81.0</cell></row><row><cell>7</cell><cell>76.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>This work was partially supported by a Google Research Award Latin America. We are grateful to NVIDIA Corporation for donating the GPUs used in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to transfer: Transferring latent task structures and its application to person-specific facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large displacement optical flow: descriptor matching in variational motion estimation. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<title level="m">Carlos Niebles. Activitynet: A large-scale video bench</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">mark for human activity understanding</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented optical flow and binetcauchy kernels on nonlinear dynamical systems for the recognition of human actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">P-cnn: Posebased cnn features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Modeling spatial and temporal cues for multi-label facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00911</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Facial action coding system</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<idno>ECCV. 1998. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Compound facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Facial action coding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Rosenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiconditional latent variable model for joint facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning based facs action unit occurrence and intensity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Tasli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Uyl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maroulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic facial expression recognition using features of salient facial patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Happy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Routray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi view facial action unit detection based on cnn and blstm-rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Automatic Face &amp; Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Open source computer vision library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itseez</surname></persName>
		</author>
		<idno>2015. 7</idno>
		<ptr target="https://github.com/itseez/opencv" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning the dynamic appearance and shape of facial action units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facial expression analysis based on high dimensional binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Froumenty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Latent trees for estimating intensity of facial action units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaltwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Eac-net: A region-based deep enhancing and cropping approach for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Abtahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02925</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Au-aware deep networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deeply learning deformable facial action parts model for dynamic expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Facial expression recognition via a boosted deep belief network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A main directional mean optical flow feature for spontaneous micro-expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Object recognition from local scaleinvariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The proceedings of the seventh IEEE international conference on</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
	<note>Computer vision</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Extended disfa dataset: Investigating posed and spontaneous facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Performance evaluation of texture measures with classification based on kullback discrimination of distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IAPR International Conference on</title>
		<meeting>the 12th IAPR International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="582" to="585" />
		</imprint>
	</monogr>
	<note>Computer Vision &amp; Image Processing</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-region two-stream r-cnn for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">View-independent facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Automatic Face &amp; Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Meta-analysis of the first facial expression recognition challenge. Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="966" to="979" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sánchez-Lozano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04174</idno>
		<title level="m">Fera 2017-addressing head pose in the third facial expression recognition and analysis challenge</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Capturing global semantic relationships for facial action unit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Object contour detection with a fully convolutional encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Lipson. Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Confidence preserving machine for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multimodal spontaneous emotion corpus for human behavior analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Joint patch and multi-label learning for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep region and multi-label learning for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
