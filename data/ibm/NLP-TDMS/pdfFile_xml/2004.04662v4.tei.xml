<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Residual Shuffle-Exchange Network for Fast Processing of Long Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andis</forename><surname>Draguns</surname></persName>
							<email>andis.draguns@lumii.lv</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Latvia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emīls</forename><surname>Ozoliņš</surname></persName>
							<email>emils.ozolins@lumii.lv</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Latvia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matīss</forename><surname>Agrisšostaks</surname></persName>
							<email>agris.sostaks@lumii.lv</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Latvia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kārlis</forename><surname>Apinis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Latvia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freivalds</surname></persName>
							<email>karlis.freivalds@lumii.lv</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Latvia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Residual Shuffle-Exchange Network for Fast Processing of Long Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention is a commonly used mechanism in sequence processing, but it is of O(n 2 ) complexity which prevents its application to long sequences. The recently introduced neural Shuffle-Exchange network offers a computation-efficient alternative, enabling the modelling of long-range dependencies in O(n log n) time. The model, however, is quite complex, involving a sophisticated gating mechanism derived from the Gated Recurrent Unit. In this paper, we present a simple and lightweight variant of the Shuffle-Exchange network, which is based on a residual network employing GELU and Layer Normalization. The proposed architecture not only scales to longer sequences but also converges faster and provides better accuracy. It surpasses the Shuffle-Exchange network on the LAMBADA language modelling task and achieves state-ofthe-art performance on the MusicNet dataset for music transcription while being efficient in the number of parameters. We show how to combine the improved Shuffle-Exchange network with convolutional layers, establishing it as a useful building block in long sequence processing applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>More and more applications of sequence processing performed by neural networks require dealing with long inputs. A key requirement is to allow modelling of dependencies between distant parts of the sequences. Such long-range dependencies occur in natural language when the meaning of some word depends on other words in the same or previous sentence. There are important cases, e.g., to resolve coreferences, when such distant information may not be disregarded.</p><p>In music, dependencies occur on several scales. At the finest scale samples of the waveform correlate to form note pitches, at medium scale neighbouring notes relate to each other by forming melodies and chord progressions, at coarse scale common melodies reappear throughout the entire piece creating a coherent musical form <ref type="bibr" target="#b26">(Thickstun, Harchaoui, and Kakade 2017;</ref><ref type="bibr" target="#b16">Huang et al. 2019)</ref>. Dealing with such dependencies requires processing very long sequences (several pages of text or the entire musical composition) in a manner that aggregates information from their distant parts. Especially challenging are approaches that work directly on the raw audio waveform.</p><p>The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21).</p><p>The ability to combine distant information is even more important for algorithmic tasks where each output symbol typically depends on every input symbol. The goal of algorithm induction is to derive an algorithm from input-output examples which are often given as sequences. Algorithmic tasks are especially challenging due to the need for processing sequences of unlimited length. Also, generalization plays an important role since training is often performed on short sequences but testing on long ones.</p><p>The commonly used attention mechanism (for example, in Transformers) can deal with such long-range dependencies but its time and space complexity is quadratic depending on the sequence length, therefore, is not an attractive choice for long sequences. The attention mechanism's complexity also makes it slower at inference time, making it less suitable for tasks with strict latency requirements, such as realtime sound processing. The recently introduced neural Shuffle-Exchange networks allow modelling of long-range dependencies in sequences in O(n log n) time <ref type="bibr" target="#b12">(Freivalds, Ozoliņš, andŠostaks 2019)</ref>. The idea is very promising and offers a computation-efficient alternative to the attention mechanism. However, the model is quite complex, involving a sophisticated gating mechanism derived from the Gated Recurrent Unit .</p><p>In this paper, we present a much simpler and faster version of the neural Shuffle-Exchange network which is based on the residual network idea employing Gaussian Error Linear Units <ref type="bibr" target="#b15">(Hendrycks and Gimpel 2016)</ref> and Layer Normalization <ref type="bibr" target="#b1">(Ba, Kiros, and Hinton 2016)</ref> instead of gates.</p><p>We empirically validate our improved model on algorithmic tasks, LAMBADA question answering and multiinstrument musical note recognition (MusicNet dataset). It surpasses the original Shuffle-Exchange network by 2.1% on the LAMBADA language modelling task and achieves state-of-the-art 78.02% average precision score on Music-Net.</p><p>We introduce a modification where we prepend our proposed model with strided convolutions to increase the speed and applicability to long sequences even more. This change enables processing a sequence of length 2M symbols in only 3.97 seconds.</p><p>Our main contributions are: • We propose a much simpler and faster Switch Unit -the core part of the Shuffle-Exchange network. This improve-ment leads to higher accuracy and scaling to longer sequences. It also makes the model approximately 4 times faster to train and 2 times faster on inference.</p><p>• We surpass the previous state-of-the-art on MusicNet while being efficient in the number of parameters. Our proposed improvements to the architecture enable this state-of-the-art achieving model to run inference on a single GPU fast enough to be suitable for realtime audio processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The attention mechanism  has become a standard choice in numerous neural models, including Transformer <ref type="bibr" target="#b28">(Vaswani et al. 2017</ref>) and BERT <ref type="bibr" target="#b10">(Devlin et al. 2018</ref>) which achieved state-of-the-art accuracy in NLP and related tasks. However, the complexity of the attention mechanism is quadratic depending on the input length and does not scale to long sequences. One way to overcome the complexity of attention is cutting the sequence into short segments and using attention only within the segment boundaries <ref type="bibr" target="#b0">(Al-Rfou et al. 2018)</ref>. Various sparse attention mechanisms have been proposed to deal with the quadratic complexity of dense attention by attending only to a small predetermined subset of locations <ref type="bibr" target="#b3">(Beltagy, Peters, and Cohan 2020;</ref><ref type="bibr" target="#b34">Zaheer et al. 2020</ref>). Reformer <ref type="bibr" target="#b20">(Kitaev, Kaiser, and Levskaya 2020)</ref> uses localitysensitive hashing to approximate attention in time O(n log n). Linformer ) uses a linear complexity approximation to the original attention by creating a lowrank factorization of the attention matrix.</p><p>Another option for processing long sequences is using convolutional architectures <ref type="bibr" target="#b13">(Gehring et al. 2017)</ref>. However, convolutions are inherently local − the value of a particular neuron depends on a small neighbourhood of the previous layer. One way to capture long-range structure is to increase the receptive field of convolution by using dilated (atrous) convolution, where the convolution mask is spread out at regular spatial intervals. Dilated architectures have achieved great success in image segmentation <ref type="bibr" target="#b33">(Yu and Koltun 2015)</ref> and audio generation <ref type="bibr" target="#b27">(van den Oord et al. 2016</ref>).</p><p>An important use of sequence processing models is in learning algorithmic tasks (see <ref type="bibr">Kant (2018)</ref> for a good overview) where the way how memory is accessed is crucial. Neural GPU <ref type="bibr" target="#b18">(Kaiser and Sutskever 2015)</ref> utilizes active memory <ref type="bibr" target="#b17">(Kaiser and Bengio 2016)</ref> where computation is coupled with memory access. <ref type="bibr" target="#b11">Freivalds and Liepins (2018)</ref> proposes DNGPU where the flow of information in the Neural GPU is facilitated by introducing diagonal gates that improves training and generalization but does not address the performance problem caused by many layers.</p><p>For music transcription tasks convolutional architectures are common. See <ref type="bibr" target="#b4">(Benetos et al. 2019</ref>) for a good overview. <ref type="bibr" target="#b27">Trabelsi et al. (2018)</ref> achieves notable performance on Mu-sicNet by using a convolutional network based on complex numbers. <ref type="bibr" target="#b32">Yang et al. (2020)</ref> recently proposed a Transformer network that employs the Fourier transform of the waveform in the complex domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Shuffle-Exchange networks</head><p>Neural Shuffle-Exchange network <ref type="bibr">(Freivalds, Ozoliņš, anď Sostaks 2019)</ref> has been recently proposed as an efficient alternative to the attention mechanism that allows modelling of long-range dependencies in sequences in O(n log n) time. The neural Shuffle-Exchange network is the neural adaption of the Shuffle-Exchange and the Beneš networks which are well-known from packet routing tasks in computer networks. These networks consist of interleaved shuffle and switch layers. The shuffle layer permutes the signals. The switch layer consists of switches. Each switch is configured to either swap two adjacent signals or leave them unchanged. The neural Shuffle-Exchange network adapts the structure of Beneš network and replaces each switch with a Switch Unit -a learnable 2-to-2 function. See <ref type="figure">Fig. 1</ref> for an example of a neural Shuffle-Exchange network routing signals from 8 input sequence addresses to 8 output sequence addresses.</p><p>The input to the neural Shuffle-Exchange network is a sequence of length n = 2 k , k ∈ N, and each element of the sequence is a vector with m dimensions. The input sequence is padded to the nearest power of two. The first layer of the model is a Switch Layer, which consists of n/2 Switch Units (SU). The input sequence is split into non-overlapping pairs of adjacent elements, and each pair is processed by a Switch Unit. Each Switch Unit is a neural network that computes a 2-to-2 function from a pair of elements. The second layer of the network is a Shuffle Layer, which permutes the inputs according to the perfect shuffle permutation. Perfect shuffle permutation maps each input address to an address that is circularly bit-shifted to the left (e.g. 101 to 011).</p><p>Note that all Shuffle Layers are identical and have no learnable parameters. It is also worth noting that both types of layers leave the dimensions of the input unchanged. The third layer of the network is the same as the first layera Switch Layer. The layers keep alternating between Shuffle Layers and Switch Layers until there are a total of log(n)−1 from each type of layer. This arrangement of layers constitutes the Shuffle-Exchange network.  <ref type="figure">Figure 1</ref>: Shuffle-Exchange network routing signals from 8 input sequence addresses (sequence element locations) from the left to 8 output sequence addresses to the right. Each green block is a Switch Unit that takes two input elements and either swaps or leaves them unchanged. A column of Switch Units forms a Switch Layer. The arrows between two Switch Layers represent a Shuffle Layer that permutes the elements. In this figure, the switches are configured to route an element from address 000 to 111.  The Shuffle-Exchange network is followed by a reversed Shuffle-Exchange network. The only difference between these two is that in the reversed Shuffle-Exchange network, all Shuffle Layers are replaced by inverse Shuffle Layers. Inverse Shuffle Layer permutes inputs like the regular Shuffle Layer, but the bit shift direction is to the right (e.g. 011 to 101). This combination of regular Shuffle-Exchange network followed by a reversed Shuffle-Exchange network forms a Beneš block -a building block of the neural Shuffle-Exchange network. It has been shown <ref type="bibr" target="#b8">(Dally and Towles 2004)</ref> that such a Beneš block can connect any input to any output for each input-output simultaneously. Therefore, the neural Shuffle-Exchange network has a 'receptive field' of the size of the whole sequence, and it has no bottleneck. These properties hold for dense attention but have not been shown for many sparse attention and dilated convolutional architectures.</p><p>Multiple Beneš blocks can be stacked one after another to increase the depth of the model. After the last Beneš block, a final Switch Layer is added to complete the model. Within each Beneš block the weights are shared for each Switch Unit in the Shuffle-Exchange network and each Switch Unit in the reversed Shuffle-Exchange network (see <ref type="figure" target="#fig_1">Fig. 2</ref>). Such weight sharing enables generalization on algorithmic tasks because otherwise there would be no straightforward way of scaling up the model for sequences that are longer than the ones observed during training. No significant decrease in accuracy is observed on other tasks as a result of this weight sharing scheme.</p><p>At the heart of the neural Shuffle-Exchange network is the Switch Unit. The definition of the original Switch Unit from the neural Shuffle-Exchange network is:</p><formula xml:id="formula_0">s = [s 1 , s 2 ] r 1 = σ(W 1 r s + B 1 r ) r 2 = σ(W 2 r s + B 2 r ) c 1 = tanh(W c 1(r 1 s) + B 1 c ) c 2 = tanh(W c 2(r 2 s) + B 2 c ) u = σ(W u s + B u ) s = swapHalf(s 1 , s 2 ) [s 1 o , s 2 o ] = u s + (1 − u) [c 1 , c 2 ]</formula><p>For a more detailed description of how this Switch Unit works, see <ref type="bibr" target="#b12">Freivalds, Ozoliņš, andŠostaks (2019)</ref>. This formulation of the Switch Unit uses sophisticated gating mechanisms similar to Gated Recurrent Unit  and is quite complex. Because apart from Switch Units, the network consists of only fixed non-trainable permutations, the choice of Switch Unit is critical to the overall performance of the network. Some alternatives to the Switch Unit have been explored, but so far the simpler architectures have led to a decrease in performance <ref type="bibr">(Freivalds, Ozoliņš, anď Sostaks 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Residual Shuffle-Exchange Network</head><p>We propose the Residual Shuffle-Exchange network (RSE), which keeps the structure from the neural Shuffle-Exchange network but replaces the Switch Unit with our Residual Switch Unit (RSU). RSU is based on a residual network and employs Gaussian Error Linear Unit (GELU) <ref type="bibr" target="#b15">(Hendrycks and Gimpel 2016)</ref> and Layer Normalization. The unit's design is similar to the feed-forward block in the Transformer.</p><p>RSU takes as an input two vectors [i 1 , i 2 ] and produces two vectors [o 1 , o 2 ] as an output. Each of these vectors is of size m, where m is the number of feature maps.</p><p>RSU consists of two linear transformations on the feature dimension. The first linear transformation is followed by Layer Normalization (LayerNorm) without output bias and gain <ref type="bibr" target="#b31">(Xu et al. 2019</ref>) and then by GELU. By default, we use a 2x larger hidden layer size than the input of the first layer, which is a good compromise of speed and accuracy (see Section 5.4). A second linear transformation is applied after GELU. The RSU is defined as follows:</p><formula xml:id="formula_1">i = [i 1 , i 2 ] g = GELU(LayerNorm(Zi)) c = W g + B [o 1 , o 2 ] = σ(S) i + h c</formula><p>In the above equations, Z, W are weight matrices of size 2m × 4m and 4m × 2m, respectively, S is vector of size 2m and B is a bias vector − all of those are learnable parameters; h is scalar value, denotes element-wise vector multiplication and σ is the sigmoid function. For a visualization of the Residual Switch Unit, see Appendix A.</p><p>In this unit, we use a residual connection that is scaled by the learnable parameter S, which is restricted to the range [0,1] by the sigmoid function. Additionally, we scale the new value c coming out of the last linear transformation by a constant h. We initialize S and h such that the signal travelling through the network keeps its expected amplitude at 0.25 under the assumption of the normal distribution (which is observed in practice). To have that, we initialize S as σ −1 (r) and h as √ 1 − r 2 * 0.25 where r is an experimentally chosen constant close to 1. As r approaches 1, RSU and RSE both start to behave more like identity functions. We use r = 0.9, which works well. The signal amplitude after LayerNorm is 1, the weight matrix W is initialized to keep this amplitude. If the amplitude of the input is 0.25, then the expected amplitude at the output is also 0.25, which is a good range for the softmax loss. During training, the network is free to adjust these amplitudes, but this initialization provides stable convergence even for deep networks.</p><p>Besides being simpler, the improved design of the RSU allows not using skip connections between Beneš blocks that were needed in the original SE network to ensure stable convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Prepending convolutions</head><p>There are tasks, e.g. the MusicNet task, where there is a large mismatch of information content between input and the Residual Shuffle-Exchange network -each input unit contains one sample, but Residual Shuffle-Exchange network requires a large number of feature maps to work well. Encoding just one number into many feature maps is wasteful. For such tasks, we prepend the Residual Shuffle-Exchange network with several strided convolutions to increase the number of feature maps and reduce the sequence length. We use convolutions with stride 2 and apply LayerNorm and GELU after each convolution like in the RSU. Before the result is passed to the Residual Shuffle-Exchange network, a linear transformation is applied. An illustration of a concrete example model with two prepended convolution layers can be found in Appendix C.</p><p>Prepending convolutions shortens the input to the RSE network and speeds up processing. The obtained accuracy for the MusicNet is roughly the same. Note that this approach leads to a shorter output than the input. It may be necessary to append transposed convolution layers at the end of the network to upsample the signal back to its original length. For the MusicNet task, upsampling is not necessary since we utilize only a few elements of the output sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We have implemented the proposed architecture in Tensor-Flow. The code is at https://github.com/LUMII-Syslab/RSE. All models are trained on a single Nvidia RTX 2080 Ti (11GB) GPU with RAdam optimizer <ref type="bibr" target="#b21">(Liu et al. 2019)</ref> Our models were hand-tuned based on a coarse grid search of the parameters. Further increasing the model size leads to overfitting or negligible accuracy increase. For the other models, we use the hyperparameters that their authors have reported achieving the highest accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Algorithmic tasks</head><p>Let us evaluate how well the Residual Shuffle-Exchange (RSE) network performs on algorithmic tasks in comparison with the neural Shuffle-Exchange (SE) <ref type="bibr" target="#b12">(Freivalds, Ozoliņš, andŠostaks 2019)</ref>. The goal is to infer O(n log n) time algorithms purely from input-output examples. In these tasks, a single bit change in the input can lead to a completely different output. Algorithmic tasks are good benchmarks to evaluate the model's ability to develop a rich set of long-term dependencies.</p><p>We consider long binary addition, long binary multiplication and sorting, which are common benchmark tasks in several papers including <ref type="bibr" target="#b12">(Freivalds, Ozoliņš, andŠostaks 2019;</ref><ref type="bibr" target="#b19">Kalchbrenner, Danihelka, and Graves 2015;</ref><ref type="bibr" target="#b36">Zaremba and Sutskever 2015;</ref><ref type="bibr" target="#b35">Zaremba et al. 2016;</ref><ref type="bibr" target="#b17">Joulin and Mikolov 2015;</ref><ref type="bibr" target="#b14">Grefenstette et al. 2015;</ref><ref type="bibr" target="#b18">Kaiser and Sutskever 2015;</ref><ref type="bibr" target="#b11">Freivalds and Liepins 2018;</ref><ref type="bibr" target="#b9">Dehghani et al. 2018)</ref>.</p><p>The model for evaluation consists of an embedding layer where each symbol of the input is mapped to a vector of length m, one or two Beneš blocks and the output layer which performs a linear transformation to the required number of classes with a softmax cross-entropy loss for each symbol independently. We use the RSE model having one Beneš block for addition and sorting tasks, two blocks for the multiplication task and m = 192 feature maps.</p><p>We use dataset generators and curriculum learning from the article introducing neural Shuffle-Exchange networks <ref type="bibr" target="#b12">(Freivalds, Ozoliņš, andŠostaks 2019)</ref>. For training, we instantiate several models for sequence lengths (powers of 2) from 8 to 64 sharing the same weights and train each example on the smallest instance it fits. We pad the sequence up to the required length with zeroes. <ref type="figure">Figure 3</ref> shows the testing accuracy on sequences of length 64 vs training step. We can see that on the multiplication task the proposed model trains much faster than SE, reaching near-zero error in about 20K steps vs 200K steps for the SE. For addition and sorting tasks, both models perform similarly.   <ref type="figure">Figure 4</ref>: Test accuracy depending on the length for the generalization of Residual Shuffle-Exchange (RSE) and Shuffle-Exchange (SE) to longer sequences on algorithmic tasks.</p><p>We have compared the generalization performance of both models, see <ref type="figure">Fig. 4</ref>. We train both models on length up to 64 and evaluate on length up to 4K. On addition and sorting tasks, the proposed RSE model generalizes very well to length 256 but loses slightly to SE on longer sequences. For the multiplication task RSE model generalizes reasonably well to twice as long sequences but not more, where the old model does not generalize even this much. We have compared our model to the DNGPU -the improved Neural GPU <ref type="bibr" target="#b11">(Freivalds and Liepins 2018)</ref>. Our model outperforms it on all tasks except multiplication. More detailed comparison of DNGPU and RSE generalization performance can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">LAMBADA question answering</head><p>The goal of the LAMBADA task is to predict a given target word from its broad context (on average, 4.6 sentences collected from novels). The sentences in the LAMBADA dataset <ref type="bibr" target="#b23">(Paperno et al. 2016)</ref> are specially selected such that giving the right answer requires examining the whole passage. In 81% cases of the test set the target word can be found in the text, and we follow a common strategy <ref type="bibr" target="#b7">(Chu et al. 2017;</ref><ref type="bibr" target="#b9">Dehghani et al. 2018)</ref> to choose the target word as one from the text. The answer will be wrong in the remaining cases, so the achieved accuracy will not exceed 81%. Choosing a random word from the passage gives 1.6% test accuracy <ref type="bibr" target="#b23">(Paperno et al. 2016)</ref>.</p><p>We instantiate the model for input length 256 (all test and train examples fit into this length) and pad the input sequence to that length by placing the sequence at a random position and adding zeros on both ends. Randomized padding improves test accuracy. We use a pretrained fastText 1M English word embedding <ref type="bibr" target="#b22">(Mikolov et al. 2018)</ref> for the input words. The embedding layer is followed by 2 Beneš blocks with 384 feature maps. To perform the answer selection as a word from the text, each symbol of the output is linearly mapped to a single scalar and we use softmax loss over the obtained sequence to select the position of the answer word.</p><p>In <ref type="table" target="#tab_2">Table 1</ref>, we show the test accuracy and the number of learnable parameters of our model in the context of results reported in previous works. The Residual Shuffle-Exchange network scores better than SE by 2.1% while using 3x less learnable parameters <ref type="bibr" target="#b12">(Freivalds, Ozoliņš, andŠostaks 2019)</ref>. Current state-of-the-art model GPT-3 surpasses our model, achieving 86.4% accuracy while using about 16000 times more learnable parameters and pretraining on a huge dataset <ref type="bibr" target="#b5">(Brown et al. 2020)</ref>. The performance of GPT-3 is comparable to human performance on this task <ref type="bibr" target="#b7">(Chu et al. 2017</ref>). Our model scores lower than Universal Transformer by 1.66% <ref type="bibr" target="#b9">(Dehghani et al. 2018</ref>) but uses about 14 times fewer parameters. Our model also scores by 5.34% higher than the Gated-Attention reader <ref type="bibr" target="#b7">(Chu et al. 2017</ref>).  <ref type="figure">Figure 6</ref>: The MusicNet performance of our model on various window sizes. Using window sizes larger than 8K gives marginal accuracy improvement at a considerable increase in training time.</p><p>maps and 2 Beneš blocks, with total parameter count 33M and 11M, respectively. We evaluate sequence lengths that fit in the 11GB of GPU memory. The Residual Shuffle-Exchange network works faster and can be evaluated on 4x longer sequences than Shuffle-Exchange network and 128x longer sequences than the Universal Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">MusicNet</head><p>The music transcription dataset MusicNet <ref type="bibr" target="#b26">(Thickstun, Harchaoui, and Kakade 2017)</ref> consists of 330 classical music recordings paired with the MIDI transcriptions of their notes. The total length of the recordings is 34 hours, and it features 11 different instruments. The task is to classify what notes are being played at each time step given the waveform. As multiple notes can be played at the same time, this is a multi-label classification task.</p><p>For performing classification, regularly spaced windows of a given length are extracted from the waveform. We predict all the notes that are played at the midpoint of the extracted window.</p><p>We use an RSE model with two Beneš blocks with 192 feature maps. We experimentally found this to be the best configuration. To increase the training speed, we prepend two strided convolutions in front of that, see analysis of other options below. To obtain the note predictions, we use the element in the middle of the sequence output by the RSE model, linearly transform it to the 128 values, one for each note pitch, and apply the sigmoid cross-entropy loss function to perform multi-label classification.</p><p>For evaluating the model, we use the average precision score (APS), which is the area under the precision-recall curve. This metric is well suited to prediction tasks with imbalanced classes and is suggested for the MusicNet dataset in the original paper <ref type="bibr" target="#b26">(Thickstun, Harchaoui, and Kakade 2017)</ref>. We evaluate APS using the scikit-learn machine learning library <ref type="bibr" target="#b24">(Pedregosa et al. 2011)</ref>. We train the model on different window sizes ranging from 128 to 8192 (see <ref type="figure">Fig. 6</ref>). We find that larger window sizes invariably lead to better accuracy. The best APS score of 78.02% is obtained on length 8192.</p><p>The previous state-of-the-art was achieved by the Translation invariant net <ref type="bibr" target="#b25">(Thickstun et al. 2018)</ref>. They used taskspecific handcrafted filterbanks to achieve invariance in representation with respect to pitch shifts of the input audio. It incorporates the prior knowledge of invariances in the problem domain to train on an augmented version of the Mu-sicNet, where data is pitch-shifted by an integral number of semitones.</p><p>See <ref type="table" target="#tab_3">Table 2</ref> for a comparison with other works. A majority of the best results on the MusicNet have architectures that are specialized to use complex-valued data. This can give an advantage in sound processing tasks where data can be Fourier-transformed into a complex-valued representation before it is given as an input to the model. <ref type="bibr" target="#b27">Trabelsi et al. (2018)</ref> achieves 72.90% APS with Deep Complex Network -a complex-valued convolution architecture. Its real-valued counterpart Deep Real Network achieves a lower score of 69.80% APS. cgRNN is an RNN that uses a recurrent cell with complex-valued transitions <ref type="bibr" target="#b30">(Wolter and Yao 2018)</ref>. It achieves 53% APS and uses only 2.36M parameters. <ref type="bibr" target="#b32">Yang et al. (2020)</ref> proposed Complex Transformer that achieves 74.22% APS. We have tested how prepending convolutions impact the speed and accuracy of the model. We find that one convolution gives the best results, although the differences are small. We chose to use two convolutions for a good balance between speed and accuracy. We use the batch size of one example in this test to see the sequence length limit our model can be trained and tested on a single GPU. <ref type="figure">Fig. 7</ref> shows the training and evaluation speed of the model depending on the number of convolutions. Increasing the number of convolution layers improves training and evaluation speed, and the version with two convolutions can be trained on sequences up to length 128K. With 86 predictions per second as in the test set, our state-of-the-art model can perform inference 1.93 times faster than realtime. The training lines stop at the length at which the model does not fit in the GPU memory anymore. Testing lines reach a 2M technical limitation of our implementation.</p><p>In Appendix C, we provide accuracy measurements depending on the window size and the number of convolutions. We visualize the note predictions and describe the most common errors in Appendix D. Additional training details are provided in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation study</head><p>We have chosen the multiplication task as a showcase for the ablation study. It is a hard task which challenges every aspect of the model and performance differences are clearly observable. We use a model with 2 Beneš blocks, 192 feature maps and train it on length 128. We consider the following simplifications of the proposed architecture: We can see in <ref type="figure">Fig. 8</ref> that the proposed baseline performs the best. Versions without residual connection or normalization do not work well. Residual weight parameter and GELU non-linearity give a smaller contribution to the model's performance.</p><p>We investigate the effect of the RSU hidden layer size on performance. Parameter count and speed of the model is directly proportional to the hidden layer size; therefore, we want to select the smallest size, which gives a good performance. By default, we use 2m feature maps where m is the number of feature maps of the model. Versions with half as large or twice as large hidden layer size are explored. We discover that a larger hidden layer size leads to better performance. We consider the choice of 2m hidden layer size a good compromise between performance and parameter count. The experimental results for various hidden layer sizes are found in Appendix F.</p><p>We have performed ablation experiments also for LAM-BADA and MusicNet tasks with similar conclusions, but the differences are much less pronounced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have proposed a simpler and faster version of the neural Shuffle-Exchange network. It has O(n log n) complexity and enables processing of sequences up to length 2 million where standard methods, like attention, fail. While keeping the overall successful connectivity structure of the Shuffle-Exchange network, we have shown that using residual connections instead of gated connections in its design, gives a significant boost to the training speed and achieved accuracy. Additionally, we have shown how to combine the model with strided convolutions that increase its speed and sequence length that can be processed.</p><p>The proposed model achieves state-of-the-art accuracy in recognizing musical notes directly from the waveform -a task where the ability to process long sequences is crucial. Notably, our architecture uses significantly fewer parameters than most of the previous best models for this task.</p><p>Our experiments confirm the Residual Shuffle-Exchange network as a useful building block for long sequence processing applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Residual Switch Unit</head><p>We visualize our Residual Switch Unit (see <ref type="figure" target="#fig_5">Fig.9</ref>) and place it in the context of the description found in section 4. RSU consists of two linear transformations on the feature dimension. The first linear transformation is followed by Layer Normalization (LayerNorm) without output bias and gain <ref type="bibr" target="#b31">(Xu et al. 2019</ref>) and then by GELU. A second linear transformation is applied after GELU. The RSU is defined as follows:</p><formula xml:id="formula_2">i = [i 1 , i 2 ] g = GELU(LayerNorm(Zi)) c = W g + B [o 1 , o 2 ] = σ(S) i + h c</formula><p>In the above equations, Z, W are weight matrices of size 2m × 4m and 4m × 2m, respectively, S is vector of size 2m and B is a bias vector − all of those are learnable parameters; h is scalar value, denotes element-wise vector multiplication and σ is the sigmoid function. The rationale for the design of the Residual Switch Unit stems from there being two good design choices for deep networks (and our network is deep) -gated network (like LSTM) and residual network. Since the original unit was based on gates, we experimented with various choices based on a residual network. We found a design which is similar to the feed-forward block in the Transformer that works really well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Algorithmic tasks</head><p>We compare the generalization of our proposed RSE with Neural GPU with diagonal gates (DNGPU). RSE generalizes better on sorting and addition tasks, but worse on the multiplication task (see <ref type="figure">Fig.10</ref>). Training the DNGPU is done as described by <ref type="bibr" target="#b11">Freivalds and Liepins (2018</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Musicnet convolution count analysis</head><p>In the MusicNet dataset, each element of the input sequence is a single number. Residual Shuffle-Exchange network requires a large number of feature maps to work well, but encoding just one number into many feature maps is wasteful. For such tasks, we prepend the Residual Shuffle-Exchange network with several strided convolutions to increase the number of feature maps and reduce the sequence length. We use convolutions with stride 2 and apply LayerNorm and GELU after each convolution like in the RSU. Before the result is passed to the Residual Shuffle-Exchange network, a linear transformation is applied. In <ref type="figure">Fig. 11</ref> we illustrate an example model with two prepended convolution layers for an input sequence consisting of 4096 numbers.  <ref type="figure">Figure 11</ref>: The architecture with two prepended convolutions employed for the MusicNet task.</p><p>We have tested how prepending convolutions impact the speed and accuracy of the model. <ref type="table" target="#tab_6">Table 3</ref> shows the obtained accuracy on window size 1024 for a different number of convolutions. We find that one convolution gives the best results, although the differences are small. We chose to use two convolutions for a good balance between speed and accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MusicNet prediction visualization</head><p>We visualize the note predictions that our model outputs with a window size 8192 (see <ref type="figure" target="#fig_1">Fig. 12</ref>). This is the same model with which we achieved state-of-the-art accuracy. For creating input sequences for classification, regularly spaced windows of size 8192 are extracted from the waveform. Each window is shifted relative to the previous window by 128 elements of the sequence. The model predicts all the notes that are being played at the midpoint of the window. <ref type="figure" target="#fig_1">Figure 12</ref>: The top picture shows the predictions of our model on the test set. The predicted note shading represents the classification confidence. The bottom picture shows the corresponding labels. The vertical axis is the pitch of the note. The horizontal axis corresponds to the starting time of the audio segment that is given as the input to our model for predicting the notes played in the middle of the segment.</p><p>The notes are generally well predicted but their start and end times are smoothed out. This affects low-pitched notes the most. The loss term described in Appendix E was added to alleviate this by encouraging correct predictions at regularly spaced intervals of the input sequence. The added term decreased the number of errors at the edges of the notes and improved the overall performance. In real applications, an appropriate threshold should be applied to get the duration of the notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E MusicNet training details</head><p>We train the model for 800K iterations which corresponds to approximately eight epochs. We found this value by examining the training dynamics on the validation set. The MusicNet dataset does not provide a separate validation set, so we split off six recordings from the training set and use them for validation. We use the same six recordings as <ref type="bibr" target="#b27">Trabelsi et al. (2018)</ref>. Using the same procedure as <ref type="bibr" target="#b32">Yang et al. (2020)</ref>, we downsample the waveform of the recordings in the dataset from 44.1 kHz to 11 kHz.</p><p>We add a term to the loss function, which predicts the notes played at regularly spaced intervals with stride 128 in the input sequence. This term is used only during training. The term is added because using only the middle element in the loss function seems to lead to lower accuracy in predicting the beginning and the end of the notes. The loss for these additional predictions is calculated in the same way as for the middle element.</p><p>We find that adding this term to the loss function improves the training speed and the final accuracy of the model. For example, without adding this loss term, we achieve 76.84%, while with the added loss term, we achieve 78.02%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional predictions</head><p>Middle prediction <ref type="figure">Figure 13</ref>: A window of size 1024 with additional predictions for the added loss term. The predictions are regularly spaced at sequence positions that are multiples of 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional ablation study</head><p>We investigate the effect of the RSU hidden layer size on the performance. Parameter count and speed of the model is directly proportional to the hidden layer size; therefore, we want to select the smallest size, which gives a good performance. By default, we use 2m feature maps where m is the number of feature maps of the model. Versions with half as large or twice as large hidden layers are explored. In <ref type="figure" target="#fig_6">Fig.14</ref> we can see that a larger hidden layer gives a better performance. We use a hidden layer of size 2m as a good compromise between performance and parameter count. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Residual Shuffle-Exchange network with two Beneš blocks and eight inputs. All learnable parameters are within the Switch Units. The rest of the network is fixed and used for routing information. Any number of Beneš blocks can be added to increase the depth of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Training and evaluation speed (log-scale) depending on the prepended convolution count on the MusicNet task. Ablation experiments. The plot shows test error on the multiplication task length 128 vs training step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>•</head><label></label><figDesc>removing LayerNorm (without LayerNorm) • using ReLU instead of GELU • removing the residual connection; the last equation of RSU becomes [o 1 , o 2 ] = c (without residual) • setting the residual weight parameter σ(S) to a constant 1 instead of a learnable parameter; the equation becomes [o 1 , o 2 ] = i + h c (without residual scale)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Residual Switch Unit. A number of feature maps (m) is shown in parentheses. Depicted here with the default of hidden layer being 2× larger than the input (4m being the size of the hidden layer and 2m the size of the input).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 14 :</head><label>14</label><figDesc>Test error depending on the hidden layer size (m is the number of feature maps of the model).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Accuracy on LAMBADA word prediction task.</figDesc><table><row><cell>Model</cell><cell cols="2">Parameters (M) Acc (%)</cell></row><row><cell>Random word</cell><cell>-</cell><cell>1.6</cell></row><row><cell>Gated-Attention Reader</cell><cell>unknown</cell><cell>49.0</cell></row><row><cell>SE</cell><cell>33</cell><cell>52.28</cell></row><row><cell>RSE (this work)</cell><cell>11</cell><cell>54.34</cell></row><row><cell>Universal Transformer</cell><cell>152</cell><cell>56.0</cell></row><row><cell>Human performance</cell><cell>-</cell><cell>86.0</cell></row><row><cell>GPT-3</cell><cell>175000</cell><cell>86.4</cell></row><row><cell cols="3">In Fig 5, we compare the training and evaluation time of</cell></row><row><cell cols="3">Residual Shuffle-Exchange (RSE), neural Shuffle-Exchange</cell></row><row><cell cols="3">(SE) and Universal Transformer (UT) networks using con-</cell></row><row><cell cols="3">figurations that reach their best test accuracy. We use the</cell></row><row><cell cols="3">official Universal Transformer and neural Shuffle-Exchange</cell></row><row><cell cols="3">implementations and measure the time for one training and</cell></row><row><cell cols="3">evaluation step on a single sequence. For the Universal</cell></row><row><cell cols="3">Transformer, we use its base configuration with 152M learn-</cell></row><row><cell cols="3">able parameters. SE and RSE networks have 384 feature</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance on the MusicNet dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">Parameters (M) APS (%)</cell></row><row><cell>cgRNN</cell><cell>2.36</cell><cell>53.00</cell></row><row><cell>Deep Real Network</cell><cell>10.0</cell><cell>69.80</cell></row><row><cell>Deep Complex Network</cell><cell>8.8</cell><cell>72.90</cell></row><row><cell>Complex Transformer</cell><cell>11.61</cell><cell>74.22</cell></row><row><cell>Translation-invariant net</cell><cell>unknown</cell><cell>77.3</cell></row><row><cell>RSE (this work)</cell><cell>3.06</cell><cell>78.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>).</figDesc><table><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>accuracy</cell><cell>0.6 0.7</cell><cell cols="2">RSE addition DNGPU addition RSE sorting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">DNGPU sorting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell cols="2">RSE multiplication</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">DNGPU multiplication</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1K</cell><cell>2K</cell><cell>4K</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">test length</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">Figure 10: Test accuracy depending on the test length for the generalization of Residual Shuffle-Exchange (RSE) and Neural</cell></row><row><cell cols="8">GPU with diagonal gates (DNGPU) to longer sequences on algorithmic tasks.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Accuracy (APS) depending on the number of convolutional layers on length 1024.</figDesc><table><row><cell cols="2">Convolutional layer count APS (%)</cell></row><row><cell>0</cell><cell>69.29</cell></row><row><cell>1</cell><cell>69.57</cell></row><row><cell>2</cell><cell>68.95</cell></row><row><cell>3</cell><cell>67.39</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the IMCS UL Scientific Cloud for the computing power and Leo Trukšāns for the technical support. We sincerely thank all the reviewers for their comments and suggestions. This research is funded by the Latvian Council of Science, project No. lzp-2018/1-0327.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Character-level language modeling with deeper selfattention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04444</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer Normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic Music Transcription: An Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ewert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="20" to="30" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Language models are few-shot learners</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Broad Context Language Modeling as Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="52" to="57" />
		</imprint>
		<respStmt>
			<orgName>ACL (Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Principles and practices of interconnection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Towles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Universal transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving the neural GPU architecture for algorithm learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Freivalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liepins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ICML workshop Neural Abstract Machines &amp; Program Induction v2</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural Shuffle-Exchange Networks -Sequence Processing in O(n log n) Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Freivalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ozoliņš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andšostaks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional Sequence to Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Precup, D.</editor>
		<editor>and Teh, Y. W.</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to Transduce with Unbounded Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cortes, C.</editor>
		<editor>and Lee D.D. et al.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1828" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian Error Linear Units (GELUs)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Music Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">;</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Lee, D.</editor>
		<editor>and Luxburg U.V. et al.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3781" to="3789" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08228</idno>
		<title level="m">Neural GPUs learn algorithms</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Grid long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01526</idno>
		<idno>arXiv:1802.02353</idno>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Neural Program Synthesis</title>
		<meeting><address><addrLine>Kant, N</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reformer: The Efficient Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the Variance of the Adaptive Learning Rate and Beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Advances in Pre-Training Distributed Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European Language Resources Association (ELRA)</title>
		<editor>Calzolari, N.</editor>
		<editor>and Choukri, Khalid et al.</editor>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European Language Resources Association (ELRA)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: word prediction requiring a broad discourse context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fernández</surname></persName>
		</author>
		<idno>1525-1534. ACL</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Invariances and Data Augmentation for Supervised Music Transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Features of Music from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">WaveNet: A Generative Model for Raw Audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09792</idno>
	</analytic>
	<monogr>
		<title level="m">SSW -the 9th ISCA Speech Synthesis Workshop</title>
		<meeting><address><addrLine>Sunnyvale, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">125</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deep Complex Networks</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Guyon, I.</editor>
		<editor>and Luxburg U.V. et al.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Linformer: Self-Attention with Linear Complexity</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Complex gated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10536" to="10546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding and Improving Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4383" to="4393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Complex Transformer: A Framework for Modeling Complex-Valued Sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4232" to="4236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<title level="m">Big bird: Transformers for longer sequences</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning simple algorithms from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="421" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00521</idno>
		<title level="m">Reinforcement Learning Neural Turing Machines-Revised</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
