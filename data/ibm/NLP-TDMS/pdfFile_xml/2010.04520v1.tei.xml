<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Back-Parsing for AMR-to-Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename><forename type="middle">♥</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>Tencent, WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♣</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute of Advanced Technology</orgName>
								<orgName type="institution">Westlake Institute for Advanced Study</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Online Back-Parsing for AMR-to-Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph. Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being used to generate outputs. We propose a decoder that back predicts projected AMR graphs on the target sentence during text generation. As the result, our outputs can better preserve the input meaning than standard decoders. Experiments on two AMR benchmarks show the superiority of our model over the previous state-of-the-art system based on graph Transformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abstract meaning representation (AMR) <ref type="bibr" target="#b1">(Banarescu et al., 2013)</ref> is a semantic graph representation that abstracts meaning away from a sentence. <ref type="figure" target="#fig_0">Figure 1</ref> shows an AMR graph, where the nodes, such as "possible-01" and "police", represent concepts, and the edges, such as "ARG0" and "ARG1", indicate relations between the concepts they connect. The task of AMR-to-text generation <ref type="bibr" target="#b18">(Konstas et al., 2017)</ref> aims to produce fluent sentences that convey consistent meaning with input AMR graphs. For example, taking the AMR in <ref type="figure" target="#fig_0">Figure 1</ref> as input, a model can produce the sentence "The police could help the victim". AMR-to-text generation has been shown useful for many applications such as machine translation <ref type="bibr" target="#b31">(Song et al., 2019)</ref> and summarization <ref type="bibr" target="#b20">(Liu et al., 2015;</ref><ref type="bibr" target="#b40">Yasunaga et al., 2017;</ref><ref type="bibr" target="#b19">Liao et al., 2018;</ref><ref type="bibr" target="#b15">Hardy and Vlachos, 2018)</ref>. In addition, AMR-totext generation can be a good test bed for general graph-to-sequence problems <ref type="bibr" target="#b4">(Belz et al., 2011;</ref><ref type="bibr" target="#b12">Gardent et al., 2017)</ref>.</p><p>AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks <ref type="bibr" target="#b3">(Beck et al., 2018;</ref><ref type="bibr" target="#b33">Song et al., 2018;</ref><ref type="bibr" target="#b13">Guo et al., 2019)</ref> and richer graph representations <ref type="bibr" target="#b8">(Damonte and Cohen, 2019;</ref><ref type="bibr" target="#b14">Hajdik et al., 2019;</ref><ref type="bibr" target="#b29">Ribeiro et al., 2019)</ref> have been shown to give better performances than RNN-based models <ref type="bibr" target="#b18">(Konstas et al., 2017)</ref> on linearized graphs. Subsequent work exploited graph Transformer <ref type="bibr" target="#b43">(Zhu et al., 2019;</ref><ref type="bibr" target="#b6">Cai and Lam, 2020;</ref><ref type="bibr" target="#b36">Wang et al., 2020)</ref>, achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current state-of-the-art models use a rather standard decoder: it functions as a language model, where each word is generated given only the previous words. As a result, one limitation of such decoders is that they tend to produce fluent sentences that may not retain the meaning of input AMRs.</p><p>We investigate enhancing AMR-to-text decoding by integrating online back-parsing, simultaneously predicting a projected AMR graph on the target sentence while it is being constructed. This is largely inspired by work on backtranslation <ref type="bibr" target="#b30">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b34">Tu et al., 2017)</ref>, which shows that back predicting the source sentence given a target translation output can be useful for strengthening neural machine translation. We perform online back parsing, where the AMR graph structure is constructed through the autoregressive sentence construction process, thereby saving the need for training a separate AMR parser. By adding online back parsing to the decoder, structural information of the source graph can intuitively be better preserved in the decoder network. <ref type="figure" target="#fig_1">Figure 2</ref> visualizes our structure-integrated decoding model when taking the AMR in <ref type="figure" target="#fig_0">Figure  1</ref> as input. In particular, at each decoding step, the model predicts the current word together with its corresponding AMR node and outgoing edges to the previously generated words. The predicted word, AMR node and edges are then integrated as the input for the next decoding step. In this way, the decoder can benefit from both more informative loss via multi-task training and richer features taken as decoding inputs.</p><p>Experiments on two AMR benchmark datasets (LDC2015E86 and LDC2017T10 1 ) show that our model significantly outperforms a state-ofthe-art graph Transformer baseline by 1.8 and 2.5 BLEU points, respectively, demonstrating the advantage of structure-integrated decoding for AMR-to-text generation. Deep analysis and human evaluation also confirms the superiority of our model. Our code is available at https: //github.com/muyeby/AMR-Backparsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baseline: Graph Transformer</head><p>Formally, the AMR-to-text generation task takes an AMR graph as input, which can be denoted as a directed acyclic graph G = (V, E), where V denotes the set of nodes and E refers to the set of labeled edges. An edge can further be represented by a triple v i , r k , v j , showing that node v i and v j are connected by relation type r k . Here k ∈ [1, ..., R], and R is the total number of relation types. The goal of AMR-to-text generation is to generate a word sequence y = [y 1 , y 2 , . . . , y M ], which conveys the same meaning as G.</p><p>We take a graph Transformer model <ref type="bibr" target="#b17">(Koncel-Kedziorski et al., 2019;</ref><ref type="bibr" target="#b43">Zhu et al., 2019;</ref><ref type="bibr" target="#b6">Cai and Lam, 2020;</ref><ref type="bibr" target="#b36">Wang et al., 2020)</ref> as our baseline. Previous work has proposed several variations of graph-Transformer. We take the model of <ref type="bibr" target="#b43">Zhu et al. (2019)</ref>, which gives the state-of-the-art performance. This approach exploits a graph Transformer encoder for AMR encoding and a standard Transformer decoder for text generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Transformer Encoder</head><p>The Graph Transformer Encoder is an extension of the standard Transformer encoder <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref>, which stacks L encoder layers, each having two sublayers: a self-attention layer and a position-wise feed forward layer. Given a set of AMR nodes [v 1 , v 2 , . . . , v N ], the l-th encoder layer takes the node features [h l−1 1 , h l−1 2 , . . . , h l−1 N ] from its preceding layer as input and produces a new set of features <ref type="bibr">1, . . . , L]</ref>, and h 0 i represents the embedding of AMR node v i , which is randomly initialized.</p><formula xml:id="formula_0">[h l 1 , h l 2 , . . . , h l N ] as its output. Here h l−1 i , h l i ∈ R d , d is the feature dimension, l ∈ [</formula><p>The graph Transformer encoder extends the vanilla self-attention (SAN) mechanism by explicitly encoding the relation r k 2 between each AMR node pair (v i , v j ) in the graph. In particular, the relation-aware self-attention weights are obtained by:</p><formula xml:id="formula_1">α ij = exp(e ij ) n∈[1,...,N ] exp (e in ) , e ij = (W Q h l−1 i ) T (W K h l−1 j + W R γ k ) √ d ,<label>(1)</label></formula><p>where W Q , W K , W R are model parameters, and γ k ∈ R dr is the embedding of relation r k , which is randomly initialized and optimized during training, d r is the dimension of relation embeddings.</p><p>With α ij , the output features are:</p><formula xml:id="formula_2">h l i = j∈[1,...,N ] α ij (W V h l−1 j + W R γ k ),<label>(2)</label></formula><p>where W V is a parameter matrix. Similar to the vanilla Transformer, a graph Transformer also uses multi-head self-attention, residual connection and layer normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Standard Transformer Decoder</head><p>The graph Transformer decoder is identical to the vanilla Transformer <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref>. It consists of an embedding layer, multiple 2 Since the adjacency matrix is sparse, the graph Transformer encoder uses the shortest label path between two nodes to represent the relation (e.g. path (victim, police) = "↑ARG1 ↓ARG0", path (police, victim) = "↑ARG0 ↓ARG1"). Transformer decoder layers and a generator layer (parameterized with a linear layer followed by softmax activation). Supposing that the number of decoder layers is the same as the encoder layers, denoted as L. The decoder consumes the hidden states of the top-layer encoder H L = [h L 1 , h L 2 , . . . , h L N ] as input and generates a sentence y = [y 1 , y 2 , . . . , y M ] word-by-word, according to the hidden states of the topmost decoder layer</p><formula xml:id="formula_3">S L = [s L 1 , s L 2 , . . . , s L M ]</formula><p>. Formally, at time t, the l-th decoder layer (l ∈ [1, . . . , L]) updates the hidden state as:</p><formula xml:id="formula_4">s l t = SAN(s l−1 1 , s l−1 2 , . . . , s l−1 t ), c l t = AN(ŝ l t , H L ), s l t = FF(c t ,ŝ l t ),<label>(3)</label></formula><p>where FF denotes a position-wise feed-forward layer, [s l−1 1 , s l−1 2 , . . . , s l−1 t ] represent the hidden states of the l − 1th decoder layer, [s 0 1 , s 0 2 , . . . , s 0 t ] are embeddings of [y s , y 1 , . . . , y t−2 , y t−1 ], and y s denotes the start symbol of a sentence.</p><p>In Eq 3, AN is a standard attention layer, which computes a set of attention scores β ti (i ∈ [1, . . . , N ]) and a context vector c t :</p><formula xml:id="formula_5">β ti = exp(f (ŝ l t , h L i )) j∈[1,...,N ] exp (f (ŝ l t , h L j )) , c l t = i∈[1,...,N ] β ti h L i ,<label>(4)</label></formula><p>where f is a scaled dot-product attention function. Denoting the output hidden state of the L-th decoder layer at time t as s L t , the generator layer predicted the probability of a target word y t as:</p><formula xml:id="formula_6">p(y t |y &lt;t , G) = softmax(W g s L t ),<label>(5)</label></formula><p>where y &lt;t = [y 1 , y 2 , . . . , y t−1 ], and W g is a model parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Objective</head><p>The training objective of the baseline model is to minimize the negative log-likelihood of conditional word probabilities:</p><formula xml:id="formula_7">std = − t∈[1,...,M ] log p(y t |y &lt;t , G) = − t∈[1,...,M ] log p(y t |s L t ; Θ),<label>(6)</label></formula><p>where Θ denotes the full set of parameters.</p><p>3 Model with Back-Parsing <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the proposed model. We adopt the baseline graph encoder described in Section 2.1 for AMR encoding, while enhancing the baseline decoder (Section 2.2) with AMR graph prediction for better structure preservation.</p><p>In particular, we train the decoder to reconstruct the AMR graph (so called "back-parsing") by jointly predicting the corresponding AMR nodes and projected relations when generating a new word. In this way, we expect that the model can better memorize the AMR graph and generate more faithful outputs. In addition, our decoder is trained in an online manner, which uses the last node and edge predictions to better inform the generation of the next word. Specifically, the encoder hidden states are first calculated given an AMR graph. At each decoding time step, the proposed decoder takes the encoder states as inputs and generates a new word (as in Section 2.2), together with its corresponding AMR node (Section 3.1) and its outgoing edges (Section 3.2), These predictions are then used inputs to calculate the next state (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Node Prediction</head><p>We first equip a standard decoder with the ability to make word-to-node alignments while generating target words. Making alignments can be formalized as a matching problem, which aims to find the most relevant AMR graph node for each target word. Inspired by previous work <ref type="bibr" target="#b21">(Liu et al., 2016;</ref><ref type="bibr" target="#b23">Mi et al., 2016)</ref>, we solve the matching problem by supervising the word-tonode attention scores given by the Transformer decoder. In order to deal with words without alignments, we introduce a NULL node v ∅ into the input AMR graph (as shown in <ref type="figure" target="#fig_1">Figure 2</ref>) and align such words to it. <ref type="bibr">3</ref> More specifically, at each decoding step t, our Transformer decoder first calculates the top decoder layer word-to-node attention distribution β t = [β t0 , β t1 , ..., β tN ] (Eq 3 and Eq 4) after taking the encoder states</p><formula xml:id="formula_8">H L = [h L 0 , h L 1 , h L 2 , . . . , h L N ]</formula><p>together with the previously generated sequence y &lt;t = [y 1 , y 2 , . . . , y t−1 ] (β t0 and h L 0 are the probability and encoder state for the NULL node v ∅ ). Then the probability of aligning the current decoder state to node v i is defined as:</p><formula xml:id="formula_9">p(ALI(s t ) = v i |H L , y &lt;t ) = β ti ,<label>(7)</label></formula><p>where ALI is the sub-network for finding the best aligned AMR node for a given decoder state.</p><p>Training. Supposing that the gold alignment (refer to Section 4.1) at time t isβ t , the training objective for node prediction is to minimize the loss defined as the distance between β t andβ t :</p><formula xml:id="formula_10">node = t∈[1,...,M ] ∆(β t ,β t ),<label>(8)</label></formula><p>where ∆ denotes a discrepancy criterion that can quantify the distance between β t andβ t . We take two common alternatives: (1) Mean Squared Error (MSE), and (2) Cross Entropy Loss (CE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Edge Prediction</head><p>The edge prediction sub-task aims to preserve the node-to-node relations in an AMR graph during text generation. To this end, we project the edges of each input AMR graph onto the corresponding sentence according to their nodeto-word alignments, before training the decoder to generate the projected edges along with target words. For words without outgoing edges, we add a "self-loop" edge for consistency. Formally, at decoding step t, each relevant directed edge (or arc) with relation label r k starting from y t can be represented as y j , r k , y t , where j ≤ t, y j , y t and r k are called "arc to", "arc from", and "label" respectively. We modify the deep biaffine attention classifier <ref type="bibr" target="#b10">(Dozat and Manning, 2016)</ref> to model these edges.</p><p>In particular, we factorize the probability for each labeled edge into the "arc" and "label" parts, computing both based on the current decoder hidden state and the states of all previous words. The "arc" score ψ arc tj ∈ R 1 , which measures whether or not a directed edge from y t to y j exists, is calculated as:</p><formula xml:id="formula_11">b arc to j , b arc from t = FF arc to (s L j ), FF arc from (s L t ), ψ arc tj = Biaff arc (b arc to j , b arc from t ), ψ arc t1 , ψ arc t2 , ..., ψ arc tj , ..., ψ arc tt = softmax(ψ arc t1 ,ψ arc t2 , ...,ψ arc tj , ...,ψ arc tt ).<label>(9)</label></formula><p>Similarly, the "label" score ψ label tj ∈ R R , which is used to predict a label for potential word pair (y j , y t ), is given by:</p><formula xml:id="formula_12">b label to j ,b label from t = FF label to (s L j ), FF label from (s L t ), ψ label tj = softmax Biaff label (b label to j , b label from t ) .<label>(10)</label></formula><p>In Eq 9 and Eq 10, FF arc to , FF arc from , FF label to and FF label from are linear transformations. Biaff arc and Biaff label are biaffine transformations:</p><formula xml:id="formula_13">Biaff(x 1 , x 2 ) = x T 1 U x 2 + W (x 1 ⊕ x 2 ) + b,<label>(11)</label></formula><p>where ⊕ denotes vector concatenation, U, W and b are model parameters. U is a (d × 1 × d) tensor for unlabeled classification (Eq 9) and a (d × R × d) tensor for labeled classification (Eq 10), where d is the hidden size. Defining p(y j |y t ) as ψ arc tj and p(r k |y j , y t ) as ψ label tj [k], the probability of a labeled edge y j , r k , y t is calculated by the chain rule:</p><formula xml:id="formula_14">p(r k , y j |y t ) = p(r k |y j , y t )p(y j |y t ) = ψ label tj [k] · ψ arc tj .<label>(12)</label></formula><p>Training. The training objective for the edge prediction task is the negative log-likelihood over all projected edges E :</p><formula xml:id="formula_15">label = − y j ,r k ,y i ∈E log p(r k , y j |y i ) (13)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Next State Calculation</head><p>In addition to simple "one-way" AMR backparsing (as shown in Section 3.1 and 3.2), we also study integrating the previously predicted AMR nodes and outgoing edges as additional decoder inputs to help generate the next word.</p><p>In particular, for calculating the decoder hidden states [s 1 t+1 , s 2 t+1 , ..., s L t+1 ] at step t + 1, the input feature to our decoder is a triple y t , v t , e t instead of a single value y t , which the baseline has.</p><p>Here y t , v t and e t are vector representations of the predicted word, AMR node and edges at step t, respectively. More specifically, v t is a weighted sum of the top-layer encoder hidden</p><formula xml:id="formula_16">states [h L 0 , h L 1 , ..., h L N ]</formula><p>, and coefficients are from the distribution of β t in Eq 7:</p><formula xml:id="formula_17">v t = i∈[0,...,N ] β ti h L i ,<label>(14)</label></formula><p>where is the operation for scalar-tensor product.</p><p>Similarly, e t is calculated as:</p><formula xml:id="formula_18">e t = r t ⊕ s t , r t = |R| k=1 t j=1 p(r k , y j |y t )γ k , s t = t j=1 p(y j |y t )s L j ,<label>(15)</label></formula><p>where ⊕ concatenates two tensors, p(r k , y j |y t ) and p(y j |y t ) are probabilities given in Eq 12, γ k is a relation embedding, and s L j is the decoder hidden state at step j. e t−1 is a vector concatenation of r t and s t , which are weighted sum of relation embeddings and weighted sum of previous decoder hidden states, respectively.</p><p>In contrast to the baseline in Eq 3, at time t+1, the hidden state of the first decoder layer is calculated as:</p><formula xml:id="formula_19">s 1 t+1 = SAN(s 0 1 , ..., s 0 t , y t , v t , e t ), c 1 t+1 = AN(ŝ 1 t+1 , H L ), s 1 t+1 = FF(c 1 t+1 ,ŝ 1 t+1 ),<label>(16)</label></formula><p>where the definition of H L , SAN, AN, FF and [s 0 1 , . . . , s 0 t ] are the same as Eq 3. v 0 and e 0 (as shown in <ref type="figure" target="#fig_1">Figure 2</ref>) are defined as zero vectors. The hidden states of upper decoder layers ([s 2 t+1 , ..., s L t+1 ]) are updated in the same way as Eq 3.</p><p>Following previous work on syntactic text generation <ref type="bibr" target="#b39">(Wu et al., 2017;</ref>, we use gold AMR nodes and outgoing edges as inputs for training, while we take automatic predictions for decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Objective</head><p>The overall training objective is:</p><formula xml:id="formula_20">total = std + λ 1 node + λ 2 label ,<label>(17)</label></formula><p>where λ 1 and λ 2 are weighting hyper-parameters for node and label , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on two benchmark AMR-to-text generation datasets, including LDC2015E86 and LDC2017T10. These two datasets contain 16,833 and 36,521 training examples, respectively, and share a common set of 1,368 development and 1,371 test instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Data preprocessing. Following previous work <ref type="bibr" target="#b33">(Song et al., 2018;</ref><ref type="bibr" target="#b43">Zhu et al., 2019)</ref>, we take a standard simplifier <ref type="bibr" target="#b18">(Konstas et al., 2017)</ref> to preprocess AMR graphs, adopting the Stanford tokenizer 4 and Subword Tool 5 to segment text into subword units. The node-to-word alignments are generated by ISI aligner <ref type="bibr" target="#b26">(Pourdamghani et al., 2014)</ref>. We then project the source AMR graph onto the target sentence according to such alignments.</p><p>For node prediction, the attention distributions are normalized, but the alignment scores generated by the ISI aligner are unnormalized hard 0/1 values. To enable cross entropy loss, we follow previous work <ref type="bibr" target="#b23">(Mi et al., 2016)</ref> to normalize the gold-standard alignment scores. Hyperparameters. We choose the feature-based model 6 of <ref type="bibr" target="#b43">Zhu et al. (2019)</ref> as our baseline (G-Trans-F-Ours). Also following their settings, both the encoder and decoder have 6 layers, with each layer having 8 attention heads. The sizes of hidden layers and word embeddings are 512, and the size of relation embedding is 64. The hidden size of the biaffine attention module is 512. We use Adam (Kingma and Ba, 2015) with a learning rate of 0.5 for optimization. Our models are trained for 500K steps on a single 2080Ti GPU. We tune these hyperparameters on the LDC2015E86 development set and use the selected values for testing 7 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Evaluation.</head><p>We set the decoding beam size as 5 and take BLEU <ref type="bibr" target="#b24">(Papineni et al., 2002)</ref> and Meteor <ref type="bibr" target="#b2">(Banerjee and Lavie, 2005;</ref><ref type="bibr" target="#b9">Denkowski and Lavie, 2014)</ref> as automatic evaluation metrics. We also employ human evaluation to assess the semantic faithfulness and generation fluency of compared methods by randomly selecting 50 AMR graphs for comparison. Three people familiar with AMR are asked to score the generation quality with regard to three aspects -concept preservation rate, relation preservation rate and fluency (on a scale of <ref type="bibr">[0,</ref><ref type="bibr">5]</ref>). Details about the criteria are:</p><p>• Concept preservation rate assesses to what extent the concepts in input AMR graphs are involved in generated sentences. • Relation preservation rate measures to what extent the relations in input AMR graphs exist in produced utterances. • Fluency evaluates whether the generated sentence is fluent and grammatically correct.</p><p>Recently, significant progress <ref type="bibr" target="#b29">(Ribeiro et al., 2019;</ref><ref type="bibr" target="#b42">Zhang et al., 2020;</ref><ref type="bibr" target="#b45">Ç elikyilmaz et al., 2020)</ref> in developing new metrics for NLG evaluation has made. We leave evaluation on these metrics for future work. <ref type="table">Table 1</ref> shows the performances on the devset of LDC2015E86 under different model settings. For the node prediction task, it can be observed that both cross entropy loss (CE) and mean squared error loss (MSE) give significantly better results than the baseline, with 0.46 and 0.65 improvement in terms of BLEU, respectively. In addition, CE gives a better result than MSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Development Experiments</head><p>Regarding edge prediction, we investigate two settings, with relation embeddings being shared by the encoder and decoder, or being separately constructed, respectively. Both settings give large improvements over the baseline. Compared with the model using independent relation embeddings, the model with shared relation embeddings gives slightly better results with less parameters, indicating that the relations in an AMR graph and the relations between words are consistent. We therefore adopt the CE loss and shared relation embeddings for the remaining experiments. <ref type="figure" target="#fig_2">Figure 3</ref> presents the BLEU scores of integrating standard AMR-to-text generation with node prediction or edge prediction under different λ 1 and λ 2 values, respectively.</p><p>There are improvements when increasing the coefficient from 0, demonstrating that both node prediction and edge prediction have positive influence on AMR-to-text generation. The BLEU of the two models reach peaks at λ 1 = 0.01 and λ 2 = 0.1, respectively. When further increasing the coefficients, the BLEU scores start to decrease. We thus set λ 1 = 0.01, λ 2 = 0.1 for the rest of our experiments. <ref type="table" target="#tab_2">Table 2</ref> shows the automatic evaluation results, where "G-Trans-F-Ours" and "Ours Back-Parsing" represent the baseline and our full model, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Automatic Evaluation</head><p>The top group of the table shows the previous state-of-the-art results on the LDC2015E86 and LDC2017T10 testsets. Our systems give significantly better results than the previous systems using different encoders, including LSTM <ref type="bibr" target="#b18">(Konstas et al., 2017)</ref>, graph gated neural network (GGNN; <ref type="bibr" target="#b3">Beck et al., 2018)</ref>, graph recurrent network (GRN; <ref type="bibr" target="#b33">Song et al., 2018)</ref>, densely connected graph convolutional network (DCGCN; <ref type="bibr" target="#b13">Guo et al., 2019)</ref> and various graph transformers (G-Trans-F, G-Trans-SA, G-Trans-C, G-Trans-W). Our baseline also achieves better Model LDC15 LDC17</p><p>LSTM <ref type="bibr" target="#b18">(Konstas et al., 2017)</ref> 22.00 -GGNN <ref type="bibr" target="#b3">(Beck et al., 2018)</ref> -23.30 GRN <ref type="bibr" target="#b33">(Song et al., 2018)</ref> 23.30 -DCGCN <ref type="bibr" target="#b13">(Guo et al., 2019)</ref> 25.9 27.9 G-Trans-F <ref type="bibr" target="#b43">(Zhu et al., 2019)</ref> 27.23 30.18 G-Trans-SA <ref type="bibr" target="#b43">(Zhu et al., 2019)</ref> 29.66 31.54 G-Trans-C <ref type="bibr" target="#b6">(Cai and Lam, 2020)</ref> 27.4 29.8 G-Trans-W <ref type="bibr" target="#b36">(Wang et al., 2020)</ref> 25 with external data LSTM (20M) <ref type="bibr" target="#b18">(Konstas et al., 2017)</ref> 33.8 -GRN (2M) <ref type="bibr" target="#b33">(Song et al., 2018)</ref> 33.6 -G-Trans-W (2M) <ref type="bibr" target="#b36">(Wang et al., 2020)</ref> 36.4 - BLEU scores than the corresponding models of <ref type="bibr" target="#b43">Zhu et al. (2019)</ref>. The main reason is that we train with more steps (500K vs 300K) and we do not prune low-frequency vocabulary items after applying BPE. Note that we do not compare our model with methods by using external data. Compared with our baseline (G-Trans-F-Ours), the proposed approach achieves significant (p &lt; 0.01) improvements, giving BLEU scores of 31.48 and 34.19 on LDC2015E86 and LDC2017T10, respectively, which are to our knowledge the best reported results in the literature. In addition, the outputs of our model have 0.8 more words than the baseline on average. Since the BLEU metric tend to prefer shorter results, this confirm that our model indeed recovers more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Human Evaluation</head><p>As shown in <ref type="table" target="#tab_4">Table 3</ref>, our model gives higher scores of concept preservation rate than the baseline on both datasets, with improvements of 3.6 and 3.3, respectively. In addition, the relation preservation rate of our model is also better than the baseline. This indicating that our model can preserve more concepts and relations than the baseline method, thanks to the back-parsing mechanism. With regard to the generation fluency, our model also gives better results than baseline. The main reason is that the relations between concepts such as subject-predicate relation and modified relation are helpful for generating fluency sentences.</p><p>Apart from that, we study discourse <ref type="bibr" target="#b28">(Prasad et al., 2008)</ref> relations, which are essential for generating a good sentence with correct meaning. Specifically, we consider 4   common discourse relations ("Cause", "Contrast", "Condition", "Coordinating"). For each type of discourse, we randomly select 50 examples from the test set and ask 3 linguistic experts to calculate the discourse preservation accuracy by checking if the generated sentence preserves such information. <ref type="table" target="#tab_5">Table 4</ref> gives discourse preservation accuracy results of the baseline and our model, respectively. The baseline already performs well, which is likely because discourse information can somehow be captured through co-occurrence in each (AMR, sentence) pair. Nevertheless, our approach achieves better results, showing that our back-parsing mechanism is helpful for preserving discourse relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>Ablation We conduct ablation tests to study the contribution of each component to the proposed model. In particular, we evaluate models with only the node prediction loss (Node Prediction, Section 3.1) and the edge prediction loss (Edge Prediction, Section 3.2), respectively, and further investigate the effect of integrating node and edge information into the next state computation (Section 3.3) by comparing models without and with (Int.) such integration. <ref type="table" target="#tab_7">Table 5</ref> shows the BLEU and Meteor scores on the LDC2015E86 testset. Compared with the baseline, we observe a performance improvement of 0.34 BLEU by adding the node prediction loss only. When using the predicted AMR graph nodes as additional input for next state computation (i.e.,   Node Prediction (Int.)), the BLEU score increases from 30.49 to 30.72, and the Meteor score reaches 35.94, showing that the previously predicted nodes are beneficial for text generation. Such results are consistent with our expectation that predicting the corresponding AMR node can help the generation of correct content words (a.k.a. concepts).</p><p>Similarly, edge prediction also leads to performance boosts. In particular, integrating the predicted relations for next state computation (Edge Prediction (Int.)) gives an improvement of 0.92 BLEU over the baseline. Edge prediction results in larger improvements than node prediction, indicating that relation knowledge is more informative than word-to-node alignment.</p><p>In addition, combining the node prediction and edge prediction losses (Both Prediction) leads to better model performance, which indicates that node prediction and edge prediction have mutual benefit.</p><p>Integrating both node and edge predictions (Both Prediction (Int.)) further improves the system to <ref type="bibr">31.48 BLEU and 36.15 Meteor,</ref><ref type="bibr">respectively.</ref> Correlation between Prediction Accuracy and Model Performance We further investigate the influence of AMR-structure preservation on the performance of the main text generation task. Specifically, we first force our model to generate a gold sentence in order to calculate the accuracies   for node prediction and edge prediction. We then calculate the corresponding BLEU score for the sentence generated by our model on the same input AMR graph without forced decoding, before drawing correlation between the accuracies and the BLEU score. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>(a) and 4(b) 8 , both node accuracy and edge accuracy have a strong positive correlation with the BLEU score, indicating that the more structural information is retained, the better the generated text is. We also evaluate the pearson (ρ) correlation coefficients between BLEU scores and node (edge) prediction accuracies. Results are given in <ref type="table" target="#tab_9">Table 6</ref>. Both types of prediction accuracies have strong positive correlations with the final BLEU scores, and their combination yields further boost on the correlation coefficient, indicating the necessity of jointly predicting the nodes and edges. Performances VS AMR Graphs Sizes <ref type="figure" target="#fig_4">Figure 5</ref> compares the BLEU scores of the baseline and our model on different AMR sizes. Our model is consistently better than the baseline for most length brackets, and the advantage is more obvious for large AMRs (size 51+).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>We provide two examples in <ref type="table" target="#tab_10">Table 7</ref> to help better understand the proposed model. Each example consists of an AMR graph, a reference sentence (REF), the output of baseline model (Baseline) and the sentence generated by our method (Ours).</p><p>As shown in the first example, although the baseline model maintains the main idea of the original text, it fails to recognize the AMR graph nodes "local" and "problem". In contrast, our model successfully recovers these two nodes and generates a sentence which is more faithful to the reference. We attribute this improvement to node prediction. To verify this, we visualize the word-to-node attention scores of both approaches in <ref type="figure" target="#fig_5">Figure 6</ref>. As shown in the figure, the baseline model gives little attention to the AMR node "local" and "problem" during text generation. In contrast, our system gives a more accurate alignment to the relevant AMR nodes in decoding.</p><p>In the second example, the baseline model incorrectly positions the terms "doctor", "see" and "worse cases" while our approach generates a more natural sentence. This can be attributed to the edge prediction task, which can inform the decoder to preserve the relation that "doctor" is the subject of "see" and "worse cases" is the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Early studies on AMR-to-text generation rely on statistical methods. <ref type="bibr" target="#b11">Flanigan et al. (2016)</ref> convert input AMR graphs to trees by splitting re-entrances, before translating these trees into target sentences with a tree-to-string transducer; <ref type="bibr" target="#b27">Pourdamghani et al. (2016)</ref> apply a phrase-based MT system on linearized AMRs; <ref type="bibr" target="#b32">Song et al. (2017)</ref> design a synchronous node replacement grammar to parse input AMRs while generating target sentences.</p><p>These approaches show comparable or better results than early neural (1) (o / obvious-01 :ARG1 (p / problem :ARG1-of (l / local-02)) :ARG1-of (c / cause-01 :ARG0 (l2 / lumpy :domain (d / dough :mod (c2 / cookie) :mod (t / this))))) REF: Obviously there are local problems because this cookie dough is lumpy . Baseline: It is obvious that these cookie dough were a lumpy . Ours: Obviously there is a local problem as this cookie dough is a lumpy .</p><p>(2) (c / cause-01 :ARG0 (s / see-01 :ARG0 (d / doctor) :ARG1 (c2 / case :ARG1-of (b / bad-05 :degree (m / more :quant (m2 / much))))) :ARG1 (w / worry :polarity -:mode imperative :ARG0 (y / you) :ARG1 (t / that))) REF: Doctors have seen much worse cases so don't worry about that ! Baseline: Don't worry about that see much worse cases by doctors . Ours: Don't worry that , as a doctor saw much worse cases . models <ref type="bibr" target="#b18">(Konstas et al., 2017)</ref>. However, recent neural approaches <ref type="bibr" target="#b33">(Song et al., 2018;</ref><ref type="bibr" target="#b43">Zhu et al., 2019;</ref><ref type="bibr" target="#b6">Cai and Lam, 2020;</ref><ref type="bibr" target="#b36">Wang et al., 2020;</ref><ref type="bibr" target="#b22">Mager et al., 2020)</ref> have demonstrated the stateof-the-art performances thanks to the use of contextualized embeddings.</p><p>Related work on NMT studies back-translation loss <ref type="bibr" target="#b30">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b34">Tu et al., 2017)</ref> by translating the target reference back into the source text (reconstruction), which can help retain more comprehensive input information. This is similar to our goal. <ref type="bibr" target="#b38">Wiseman et al. (2017)</ref> extended the reconstruction loss of <ref type="bibr" target="#b34">Tu et al. (2017)</ref> for table-to-text generation. We study a more challenging topic on how to retain the meaning of a complex graph structure rather than a sentence or a table. In addition, rather than reconstructing the input after the output is produced, we predict the input while the output is constructed, thereby allowing stronger information sharing.</p><p>Our work is also remotely related to previous work on string-to-tree neural machine translation (NMT) <ref type="bibr" target="#b0">(Aharoni and Goldberg, 2017;</ref><ref type="bibr" target="#b39">Wu et al., 2017;</ref>, which aims at generating target sentences together with their syntactic trees.</p><p>One major difference is that their goal is producing grammatical outputs, while ours is preserving input structural information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We investigated back-parsing for AMR-to-text generation by integrating the prediction of projected AMRs into sentence decoding. The resulting model benefits from both richer loss and more structual features during decoding. Experiments on two benchmarks show advantage of our model over a state-of-the-art baseline.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Full Experimental Settings</head><p>Table 8 lists all model hyperparameters used for experiments. Specifically, we share the vocabulary of AMR node BPEs and target word BPEs. Our implementation is based on the model of <ref type="bibr" target="#b43">Zhu et al. (2019)</ref>, which is available at https://github. com/Amazing-J/structural-transformer.</p><p>Our re-implementation and the proposed model are released at https://github.com/muyeby/ AMR-Backparsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 More Results</head><p>We compare our model with more baselines and use more evaluation metrics (BLEU <ref type="bibr" target="#b24">(Papineni et al., 2002)</ref>, Meteor <ref type="bibr" target="#b2">(Banerjee and Lavie, 2005;</ref><ref type="bibr" target="#b9">Denkowski and Lavie, 2014)</ref> and CHRF++ <ref type="bibr" target="#b25">(Popović, 2017)</ref>).</p><p>The results are shown in <ref type="table" target="#tab_14">Table 9</ref>. It can be observed that our approach achieves the best performance on both datasets regardless of the evaluation metrics. This observation is consistent with <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LDC2015E86 LDC2017T10</head><p>BLEU Meteor CHRF++ BLEU Meteor CHRF++ LSTM <ref type="bibr" target="#b18">(Konstas et al., 2017)</ref> 22.00 -----GRN <ref type="bibr" target="#b33">(Song et al., 2018)</ref> 23.30 -----Syntax-G <ref type="bibr" target="#b7">(Cao and Clark, 2019)</ref> 23.5 --26.8 --S-Enc <ref type="bibr" target="#b8">(Damonte and Cohen, 2019)</ref> 24.40 23.60 -24.54 24.07 -DCGCN <ref type="bibr" target="#b13">(Guo et al., 2019)</ref> 25.9 --27.9 -57.3 G-Trans-F <ref type="bibr" target="#b43">(Zhu et al., 2019)</ref> 27  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example AMR graph meaning "The police could help the victim."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>BLEU scores on the LDC2015E86 devset against different hyperparameter values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Performance (in BLEU) on the test set with respect to the node (a) and edge (b) prediction accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Performances (in BLEU) on the test set with respect to the size of the input AMR graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of word-to-node attention obtained from the baseline graph Transformer (left) and our model with node prediction loss (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Test-set BLEU scores on LDC2015E86</cell></row><row><cell>(LDC15) and LDC2017T10 (LDC17).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Human evaluation of the sentences generated by different systems on concept presevation rate (CPR), relation preservation rate (RPR) and fluency.</figDesc><table><row><cell>Model</cell><cell cols="4">Cause Contrast Condition Coord.</cell></row><row><cell>Baseline</cell><cell>0.84</cell><cell>0.92</cell><cell>0.91</cell><cell>0.98</cell></row><row><cell>Ours</cell><cell>0.96</cell><cell>0.98</cell><cell>0.95</cell><cell>0.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Human study for discourse preservation</cell></row><row><cell>accuracy on LDC2015E86.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on LDC2015E86 test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>The pearson correlation coefficient ρ between the prediction accuracy and BLEU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Examples for case study.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Full list of model parameters on the LDC2015E86.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Main test results on LDC2015E86 and LDC2017T10.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://amr.isi.edu/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This node is set as the parent of the original graph root (e.g. possible-01 inFigure 2) with relation "root".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://nlp.stanford.edu/software/tokenizer.shtml 5 https://github.com/rsennrich/subword-nmt6  We do not choose their best model (G-Trans-SA) due to its large GPU memory consumption, and its performance is actually comparable with G-Trans-F in our experiments.7Table 8in Appendix shows the full set of parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">For clear visualization, we only select the first one out of every 30 sentences from the LDC2015E86 testset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Yue Zhang is the corresponding author. We would like to thank the anonymous reviewers for their insightful comments and Yulong Chen for his fruitful inspiration. This work has been supported by National Natural Science Foundation of China under grant No.61976180 and a Xiniuniao grant of Tencent.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards string-to-tree neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph-to-sequence learning using gated graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The first surface realisation shared task: Overview and evaluation results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th</title>
		<meeting>the 13th</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">European workshop on natural language generation</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph transformer for graph-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<meeting>Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Factorising AMR generation through syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2157" to="2163" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structural neural encoders for amr-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shay B Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generation from abstract meaning representation using tree transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The webnlg challenge: Generating text from rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
		<meeting>the 10th International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected graph convolutional networks for graph-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="297" to="312" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural text generation from rich semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerie</forename><surname>Hajdik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Wayne</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Guided neural language generation for abstractive summarization using abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hardy</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Text generation from knowledge graphs with graph transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhanush</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural amr: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward abstractive summarization using semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural machine translation with supervised attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GPT-too: A language-model-first approach for AMR-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Mager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arafat</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Suk</forename><surname>Sultan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roukos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.167</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1846" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Supervised attentions for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">chrF++: words helping character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aligning English strings with abstract meaning representation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Pourdamghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generating english from abstract meaning representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Pourdamghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th international natural language generation conference</title>
		<meeting>the 9th international natural language generation conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Penn discourse TreeBank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enhancing amr-to-text generation with dual graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Leonardo Fr Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic neural machine translation using amr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Amr-to-text generation with synchronous node replacement grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A graph-to-sequence model for amr-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural machine translation with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Amr-to-text generation with graph transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="19" to="33" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A tree-based decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence-to-dependency neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph-based neural multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitijh</forename><surname>Meelu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st</title>
		<meeting>the 21st</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<title level="m">Conference on Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modeling graph structure in transformer for better amr-totext generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019</title>
		<meeting>the 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<title level="m">Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Evaluation of text generation: A survey. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
