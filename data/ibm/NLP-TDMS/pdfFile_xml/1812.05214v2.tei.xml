<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Learn from Noisy Labeled Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
							<email>lijunnan@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
							<email>yongkang.wong@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
							<email>qzhao@cs.umn.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Minnesota</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Learn from Noisy Labeled Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the success of deep neural networks (DNNs) in image classification tasks, the human-level performance relies on massive training data with high-quality manual annotations, which are expensive and time-consuming to collect. There exist many inexpensive data sources on the web, but they tend to contain inaccurate labels. Training on noisy labeled datasets causes performance degradation because DNNs can easily overfit to the label noise. To overcome this problem, we propose a noise-tolerant training algorithm, where a meta-learning update is performed prior to conventional gradient update. The proposed meta-learning method simulates actual training by generating synthetic noisy labels, and train the model such that after one gradient update using each set of synthetic noisy labels, the model does not overfit to the specific noise. We conduct extensive experiments on the noisy CIFAR-10 dataset and the Clothing1M dataset. The results demonstrate the advantageous performance of the proposed method compared to state-of-the-art baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the key reasons why deep neural networks (DNNs) have been so successful in image classification is the collections of massive labeled datasets such as COCO <ref type="bibr" target="#b13">[14]</ref> and ImageNet <ref type="bibr" target="#b19">[20]</ref>. However, it is time-consuming and expensive to collect such high-quality manual annotations. A single image often requires agreement from multiple annotators to reduce label error. On the other hand, there exist other less expensive sources to collect labeled data, such as search engines, social media websites, or reducing the number of annotators per image. However, those low-cost approaches introduce low-quality annotations with label noise. Many studies have shown that label noise can significantly affect the accuracy of the learned classifiers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b32">32]</ref>. In this work, we address the following problem: how to effectively train on noisy labeled datasets? Some methods learn with label noise by relying on human supervision to verify seed images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">29]</ref> or estimate label confusion <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">31]</ref>. However, those methods exhibit a disadvantage in scalability for large datasets. On the other hand, methods without human supervision (e.g. label correction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">24]</ref> and noise correction layers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">23]</ref>) are scalable but less effective and more heuristic. In this work we propose a meta-learning based noise-tolerant (MLNT) training to learn from noisy labeled data without human supervision or access to any clean labels. Rather than designing a specific model, we propose a model-agnostic training algorithm, which is applicable to any model that is trained with gradient-based learning rule.</p><p>The prominent issue in training DNNs on noisy labeled data is that DNNs often overfit to the noise, which leads to performance degradation. Our method addresses this issue by optimizing for a model's parameters that are less prone to overfitting and more robust against label noise. Specifically, for each mini-batch, we propose a meta-objective to train the model, such that after the model goes through conventional gradient update, it does not overfit to the label noise. The proposed meta-objective encourages the model to produce consistent predictions after it is trained on a variety of synthetic noisy labels. The key idea of our method is: a noise-tolerant model should be able to consistently learn the underlying knowledge from data despite different label noise. The main contribution of this work are as follows.</p><p>• We propose a noise-tolerant training algorithm, where a meta-objective is optimized before conventional training. Our method can be theoretically applied to any model trained with gradient-based rule. We aim to optimize for a model that does not overfit to a wide spectrum of artificially generated label noise.</p><p>• We formulate our meta-objective as: train the model such that after it learns from various synthetic noisy labels using gradient update, the updated models give consistent predictions with a teacher model. We adapt a self-ensembling method to construct the teacher model, which gives more reliable predictions unaffected by the synthetic noise.</p><p>• We perform experiments on two datasets with synthetic and real-world label noise, and demonstrate the advantageous performance of the proposed method in image classification tasks compared to state-of-the-art methods. In addition, we conduct extensive ablation study to examine different components of the proposed method. Our code is publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning with label noise. A number of approaches have been proposed to train DNNs with noisy labeled data. One line of approaches formulate explicit or implicit noise models to characterize the distribution of noisy and true labels, using neural networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b29">29]</ref>, directed graphical models <ref type="bibr" target="#b31">[31]</ref>, knowledge graphs <ref type="bibr" target="#b12">[13]</ref>, or conditional random fields <ref type="bibr" target="#b27">[27]</ref>. The noise models are then used to infer the true labels or assign smaller weights to noisy samples. However, these methods often require a small set of data with clean labels to be available, or use expensive estimation methods. They also rely on specific assumptions about the noise model, which may limit their effectiveness with complicated label noise. Another line of approaches use correction methods to reduce the influence of noisy labels. Bootstrap method <ref type="bibr" target="#b17">[18]</ref> introduces a consistency objective that effectively re-labels the data during training. Tanaka et al. <ref type="bibr" target="#b24">[24]</ref> propose to jointly optimize network parameters and data labels. An iterative training method is proposed to identify and downweight noisy samples <ref type="bibr" target="#b30">[30]</ref>. A few other methods have also been proposed that use noisetolerant loss functions to achieve robust learning under label noise <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">28]</ref>. Meta-Learning. Recently, meta-learning methods for DNNs have resurged in its popularity. Meta-learning generally seeks to perform the learning at a level higher than where conventional learning occurs, e.g. learning the update rule of a learner <ref type="bibr" target="#b16">[17]</ref>, or finding weight initializations that can be easily fine-tuned <ref type="bibr" target="#b0">[1]</ref> or transferred <ref type="bibr" target="#b11">[12]</ref>. Our approach is most related to MAML <ref type="bibr" target="#b0">[1]</ref>, which aims to train model parameters that can learn well based on a few examples and a few gradient descent steps. Both MAML and our method are model-agnostic and perform training by doing gradient updates on simulated meta-tasks. However, our objective and algorithm are different from that of MAML. MAML addresses few-shot transfer to new tasks, whereas we aim to learn a noise-tolerant model. Moreover, MAML trains using classification loss on a meta-test set, whereas we use a consistency loss with a teacher model. Self-Ensembling. Several recent methods based on selfensembling have improved the state-of-the-art results for semi-supervised learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b25">25]</ref>, where labeled samples are scarce and unlabeled samples are abundant. These methods apply a consistency loss to the unlabeled samples, which regularizes a neural network to make consistent pre-1 https://github.com/LiJunnan1992/MLNT <ref type="figure">Figure 1</ref>: Left: conventional gradient update with cross entropy loss may overfit to label noise. Right: a meta-learning update is performed beforehand using synthetic label noise, which encourages the network parameters to be noise-tolerant and reduces overfitting during the conventional update.</p><p>dictions for the same samples under different data augmentation <ref type="bibr" target="#b21">[21]</ref>, dropout and noise conditions <ref type="bibr" target="#b9">[10]</ref>. We focus in particular on the self-ensembling approach proposed by Tarvainen &amp; Valpola <ref type="bibr" target="#b25">[25]</ref> as it forms one of the basis of our approach. Their approach proposes two networks: a student network and a teacher network, where the weights of the teacher are the exponential moving average of those of the student. They enforce the student network to make consistent predictions with the teacher network. In our method, we use the teacher network in meta-test to train the student network such that it is more tolerant to label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Statement</head><p>We consider a classification problem with a training set D = {(x 1 , y 1 ), ..., (x n , y n )}, where x i denotes the i th sample and y i ∈ {0, 1} c is a one-hot vector representing the corresponding noisy label over c classes. Let f (x i , θ) denotes the discriminative function of a neural network parameterized by θ, which maps an input to an output of the c-class softmax layer. The conventional objective for supervised classification is to minimize an empirical risk, such as the cross entropy loss:</p><formula xml:id="formula_0">L c = − 1 n n i=1 y i · log(f (x i , θ)),<label>(1)</label></formula><p>where · denotes dot product. However, since y i contains noise, the neural network can overfit and perform poorly on the test set. We propose a meta-objective that encourages the network to learn noisetolerant parameters. The details are delineated next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Meta-Learning based Noise-Tolerant Training</head><p>Our method can learn the parameters of a DNN model in such a way as to "prepare" the model for label noise. The intuition behind our method is that when training with a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Train</head><formula xml:id="formula_1">( , ′ , � ) ( , 1 ′ , � ) ℒ meta ( )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Loss</head><p>Consistency Loss</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Test</head><p>Meta Loss loss is minimized before training on the conventional classification loss. We first generate multiple mini-batches of synthetic noisy labels with random neighbor label transfer (marked by orange arrow). The random neighbor label transfer can preserve the underlying noise transition (e.g. DEER → HORSE, CAT ↔ DOG), therefore generating synthetic label noise in a similar distribution as the original data. For each synthetic mini-batch, we update the parameters with gradient descent, and enforce the updated model to give consistent predictions with a teacher model. The meta-objective is to minimize the consistency loss across all updated models w.r.t θ.</p><p>gradient-based rule, some network parameters are more tolerant to label noise than others. How can we encourage the emergence of such noise-tolerant parameters? We achieve this by introducing a meta-learning update before the conventional update for each mini-batch. The meta-learning update simulates the process of training with label noise and makes the network less prone to over-fitting. Specifically, for each mini-batch of training data, we generate a variety of synthetic noisy labels on the same images. With each set of synthetic noisy labels, we update the network parameters using one gradient update, and enforce the updated network to give consistent predictions with a teacher model unaffected by the synthetic noise. As shown in <ref type="figure">Figure 1</ref>, the meta-learning update optimizes the model so that it can learn better with conventional gradient update on the original mini-batch. In effect, we aim to find model parameters that are less sensitive to label noise and can consistently learn the underlying knowledge from data despite label noise. The proposed meta-learning update consists of two procedures: meta-train and meta-test. Meta-Train. Formally, at each training step, we consider a mini-batch of data (X, Y ) sampled from the training set, where X = {x 1 , ..., x k } are k samples, and Y = {y 1 , ..., y k } are the corresponding noisy labels. We want to generate multiple mini-batches of noisy labels {Ŷ 1 , ...,Ŷ M } with similar label noise distribution as Y . We will describe the procedure for generating one set of noisy labelsŶ m = {ŷ m 1 , ...,ŷ m k }. First, we randomly select ρ samples out of the mini-batch of k samples. For each selected sample x i , we rank its neighbors within the mini-batch. Then we randomly select a neighbor x j from its top 10 nearest neighbors (10 is experimentally determined), and use the neighbor's label y j to replace the label for x i ,ŷ m i = y j . Because we transfer labels among neighbors, the synthetic noisy labels are from a similar distribution as the original noisy labels. We repeat the above procedure M times to generate M mini-batches of synthetic noisy labels. Note that we compute nearest neighbors based on the Euclidean distance between feature representations (pre-softmax layer activations) generated by a DNN pretrained on the entire noisy training set D.</p><p>Let θ denote the current model's parameters, for each synthetic mini-batch (X,Ŷ m ), we update θ to θ m using one gradient descent step on the mini-batch.</p><formula xml:id="formula_2">θ m = θ − α∇ θ L c (X,Ŷ m , θ),<label>(2)</label></formula><p>where L c (X,Ŷ m , θ) is the cross entropy loss described in equation 1, and α is the step size.</p><p>Meta-Test. Our meta-objective is to train θ such that each updated parameters θ m do not overfit to the specific noisy labelsŶ m . We achieve this by enforcing each updated model to give consistent predictions with a teacher model. We consider the model parameterized by θ as the student model, and construct the teacher model parameterized byθ following the self-ensembling <ref type="bibr" target="#b25">[25]</ref> method. The teacher model usually gives better predictions than the student model. Its parameters are computed as the exponential moving average (EMA) of the student's parameters. Specifically, at each training step, we updateθ with:</p><formula xml:id="formula_3">θ = γθ + (1 − γ)θ,<label>(3)</label></formula><p>where γ is a smoothing coefficient hyper-parameter. Since the teacher model is unaffected by the synthetic label noise, we enforce a consistency loss J (θ m ) that encourages each updated model (with parameters θ m ) to give consistent predictions with the teacher model on the same input X. We define J (θ m ) as the Kullback-Leibler (KL)divergence between the softmax predictions from the updated model f (X, θ m ) and the softmax predictions from the teacher model f (X,θ). We find that KL-divergence produces better results compared to the mean squared error used in <ref type="bibr" target="#b25">[25]</ref>.</p><formula xml:id="formula_4">J (θ m ) = 1 k k i=1 D KL (f (x i ,θ)||f (x i , θ m )) (4) = 1 k k i=1 E(log(f (x i ,θ)) − log(f (x i , θ m ))). (5)</formula><p>We want to minimize the consistency loss for all of the M updated models with parameters {θ 1 , ..., θ M }. Therefore, the meta loss is defined as the average of all consistency losses:</p><formula xml:id="formula_5">L meta (θ) = 1 M M m=1 J (θ m ) (6) = 1 M M m=1 J (θ − α∇ θ L c (X,Ŷ m , θ)). (7)</formula><p>Although L meta (θ) is computed using the updated model parameters θ m , the optimization is performed over the student model parameters θ. We perform stochastic gradient descent (SGD) to minimize the meta loss. The model parameters θ are updated as follows:</p><formula xml:id="formula_6">θ ← θ − η∇L meta (θ),<label>(8)</label></formula><p>where η is the meta-learning rate.</p><p>After the meta-learning update, we perform SGD to optimize the classification loss on the original mini-batch</p><formula xml:id="formula_7">(X, Y ). θ ← θ − β∇L c (X, Y , θ),<label>(9)</label></formula><p>where β is the learning rate. The full algorithm is outlined in Algorithm 1. Sample a mini-batch (X, Y ) of size k from D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>for m = 1 : M do 6:</p><p>Generate synthetic noisy labelsŶ m by random neighbor label transfer <ref type="bibr">7:</ref> Compute updated parameters with gradient descent:</p><formula xml:id="formula_8">θ m = θ − α∇ θ L c (X,Ŷ m , θ) 8:</formula><p>Evaluate consistency loss with teacher:</p><formula xml:id="formula_9">J (θ m ) = 1 k k i=1 D KL (f (x i ,θ)||f (x i , θ m )) 9:</formula><p>end for 10:</p><formula xml:id="formula_10">Evaluate L meta (θ) = 1 M M m=1 J (θ m ) 11:</formula><p>Meta-learning update θ ← θ − η∇L meta (θ)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Evaluate classification loss L c (X, Y , θ) <ref type="bibr" target="#b12">13</ref>:</p><formula xml:id="formula_11">Update θ ← θ − β∇L c (X, Y , θ)</formula><p>14:</p><p>Update teacher model:θ = γθ + (1 − γ)θ 15: end while Note that the meta-gradient ∇L meta (θ) involves a gradient through a gradient, which requires calculating the second-order derivatives with respect to θ. In our experiments we use a first-order approximation by omitting the second-order derivatives, which can significantly increase the computation speed. The comparison in Section 4.4 shows that this approximation performs almost as well as using second-order derivatives. This provides another intuition to explain our method: The first-order approximation considers the term α∇ θ L c (X,Ŷ m , θ) in equation 7 as a constant. Therefore, we can consider the update θ − α∇ θ L c (X,Ŷ m , θ) as injecting data-dependent noise to the parameters, and adding noise to the network during training has been shown by many studies to have a regularization effect <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Iterative Training</head><p>We propose an iterative training scheme for two purposes: (1) Remove samples with potentially wrong class labels from the classification loss L c (X, Y , θ). (2) Improve the predictions from the teacher model f (x i ,θ) so that the consistency loss is more effective.</p><p>First, we perform an initial training iteration following the method described in Algorithm 1, and acquire a model with the best validation accuracy (usually the teacher). We name that model as mentor and use θ * to denote its parameters. In the second training iteration, we repeat the steps in Algorithm 1 with two changes described as follows.</p><p>First, if the classification loss L c (X, Y , θ) is applied to the entire training set D, samples with wrong ground-truth labels can corrupt training. Therefore, we remove a sam-ple from the classification loss if the mentor model assigns a low probability to the ground-truth class. In effect, the classification loss would now sample batches from a filtered training set D which contains fewer corrupted samples.</p><formula xml:id="formula_12">D = {(x i , y i ) ∈ D | y i · f (x i , θ * ) &gt; τ },<label>(10)</label></formula><p>where f (x i , θ * ) is the softmax prediction of the mentor model, and τ is a threshold to control the balance between the quality and quantity of D . Second, we improve the effectiveness of the consistency loss by merging the predictions from the mentor model and the teacher model to produce more reliable predictions. The new consistency loss is:</p><formula xml:id="formula_13">J (θ m ) = 1 k k i=1 D KL (s||f (x i , θ m )),<label>(11)</label></formula><formula xml:id="formula_14">s = λf (x i ,θ) + (1 − λ)f (x i , θ * )),<label>(12)</label></formula><p>where λ is a weight to control the importance of the teacher model and the mentor model. It is ramped up from 0 to 0.5 as training proceeds. We train for three iterations in our experiments. The mentor model is the best model from the previous iteration. We observe that further training iterations beyond three do not give noticeable performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conduct experiments on two datasets, namely CIFAR-10 <ref type="bibr" target="#b8">[9]</ref> and Clothing1M <ref type="bibr" target="#b31">[31]</ref>. We follow the same experimental setting as previous studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27]</ref> for fair comparison.</p><p>For CIFAR-10, we split 10% of the training data for validation, and artificially corrupt the rest of the training data with two types of label noise: symmetric and asymmetric. The symmetric label noise is injected by using a random one-hot vector to replace the ground-truth label of a sample with a probability of r. The asymmetric label noise is designed to mimic some of the structure of real mistakes for similar classes <ref type="bibr" target="#b15">[16]</ref>: TRUCK → AUTOMOBILE, BIRD → AIRPLANE, DEER → HORSE, CAT ↔ DOG. Label transitions are parameterized by r ∈ [0, 1] such that true class and wrong class have probability of 1 − r and r, respectively.</p><p>Clothing1M <ref type="bibr" target="#b31">[31]</ref> consists of 1M images collected from online shopping websites, which are classified into 14 classes, e.g. t-shirt, sweater, jacket. The labels are generated using surrounding texts provided by sellers, which contain real-world errors. We use the 14k and 10k clean data for validation and test, respectively, but we do not use the 50k clean training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation</head><p>For experiments on CIFAR-10, we follow the same experimental setting as <ref type="bibr" target="#b24">[24]</ref> and use the network based on Pre-Act ResNet-32 <ref type="bibr" target="#b6">[7]</ref>. By common practice <ref type="bibr" target="#b24">[24]</ref>, we normalize the images, and perform data augmentation by random horizontal flip and 32 × 32 random cropping after padding 4 pixels per side. We use a batch size k = 128, a step size α = 0.2, a learning rate β = 0.2, and update θ using SGD with a momentum of 0.9 and a weight decay of 10 −4 . For each training iteration, we divide the learning rate by 10 after 80 epochs, and train until 120 epochs. For the initial iteration, we ramp up η (meta-learning rate) from 0 to 0.4 during the first 20 epochs, and keep η = 0.4 for the rest of the training. In terms of the EMA decay γ, we use γ = 0.99 for the first 20 epochs and γ = 0.999 later on, because the student improves quickly early in the training, and thus the teacher should have a shorter memory <ref type="bibr" target="#b25">[25]</ref>. In the ablation study (Section 4.4), we will show the effect of the three important hyper-parameters, namely M , the number of synthetic mini-batches, ρ, the number of samples with label replacement, and the threshold τ for data filtering. The value for all hyper-parameters are determined via validation.</p><p>For experiments on Clothing1M, we follow previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">24]</ref> and use the ResNet-50 <ref type="bibr" target="#b5">[6]</ref> pre-trained on ImageNet. For preprocessing, we resize the image to 256 × 256, crop the middle 224 × 224 as input, and perform normalization. We use a batch size k = 32, a step size α = 0.02, a learning rate β = 0.0008, and update θ using SGD with a momentum of 0.9 and a weight decay of 10 −3 . We train for 3 epochs for each iteration. During the first 2000 mini-batches in the initial iteration, we ramp up η from 0 to 0.0008, and set γ = 0.99. For the rest of the training, we use η = 0.0008 and γ = 0.999. Other hyperparameters are set as M = 10, ρ = 0.5k, and τ = 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on CIFAR-10</head><p>We compare the proposed MLNT with multiple baseline methods using CIFAR-10 dataset with symmetric label noise (noise ratio r = 0, 0.1, 0.3, 0.5, 0.7, 0.9) and asymmetric label noise (noise ratio r = 0.1, 0.2, 0.3, 0.4, 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The baselines include:</head><p>(1) Cross Entropy: conventional training without the metalearning update. We report both the results from <ref type="bibr" target="#b24">[24]</ref> and from our own implementation.</p><p>(2) Forward <ref type="bibr" target="#b15">[16]</ref>: forward loss correction based on the noise transition matrix. Both Forward <ref type="bibr" target="#b15">[16]</ref> and CNN-CRF <ref type="bibr" target="#b27">[27]</ref> require the ground-truth noise transition matrix, and Joint Optimiza-  tion <ref type="bibr" target="#b24">[24]</ref> requires the ground-truth class distribution among training data. Our method does not require any prior knowledge on the data, thus is more general. Note that all baselines use the same network architecture as our method. We report the numbers published in <ref type="bibr" target="#b24">[24]</ref>. <ref type="table" target="#tab_1">Table 1</ref> and <ref type="table" target="#tab_2">Table 2</ref> show the results for symmetric and asymmetric label noise, respectively. Our implementation of Cross Entropy has lower overall accuracy compared to <ref type="bibr" target="#b24">[24]</ref>. The reason could be the different programming frameworks used (we use PyTorch <ref type="bibr" target="#b14">[15]</ref>, whereas <ref type="bibr" target="#b24">[24]</ref> used Chainer <ref type="bibr" target="#b26">[26]</ref>). For both types of noise, the proposed MLNT method with one training iteration significantly improves accuracy compared to Cross Entropy (reproduced), and achieves comparable performance to state-of-the-art methods. Iterative training further improves the performance. MLNT-teacher after three iterations significantly outperforms previous methods. An exception where MLNT does not outperform baselines is with 50% asymmetric label noise. This is because that asymmetric label noise is generated by exchanging CAT and DOG classes, and it is theoretically impossible to distinguish them without prior knowledge when the noise ratio is 50%.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, we also show the results with clean training data (r = 0). The proposed MLNT can achieve an improvement of +1.68 in accuracy compared to Cross Entropy, which shows the regularization effect of the proposed meta-objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Progressive Comparison. <ref type="figure" target="#fig_2">Figure 3</ref> plots the model's accuracy on noisy training data and its test accuracy on clean test data as training proceeds. We show a representative training process using asymmetric label noise with r = 0.4. Accuracy is calculated every epoch, and training accuracy is computed across all mini-batches within the epoch. Comparing the proposed MLNT methods (1st iter.) with Cross Entropy, MLNT learns more quickly during the beginning of training, as shown by the higher test accuracy. Cross Entropy has the most unstable training process, as shown by its fluctuating test accuracy curve. MLNT-student is more stable because of the regularization from the metaobjective, whereas MLNT-teacher is extremely stable because its parameters change smoothly during training. At the 80th epoch, the learning rate is divided by 10, which   causes a drastic increase in both training and test accuracy for MLNT-student and Cross Entropy. After the 80th epoch, the model begins to overfit because of the small learning rate. However, the proposed MLNT-student suffers less overfitting compared to Cross Entropy, as shown by its lower training accuracy and higher test accuracy. Hyper-parameters. We conduct ablation study to examine the effect of three hyper-parameters: M, ρ, τ . M is the number of mini-batches {(X,Ŷ m )} M m=1 with synthetic noisy labels that we generate for each mini-batch (X, Y ) from the original training data. Intuitively, with larger M , the model is exposed to a wider variety of label noise, and thus can learn to be more noise-tolerant. In <ref type="figure" target="#fig_5">Figure 4</ref> we show the test accuracy on CIFAR-10 for MLNT-student (1st iter.) with M = 0, 5, 10, 15 (M = 0 is the same as Cross Entropy) trained using labels with symmetric noise (SN) and asymmetric noise (AN) of different ratio. The result shows that the accuracy indeed increases as M increases. The increase is most significant when M changes from 0 to 5, and is marginal when M changes from 10 to 15. Therefore, the experiments in this paper are conducted using M = 10, as a trade-off between the training speed and the model's performance.</p><p>ρ is the number of samples whose labels are changed in each synthetic mini-batchŶ m of size k. We experiment     with ρ = 0.1k, 0.2k, 0.3k, 0.4k, 0.5k, which correspond to 13, 26, 39, 51, 64 samples with a batch size of 128. <ref type="figure" target="#fig_6">Figure 5</ref> shows the performance of MLNT-student (1st iter.) using different ρ trained on CIFAR-10 with different ratio of asymmetric label noise. The performance is insensitive to the value of ρ. For different noise ratio, the optimal ρ generally falls into the range of [0.3k, 0.5k].</p><p>τ is the threshold to determine which samples are filtered out by the mentor model during the 2nd and 3nd training iteration. It controls the balance between the quality and quantity of the data that is used by the classification loss. In  <ref type="figure" target="#fig_8">Figure 6</ref> we show the performance of MLNT-student (2nd iter.) trained using different value of τ . As the noise ratio r increases, the optimal value of τ also increases to filter out more samples. <ref type="figure" target="#fig_9">Figure 7</ref> shows some example images from Clothing1M dataset that are filtered out and their corresponding probability scores given by the mentor model. Full optimization. We have been using a first-order approximation to optimize L meta for faster computation speed. Here we conduct experiments using full optimization by including the second-order derivative with respect to θ. <ref type="table" target="#tab_5">Table 3</ref> shows the comparison on four representative sets of experiments with different label noise. We show the test accuracy (averaged across 5 runs) of MLNT-student (1st iter.) trained with full optimization and first-order approximation. The result shows that the performance from first-order approximation is nearly the same as that obtained with full second derivatives. This suggests that the improvement of MLNT mostly comes from the gradients of the meta loss at the updated parameter values, rather than the secondorder gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Experiments on Clothing1M</head><p>We demonstrate the efficacy of the proposed method on real-world noisy labels using the Clothing1M dataset. The results are shown in <ref type="table" target="#tab_6">Table 4</ref>. We show the accuracy for baselines #1 and #3 reported in <ref type="bibr" target="#b24">[24]</ref>, and the accuracy for #2 reported in <ref type="bibr" target="#b15">[16]</ref>. The proposed MLNT method with one training iteration achieves better performance compared to state-of-the-art methods. After three training iterations, MLNT achieves a significant improvement in accuracy of +4.19 over Cross Entropy, and an improvement of +1.31 over the best baseline method #3.  Method Accuracy #1 Cross Entropy <ref type="bibr" target="#b24">[24]</ref> 69.15 Cross Entropy (reproduced) 69.28 #2 Forward <ref type="bibr" target="#b15">[16]</ref> 69.84 #3 Joint Optimization <ref type="bibr" target="#b24">[24]</ref> 72.16</p><p>MLNT-student (1st iter.) 72.34 MLNT-teacher (1st iter.) 72.08</p><p>MLNT-student (2nd iter.) 73.13 MLNT-teacher (2nd iter.) 73.10</p><p>MLNT-student (3nd iter.) 73.44 MLNT-teacher (3nd iter.) 73.47</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a meta-learning method to learn from noisy labeled data, where a meta-learning update is performed prior to conventional gradient update. The proposed meta-objective aims to find noise-tolerant model parameters that are less prone to overfitting. In the metatrain step, we generate multiple mini-batches with synthetic noisy labels, and use them to update the parameters. In the meta-test step, we apply a consistency loss between each updated model and a teacher model, and train the original parameters to minimize the total consistency loss. In addition, we propose an iterative training scheme, where the model from previous iteration is used to clean data and refine predictions. We evaluate the proposed method on two datasets. The results validate the advantageous performance of our method compared to state-of-the-art methods. For future work, we plan to explore using the proposed modelagnostic method to other domains with different model architectures, such as learning Recurrent Neural Networks for machine translation with corrupted ground-truth sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the proposed meta-learning based noise-tolerant (MLNT) training. For each mini-batch of training data, a meta</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 3 )</head><label>3</label><figDesc>CNN-CRF<ref type="bibr" target="#b27">[27]</ref>: a CRF model is proposed to represent the relationship between noisy and clean labels. It requires a small set of clean labels during training. (4) Joint Optimization<ref type="bibr" target="#b24">[24]</ref>: alternatively updates network parameters and corrects labels during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Test accuracy on clean test data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Progressive performance comparison of the proposed MLNT and Cross Entropy as training proceeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Performance of MLNT-student (1st iter.) on CIFAR-10 trained with different number of synthetic mini-batches M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Performance of MLNT-student (1st iter.) on CIFAR-10 trained with asymmetric label noise using different ρ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>r=0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Performance of MLNT-student (2nd iter.) on CIFAR-10 trained with asymmetric label noise using different τ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Example images from Clothing1M that are filtered out by the mentor model. We show the ground-truth label (red) and the label predicted by the mentor (blue) with their corresponding probability scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy (%) on CIFAR-10 test set for different methods trained with symmetric label noise. We report the mean and standard error across 5 runs.</figDesc><table><row><cell>Method</cell><cell>r = 0</cell><cell>r = 0.1</cell><cell>r = 0.3</cell><cell>r = 0.5</cell><cell>r = 0.7</cell><cell>r = 0.9</cell></row><row><cell>Cross Entropy [24]</cell><cell>93.5</cell><cell>91.0</cell><cell>88.4</cell><cell>85.0</cell><cell>78.4</cell><cell>41.1</cell></row><row><cell>Cross Entropy (reproduced)</cell><cell>91.84±0.05</cell><cell>90.33±0.06</cell><cell>87.85±0.08</cell><cell>84.62±0.08</cell><cell>78.06±0.16</cell><cell>45.85±0.91</cell></row><row><cell>Joint Optimization [24]</cell><cell>93.4</cell><cell>92.7</cell><cell>91.4</cell><cell>89.6</cell><cell>85.9</cell><cell>58.0</cell></row><row><cell>MLNT-student (1st iter.)</cell><cell>93.18±0.07</cell><cell>92.16±0.05</cell><cell>90.57±0.08</cell><cell>87.68±0.06</cell><cell>81.96±0.19</cell><cell>55.45±1.11</cell></row><row><cell>MLNT-teacher (1st iter.)</cell><cell>93.21±0.07</cell><cell>92.43±0.05</cell><cell>91.06±0.07</cell><cell>88.43±0.05</cell><cell>83.27±0.22</cell><cell>57.39±1.13</cell></row><row><cell>MLNT-student (2nd iter.)</cell><cell>93.24±0.09</cell><cell>92.63±0.07</cell><cell>91.99±0.13</cell><cell>89.71±0.07</cell><cell>86.28±0.19</cell><cell>58.21±1.09</cell></row><row><cell>MLNT-teacher (2nd iter.)</cell><cell>93.35±0.07</cell><cell>92.91±0.09</cell><cell>91.89±0.06</cell><cell>90.03±0.08</cell><cell>86.24±0.18</cell><cell>58.33±1.10</cell></row><row><cell>MLNT-student (3nd iter.)</cell><cell>93.29±0.08</cell><cell>92.91±0.10</cell><cell>92.02±0.09</cell><cell>90.27±0.10</cell><cell>86.95±0.17</cell><cell>58.57±1.12</cell></row><row><cell>MLNT-teacher (3nd iter.)</cell><cell>93.52±0.08</cell><cell>93.24±0.08</cell><cell>92.50±0.07</cell><cell>90.65±0.09</cell><cell>87.11±0.19</cell><cell>59.09±1.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy (%) on CIFAR-10 test set for different methods trained with asymmetric label noise. We report the mean</figDesc><table><row><cell>and standard error across 5 runs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>r = 0.1</cell><cell>r = 0.2</cell><cell>r = 0.3</cell><cell>r = 0.4</cell><cell>r = 0.5</cell></row><row><cell>Cross Entropy [24]</cell><cell>91.8</cell><cell>90.8</cell><cell>90.0</cell><cell>87.1</cell><cell>77.3</cell></row><row><cell>Cross Entropy (reproduced)</cell><cell>91.04±0.07</cell><cell>90.19±0.09</cell><cell>88.88±0.06</cell><cell>86.34±0.22</cell><cell>77.48±0.79</cell></row><row><cell>Forward [16]</cell><cell>92.4</cell><cell>91.4</cell><cell>91.0</cell><cell>90.3</cell><cell>83.8</cell></row><row><cell>CNN-CRF [27]</cell><cell>92.0</cell><cell>91.5</cell><cell>90.7</cell><cell>89.5</cell><cell>84.0</cell></row><row><cell>Joint Optimization [24]</cell><cell>93.2</cell><cell>92.7</cell><cell>92.4</cell><cell>91.5</cell><cell>84.6</cell></row><row><cell>MLNT-student (1st iter.)</cell><cell>92.89±0.11</cell><cell>91.84±0.10</cell><cell>90.55±0.09</cell><cell>88.70±0.13</cell><cell>79.95±0.71</cell></row><row><cell>MLNT-teacher (1st iter.)</cell><cell>93.05±0.10</cell><cell>92.19±0.09</cell><cell>91.47±0.04</cell><cell>88.69±0.08</cell><cell>78.44±0.45</cell></row><row><cell>MLNT-student (2nd iter.)</cell><cell>93.01±0.12</cell><cell>92.65±0.11</cell><cell>91.87±0.12</cell><cell>90.60±0.12</cell><cell>81.53±0.66</cell></row><row><cell>MLNT-teacher (2nd iter.)</cell><cell>93.33±0.13</cell><cell>92.97±0.11</cell><cell>92.43±0.19</cell><cell>90.93±0.15</cell><cell>81.47±0.54</cell></row><row><cell>MLNT-student (3nd iter.)</cell><cell>93.36±0.14</cell><cell>92.98±0.13</cell><cell>92.59±0.10</cell><cell>91.87±0.12</cell><cell>82.25±0.68</cell></row><row><cell>MLNT-teacher (3nd iter.)</cell><cell>93.61±0.10</cell><cell>93.25±0.12</cell><cell>92.82±0.18</cell><cell>92.30±0.10</cell><cell>82.09±0.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy (%) on CIFAR-10 for MNLT-student (1st iter.) with full optimization of the meta-loss and its first-order approximation.OptimizationSN AN r = 0.3 r = 0.7 r = 0.2 r = 0.4</figDesc><table><row><cell>First-order approx.</cell><cell>90.57</cell><cell>81.96</cell><cell>91.84</cell><cell>88.70</cell></row><row><cell>Full</cell><cell>90.74</cell><cell>82.05</cell><cell>91.89</cell><cell>88.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracy (%) of different methods on the Clothing1M test set.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research is supported by the National Research Foundation, Prime Minister's Office, Singapore under its Strategic Capability Research Centres Funding Initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Frénay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="845" to="869" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1919" to="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making risk minimization tolerant to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2309" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images. Mater&apos;s thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1928" to="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2233" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4331" to="4340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Brendan Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning from noisy largescale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6575" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8688" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
