<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie√üner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available -current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval.</p><p>arXiv:1702.04405v2 [cs.CV] 11 Apr 2017 1 A comprehensive and detailed overview of publicly-accessible RGB-D datasets is given by <ref type="bibr" target="#b20">[20]</ref> at http://www0.cs.ucl.ac.uk/ staff/M.Firman/RGBDdatasets/, which is updated on a regular basis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the introduction of commodity RGB-D sensors, such as the Microsoft Kinect, the field of 3D geometry capture has gained significant attention and opened up a wide range of new applications. Although there has been significant effort on 3D reconstruction algorithms, general 3D scene understanding with RGB-D data has only very recently started to become popular. Research along semantic understanding is also heavily facilitated by the rapid progress of modern machine learning methods, such as neural models. One key to successfully applying theses approaches is the availability of large, labeled datasets. While much effort has been made on 2D datasets <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b47">47]</ref>, where images can be downloaded from the web and directly annotated, the situation for 3D data is more challenging. Thus, many of the current RGB-D datasets <ref type="bibr" target="#b75">[74,</ref><ref type="bibr" target="#b93">92,</ref><ref type="bibr" target="#b78">77,</ref><ref type="bibr" target="#b32">32]</ref> are orders of magnitude smaller than their 2D counterparts. Typically, 3D deep learning methods use synthetic data to mitigate this lack of real-world data <ref type="bibr" target="#b92">[91,</ref><ref type="bibr" target="#b6">6]</ref>.</p><p>One of the reasons that current 3D datasets are small is because their capture requires much more effort, and effi- ciently providing (dense) annotations in 3D is non-trivial. Thus, existing work on 3D datasets often fall back to polygon or bounding box annotations on 2.5D RGB-D images <ref type="bibr" target="#b75">[74,</ref><ref type="bibr" target="#b93">92,</ref><ref type="bibr" target="#b78">77]</ref>, rather than directly annotating in 3D. In the latter case, labels are added manually by expert users (typically by the paper authors) <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b72">71]</ref> which limits their overall size and scalability.</p><p>In this paper, we introduce ScanNet, a dataset of richlyannotated RGB-D scans of real-world environments containing 2.5M RGB-D images in 1513 scans acquired in 707 distinct spaces. The sheer magnitude of this dataset is larger than any other <ref type="bibr" target="#b59">[58,</ref><ref type="bibr" target="#b82">81,</ref><ref type="bibr" target="#b93">92,</ref><ref type="bibr" target="#b76">75,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b72">71,</ref><ref type="bibr" target="#b32">32]</ref>. However, what makes it particularly valuable for research in scene understanding is its annotation with estimated calibration parameters, camera poses, 3D surface reconstructions, textured meshes, dense object-level semantic segmentations, and aligned CAD models (see <ref type="figure">Fig. 2</ref>). The semantic segmentations are more than an order of magnitude larger than any previous RGB-D dataset.</p><p>In the collection of this dataset, we have considered two main research questions: 1) how can we design a framework that allows many people to collect and annotate large Dataset Size Labels Annotation Tool Reconstruction CAD Models NYU v2 <ref type="bibr" target="#b59">[58]</ref> 464 scans 1449 frames 2D LabelMe-style <ref type="bibr" target="#b70">[69]</ref> none some <ref type="bibr" target="#b25">[25]</ref> TUM <ref type="bibr" target="#b82">[81]</ref> 47 scans none -aligned poses (Vicon) no SUN 3D <ref type="bibr" target="#b93">[92]</ref> 415 scans 8 scans 2D polygons aligned poses <ref type="bibr" target="#b93">[92]</ref> no SUN RGB-D <ref type="bibr" target="#b76">[75]</ref> 10k frames 10k frames 2D polygons + bounding boxes aligned poses <ref type="bibr" target="#b93">[92]</ref> no BuildingParser <ref type="bibr" target="#b2">[3]</ref> 265 rooms 265 rooms CloudCompare <ref type="bibr" target="#b24">[24]</ref> point cloud no PiGraphs <ref type="bibr" target="#b72">[71]</ref> 26 scans 26 scans dense 3D, by the authors <ref type="bibr" target="#b72">[71]</ref> dense 3D <ref type="bibr" target="#b63">[62]</ref> no SceneNN <ref type="bibr" target="#b32">[32]</ref> 100 scans 100 scans dense 3D, by the authors <ref type="bibr" target="#b61">[60]</ref> dense 3D <ref type="bibr" target="#b9">[9]</ref> no ScanNet (ours) 1513 scans 1513 scans dense 3D, crowd-sourced MTurk dense 3D <ref type="bibr" target="#b12">[12]</ref> yes 2.5M frames labels also proj. to 2D frames <ref type="table">Table 1</ref>. Overview of RGB-D datasets for 3D reconstruction and semantic scene understanding. Note that in addition to the 1513 scans in ScanNet, we also provided dense 3D reconstruction and annotations on all NYU v2 sequences.</p><p>amounts of RGB-D data, and 2) can we use the rich annotations and data quantity provided in ScanNet to learn better 3D models for scene understanding?</p><p>To investigate the first question, we built a capture pipeline to help novices acquire semantically-labeled 3D models of scenes. A person uses an app on an iPad mounted with a depth camera to acquire RGB-D video, and then we processes the data off-line and return a complete semantically-labeled 3D reconstruction of the scene. The challenges in developing such a framework are numerous, including how to perform 3D surface reconstruction robustly in a scalable pipeline and how to crowdsource semantic labeling. The paper discusses our study of these issues and documents our experience with scaling up RGB-D scan collection (20 people) and annotation (500 crowd workers).</p><p>To investigate the second question, we trained 3D deep networks with the data provided by ScanNet and tested their performance on several scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval. For the semantic voxel labeling task, we introduce a new volumetric CNN architecture.</p><p>Overall, the contributions of this paper are:</p><p>‚Ä¢ A large 3D dataset containing 1513 RGB-D scans of over 707 unique indoor environments with estimated camera parameters, surface reconstructions, textured meshes, semantic segmentations. We also provide CAD model placements for a subset of the scans. ‚Ä¢ A design for efficient 3D data capture and annotation suitable for novice users. ‚Ä¢ New RGB-D benchmarks and improved results for state-of-the art machine learning methods on 3D object classification, semantic voxel labeling, and CAD model retrieval.</p><p>‚Ä¢ A complete open source acquisition and annotation framework for dense RGB-D reconstructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Work</head><p>A large number of RGB-D datasets have been captured and made publicly available for training and benchmarking <ref type="bibr" target="#b57">[56,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b66">65,</ref><ref type="bibr" target="#b80">79,</ref><ref type="bibr" target="#b84">83,</ref><ref type="bibr" target="#b75">74,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b59">58,</ref><ref type="bibr" target="#b82">81,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b56">55,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b69">68,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b93">92,</ref><ref type="bibr" target="#b81">80,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b73">72,</ref><ref type="bibr" target="#b94">93,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b58">57,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b71">70,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b96">95,</ref><ref type="bibr" target="#b76">75,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b86">85,</ref><ref type="bibr" target="#b72">71,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b79">78,</ref><ref type="bibr" target="#b1">2]</ref>. <ref type="bibr" target="#b0">1</ref> These datasets have been used to train models for many 3D scene understanding tasks, including semantic segmentation <ref type="bibr" target="#b68">[67,</ref><ref type="bibr" target="#b59">58,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b87">86]</ref>, 3D object detection <ref type="bibr" target="#b74">[73,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b77">76,</ref><ref type="bibr" target="#b78">77]</ref>, 3D object classification <ref type="bibr" target="#b92">[91,</ref><ref type="bibr" target="#b54">53,</ref><ref type="bibr" target="#b67">66]</ref>, and others <ref type="bibr" target="#b95">[94,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref>.</p><p>Most RGB-D datasets contain scans of individual objects. For example, the Redwood dataset <ref type="bibr" target="#b10">[10]</ref> contains over 10,000 scans of objects annotated with class labels, 1,781 of which are reconstructed with KinectFusion <ref type="bibr" target="#b60">[59]</ref>. Since the objects are scanned in isolation without scene context, the dataset's focus is mainly on evaluating surface reconstruction quality rather than semantic understanding of complete scenes.</p><p>One of the earliest and most popular datasets for RGB-D scene understanding is NYU v2 <ref type="bibr" target="#b75">[74]</ref>. It is composed of 464 short RGB-D sequences, from which 1449 frames have been annotated with 2D polygons denoting semantic segmentations, as in LabelMe <ref type="bibr" target="#b70">[69]</ref>. SUN RGB-D <ref type="bibr" target="#b76">[75]</ref> follows up on this work by collecting 10,335 RGB-D frames annotated with polygons in 2D and bounding boxes in 3D. These datasets have scene diversity comparable to ours, but include only a limited range of viewpoints, and do not provide complete 3D surface reconstructions, dense 3D semantic segmentations, or a large set of CAD model alignments.</p><p>One of the first RGB-D datasets focused on long RGB-D sequences in indoor environments is SUN3D. It contains a set of 415 Kinect v1 sequences of 254 unique spaces. Although some objects were annotated manually with 2D polygons, and 8 scans have estimated camera poses based on user input, the bulk of the dataset does not include camera poses, 3D reconstructions, or semantic annotations.</p><p>Recently, Armeni et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref> introduced an indoor dataset containing 3D meshes for 265 rooms captured with a custom Matterport camera and manually labeled with semantic annotations. The dataset is high-quality, but the cap-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB-D Scanning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Reconstruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Upload</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation</head><p>Semantic Labeling Retrieval + Alignment Crowdsourcing <ref type="figure">Figure 2</ref>. Overview of our RGB-D reconstruction and semantic annotation framework. Left: a novice user uses a handheld RGB-D device with our scanning interface to scan an environment. Mid: RGB-D sequences are uploaded to a processing server which produces 3D surface mesh reconstructions and their surface segmentations. Right: Semantic annotation tasks are issued for crowdsourcing to obtain instance-level object category annotations and 3D CAD model alignments to the reconstruction.</p><p>ture pipeline is based on expensive and less portable hardware. Furthermore, only a fused point cloud is provided as output. Due to the lack of raw color and depth data, its applicability to research on reconstruction and scene understanding from raw RGB-D input is limited. The datasets most similar to ours are SceneNN <ref type="bibr" target="#b32">[32]</ref> and PiGraphs <ref type="bibr" target="#b72">[71]</ref>, which are composed of 100 and 26 densely reconstructed and labeled scenes respectively. The annotations are done directly in 3D <ref type="bibr" target="#b61">[60,</ref><ref type="bibr" target="#b72">71]</ref>. However, both scanning and labeling are performed only by expert users (i.e. the authors), limiting the scalability of the system and the size of the dataset. In contrast, we design our RGB-D acquisition framework specifically for ease-of-use by untrained users and for scalable processing through crowdsourcing. This allows us to acquire a significantly larger dataset with more annotations (currently, 1513 sequences are reconstructed and labeled).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset Acquisition Framework</head><p>In this section, we focus on the design of the framework used to acquire the ScanNet dataset ( <ref type="figure">Fig. 2</ref>). We discuss design trade-offs in building the framework and relay findings on which methods were found to work best for large-scale RGB-D data collection and processing.</p><p>Our main goal driving the design of our framework was to allow untrained users to capture semantically labeled surfaces of indoor scenes with commodity hardware. Thus the RGB-D scanning system must be trivial to use, the data processing robust and automatic, the semantic annotations crowdsourced, and the flow of data through the system handled by a tracking server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">RGB-D Scanning</head><p>Hardware. There is a spectrum of choices for RGB-D sensor hardware. Our requirement for deployment to large groups of inexperienced users necessitates a portable and low-cost RGB-D sensor setup. We use the Structure sensor <ref type="bibr" target="#b64">[63]</ref>, a commodity RGB-D sensor with design similar to the Microsoft Kinect v1. We attach this sensor to a handheld device such as an iPhone or iPad (see <ref type="figure">Fig. 2</ref> left) -results in this paper were collected using iPad Air2 devices. The iPad RGB camera data is temporally synchronized with the depth sensor via hardware, providing synchronized depth and color capture at 30 Hz. Depth frames are captured at a resolution of 640 √ó 480 and color at 1296 √ó 968 pixels. We enable auto-white balance and auto-exposure by default.</p><p>Calibration. Our use of commodity RGB-D sensors necessitates unwarping of depth data and alignment of depth and color data. Prior work has focused mostly on controlled lab conditions with more accurate equipment to inform calibration for commodity sensors (e.g., Wang et al. <ref type="bibr" target="#b88">[87]</ref>). However, this is not practical for novice users. Thus the user only needs to print out a checkerboard pattern, place it on a large, flat surface, and capture an RGB-D sequence viewing the surface from close to far away. This sequence, as well as a set of infrared and color frame pairs viewing the checkerboard, are uploaded by the user as input to the calibration. Our system then runs a calibration procedure based on <ref type="bibr" target="#b85">[84,</ref><ref type="bibr" target="#b14">14]</ref> to obtain intrinsic parameters for both depth and color sensors, and an extrinsic transformation of depth to color. We find that this calibration procedure is easy for users and results in improved data and consequently enhanced reconstruction quality.</p><p>User Interface. To make the capture process simple for untrained users, we designed an iOS app with a simple live RGB-D video capture UI (see <ref type="figure">Fig. 2 left)</ref>. The user provides a name and scene type for the current scan and proceeds to record a sequence. During scanning, a log-scale RGB feature detector point metric is shown as a "featurefulness" bar to provide a rough measure of tracking robustness and reconstruction quality in different regions being scanned. This feature was critical for providing intuition to users who are not familiar with the constraints and limitations of 3D reconstruction algorithms.</p><p>Storage. We store scans as compressed RGB-D data on the device flash memory so that a stable internet connection is not required during scanning. The user can upload scans to the processing server when convenient by pressing an "upload" button. Our sensor units used 128 GB iPad Air2 devices, allowing for several hours of recorded RGB-D video. In practice, the bottleneck was battery life rather than storage space. Depth is recorded as 16-bit unsigned short values and stored using standard zLib compression. RGB data is encoded with the H.264 codec with a high bitrate of 15 Mbps to prevent encoding artifacts. In addition to the RGB-D frames, we also record Inertial Measurement Unit (IMU) data, including acceleration, and angular velocities, from the Apple SDK. Timestamps are recorded for IMU, color, and depth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Surface Reconstruction</head><p>Once data has been uploaded from the iPad to our server, the first processing step is to estimate a denselyreconstructed 3D surface mesh and 6-DoF camera poses for all RGB-D frames. To conform with the goal for an automated and scalable framework, we choose methods that favor robustness and processing speed such that uploaded recordings can be processed at near real-time rates with little supervision.</p><p>Dense Reconstruction. We use volumetric fusion <ref type="bibr" target="#b11">[11]</ref> to perform the dense reconstruction, since this approach is widely used in the context of commodity RGB-D data. There is a large variety of algorithms targeting this scenario <ref type="bibr" target="#b60">[59,</ref><ref type="bibr" target="#b89">88,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b63">62,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b90">89,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b91">90,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b12">12]</ref>. We chose the BundleFusion system <ref type="bibr" target="#b12">[12]</ref> as it was designed and evaluated for similar sensor setups as ours, and provides real-time speed while being reasonably robust given handheld RGB-D video data.</p><p>For each input scan, we first run BundleFusion <ref type="bibr" target="#b12">[12]</ref> at a voxel resolution of 1 cm 3 . BundleFusion produces accurate pose alignments which we then use to perform volumetric integration through VoxelHashing <ref type="bibr" target="#b63">[62]</ref> and extract a high resolution surface mesh using the Marching Cubes algorithm on the implicit TSDF (4 mm 3 voxels). The mesh is then automatically cleaned up with a set of filtering steps to merge close vertices, delete duplicate and isolated mesh parts, and finally to downsample the mesh to high, medium, and low resolution versions (each level reducing the number of faces by a factor of two).</p><p>Orientation. After the surface mesh is extracted, we automatically align it and all camera poses to a common coordinate frame with the z-axis as the up vector, and the xy plane aligned with the floor plane. To perform this alignment, we first extract all planar regions of sufficient size, merge regions defined by the same plane, and sort them by normal (we use a normal threshold of 25 ‚Ä¢ and a planar offset threshold of 5 cm). We then determine a prior for the up vector by projecting the IMU gravity vectors of all frames into the coordinates of the first frame. This allows us to select the floor plane based on the scan bounding box and the normal most similar to the IMU up vector direction. Finally, we use a PCA on the mesh vertices to determine the rotation around the z-axis and translate the scan such that its bounds are within the positive octant of the coordinate system. Validation. This reconstruction process is automatically triggered when a scan is uploaded to the processing server and runs unsupervised. In order to establish a clean snapshot to construct the ScanNet dataset reported in this paper, we automatically discard scan sequences that are short, have high residual reconstruction error, or have low percentage of aligned frames. We then manually check for and discard reconstructions with noticeable misalignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semantic Annotation</head><p>After a reconstruction is produced by the processing server, annotation HITs (Human Intelligence Tasks) are issued on the Amazon Mechanical Turk crowdsourcing market. The two HITs that we crowdsource are: i) instancelevel object category labeling of all surfaces in the reconstruction, and ii) 3D CAD model alignment to the reconstruction. These annotations are crowdsourced using webbased interfaces to again maintain the overall scalability of the framework.</p><p>Instance-level Semantic Labeling. Our first annotation step is to obtain a set of object instance-level labels directly on each reconstructed 3D surface mesh. This is in contrast to much prior work that uses 2D polygon annotations on RGB or RGB-D images, or 3D bounding box annotations.</p><p>We developed a WebGL interface that takes as input the low-resolution surface mesh of a given reconstruction and a conservative over-segmentation of the mesh using a normalbased graph cut method <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b39">39]</ref>. The crowd worker then selects segments to annotate with instance-level object category labels (see <ref type="figure" target="#fig_1">Fig. 3</ref>). Each worker is required to annotate at least 25% of the surfaces in a reconstruction, and encouraged to annotate more than 50% before submission. Each scan is annotated by multiple workers (scans in ScanNet are annotated by 2.3 workers on average).</p><p>A key challenge in designing this interface is to enable efficient annotation by workers who have no prior experience with the task, or 3D interfaces in general. Our interface uses a simple painting metaphor where clicking and drag- . Crowdsourcing interface for aligning CAD models to objects in a reconstruction. Objects can be clicked to initiate an assisted search for CAD models (see list of bookshelves in middle). A suggested model is placed at the position of the clicked object, and the user then refines the position and orientation. A desk, chair, and nightstand have been already placed here.</p><p>ging over surfaces paints segments with a given label and corresponding color. This functions similarly to 2D painting and allows for erasing and modifying existing regions.</p><p>Another design requirement is to allow for freeform text labels, to reduce the inherent bias and scalability issues of pre-selected label lists. At the same time, it is desirable to guide users for consistency and coverage of basic object types. To achieve this, the interface provides autocomplete functionality over all labels previously provided by other workers that pass a frequency threshold (&gt; 5 annotations). Workers are always allowed to add arbitrary text labels to ensure coverage and allow expansion of the label set.</p><p>Several additional design details are important to ensure usability by novice workers. First, a simple distance check for connectedness is used to disallow labeling of disconnected surfaces with the same label. Earlier experiments without this constraint resulted in two undesirable behaviors: cheating by painting many surfaces with a few labels, and labeling of multiple object instances with the same label. Second, the 3D nature of the data is challenging for novice users. Therefore, we first show a full turntable rotation of each reconstruction and instruct workers to change the view using a rotating turntable metaphor. Without the turntable rotation animation, many workers only annotated from the initial view and never used camera controls despite the provided instructions.</p><p>CAD Model Retrieval and Alignment. In the second annotation task, a crowd worker was given a reconstruction already annotated with object instances and asked to place appropriate 3D CAD models to represent major objects in the scene. The challenge of this task lies in the selection of closely matching 3D models from a large database, and in precisely aligning each model to the 3D position of the corresponding object in the reconstruction.</p><p>We implemented an assisted object retrieval interface  <ref type="bibr" target="#b32">[32]</ref>). ScanNet has an order of magnitude more scans, with 3D surface mesh reconstructions covering more than ten times the floor and surface area, and with more than 36,000 annotated object instances.</p><p>where clicking on a previously labeled object in a reconstruction immediately searched for CAD models with the same category label in the ShapeNetCore <ref type="bibr" target="#b6">[6]</ref> dataset, and placed one example model such that it overlaps with the oriented bounding box of the clicked object (see <ref type="figure" target="#fig_2">Fig. 4</ref>). The worker then used keyboard and mouse-based controls to adjust the alignment of the model, and was allowed to submit the task once at least three CAD models were placed. Using this interface, we collected sets of CAD models aligned to each ScanNet reconstruction. Preliminary results indicate that despite the challenging nature of this task, workers select semantically appropriate CAD models to match objects in the reconstructions. The main limitation of this interface is due to the mismatch between the corpus of available CAD models and the objects observed in the ScanNet scans. Despite the diversity of the ShapeNet CAD model dataset (55K objects), it is still hard to find exact instance-level matches for chairs, desks and more rare object categories. A promising way to alleviate this limitation is to algorithmically suggest candidate retrieved and aligned CAD models such that workers can perform an easier verification and adjustment task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ScanNet Dataset</head><p>In this section, we summarize the data we collected using our framework to establish the ScanNet dataset. This dataset is a snapshot of available data from roughly one month of data acquisition by 20 users at locations in several countries. It has annotations by more than 500 crowd workers on the Mechanical Turk platform. Since the presented framework runs in an unsupervised fashion and people are continuously collecting data, this dataset continues to grow organically. Here, we report some statistics for an initial snapshot of 1513 scans, which are summarized in <ref type="table">Table 2</ref>. <ref type="figure" target="#fig_3">Fig. 5</ref> plots the distribution of scanned scenes over different types of real-world spaces. ScanNet contains a variety of spaces such as offices, apartments, and bathrooms. The dataset contains a diverse set of spaces ranging from small (e.g., bathrooms, closets, utility rooms) to large (e.g., apartments, classrooms, and libraries). Each scan has been annotated with instance-level semantic category labels through our crowdsourcing task. In total, we deployed 3,391 annotation tasks to annotate all 1513 scans.</p><p>The text labels used by crowd workers to annotate object instances are all mapped to the object category sets of NYU v2 <ref type="bibr" target="#b59">[58]</ref>, ModelNet <ref type="bibr" target="#b92">[91]</ref>, ShapeNet <ref type="bibr" target="#b6">[6]</ref>, and WordNet <ref type="bibr" target="#b18">[18]</ref> synsets. This mapping is made more robust by a preprocess that collapses the initial text labels through synonym and misspelling detection.</p><p>In addition to reconstructing and annotating the 1513 ScanNet scans, we have processed all the NYU v2 RGB-D sequences with our framework. The result is a set of dense reconstructions of the NYU v2 spaces with instance-level object annotations in 3D that are complementary in nature to the existing image-based annotations.</p><p>We also deployed the CAD model alignment crowdsourcing task to collect a total of 107 virtual scene interpretations consisting of aligned ShapeNet models placed on a subset of 52 ScanNet scans by 106 workers. There were a total of 681 CAD model instances (of 296 unique models) retrieved and placed on the reconstructions, with an average of 6.4 CAD model instances per annotated scan.</p><p>For more detailed statistics on this first ScanNet dataset snapshot, please see the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Tasks and Benchmarks</head><p>In this section, we describe the three tasks we developed as benchmarks for demonstrating the value of ScanNet data.</p><p>Train/Test split statistics. <ref type="table">Table 3</ref> shows the test and training splits of ScanNet in the context of the object classification and dense voxel prediction benchmarks. Note that our data is significantly larger than any existing comparable dataset. We use these tasks to demonstrate that Scan-Net enables the use of deep learning methods for 3D scene understanding tasks with supervised training, and compare performance to that using data from other existing datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">3D Object Classification</head><p>With the availability of large-scale synthetic 3D datasets such as <ref type="bibr" target="#b92">[91,</ref><ref type="bibr" target="#b6">6]</ref> and recent advances in 3D deep learn-  <ref type="table">Table 3</ref>. Train/Test split for object classification and dense voxel prediction tasks. Note that the number of instances does not include the rotation augmentation.</p><p>ing, research has developed approaches to classify objects using only geometric data with volumetric deep nets <ref type="bibr" target="#b92">[91,</ref><ref type="bibr" target="#b83">82,</ref><ref type="bibr" target="#b53">52,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b67">66]</ref>. All of these methods train on purely synthetic data and focus on isolated objects. Although they show limited evaluation on real-world data, a larger evaluation on realistic scanning data is largely missing. When training data is synthetic and test is performed on real data, there is also a significant discrepancy of test performance, as data characteristics, such as noise and occlusions patterns, are inherently different.</p><p>With ScanNet, we close this gap as we have captured a sufficiently large amount of 3D data to use real-world RGB-D input for both training and test sets. For this task, we use the bounding boxes of annotated objects in ScanNet, and isolate the contained geometry. As a result, we obtain local volumes around each object instance for which we know the annotated category. The goal of the task is to classify the object represented by a set of scanned points within a given bounding box. For this benchmark, we use 17 categories, with 9, 677 train instances and 2, 606 test instances.</p><p>Network and training. For object classification, we follow the network architecture of the 3D Network-in-Network of <ref type="bibr" target="#b67">[66]</ref>, without the multi-orientation pooling step. In order to classify partial data, we add a second channel to the 30 3 occupancy grid input, indicating known and unknown regions (with 1 and 0, respectively) according to the camera scanning trajectory. As in Qi et al. <ref type="bibr" target="#b67">[66]</ref>, we use an SGD solver with learning rate 0.01 and momentum 0.9, decaying the learning rate by half every 20 epochs, and training the model for 200 epochs. We augment training samples with 12 instances of different rotations (including both elevation and tilt), resulting in a total training set of 111, 660 samples.</p><p>Benchmark performance. As a baseline evaluation, we run the 3D CNN approach of Qi et al. <ref type="bibr" target="#b67">[66]</ref>. <ref type="table" target="#tab_3">Table 4</ref> shows the performance of 3D shape classification with different train and test sets. The first two columns show results on synthetic test data from ShapeNet <ref type="bibr" target="#b6">[6]</ref> including both complete and partial data. Naturally, training with the corresponding synthetic counterparts of ShapeNet provides the best performance, as data characteristics are shared. However, the more interesting case is real-world test data (right-most two columns); here, we show results on test sets of SceneNN <ref type="bibr" target="#b32">[32]</ref> and ScanNet. First, we see that training on synthetic data allows only for limited knowledge transfer (first two rows). Second, although the relatively small Sce-neNN dataset is able to learn within its own dataset to a reasonable degree, it does not generalize to the larger variety of environments found in ScanNet. On the other hand, training on ScanNet translates well to testing on SceneNN; as a result, the test results on SceneNN are significantly improved by using the training data from ScanNet. Interestingly enough, these results can be slightly improved when mixing training data of ScanNet with partial scans of ShapeNet (last row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic Test Sets</head><p>Real Test Sets   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Semantic Voxel Labeling</head><p>A common task on RGB data is semantic segmentation (i.e. labeling pixels with semantic classes) <ref type="bibr" target="#b49">[49]</ref>. With our data, we can extend this task to 3D, where the goal is to predict the semantic object label on a per-voxel basis. This task of predicting a semantic class for each visible 3D voxel has been addressed by some prior work, but using handcrafted features to predict a small number of classes <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b87">86]</ref>, or focusing on outdoor environments <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b5">5]</ref>.</p><p>Data Generation. We first voxelize a scene and obtain a dense voxel grid with 2cm 3 voxels, where every voxel stores its TSDF value and object class annotation (empty space and unlabeled surface points have their own respective classes). We now extract subvolumes of the scene volume, of dimension 2 √ó 31 √ó 31 √ó 62 and spatial extent 1.5m √ó 1.5m √ó 3m; i.e., a voxel size of ‚âà 4.8cm 3 ; the two channels represent the occupancy and known/unknown space according to the camera trajectory. These sample volumes are aligned with the xy-ground plane.For ground truth data generation, voxel labels are propagated from the scene voxelization to these sample volumes. The samples are chosen that ‚â• 2% of the voxels are occupied (i.e., on the surface), and ‚â• 70% of these surface voxels have valid annotations; samples not meeting these criteria are discarded. Across ScanNet, we generate 93, 721 subvolume examples for training, augmented by 8 rotations each (i.e., 749, 768 training samples), from 1201 training scenes. In addition, we extract 18, 750 sample volumes for testing, which are also augmented by 8 rotations each (i.e., 150, 000 test samples) from 312 test scenes. We have 20 object class labels plus 1 class for free space.</p><p>Network and training. For the semantic voxel labeling task, we propose a network which predicts class labels for a column of voxels in a scene according to the occupancy characteristics of the voxels' neighborhood. In order to infer labels for an entire scene, we use the network to predict a label for every voxel column at test time (i.e., every xy position that has voxels on the surface). The network takes as input a 2 √ó 31 √ó 31 √ó 62 volume and uses a series of fully convolutional layers to simultaneously predict class scores for the center column of 62 voxels. We use ReLU and batch normalization for all layers (except the last) in the network. To account for the unbalanced training data over the class labels, we weight the cross entropy loss with the inverse log of the histogram of the train data.</p><p>We use an SGD solver with learning rate 0.01 and momentum 0.9, decaying the learning rate by half every 20 epochs, and train the model for 100 epochs.</p><p>Quantitative Results. The goal of this task is to predict semantic labels for all visible surface voxels in a given 3D scene; i.e., every voxel on a visible surface receives one of the 20 object class labels. We use NYU2 labels, and list voxel classification results on ScanNet in <ref type="table">Table 7</ref>. We achieve an voxel classification accuracy of 73.0% over the set of 312 test scenes, which is based purely on the geometric input (no color is used).</p><p>In <ref type="table" target="#tab_5">Table 5</ref>, we show our semantic voxel labeling results on the NYU2 dataset <ref type="bibr" target="#b59">[58]</ref>. We are able to outperform previous methods which are trained on limited sets of real-world data using our volumetric classification network. For instance, Hermans et al. <ref type="bibr" target="#b31">[31]</ref> classify RGB-D frames using a dense random decision forest in combination with a conditional random field. Additionally, SemanticFusion <ref type="bibr" target="#b55">[54]</ref> uses a deep net trained on RGB-D frames, and regularize the predictions with a CRF over a 3D reconstruction of the frames; note that we compare to their classification results floor wall chair before the CRF regularization. SceneNet trains on a large synthetic dataset and fine-tunes on NYU2. Note that in contrast to Hermans et al. and SemanticFusion, neither we nor SceneNet use RGB information. Note that we do not explicitly enforce prediction consistency between neighboring voxel columns when the test volume is slid across the xy plane. This could be achieved with a volumetric CRF <ref type="bibr" target="#b65">[64]</ref>, as used in <ref type="bibr" target="#b87">[86]</ref>; however, our goal in this task to focus exclusively on the per-voxel classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">3D Object Retrieval</head><p>Another important task is retrieval of similar CAD models given (potentially partial) RGB-D scans. To this end, one wants to learn a shape embedding where a feature descriptor defines geometric similarity between shapes. The core idea is to train a network on a shape classification task where a shape embedding can be learned as byproduct of the classification task. For instance, Wu et al. <ref type="bibr" target="#b92">[91]</ref> and Qi et al. <ref type="bibr" target="#b67">[66]</ref> use this technique to perform shape retrieval queries within the ShapeNet database.</p><p>With ScanNet, we have established category-level correspondences between real-world objects and ShapeNet models. This allows us to train on a classification problem where both real and synthetic data are mixed inside of each cate-gory using real and synthetic data within shared class labels. Thus, we can learn an embedding between real and synthetic data in order to perform model retrieval for RGB-D scans. To this end, we use the volumetric shape classification network by Qi et al. <ref type="bibr" target="#b67">[66]</ref>, we use the same training procedure as in Sec. 5.1. Nearest neighbors are retrieved based on the 2 distance between the extracted feature descriptors, and measured against the ground truth provided by the CAD model retrieval task. In <ref type="table" target="#tab_4">Table 6</ref>, we show object retrieval results using objects from ScanNet to query for nearest neighbor models from ShapeNetCore. Note that training on ShapeNet and ScanNet independently results in poor retrieval performance, as neither are able to bridge the gap between the differing characteristics of synthetic and real-world data. Training on both ShapeNet and ScanNet together is able to find an embedding of shape similarities between both data modalities, resulting in much higher retrieval accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper introduces ScanNet: a large-scale RGB-D dataset of 1513 scans with surface reconstructions, instance-level object category annotations, and 3D CAD model placements. To make the collection of this data possible, we designed a scalable RGB-D acquisition and semantic annotation framework that we provide for the benefit of the community. We demonstrated that the richlyannotated scan data collected so far in ScanNet is useful in achieving state-of-the-art performance on several 3D scene understanding tasks; we hope that ScanNet will inspire future work on many other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Statistics and Comparisons</head><p>In this section, we provide thorough statistics on the construction and composition of ScanNet dataset, and also compare it to the most similar datasets from prior work. <ref type="figure" target="#fig_4">Fig. 6</ref> shows six example annotated reconstructions for a variety of spaces. For each reconstruction, the surface mesh with colors is shown, as well as a visualization with category labels for each object collected using our crowdsourced annotation interface. Category labels are consistent between spaces and are mapped to WordNet <ref type="bibr" target="#b18">[18]</ref> synsets. In addition to the category label, separate object instance labels are also available to indicate multiple instances of a given category, such as distinct chairs around a conference table in the fourth row of <ref type="figure" target="#fig_4">Fig. 6</ref>. <ref type="figure" target="#fig_5">Fig. 7</ref> shows a larger set of reconstructed spaces in Scan-Net to illustrate the variety of spaces that are part of the dataset. The scans range from small spaces with just a few objects (e.g., toilets), to large areas with dozens of objects (e.g., classrooms and studio apartments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Example Annotated Reconstructions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Dataset Construction Statistics</head><p>The construction of ScanNet was carried out with the RGB-D acquisition and annotation framework described in the main paper. In order to provide an intuition of the <ref type="table" target="#tab_3">ScanNet   Category  Count   wall  6226  chair  4279  floor  3212  table  2223  door  1181  couch  1048  cabinet  937  desk  733  shelf  732  bed  699  office chair  669  trashcan  561  pillow  490  sink  470  window  398  toilet  397  picture  351  bookshelf  328  monitor  308  curtain  280  computer  274  armchair  264  bathtub  253  coffee table  239  box  231  dining chair  230  refrigerator  226  book  221  lamp  218  towel  216  kitchen cabinet  203  drawer  202  tv  187  nightstand  182  counter  179  dresser  177  clothes  164  countertop  163  stool  130  plant  130  cushion  116  ceiling  114  bedframe  111  keyboard  107  end table  105  toilet paper  104  bag  104  backpack  100  blanket  94  dining table  94</ref> SceneNN <ref type="bibr" target="#b32">[32]</ref> Category <ref type="table" target="#tab_3">Count   chair  194  table  53  floor  44  seat  41  desk  39  monitor  31  sofa  25  cabinet  25  door  24  box  23  keyboard  23  trash bin  21  wall  20  pillow  19  fridge  18  stand  18  bag  17  bed  16  window  14  sink  13  printer  12  computer  12  chair01  12  desk1  11  monitor01  10  shelves  10  shelf  10  chair1  10  chair02  10  fan  9  basket  9  desk2  9  laptop  9  trashbin  9  kettle  9  microwave  9  monitor1  8  stove  8  chair2  8  bike  7  blanket  7  drawer  7  lamp  7  wall02  7  wall01  7  wall04  7  backpack  7  cup  7  chair3  7  whiteboard  7   Table 8</ref>. Total counts of annotated object instances of the 50 largest categories in ScanNet (left), and in SceneNN <ref type="bibr" target="#b32">[32]</ref> (right), the most similar annotated RGB-D reconstruction dataset. ScanNet contains far more annotated object instances, and the annotated labels are processed for consistency to remove duplicates such as "chair01" in SceneNN.</p><p>scalability of our framework, we report timing statistics for both the reconstruction and annotation steps. The median reconstruction processing time (including data conversion, dense voxel fusion, surface mesh extraction, align-  ment, cleanup, and preview thumbnail image rendering) is 11.3 min for each scene. A few outliers exist with significantly higher processing times (on the order of hours), due to unplanned processing server downtime during our data collection (mainly software updates), resulting in a higher mean reconstruction time of 14.9 min. After reconstruction is complete, each scan is annotated by several crowd workers on Amazon Mechanical Turk (2.3 workers on average per scan). The median annotation time per crowd worker is 12.0 min (mean time is 17.3 min, again due to a few outlier workers who take significantly longer). Aggregating the time taken across workers for annotating each of the 1513 scans in ScanNet, the median time per scan is 16.8 min, and the mean time per scan is 22.3 min.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Dataset Composition Statistics</head><p>The construction of the ScanNet dataset is motivated by the lack of large, annotated, densely reconstructed RGB-D dataset of 3D scenes that are publicly available in the academic community. Existing RGB-D datasets either have full scene-level annotations only for a subset of RGB-D frames (e.g., NYU v2 depth <ref type="bibr" target="#b59">[58]</ref>), or they focus on annotating decontextualized objects and not scenes (e.g., Choi et al. <ref type="bibr" target="#b10">[10]</ref>). The two datasets that do annotate densely reconstructed RGB-D spaces at the scene level are the SceneNN dataset by Hua et al. <ref type="bibr" target="#b32">[32]</ref> and the smaller PiGraphs dataset by Savva et al. <ref type="bibr" target="#b72">[71]</ref>.</p><p>SceneNN consists of 94 RGB-D scans captured using Asus Xtion Pro devices and reconstructed with the method of Choi et al. <ref type="bibr" target="#b9">[9]</ref>. The resulting densely-fused surface meshes are fully segmented at the level of meaningful objects. However, only a small set of segments are annotated with semantic labels. On the other hand, the PiGraphs <ref type="bibr" target="#b72">[71]</ref> dataset consists of 26 RGB-D scans captured with Kinect v1 devices and reconstructed with the VoxelHashing approach of Nie√üner et al. <ref type="bibr" target="#b63">[62]</ref>. This dataset has more complete and clean semantic labels, including object parts and object instances. However, it contains very few scenes and is limited in the variety of environments, consisting mostly of offices and conference rooms. To illustrate the large gap in quantity of annotated semantic labels between these two datasets and ScanNet, <ref type="figure">Fig. 8</ref> plots histograms of the total number of labeled object instances and the total numbers of unique semantic labels for each scan.</p><p>In order to demonstrate how our category labels map to other data, we plot the distribution of annotated object labels corresponded to the ShapeNetCore 3D CAD model categories in <ref type="figure">Fig. 9</ref>. This mapping is leveraged during our CAD model alignment and retrieval task to automatically suggest instances of CAD models from ShapeNet that match the label of a given object category in the reconstruction.</p><p>We can also obtain 2D annotations on the input RGB-D <ref type="figure">Figure 9</ref>. Top 25 most frequent annotation labels in ScanNet scans mapped to ShapeNetCore classes. ScanNet has thousands of 3D reconstructed instances of common objects such as chairs, tables, and cabinets.</p><p>sequences by projecting our 3D annotations into each frame using the corresponding camera pose. This way, we obtain an average of 76% annotation coverage of all pixels per scene by using the previously obtained 3D annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. NYUv2 Reconstruction and Comparison</head><p>Here, we discuss how ScanNet relates to NYUv2, one of the most popular RGB-D dataset with annotations. In order to compare the data in ScanNet with the data in NYUv2, we reconstructed and annotated all the RGB-D sequences in NYUv2 using our framework. (Note that for 9 sequences of the NYUv2 dataset, our framework did not obtain valid camera poses for &gt; 50% of the frames, so we did not compute reconstructions and annotations for these sequences.) Moreover, we created a set of surface mesh semantic annotations for the NYUv2 reconstructions by projecting every pixel of the annotated RGB-D frames with valid depth and label into world space using our computed camera poses, and assigning the corresponding object label to the closest surface mesh vertices (within 0.04cm, using a kd-tree lookup).</p><p>We then compare the total surface area of the reconstructed meshes that was annotated using projection from the annotated NYUv2 frames, and using our annotation pipeline. <ref type="figure" target="#fig_0">Fig. 10</ref> plots the percentage of reconstructed surfaces in NYUv2 that were annotated with each approach, as well as the percentage distribution for the ScanNet reconstructions for comparison. Note that we exclude the 9 sequences for which we do not have enough valid camera poses.</p><p>A noticeable difference between the RGB-D sequences in NYUv2 and those in ScanNet is that overall, the ScanNet sequences are more complete surface reconstructions of the real-world spaces. Most importantly, the NYUv2 original frames in general do not cover a sufficient number of viewpoints of the space to ensure full reconstruction of semantically meaningful complete objects. <ref type="figure" target="#fig_0">Fig. 11</ref> shows a comparison of several reconstructed scenes from NYUv2 RGB-D  sequences vs comparable reconstructions from ScanNet. As shown in the top-down views, the NYU reconstructions are much more sparse than the ScanNet reconstructions. This disparity makes a more direct comparison with ScanNet reconstructions hard to quantify. However, we can conclude that projecting the annotated NYUv2 RGB-D frames to reconstructions is not sufficient to semantically annotate the spaces, as is clear from the far lower surface coverage distribution for NYUv2 in <ref type="figure" target="#fig_0">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tasks</head><p>Here we provide more details about the 3D scene understanding tasks and benchmarks discussed in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Semantic Voxel Labeling</head><p>For the semantic voxel labeling task, we propose a network which predicts class labels for each column of a voxelized scene. As shown in <ref type="figure" target="#fig_0">Fig. 12</ref>, our network takes as input a 2 √ó 31 √ó 31 √ó 62 volume and uses a series of fully convolutional layers to simultaneously predict class scores for the center column of 62 voxels. We leverage information from the voxel neighborhood of both occupied space (voxels on the surface) and known space (voxels in front of a surface according to the camera trajectory) to describe the input partial data from a scan.</p><p>At test time, we slide the network through a scan through a voxelized scan along the xy-plane, and each column is predicted independently. <ref type="figure" target="#fig_0">Fig. 13</ref> visualizes several ScanNet test scans with voxel label predictions, alongside the ground truth annotations from our crowdsourced labeling task. For each NYU scene, we show an example color frame, the rough corresponding region of the view in the reconstructed scene (light blue box), and a top down view of the reconstruction. While NYUv2 reconstruction look complete from some viewpoints, much of the scene is left uncovered (see top down views). In constrast, ScanNet reconstruction have a much more complete coverage of the space and allow for denser annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dataset Acquisition Framework</head><p>This section provides more details for specific steps in our RGB-D data acquisition framework which was described in the main paper. To enable scalable dataset acquisition, we designed our data acquisition framework for 1) ease of use during capture, 2) robust reconstruction, 3) rapid crowdsourcing, 4) visibility into the collected data and its metadata. For 1) we developed an iPad app (see Ap- <ref type="bibr">Figure 12</ref>. Deep Neural Network architecture for our semantic voxel label prediction task. The network is mainly composed of 3D convolutions that process the geometry of a scene using a 3D voxel grid representation. <ref type="figure" target="#fig_0">Figure 14</ref>. Our RGB-D recording app on an iPad Air2 with attached Structure sensor (showing color stream at the top and depth stream at the bottom). The app allows novice users to record RGB-D videos and upload to a server for reconstruction and annotation. pendix C.1) with an easy-to-use interface, reasonable scanning presets, and minimalistic user controls. To ensure good reconstruction with minimal user interaction during scanning, we tested different exposure time settings and enabled auto white balancing (see Appendix C.1). We also established a simple calibration process that novice users could carry out (see Appendix C.2), and offloaded RGB-D reconstruction to the cloud (see Appendix C.3). Finally, we developed web-based UIs for crowdsourcing semantic annotation tasks as described in Appendix C.4, and for managing the collected data as described in Appendix C.6. <ref type="figure" target="#fig_0">Fig. 14</ref> shows our RGB-D recording app on the iPad. We designed an iPad app with a simple camera-based UI and a minimalistic set of controls. Before scanning, the user enters a user name, a scene name, and selects the type of room being scanned. The user then presses a single button to start and stop a scan recording. The interface can be toggled between visualizing the color stream and the depth stream overlaid on the color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. RGB-D Acquisition UI</head><p>We found that the most challenging part of scanning for novice users was acquiring an intuition as to what regions during scanning are likely to result in poor tracking and failed reconstruction. To alleviate this, we added a "progress bar"-style visualization during active scanning which indicates the featurefulness of the region being scanned. The bar ranges from full green, indicating high feature count, to near-empty black, indicating low feature count and high likelihood of tracking loss. This UI element was helpful for quickly familiarizing users with the scanning process. After scanning, the user can view a list of scans on the device and select to upload the scan data to a processing server. During upload, a progress bar is shown and scanning is disabled. Upon completion of the upload, the checksums of scan data on the server are verified against local data and the scans are automatically deleted to provide more memory for scanning.</p><p>Auto white balancing and Exposure Settings Another challenge towards performing reconstruction in uncontrolled scenarios is the wide variety of illumination conditions. Since our scanning app was designed for novice users, we opted to provide a reasonable set of presets and allow for manual override only when deemed necessary. By default, we enabled continuous automatic whitepoint balancing as implemented by the iOS SDK. We also enabled dynamic exposure selection again as implemented by the iOS SDK, but instructed users that they could manually adjust exposure if necessary to make overly dark locations brighter, or overly bright locations darker. The exposure setting can have a significant impact on the amount of motion blur during scanning. However, we found that inexperienced users preferred to rely on dynamic exposure, and typically moved relatively slowly during scanning, making motion blur less of an issue. The average exposure time during scans with dynamic exposure was close to 30 ms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Sensor Calibration</head><p>Sensor calibration is a critical, yet often overlooked part of RGB-D data acquisition. Our experiments showed that depth-to-color calibration is an important step in acquiring good 3D reconstructions from RGB-D sequences (see <ref type="figure" target="#fig_0">Fig. 15</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth To Color Calibration</head><p>To align a depth image D to color image C, we need to estimate intrinsic parameters of both sensors, the infrared camera K D and color camera K C , as well as extrinsic transformation T D‚ÜíC . In our experiments we have found that using the set of intrinsic parameters of focal length, center of projection, and two barrel distortion coefficients models worked well for the used cameras. To obtain calibration parameters K D and K C we capture a series of color-infrared pairs showing an asymmetric checkerboard grid. We then estimate calibration parameters for each camera with Matlab's CameraCalibrator application. During this procedure we additionally obtain the world positions of calibration grid corners, and use them to estimate the transformation T D‚ÜíC .</p><p>Depth Distortion Calibration Previous work suggests that for consumer-level depth cameras there exists depthdependent distortion that increases as camera moves away from the surface. Thus, we decided to augment our set of intrinsic parameters for depth cameras with a undistortion lookup table, as first suggested in Teichman et al. <ref type="bibr" target="#b85">[84]</ref>. This look up table is a function f (x, y, d), of spatial coordinates x, y and observed depth d, returning a multiplication factor m used to obtain undistorted depth d = md. The table is computed from training pairs of observed and ground truth depths d and d t . However, unlike Teichman's unsupervised approach, which produces training pairs using carefully taken 'calibration sequences', we decided to design a supervised approach similar to that of Di Cicco <ref type="bibr" target="#b14">[14]</ref>. However, we found that at large distances the depth distortion becomes so severe that approaches based on fitting planes to depth data are bound to fail. Thus to obtain training pairs {d, d t }, we capture a color-depth video sequence of a large flat wall with a calibration target at the center, as the user moves away and towards the wall. To ensure successfull calibration process user needs to ensure that the viewed wall is the only observed surface and that it covers the entire field of view. With the captured color-depth sequence and previously estimated K D , K C , T D‚ÜíC we can recover the the world positions of the calibration grid corners, effectively obtaining the ground truth plane locations for each of the captured depth images. For each pixel x, y with depth d, we then shoot a ray through x, y to intersect with the related plane. d t can be recovered from the point of intersection. The rest of our undistortion pipeline follows closely the that <ref type="figure" target="#fig_0">Figure 15</ref>. Comparison of calibration results. In the top row, we show results of calibration on a flat wall. As the distance increases the distortion becomes quite severe, motivating the need for depth distortion calibration. In the bottom row, we show results of frameto-frame tracking on raw and calibrated data. of Teichman et al. <ref type="bibr" target="#b85">[84]</ref>. We found that undistorting depth images obtained by a Structure sensor leads to significantly improved tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Surface Reconstruction</head><p>Given a calibrated RGB-D sequence as input, a fused 3D surface reconstruction is obtained using the BundleFusion framework <ref type="bibr" target="#b12">[12]</ref>, as described in the main paper. The reconstruction is then cleaned by merging vertices within 1 mm of each other, and removing connected components with fewer than 7500 triangles. Following this cleanup step, two quadric edge collapse decimation steps are performed to produce lower triangle count versions of each surface mesh. Each decimation halves the number of triangles in the surface mesh, reducing the size of the original meshes from an average of 146 MB to 5.82 MB for the low resolution mesh. The mesh decimation step is important for reducing data transfer requirements and improving loading times during the crowdsourced annotation using our webbased UI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Crowdsourced Annotation UI</head><p>We deployed our semantic annotation task to crowd workers on the Amazon Mechanical Turk platform. Each annotation task began with an introduction (see <ref type="figure" target="#fig_0">Fig. 16</ref>) providing a basic overview of the task. The worker was then shown a reconstruction and asked to paint all object instances with a color and corresponding label. The worker was required to annotate at least 25% of the surface area of the reconstruction, and encouraged to cover at least 50%. Once the worker was done, they could submit by pressing a button. Workers were compensated with $0.50 for each annotation task performed.</p><p>The CAD model retrieval and alignment task began with a view of an already semantically annotated reconstruction and asked workers to click on objects to retrieve and place appropriate CAD models. <ref type="figure" target="#fig_0">Fig. 17</ref> shows the initial instructions for an example reconstruction with several chairs. Workers for this task were required to place at least three objects before submitting. Once the worker was done, they were compensated with $1.00 for each completed task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Label cleaning and propagation</head><p>Labeling is performed on the surface mesh reconstruction, with several workers labeling each scan. To ensure that labels are consistent across workers, we use standard NLP techniques to clean up the labels. First, we use a manually curated list of good labels and their synonyms to compute a map to a single canonical label for each set, also including common misspellings by a small edit distance threshold of the given label. Labels with less than 5 counts are deemed unreliable and ignored in all statistics. Labels with more than 20 counts are manually examined and added to the list of good labels or collapsed as a synonym of a good label. The list of these frequent collapsed labels is also mapped to WordNet <ref type="bibr" target="#b18">[18]</ref> synsets when possible, and to other common label sets that are commonly used for RGB-D and 3D CAD data (NYUv2 <ref type="bibr" target="#b59">[58]</ref>, ModelNet <ref type="bibr" target="#b92">[91]</ref>, and ShapeNetCore <ref type="bibr" target="#b6">[6]</ref>).</p><p>Using the cleaned labels, we then compute an aggregated consensus labeling of each scene, since any individual crowdsourced annotation of a scene may not cover the entire scene, or may contain some errors. For each segment in the over-segmentation of a scene mesh, we first take the majority vote label. This groups together instances of the same class of objects, so we also compute a labeling purely based on geometric overlap; that is, we greedily take the unions of annotations which have ‚â• 50% overlap of segments. We then take the maximal intersections between these two labelings to obtain the final consensus.</p><p>After we have obtained the aggregated consensus semantic annotation for a scene, we then propagate these labels to the high-resolution mesh as well as to the 2D frames of the input RGB-D sequence. To propagate the labels to the high resolution mesh, we compute a kd-tree over the mesh vertices of the labeled coarse mesh, and we label each vertex of the high resolution mesh according to a nearest neighbor lookup in the kd-tree. We project the 3D semantic annotations to the input 2D frames by rendering the labeled mesh from the camera poses of each frame, and follow this with a joint dilation filter with the original RGB image and joint erosion filter with the original RGB image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6. Management UI</head><p>To enable scalability of our RGB-D acquisition and annotation, and continual transparency into the progress of scans throughout our framework, we created a web-based management UI to track and organize all data (see <ref type="figure" target="#fig_0">Fig. 19</ref>). When a user is finished scanning and presses the upload button on an iPad device, their scan data is automatically <ref type="bibr">Figure 16</ref>. Instructions provided to crowd workers for our semantic annotation task. Top: instructions before the beginning of the task. Bottom: interface instructions during annotation. uploaded to our processing server, placed into a reconstruction queue, and immediately made visible in the management UI. As the reconstruction proceeds through the var- <ref type="figure" target="#fig_0">Figure 17</ref>. Instructions provided to crowd workers for our CAD model alignment task. The worker clicks on colored objects to retrieve and place CAD models. <ref type="figure" target="#fig_0">Figure 18</ref>. ShapeNetCore <ref type="bibr" target="#b6">[6]</ref> CAD models retrieved and placed on ScanNet scans by crowd workers (scan mesh is transparent and CAD models are opaque). From top left clockwise: a classroom, bedroom, bathroom, and lounge scan. ious stages of data conversion, calibration, pose optimization and RGB-D fusion, alignment, cleanup, decimation, and segmentation, progress is visualized in the management UI. Thumbnail renderings of the generated surface reconstruction, and statistics such as total number of frames, reconstructed floor area etc. are automatically computed and can be used for filtering and sorting of the reconstructions. Similarly, during crowdsourced annotation, worker progress and aggregated annotated surface area statistics are visible and usable for sorting and filtering of the scan database. <ref type="figure" target="#fig_0">Figure 19</ref>. Our web-based data management UI for ScanNet scan data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example reconstructed spaces in ScanNet annotated with instance-level object category labels through our crowdsourced annotation framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Our web-based crowdsourcing interface for annotating a scene with instance-level object category labels. The right panel lists object instances already annotated in the scene with matching painted colors. This annotation is in progress at ‚âà 35%, with gray regions indicating unannotated surfaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>Figure 4. Crowdsourcing interface for aligning CAD models to objects in a reconstruction. Objects can be clicked to initiate an assisted search for CAD models (see list of bookshelves in middle). A suggested model is placed at the position of the clicked object, and the user then refines the position and orientation. A desk, chair, and nightstand have been already placed here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Distribution of the scans in ScanNet organized by type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Example annotated scans in ScanNet. Left: reconstructed surface mesh with original colors. Middle: color indicates category label consistently across all scans. Right: each object instance shown with a different randomly assigned color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>A variety of example annotated scans in ScanNet. Colors indicate category consistently across all scans.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .Figure 10 .</head><label>810</label><figDesc>Histograms of the total number of objects labeled per scan (top) and total number of unique labels per scan (bottom) in the PiGraphs<ref type="bibr" target="#b72">[71]</ref>, SceneNN<ref type="bibr" target="#b32">[32]</ref> and our dataset (ScanNet). The histograms show that ScanNet has many annotated objects over a larger number of scans, ranging in complexity with regards to the total number of objets per scan. Histograms of the percentage of total reconstruction surface area per scan that is semantically labeled for: NYU v2 reconstructions using projection of RGB-D annotated frames (left), for NYU v2 reconstructions using our 3D annotation interface (middle), and for ScanNet reconstructions similarly annotated with our interface (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Comparison of reconstructed Bathroom (top), Bedroom (middle), and Kitchen (bottom) from NYUv2 RGB-D frames (left), and a comparable reconstruction from ScanNet (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .</head><label>13</label><figDesc>Semantic voxel labeling of 3D scans in ScanNet using our 3D CNN architecture. Voxel colors indicate predicted or ground truth category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Retrieval from ShapeNet</cell></row><row><cell>Train</cell><cell>Top 1 NN</cell><cell>Top 3 NNs</cell></row><row><cell>ShapeNet</cell><cell>10.4%</cell><cell>8.0%</cell></row><row><cell>ScanNet</cell><cell>12.7%</cell><cell>11.7%</cell></row><row><cell>ShapeNet + ScanNet</cell><cell>77.5%</cell><cell>77.0%</cell></row></table><note>3D object classification benchmark performance. Per- centages give the classification accuracy over all models in each test set (average instance accuracy).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table /><note>3D model retrieval benchmark performance. Nearest neighbor models are retrieved for ScanNet objects from ShapeNet- Core. Percentages indicate average instance accuracy of retrieved model to query region.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Dense pixel classification accuracy on NYU2<ref type="bibr" target="#b59">[58]</ref>. Note that both SemanticFusion<ref type="bibr" target="#b55">[54]</ref> and Hermans et. al.<ref type="bibr" target="#b31">[31]</ref> use both geometry and color, and that Hermans et al. uses a CRF, unlike our approach which is geometry-only and has only unary predictions. The reported SemanticFusion classification is on the 13 class task (13 class average accuracy of 58.9%).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">table window</cell><cell>bed</cell><cell>sofa</cell><cell>tv</cell><cell>objs. furn. ceil.</cell><cell>avg.</cell></row><row><cell cols="2">Hermans et al. [31]</cell><cell>91.5 71.8</cell><cell>41.9</cell><cell>27.7</cell><cell>46.1</cell><cell cols="3">68.4 28.5 38.4</cell><cell>8.6</cell><cell>37.1 83.4</cell><cell>49.4</cell></row><row><cell cols="2">SemanticFusion [54]  *</cell><cell>92.6 86.0</cell><cell>58.4</cell><cell>34.0</cell><cell>60.5</cell><cell cols="4">61.7 47.3 33.9 59.1</cell><cell>63.7 43.4</cell><cell>58.2</cell></row><row><cell>SceneNet [28]</cell><cell></cell><cell>96.2 85.3</cell><cell>61.0</cell><cell>43.8</cell><cell>30.0</cell><cell cols="4">72.5 62.8 19.4 50.0</cell><cell>60.4 74.1</cell><cell>59.6</cell></row><row><cell cols="3">Ours (ScanNet + NYU) 99.0 55.8</cell><cell>67.6</cell><cell>50.9</cell><cell>63.1</cell><cell cols="4">81.4 67.2 35.8 34.6</cell><cell>65.6 46.2</cell><cell>60.7</cell></row><row><cell>Class</cell><cell cols="2">% of Test Scenes</cell><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Floor</cell><cell></cell><cell>35.7%</cell><cell>90.3%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wall</cell><cell></cell><cell>38.8%</cell><cell>70.1%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chair</cell><cell></cell><cell>3.8%</cell><cell>69.3%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sofa</cell><cell></cell><cell>2.5%</cell><cell>75.7%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Table</cell><cell></cell><cell>3.3%</cell><cell>68.4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Door</cell><cell></cell><cell>2.2%</cell><cell>48.9%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cabinet</cell><cell></cell><cell>2.4%</cell><cell>49.8%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bed</cell><cell></cell><cell>2.0%</cell><cell>62.4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Desk</cell><cell></cell><cell>1.7%</cell><cell>36.8%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Toilet</cell><cell></cell><cell>0.2%</cell><cell>69.9%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sink</cell><cell></cell><cell>0.2%</cell><cell>39.4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Window</cell><cell></cell><cell>0.4%</cell><cell>20.1%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Picture</cell><cell></cell><cell>0.2%</cell><cell>3.4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bookshelf</cell><cell></cell><cell>1.6%</cell><cell>64.6%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Curtain</cell><cell></cell><cell>0.7%</cell><cell>7.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Shower Curtain</cell><cell></cell><cell>0.04%</cell><cell>46.8%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Counter</cell><cell></cell><cell>0.6%</cell><cell>32.1%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Refrigerator</cell><cell></cell><cell>0.3%</cell><cell>66.4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bathtub</cell><cell></cell><cell>0.2%</cell><cell>74.3%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OtherFurniture</cell><cell></cell><cell>2.9%</cell><cell>19.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell></cell><cell>-</cell><cell>73.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>0% Table 7. Semantic voxel label prediction accuracy on ScanNet test scenes.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project is funded by Google Tango, Intel, NSF (IIS-1251217 and VEC 1539014/1539099), and a Stanford Graduate fellowship. We also thank Occipital for donating structure sensors and Nvidia for hardware donations, as well as support by the Max-Planck Center for Visual Computing and the Stanford CURIS program. Further, we thank Toan Vuong, Joseph Chang, and Helen Jiang for help on the mobile scanning app and the scanning process, and Hope Casey-Allen and Duc Nugyen for early prototypes of the annotation interfaces. Last but not least, we would like to thank all the volunteers who helped with scanning and get-ting us access to scanning spaces.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A global hypotheses verification method for 3D object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aldoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="511" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D semantic parsing of largescale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Re-identification with RGB-D sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale semantic 3d reconstruction: an adaptive multi-resolution model for multi-class volumetric labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3176" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3D model repository</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable real-time volumetric surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bautembach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">113</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-label semantic 3d reconstruction using voxel blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cherabier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>H√§ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust reconstruction of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02481</idno>
		<title level="m">A large dataset of object scans</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 23rd annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">BundleFusion: Real-time globally consistent 3D reconstruction using on-the-fly surface re-integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zoll√∂fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01093</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Shape completion using 3d-encoder-predictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00101</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Non-parametric calibration for depth sensors. Robotics and Autonomous Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Di</forename><surname>Cicco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Iocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grisetti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An evaluation of the RGB-D SLAM system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2012 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1691" to="1696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spoofing in 2D face recognition with 3D masks and anti-spoofing with Kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics: Theory, Applications and Systems (BTAS), 2013 IEEE Sixth International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wordnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley Online Library</publisher>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">RGBD datasets: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Large Scale 3D Data: Acquisition, Modelling and Analysis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Instructing people for training gestural interactive systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fothergill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mentis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data-driven 3D primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3392" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unfolding an indoor origami world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="687" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">CloudCompare3D point cloud and mesh processing software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Girardeau-Montaut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">OpenSource Project</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Support surface prediction in indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel√°ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Scenenet: Understanding real world indoor scenes with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07041</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1524" to="1531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recovering free space of indoor scenes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2807" to="2814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dense 3D semantic mapping of indoor scenes from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Floros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2631" to="2638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SceneNN: A scene meshes dataset with annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Innmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh√∂fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volumedeform</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08161</idno>
		<title level="m">Real-time volumetric non-rigid reconstruction</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2220" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A category-level 3D object dataset: Putting the Kinect to work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Janoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer Depth Cameras for Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="141" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very high frame rate volumetric integration of depth images on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuheng Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1241" to="1250" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time large-scale dense 3D reconstruction with loop closure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>K√§hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="500" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Object discovery in 3D scenes via shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2088" to="2095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fall detection using ceilingmounted 3D depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kepski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kwolek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Theory and Applications (VISAPP), 2014 International Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="640" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3d scene understanding by voxel-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1425" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Chisel: Real time large scale 3D reconstruction onboard a mobile device using spatially hashed signed distance fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klingensmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dryanovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from RGB-D videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="951" to="970" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Database-assisted object retrieval for real-time 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="435" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3D object detection with RGBD cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning discriminative representations from RGB-D video data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">People tracking in RGB-D data with on-line boosted target models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3844" to="3849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Object disappearance for object discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2836" to="2843" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Object detection and classification from large-scale cluttered indoor scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mattausch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Panozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pajarola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Semanticfusion: Dense 3d semantic mapping with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05130</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">When can we use KinectFusion for ground truth acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>H√§mmerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Color-Depth Camera Fusion in Robotics, IROS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On the repeatability and quality of keypoints for local feature-based 3D object retrieval from cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="348" to="361" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">KinectFaceDB: A Kinect database for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1534" to="1548" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">KinectFusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE international symposium on</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
	<note>Mixed and augmented reality (ISMAR)</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">A robust 3D-2D interactive tool for scene segmentation and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05883</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">RGBD-HuDaAct: A colordepth video database for human daily activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer Depth Cameras for Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Real-time 3D reconstruction at scale using voxel hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh√∂fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">The structure sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Occipital</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Phillip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Tracking a depth camera: Parameter exploration for fast ICP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Magnenat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3824" to="3829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Volumetric and multi-view CNNs for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03265</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">RGB-(D) scene labeling: Features and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2759" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Segmentation of unknown objects in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richtsfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>M√∂rwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prankl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zillich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4791" to="4796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">LabelMe: a database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">SceneGrok: Inferring action maps in 3D environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">212</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">PiGraphs: Learning interaction snapshots from observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2930" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Building part-based object detectors via 3D geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1745" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Indoor scene segmentation using a structured light sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision -Workshop on 3D Representation and Recognition</title>
		<meeting>the International Conference on Computer Vision -Workshop on 3D Representation and Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3D object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="634" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Deep sliding shapes for amodal 3D object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02300</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08974</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">People detection in RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3838" to="3843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing</title>
		<meeting>the 2013 ACM international joint conference on Pervasive and ubiquitous computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of RGB-D SLAM systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="573" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Human activity detection from RGBD images. plan, activity, and intent recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Unsupervised intrinsic calibration of depth sensors via SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Teichman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">248</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Learning to navigate the energy landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05772</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Se-manticPaint: Interactive 3D labeling and learning at your fingertips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">154</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Online reconstruction of indoor scenes from RGB-D streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3271" to="3279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Kintinuous: Spatially extended KinectFusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Johannsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">ElasticFusion: Dense SLAM without a pose graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Salas-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Robotics: Science and Systems</title>
		<meeting>Robotics: Science and Systems<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">ElasticFusion: Real-time dense SLAM and light source estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Salas-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">0278364916669237</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">SUN3D: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Automatic registration of RGB-D scans via salient directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2808" to="2815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Estimating the 3D layout of indoor scenes and its clutter from depth sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1273" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Shading-based refinement on volumetric signed distance functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollh√∂fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Innmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie√üner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
