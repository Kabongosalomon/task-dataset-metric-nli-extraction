<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Ultra-Scalable Spectral Clustering and Ensemble Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Dong</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Chang-Dong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jian-Sheng</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Chee-Keong</forename><surname>Kwoh</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING 1 Ultra-Scalable Spectral Clustering and Ensemble Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Data clustering</term>
					<term>Large-scale clustering</term>
					<term>Spectral clustering</term>
					<term>Ensemble clustering</term>
					<term>Large-scale datasets</term>
					<term>Nonlinearly separable datasets !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper focuses on scalability and robustness of spectral clustering for extremely large-scale datasets with limited resources. Two novel algorithms are proposed, namely, ultra-scalable spectral clustering (U-SPEC) and ultra-scalable ensemble clustering (U-SENC). In U-SPEC, a hybrid representative selection strategy and a fast approximation method for K-nearest representatives are proposed for the construction of a sparse affinity sub-matrix. By interpreting the sparse sub-matrix as a bipartite graph, the transfer cut is then utilized to efficiently partition the graph and obtain the clustering result. In U-SENC, multiple U-SPEC clusterers are further integrated into an ensemble clustering framework to enhance the robustness of U-SPEC while maintaining high efficiency. Based on the ensemble generation via multiple U-SEPC's, a new bipartite graph is constructed between objects and base clusters and then efficiently partitioned to achieve the consensus clustering result. It is noteworthy that both U-SPEC and U-SENC have nearly linear time and space complexity, and are capable of robustly and efficiently partitioning ten-million-level nonlinearly-separable datasets on a PC with 64GB memory. Experiments on various large-scale datasets have demonstrated the scalability and robustness of our algorithms. The MATLAB code and experimental data are available at https://www.researchgate.net/publication/330760669.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D ATA clustering is a fundamental problem in the field of data mining and machine learning <ref type="bibr" target="#b0">[1]</ref>, whose purpose is to partition a set of objects into a certain number of homogeneous groups, each referred to as a cluster. Out of the large number of clustering algorithms that have been developed, spectral clustering in recent years has been gaining increasing attention due to its promising ability in dealing with nonlinearly separable datasets <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. However, a critical limitation to conventional spectral clustering lies in its huge time and space complexity, which significantly restricts its application to large-scale problems.</p><p>Conventional spectral clustering typically consists of two time-and memory-consuming phases, namely, affinity matrix construction and eigen-decomposition. It generally takes O(N 2 d) time and O(N 2 ) memory to construct the affinity matrix, and takes O(N 3 ) time and O(N 2 ) memory to solve the eigen-decomposition problem <ref type="bibr" target="#b1">[2]</ref>, where N is the data size and d is the dimension. As the data size N increases, the computational burden of spectral clustering grows dramatically. For example, given a dataset with one million objects, the N ×N affinity matrix alone will consume 7450.58 GB of memory (with each entry in the matrix stored as a double-precision value), which prohibitively exceeds the memory capacity of a general-purpose machine, not to mention the next phase of eigen-decomposition.</p><p>To alleviate the huge computational burden of spectral clustering, a commonly used strategy is to sparsify the affinity matrix and solve the eigen-decomposition problem by some sparse eigen-solvers <ref type="bibr" target="#b1">[2]</ref>. The matrix sparsification strategy can reduce the memory cost of storing the affinity matrix and facilitate the eigen-decomposition, but it still requires the computation of all entries in the original affinity matrix. Besides matrix sparsification, another widelystudied strategy is based on sub-matrix construction <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The Nyström method <ref type="bibr" target="#b2">[3]</ref> randomly selects p representatives from the original dataset and builds an N × p affinity submatrix. Cai et al. <ref type="bibr" target="#b3">[4]</ref> extended the Nyström method and proposed the landmark based spectral clustering (LSC) method, which performs k-means on the dataset to get p cluster centers as the p representatives. However, these sub-matrix based spectral clustering methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> are typically restricted by an O(N p) complexity bottleneck, which has been a critical hurdle for them to deal with extremely large-scale dataset where a larger p is often desired for achieving better approximation <ref type="bibr" target="#b3">[4]</ref>. Moreover, the clustering results of these methods heavily rely on their one-shot approximation via the sub-matrix, which places an unstable factor on their clustering robustness. Despite the considerable efforts that have been made in recent years <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, it remains a highly challenging problem how to enable spectral clustering to efficiently and robustly cluster extremely large-scale datasets (which may even be nonlinearly separable) with arXiv:1903.01057v2 <ref type="bibr">[cs.</ref>LG] 5 Mar 2019 rather limited computing resources.</p><p>In light of this, this paper focuses on scalability and robustness of spectral clustering for extremely larger-scale datasets. Specifically, this paper proposes two novel largescale algorithms, namely, ultra-scalable spectral clustering (U-SPEC) and ultra-scalable ensemble clustering (U-SENC). In U-SPEC, a new hybrid representative selection strategy is presented to efficiently find a set of p representatives, which reduces the time complexity of k-means based selection from O(N pdt) to O(p 2 dt). Then, a fast approximation method for K-nearest representatives are designed to efficiently build a sparse sub-matrix with O(N p 1 2 d) time and O(N p 1 2 ) memory. With the sparse sub-matrix serving as the cross-affinity matrix, a bipartite graph is constructed between the dataset and the representative set. By taking advantage of the bipartite graph structure, the transfer cut <ref type="bibr" target="#b5">[6]</ref> is utilized to solve the eigen-decomposition problem with O(N K(K + k) + p 3 ) time, where k is the number of clusters and K is the number of nearest representatives. Finally, the k-means discretization is adopted to construct the clustering result from a set of k eigenvectors, which takes O(N k 2 t) time. As it generally holds that k, K p N , the time and space complexity of our U-SPEC algorithm are respectively dominated by O(N p 1 2 d) and O(N p <ref type="bibr">1 2</ref> ). Further, to go beyond the one-shot approximation of U-SPEC and provide better clustering robustness, the U-SENC algorithm is proposed by integrating multiple U-SPEC clusterers into a unified ensemble clustering framework, whose time and space complexity are respectively dominated by O(N mp 1 2 d) and O(N p <ref type="bibr">1 2</ref> ). Extensive experiments have been conducted on ten large-scale datasets (including five synthetic datasets and five real datasets), which have shown the superiority of our U-SPEC and U-SENC algorithms over the state-of-theart in terms of both clustering robustness and scalability.</p><p>To summarize, the main contributions of this paper are listed as follows: 1) A hybrid representative selection strategy is proposed to strike a balance between the efficiency of random selection and the effectiveness of k-means based selection. 2) A fast approximation method for K-nearest representatives is designed, which is time-and memoryefficient for constructing the sparse affinity submatrix between objects and representatives. 3) A large-scale spectral clustering algorithm termed U-SPEC is developed based on efficient affinity submatrix construction and bipartite graph formulation. Its time and space complexity are dominated by O(N p ) respectively. The notations that are used throughout the paper are summarized in <ref type="table" target="#tab_0">Table 1</ref>. The rest of the paper is organized as follows. The related work on large-scale spectral clustering and ensemble clustering is reviewed in Section 2. The proposed U-SPEC and U-SENC algorithms are described in The i-th representatives in R RC</p><p>The set of rep-clusters rc i</p><p>The i-th rep-cluster in RC y i Center of rc i z <ref type="bibr" target="#b0">1</ref> Number of rep-clusters in RC z <ref type="bibr" target="#b1">2</ref> Average number of objects in each rep-cluster K Number of nearest representatives K Candidate neighborhood size around a representative Dist(x i , rc j ) Distance between object x i and rep-cluster rc j G A bipartite graph between X and R B</p><p>Cross-affinity matrix of graph G. b ij</p><p>The (i, j)-th entry of B E Full affinity matrix of graph G L Graph Laplacian of graph G D Degree matrix of graph G u i</p><p>The i-th eigenvector of graph G γ i</p><p>The i-th eigenvalue of graph G G R A small graph with R as the node set</p><formula xml:id="formula_0">E R Affinity matrix of graph G R L R Graph Laplacian of graph G R D R Degree matrix of graph G R v i</formula><p>The i-th eigenvector of graph G R λ i</p><p>The i-th eigenvalue of graph G R D X Diagonal matrix with its (i, i)-th entry being the sum of the i-th row of B T Transition probability matrix <ref type="bibr">Π</ref> The ensemble of m base clusterings π i</p><p>The i-th base clustering in Π m</p><p>Number of base clusterings in Π U-SPEC i The clusterer to generate the i-th base clustering R i</p><p>The set of representatives in U-SPEC i r i j The j-th representatives in R i k i Number of clusters in π i k min Minimum number of clusters in a base clustering kmax Maximum number of clusters in a base clustering τ Random variable in [0, 1] C Set of all clusters in Π C i</p><p>The i-th cluster in C kc Number of clusters in C G A bipartite graph between X and C B</p><p>Cross-affinity matrix of graphG. b ij</p><p>The (i, j)-th entry ofB u i</p><p>The i-th eigenvector of graphG D X Diagonal matrix with its (i, i)-th entry being the sum of the i-th row ofB G C A small graph with C as the node set E C Affinity matrix of graph G C L C Graph Laplacian of graph G C D C Degree matrix of graph G C v i</p><p>The i-th eigenvector of graph G C λ i</p><p>The i-th eigenvalue of graph G C Section 3. The experimental results are reported in Section 4. Finally, the paper is concluded in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review the literature related to spectral clustering and ensemble clustering, with special emphasis on their recent large-scale extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spectral Clustering</head><p>Given a dataset of N objects, conventional spectral clustering <ref type="bibr" target="#b1">[2]</ref> first computes an N ×N affinity matrix, in which each entry corresponds to the similarity of two objects according to some similarity metrics. Then, the eigen-decomposition is performed on the graph Laplacian of the affinity matrix to obtain the k eigenvectors associated with the first k eigenvalues. By embedding the datasets into the low-dimensional space via the obtained k eigenvectors, the final clustering can be achieved via k-means or some other discretization techniques <ref type="bibr" target="#b1">[2]</ref>. Although spectral clustering has shown promising advantages in finding clusters of arbitrary shapes from complex data, its O(N 3 ) time complexity and O(N 2 ) space complexity significantly restrict its application in large-scale tasks. To alleviate the huge computational cost, some researchers sparsified the affinity matrix by considering Knearest neighbors or -neighbors, and then solved the eigendecomposition problem by some sparse eigen-solvers <ref type="bibr" target="#b1">[2]</ref>, which, however, still requires the computation of all the entries in the original affinity matrix.</p><p>To avoid the computation of the full affinity matrix, the sub-matrix based approximation has emerged as a powerful and efficient tool for spectral clustering <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. The Nyström approximation <ref type="bibr" target="#b2">[3]</ref> randomly selects p representatives from the dataset and builds an N × p affinity submatrix between the N objects and the p representatives. The sub-matrix construction takes O(N pd) time and O(N p) memory, which are much lower than the full affinity matrix construction. Although the random representative selection is very efficient, it is often unstable with regard to the quality of the selected representatives (see <ref type="figure" target="#fig_2">Fig. 1</ref>). Moreover, while it has been shown that a larger p is often favorable for better approximation <ref type="bibr" target="#b2">[3]</ref>, the O(N p) memory cost of the sub-matrix construction can still be a critical bottleneck when dealing with very large datasets. To address the potential instability of random selection, Cai and Chen <ref type="bibr" target="#b3">[4]</ref> proposed the LSC algorithm, which first partitions the dataset into p clusters via k-means and then uses the p cluster centers as the representatives. With the N × p submatrix constructed, they further sparsified it by preserving the K-nearest representatives for each row and zeroing out the others <ref type="bibr" target="#b3">[4]</ref>. Despite its progress over the previous methods, there are still three computational bottlenecks in the LSC algorithm <ref type="bibr" target="#b3">[4]</ref>. First, although the k-means based selection often provides a better set of representatives, it comes with the time complexity of O(N pdt). Second, the calculation of all possible entries in the N × p sub-matrix is still required before the sparsification, which comes with the time complexity of O(N pd). Third, the computation of the K-nearest representatives for all objects comes with the time complexity of O(N pK). More recently, instead of using p representatives, He et al. <ref type="bibr" target="#b4">[5]</ref> used Fourier features to represent data objects in kernel space, and built an N × p sub-matrix between the N objects and the p selected Fourier features, upon which the efficient eigen-decomposition can be performed. The time and space complexity of the fast explicit spectral clustering (FastESC) algorithm in <ref type="bibr" target="#b4">[5]</ref> are respectively O(N pd+p 3 ) and O(N p), which are still restricted by the O(N p) complexity bottleneck. By incorporating a newly-designed positive Euler kernel, Wu et al. <ref type="bibr" target="#b6">[7]</ref> proposed the Euler spectral clustering (EulerSC) method and proved that the EulerSC is equivalent to the weighted positive Euler k-means, which can be iteratively optimized with O(N dkt) time. However, EulerSC can only use the positive Euler kernel to define the pair-wise similarity, and is not feasible for the general spectral clustering formulation with other similarity metrics. Moreover, its clustering robustness heavily relies on the proper selection of the Euler kernel parameter, which is difficult to find without prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ensemble Clustering</head><p>Ensemble clustering has been a popular technique in recent years, which aims to combine multiple base clusterings into a better and more robust consensus clustering <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. The existing ensemble clustering algorithms can be mainly classified into three categories.</p><p>The first category is the pair-wise co-occurrence based methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Fred and Jain <ref type="bibr" target="#b7">[8]</ref> proposed the evidence accumulation clustering (EAC) method, which makes use of the co-association matrix by considering the frequency of pair-wise co-occurrence among multiple base clusterings. With the co-association matrix treated as the similarity matrix, the agglomerative clustering algorithms <ref type="bibr" target="#b0">[1]</ref> were then performed to obtain the consensus clustering. Iam-On et al. <ref type="bibr" target="#b8">[9]</ref> presented the weighted connected triple (WCT) method, which extends the EAC method by refining the co-association matrix via the common neighborhood information between clusters.</p><p>The second category is the graph partitioning based methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Strehl and Ghosh <ref type="bibr" target="#b17">[18]</ref> transformed the multiple base clusterings into a hypergraph representation, based on which three graph partitioning based ensemble clustering methods were presented. Fern and Brodley <ref type="bibr" target="#b21">[22]</ref> built a bipartite graph structure by treating both base clusters and data objects as graph nodes, and then partitioned the graph via the METIS algorithm <ref type="bibr" target="#b22">[23]</ref>.</p><p>The third category is the median partition based methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>, which cast the ensemble clustering problem into an optimization problem that aims to find a median clustering (or partition) by maximizing the similarity between this clustering and the multiple base clusterings.</p><p>Franek and Jiang <ref type="bibr" target="#b23">[24]</ref> formulated the median partition problem into a Euclidian median problem and solved it by the Weiszfeld algorithm <ref type="bibr" target="#b24">[25]</ref>. Huang et al. <ref type="bibr" target="#b16">[17]</ref> cast the median partition problem into a binary linear programming problem and solved it by the factor graph model. These ensemble clustering algorithms have shown their advantages in improving clustering accuracy and robustness. However, due to the efficiency bottleneck, most of them are not suitable for very large-scale applications. Recently some efforts have been made to (partially) address the scalability problem for ensemble clustering. To reduce the problem size, Huang et al. <ref type="bibr" target="#b10">[11]</ref> exploited the microcluster representation, which maps the N data objects onto N microclusters (N N ). Then, the set of microclusters are treated as the primitive objects, based on which two novel algorithms, i.e., the probability trajectory accumulation (PTA) and the probability trajectory based graph partitioning (PTGP), are proposed. Wu et al. <ref type="bibr" target="#b9">[10]</ref> transformed the ensemble clustering problem into a kmeans based consensus clustering (KCC) framework, which significantly facilitated the computation of the consensus function. Liu et al. <ref type="bibr" target="#b14">[15]</ref> proved that the spectral clustering of the co-association matrix is equivalent to an instance of weighted k-means clustering, and presented the spectral ensemble clustering (SEC) algorithm. While there are two phases in ensemble clustering (i.e., ensemble generation and consensus function), these algorithms <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref> generally focus on the efficiency of the consensus function. In ensemble generation, they mostly exploited k-means to produce m base clusterings <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Note that the time complexity of ensemble generation by k-means is O(N mdkt), which can still be computationally expensive when dealing with very large-scale datasets. Moreover, the performance of k-means may significantly deteriorate when handling nonlinearly separable datasets, which has a critical influence on the robustness of the ensemble clustering algorithms. Unlike the common practice that typically exploits multiple k-means clusterers as base clusterers, the proposed U-SENC algorithm integrates a diverse set of large-scale U-SPEC clusterers into a highly efficient ensemble clustering framework, which for the first time, to our knowledge, simultaneously tackles the scalability and nonlinear separability issues in both the ensemble generation and consensus function phases in ensemble clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED FRAMEWORK</head><p>In this section, we describe the proposed U-SPEC and U-SENC algorithms in Sections 3.1 and 3.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ultra-Scalable Spectral Clustering (U-SPEC)</head><p>To deal with extremely large-scale datasets, the proposed U-SPEC algorithm complies with the sub-matrix based formulation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and aims to break through the efficiency bottleneck of previous algorithms via three phases. Specifically, in the first phase, we present a hybrid representative selection strategy to strike a balance between the efficiency of the random selection and the effectiveness of the kmeans based selection. In the second phase, we develop a coarse-to-fine method to efficiently approximate the Knearest representatives for each data object, and construct a sparse affinity sub-matrix between the N objects and the p representatives. In the third phase, the N × p sub-matrix is interpreted as a bipartite graph, which can be efficiently partitioned to obtain the final clustering result. These three phases of U-SPEC will be described in Sections 3.1.1, 3.1.2, and 3.1.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Hybrid Representative Selection</head><p>Let X = {x 1 , x 2 , · · · , x N } denote a dataset with N objects, where x i ∈ R d is the i-th object and d is the dimension. To capture the relationship between all objects in X , an N × N affinity matrix can be constructed in conventional spectral clustering <ref type="bibr" target="#b1">[2]</ref>, which consumes O(N 2 d) time and O(N 2 ) memory and is not feasible for large-scale datasets. To avoid the computation of the full affinity matrix, the sub-matrix</p><formula xml:id="formula_1">1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 (b) -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 (c) Fig. 1.</formula><p>Comparison of the representatives produced by (a) random selection, (b) k-means based selection, and (c) hybrid selection. representation is often adopted in the literature of largescale spectral clustering <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The sub-matrix representation generally exploits a set of representatives to encode the overall structure of the dataset. These representatives play a crucial role in the sub-matrix representation, and can be selected by random selection <ref type="bibr" target="#b2">[3]</ref> or k-means based selection <ref type="bibr" target="#b3">[4]</ref>. Though the random selection strategy <ref type="bibr" target="#b2">[3]</ref> is highly efficient, it suffers from the inherent randomness and may lead to a set of low-quality representatives (see <ref type="figure" target="#fig_2">Fig. 1(a)</ref>). To deal with the instability of random selection, the k-means based selection <ref type="bibr" target="#b3">[4]</ref> first groups the entire dataset into p clusters via k-means and then uses the p cluster centers as the representatives. However, the k-means based selection brings in an extra time cost of O(N pdt), which restricts its feasibility for very large-scale datasets.</p><formula xml:id="formula_2">-3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 (a) -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 (b) -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -3 -2 -1 0 1 2 3 (c)</formula><p>In this paper, we propose a hybrid representative selection strategy, which is designed to find a balance between the efficiency of random selection and the effectiveness of k-means based selection. The process of the hybrid representative selection strategy is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Different from the k-means based selection which attempts to cluster the entire dataset even when the data size N is extremely large, the proposed hybrid selection strategy first randomly samples a set of p candidate representatives such that p &lt; p N . Then, upon the p candidates, we perform the k-means method to obtain p clusters and exploit the p cluster centers as the set of representatives. Empirically, the number of candidates p is suggested to be several times larger than p, e.g., p = 10p, so as to provide enough candidates while still keeping p much smaller than N in large-scale datasets. Formally, we denote the set of selected representatives as</p><formula xml:id="formula_3">R = {r 1 , r 2 , · · · , r p },<label>(1)</label></formula><p>where r i is the i-th representative in R.</p><p>By introducing an intermediate stage of random presampling, the computational complexity of the k-means based selection is reduced from O(N pdt) to O(p 2 dt). As illustrated in <ref type="figure" target="#fig_2">Fig. 1</ref>, the set of representatives produced by the hybrid selection can better reflect the data distribution than the random selection while requiring much less computational cost than the k-means based selection. To discuss this in more detail, quantitative evaluation of the performance of the proposed hybrid selection strategy against random selection and k-means based selection will be provided in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Approximation of K-Nearest Representatives</head><p>With the p representatives obtained, the next objective is to encode the pair-wise relationship of the entire dataset via the small set of representatives.</p><p>In the sub-matrix formulation of the Nyström algorithm <ref type="bibr" target="#b2">[3]</ref>, the construction of the N × p affinity sub-matrix between objects and representatives takes O(N pd) time and O(N p) memory, which is the main efficiency bottleneck of the overall algorithm <ref type="bibr" target="#b2">[3]</ref>. Given a dataset with ten million objects and a set of one thousand representatives, the storage of the N × p sub-matrix alone takes 74.51GB of memory, while the later manipulations of the sub-matrix even require more memory consumption. Cai and Chen <ref type="bibr" target="#b3">[4]</ref> proposed to sparsify the N × p affinity matrix by Knearest representatives (with K p), which, however, still requires the computation of all the distances between the N objects and the p representatives. Moreover, besides the calculation of the total of N p entries, the sparsification step also consumes O(N pK) time <ref type="bibr" target="#b3">[4]</ref>.</p><p>Before introducing our facilitation strategy, we first investigate the characteristics of the sparse sub-matrix between N objects and p representatives, where each object is only connected to its K-nearest representatives. It is obvious that there are K non-zero entries in each row of the matrix, and N K non-zero entries in the entire matrix. Assume we have p = 1, 000 and K = 5, the proportion of the nonzero entries in the matrix will be 0.5%. However, to exactly identify such a small proportion of useful entries via Knearest representatives, the entire matrix should first be calculated, which unfortunately consists of 99.5% of intermediate entries. To break the efficiency bottleneck, the key problem here is how to significantly reduce the calculation of these intermediate entries when building the sub-matrix with K-nearest representatives.</p><p>In this section, our aim is to alleviate the computational cost of the exact K-nearest representative calculation <ref type="bibr" target="#b3">[4]</ref> by designing a time-and memory-efficient approximation method. Though the K-nearest representative approximation problem and the classical K-nearest neighbor (K-NN) approximation problem <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> have some characteristics in common, they are faced with very different computational issues in actual applications. Different from the conventional K-NN approximation scenarios, which mostly deal with a general graph with an N × N affinity matrix, our aim here is to find the K-nearest representatives in a heavily imbalanced bipartite graph with an N × p affinity sub-matrix, where p is generally far smaller than N . This imbalanced nature is crucial to our K-nearest representative approximation problem. On the one hand, it makes the conventional K-NN approximation methods <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> (which are typically designed for general graphs with N ×N affinity matrices) inappropriate here. On the other hand, it may as well contribute to the design of our K-nearest representative approximation strategy. To take advantage of the imbalanced structure, it is intuitive to pre-process the graph on the side of the p representatives and minimize the computation on the other side of the N objects.</p><p>In particular, we present a new K-nearest representative approximation method based on the coarse-to-fine mechanism, and build the sparse affinity sub-matrix with O(N p 1 2 d) complexity. The main idea of our K-nearest representative approximation is to first find the nearest region, then find the nearest representative (denoted as r l ) in the nearest region, and finally find the K-nearest representatives in the neighborhood of r l . To efficiently implement the approximation, two preprocessing steps are required, that is  In pre-step 1, each rep-cluster consists of a certain number of representatives, and can be regarded as a local region of the representative set (see <ref type="figure" target="#fig_5">Fig. 3</ref>(b)). Formally, the obtained z 1 rep-clusters are denoted as</p><formula xml:id="formula_4">RC = {rc 1 , rc 2 , · · · , rc z1 },<label>(2)</label></formula><p>where rc i is the i-th rep-cluster in RC. Given an object x i ∈ X and a rep-cluster rc j ∈ RC, their distance is defined as the distance between x i and the center of rc j . That is</p><formula xml:id="formula_5">Dist(x i , rc j ) = x i − y j ,<label>(3)</label></formula><formula xml:id="formula_6">y j = 1 |rc j | r l ∈rcj r l ,<label>(4)</label></formula><p>where |rc j | denotes the number of representatives in the rep-cluster rc j and x i − y j computes the Euclidean distance between two vectors x i and y j . With the distance between objects and rep-clusters defined, for each object x i ∈ X , we approximately find its K-nearest representatives according to three main steps:</p><p>Step 1 Find the nearest rep-cluster of x i , denoted as rc j .</p><p>Step 2 Find the nearest representative of x i inside the rep-cluster rc j , denoted as r l .</p><p>Step 3 Out of r l and its K -nearest neighbors, find the K-nearest representatives of x i .</p><p>More details are illustrated in <ref type="figure" target="#fig_5">Fig. 3</ref>. For a dataset with N objects, the time cost of step 1 is O(N z 1 d). is used in this work, where · denotes the floor of a value. The candidate neighborhood size K is suggested to be several times larger than K, which can be set to K = 10K in practice. Then, the total time complexity of the K-nearest representative approximation is N (p/z 1 )d + N K d + N K K), which can be re-written as O(N (p</p><formula xml:id="formula_7">O(N z 1 d + xi R={r1 , r2 , …,r20} (a) RC={rc1 , rc2 , …,rc6} (b) xi (c) rcj xi (d) xi rcj (e) rl xi (f) xi (g) xi (h)</formula><formula xml:id="formula_8">1 2 d + Kd + K 2 )). As K p N , the dominant term in the complexity is O(N p 1 2 d).</formula><p>With the K-nearest representatives of each object obtained, a sparse N × p affinity sub-matrix can thereby be constructed. In this paper, the Gaussian kernel is used as the similarity kernel. Thus the sparse affinity sub-matrix can be represented as</p><formula xml:id="formula_9">B = {b ij } N ×p ,<label>(5)</label></formula><formula xml:id="formula_10">b ij = exp (− xi−rj 2 2σ 2 ), if r j ∈ N K (x i ), 0, otherwise,<label>(6)</label></formula><p>where N K (x i ) denotes the set of K-nearest representatives of x i and the kernel parameter σ is set to the average Euclidean distance between the objects and their K-nearest representatives. Note that B is a sparse matrix which only contains N K non-zero entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Bipartite Graph Partitioning</head><p>The affinity sub-matrix B reflects the relationship between the objects in X and the representatives in R, which can be naturally interpreted as a bipartite graph G = {X , R, B}, where X ∪ R is the node set and B is the cross-affinity matrix (as shown in <ref type="figure" target="#fig_6">Fig. 4</ref>). By taking advantage of the bipartite graph structure, the transfer cut <ref type="bibr" target="#b5">[6]</ref> can thereby be used to efficiently partition the graph and achieve the final clustering result.</p><p>To start, if we view the graph G as a general graph with N + p nodes, then its full affinity matrix can be denoted as</p><formula xml:id="formula_11">E = 0 B B 0 .<label>(7)</label></formula><p>Spectral clustering seeks to partition the graph by solving the following generalized eigen-problem <ref type="bibr" target="#b28">[29]</ref>: where L = D − E is the graph Laplacian and D ∈ R (N +p)×(N +p) is the degree matrix. By treating G as a general graph, it takes O((N + p) 3 ) time to solve the eigenproblem (8) <ref type="bibr" target="#b29">[30]</ref>, which is not computationally feasible for very large-scale datasets. By exploiting the bipartite structure, we resort to the transfer cut <ref type="bibr" target="#b5">[6]</ref> to reduce the eigen-problem (8) on the graph G (with N +p nodes) to an eigen-problem on a much smaller graph G R (with p nodes). Specifically, the graph G R is constructed as</p><formula xml:id="formula_12">Lu = γDu,<label>(8)</label></formula><formula xml:id="formula_13">G R = {R, E R }, where R is the node set, E R = B D X −1 B</formula><p>is the affinity matrix (whose computation takes O(N K 2 ) time), and D X ∈ R N ×N is a diagonal matrix with its (i, i)-th entry being the sum of the i-th row of B. Let L R = D R − E R be the graph Laplacian, where D R ∈ R p×p is the degree matrix of G R . Then, the generalized eigenproblem on the graph G R can be represented as</p><formula xml:id="formula_14">L R v = λD R v.<label>(9)</label></formula><p>It has been proved by Li et al. <ref type="bibr" target="#b5">[6]</ref> that solving the eigenproblem (8) on the graph G is equivalent to solving the eigen-problem (9) on the graph G R . Let the first k eigenpairs for the eigen-problem (9) be denoted as</p><formula xml:id="formula_15">{(λ i , v i )} k i=1</formula><p>with 0 = λ 1 ≤ λ 2 ≤ · · · ≤ λ k &lt; 1, and the first k eigenpairs for the eigen-problem (8) denoted as</p><formula xml:id="formula_16">{(γ i , u i )} k i=1 with 0 = γ 1 ≤ γ 2 ≤ · · · ≤ γ k &lt; 1.</formula><p>It has been shown that <ref type="bibr" target="#b5">[6]</ref> </p><formula xml:id="formula_17">γ i (2 − γ i ) = λ i ,<label>(10)</label></formula><formula xml:id="formula_18">u i = h i v i<label>(11)</label></formula><formula xml:id="formula_19">h i = 1 1 − γ i T v i ,<label>(12)</label></formula><p>where T = D −1 X B is the transition probability matrix. It takes O(p 3 ) time to compute the first k eigen-pairs for the eigen-problem <ref type="bibr" target="#b8">(9)</ref>. As B is a sparse matrix with N K nonzero entries, it takes O(N K) time to compute u i from v i according to Eqs. (10), <ref type="bibr" target="#b10">(11)</ref>, and <ref type="bibr" target="#b11">(12)</ref>. Therefore, the total cost of computing the first k eigenvectors for the eigen-problem (8)</p><formula xml:id="formula_20">will be O(N K 2 )+O(N Kk)+O(p 3 ) = O(N K(K +k)+p 3 ).</formula><p>With the eigen-problem solved, the obtained k eigenvectors are stacked to form an (N + p) × k matrix. By treating each row of this matrix as a new feature vector, the N rows corresponding to the N original objects are used, upon which the k-means discretization can be performed to obtain the final clustering result with O(N k 2 t) time complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Computational Complexity</head><p>In this section, we summarize the time and memory cost of our U-SPEC algorithm.</p><p>The hybrid representative selection takes O(p 2 dt) time. The affinity construction takes O(N (p </p><formula xml:id="formula_21">1 2 d + Kd + K 2 )) time.</formula><formula xml:id="formula_22">/ O(N pd) O(N p + p 3 ) LSC-R [4] / O(N pd) O(N p 2 + p 3 ) LSC-K [4] O(N pdt) O(N pd) O(N p 2 + p 3 ) U-SPEC O(p 2 dt) O(N p 1 2 d) O(N K(K+k)+p 3 ) * The final k-means discretization is O(N k 2 t) for each method.</formula><p>The eigen-decomposition takes O(N K(K + k) + p 3 ) time.</p><p>The k-means discretization takes O(N k 2 t) time. With consideration to k, K p N , the overall time complexity of U-SPEC is O(N (p <ref type="table" target="#tab_1">Table 2</ref> provides a comparison of time complexity of our U-SPEC algorithm against several other large-scale spectral clustering algorithms.</p><formula xml:id="formula_23">1 2 d + K 2 + Kk + Kd + k 2 t)), where O(N p 1 2 d) is the dominant term.</formula><p>Besides the time cost, the memory cost of U-SPEC can be either O(N K) or O(N p 1 2 ), which depends on the actual implementation of the K-nearest representative approximation. As the K-nearest representative approximation for the N objects are independent of each other, one strategy is to perform approximation for the N objects one after the other (i.e., in a serial processing manner), where the time cost is dominated by the storage of the cross-affinity matrix with N K non-zero entries. Another strategy is to first construct an affinity matrix between the N objects and the z 1 = p <ref type="bibr">1 2</ref> rep-cluster centers and then approximate the K-nearest representatives for the N objects in a batch processing manner. For some matrix-oriented software, such as MATLAB, it will be much faster to perform the approximation in a batch processing manner (with optimized matrix computation) than in a serial processing manner. To facilitate the matrix computation, our implementation of U-SPEC actually takes O(N p 1 2 ) memory. Similarly, the LSC algorithm <ref type="bibr" target="#b3">[4]</ref> also has a theoretically minimum memory cost of O(N K), but the implementation 1 provided by the authors actually takes O(N p) memory, which is also due to the matrix-computation consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ultra-Scalable Ensemble Clustering (U-SENC)</head><p>Starting from U-SPEC, this section proposes the U-SENC algorithm to integrate multiple U-SPEC's into a unified ensemble clustering framework, aiming to further enhance the clustering robustness while maintaining high efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Ensemble Generation via Multiple U-SPEC's</head><p>Ensemble clustering has been a popular research topic in recent years, due to its promising ability in enhancing clustering robustness by incorporating multiple base clusterers <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The general ensemble clustering process consists of two phases. The first phase is the ensemble generation, which involves producing a set of diverse and high-quality base clusterings. The second phase is the consensus function, which involves combining multiple 1. www.cad.zju.edu.cn/home/dengcai/Data/Clustering.html base clusterings into a better and more robust consensus clustering.</p><p>In ensemble generation, the previous ensemble clustering algorithms mostly use the k-means method to generate an ensemble of multiple base clusterings <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Though k-means has the advantage of high efficiency, it typically favors spherical distribution and lacks the ability to properly partition nonlinearly separable datasets. Some researchers have exploited the spectral clustering technique in ensemble generation <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, but the large computational cost of conventional spectral clustering significantly restricts its feasibility for scalable applications.</p><p>To address this, we utilize multiple instances of U-SPEC as the multiple base clusterers in our ensemble clustering framework. To generate an ensemble of m base clusterings, a set of m U-SPEC clusterers are required, which are denoted as U-SPEC 1 ,U-SPEC 2 , · · · ,U-SPEC m . The diversity which is highly desired in ensemble generation is incorporated from two aspects. First, the set of representatives for each base clusterer is independently obtained by the hybrid selection strategy. There are two components in hybrid selection, i.e., random pre-selection and k-means based post-selection, both of which are non-deterministic and can bring in diversity for the multiple base clusterers. Second, the number of clusters for each base clustering is randomly selected to further enhance the diversity. Formally, given the dataset X , the set of p candidate representatives for the i-th base clusterer (i.e., U-SPEC i ) are randomly selected from X . Then the k-means is used to partition the p candidates into p clusters. After that, the p cluster centers will be used as the set of p representatives for U-SPEC i , denoted as</p><formula xml:id="formula_24">R i = {r i 1 , r i 2 , · · · , r i p }.<label>(13)</label></formula><p>With the representatives obtained, the sparse affinity submatrix B i for U-SPEC i can be built between the dataset X and the representative set R i via fast approximation of Knearest representatives.</p><p>By treating X R i as the node set and B i as the crossaffinity matrix, the bipartite graph G i is built and its first k i eigenvectors are then computed via transfer cut <ref type="bibr" target="#b5">[6]</ref>. Note that the number of clusters k i is randomly selected as</p><formula xml:id="formula_25">k i = τ (k max − k min ) + k min ,<label>(14)</label></formula><p>where τ ∈ [0, 1] is a random variable and k max and k min are respectively the upper bound and lower bound of the cluster number. Then, the obtained k i eigenvectors are stacked to form a new matrix, upon which the k-means is applied to construct the base clustering result for U-SPEC i . With the m U-SPEC clusterers, the ensemble of m base clusterings can be generated, which are represented as</p><formula xml:id="formula_26">Π = {π 1 , π 2 , · · · , π m },<label>(15)</label></formula><p>where π i denotes the i-th base clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Consensus Function with Bipartite Graph</head><p>Having obtained the set of multiple base clusterings, this section presents the consensus function with bipartite graph for obtaining the consensus clustering.</p><p>Each base clustering consists of a certain number of clusters. For clarity, we denote the set of clusters in the ensemble of m base clusterings as</p><formula xml:id="formula_27">C = {C 1 , C 2 , · · · , C kc },<label>(16)</label></formula><p>where C i is the i-th cluster and k c is the total number of clusters in Π. It is obvious that k c = m i=1 k i . By treating both objects and clusters as graph nodes, the bipartite graph for the ensemble Π is defined as</p><formula xml:id="formula_28">G = {X , C,B},<label>(17)</label></formula><p>where X C is the node set andB is the cross-affinity matrix. In this bipartite graph, a (non-zero) edge exists between two nodes if and only if one node is an object and the other one is the cluster that contains it. Formally, the cross-affinity matrix is constructed as follows:</p><formula xml:id="formula_29">B = {b ij } N ×kc ,<label>(18)</label></formula><formula xml:id="formula_30">b ij = 1, if x i ∈ C j , 0, otherwise.<label>(19)</label></formula><p>Inside the same base clustering, there is no intersection between two different clusters, i.e., ∀i = j , if C i ∈ π i and C j ∈ π i , then C i C j = ∅. Obviously, each object belongs to one and only one cluster in each base clustering, and thus each object belongs exactly to m clusters in the ensemble of m base clusterings. Therefore, there are exactly m non-zero entries in each row ofB. Although the crossaffinity matrixB is an N × k c matrix, it can be stored as a sparse matrix with O(N m) memory, which corresponds to the exactly N m non-zero entries inB. Besides the memory cost, the time cost of building the sparse matrixB is O(N m).</p><p>As shown in Section 3.1.3, solving the eigen-problem for the bipartite graphG can be equivalent to solving the eigenproblem for a much smaller graph</p><formula xml:id="formula_31">G C = {C, E C }, that is L Cṽ =λD Cṽ ,<label>(20)</label></formula><p>where E C =B D −1 XB is the affinity matrix,D X ∈ R N ×N is a diagonal matrix with its (i, i)-th entry being the sum of the i-th row ofB, L C = D C − E C is the graph Laplacian, and D C ∈ R kc×kc is the degree matrix of G C . Letṽ 1 ,ṽ 2 , · · · ,ṽ k denote the first k eigenvectors for the eigen-problem <ref type="bibr" target="#b19">(20)</ref>, which can be computed with a time cost of O(k c 3 ). Based on the k eigenvectors for G C , the first k eigenvectors (denoted asũ 1 ,ũ 2 , · · · ,ũ k ) for the bipartite graphG can be computed with O(N m(m + k)) time (see Eqs. (10), <ref type="bibr" target="#b10">(11)</ref>, and <ref type="formula" target="#formula_3">(12)</ref>). Finally, by stacking the k eigenvectors to form a new matrix, the consensus clustering result in U-SENC can be obtained by k-means discretization with O(N k 2 t) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Computational Complexity</head><p>This section summarizes the time and memory cost of the proposed U-SENC algorithm.</p><p>The ensemble generation of the U-SENC algorithm takes O(N m(p   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct experiments on a variety of real and synthetic datasets to compare the proposed U-SPEC and U-SENC algorithms against several state-of-the-art spectral clustering and ensemble clustering algorithms. All experiments are conducted in Matlab 2016b on a PC with an Intel i5-6600 CPU and 64GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Measures</head><p>Our experiments are conducted on ten large-scale datasets (including five real datasets and five synthetic datasets), whose data sizes range from ten thousand to as large as twenty million. Specifically, the five real datasets are PenDigits <ref type="bibr" target="#b32">[33]</ref>, USPS <ref type="bibr" target="#b33">[34]</ref>, Letters <ref type="bibr" target="#b32">[33]</ref>, MNIST <ref type="bibr" target="#b33">[34]</ref>, and Covertype <ref type="bibr" target="#b32">[33]</ref>. The five synthetic datasets are Two Bananas-1M (TB-1M), Smiling Face-2M (SF-2M), Concentric Circles-5M (CC-5M), Circles and Gaussians-10M (CG-10M), and Flower-20M. The details of the datasets are provided in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="figure" target="#fig_8">Fig. 5</ref>.</p><p>To evaluate the clustering results by different algorithms, two widely used evaluation measures are adopted, namely, normalized mutual information (NMI) <ref type="bibr" target="#b17">[18]</ref> and clustering accuracy (CA) <ref type="bibr" target="#b34">[35]</ref>. To rule out the factor of getting lucky occasionally, in each experiment, every test method will be <ref type="bibr">TABLE 4</ref> Average NMI(%) scores (over 20 runs) by our methods and the baseline spectral clustering methods (The best score in each row is in bold).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods and Experimental Settings</head><p>In the experiments, we first compare our algorithms against the classical k-means algorithm <ref type="bibr" target="#b35">[36]</ref> as well as seven spectral clustering algorithms (including the original algorithm and six large-scale algorithms). The baseline spectral clustering algorithms are listed as follows:</p><formula xml:id="formula_32">1) SC [2]</formula><p>: original spectral clustering.</p><p>2) ESCG <ref type="bibr" target="#b36">[37]</ref>: efficient spectral clustering on graphs.</p><p>3) Nyström <ref type="bibr" target="#b2">[3]</ref>: Nyström spectral clustering. 4) LSC-K <ref type="bibr" target="#b3">[4]</ref>: landmark based spectral clustering using k-means based landmark selection. 5) LSC-R <ref type="bibr" target="#b3">[4]</ref>: landmark based spectral clustering using random landmark selection. 6) FastESC <ref type="bibr" target="#b4">[5]</ref>: fast explicit spectral clustering. 7) EulerSC <ref type="bibr" target="#b6">[7]</ref>: Euler spectral clustering.</p><p>Besides these large-scale spectral clustering algorithms, we also compare our algorithms against seven ensemble clustering algorithms, which are listed as follows: 1) EAC <ref type="bibr" target="#b7">[8]</ref>: evidence accumulation clustering. 2) WCT <ref type="bibr" target="#b8">[9]</ref>: weighted connected triple method.   3) KCC <ref type="bibr" target="#b9">[10]</ref>: k-means based consensus clustering. 4) PTGP <ref type="bibr" target="#b10">[11]</ref>: probability trajectory based graph partitioning. 5) ECC <ref type="bibr" target="#b13">[14]</ref>: entropy based consensus clustering. 6) SEC <ref type="bibr" target="#b14">[15]</ref>: spectral ensemble clustering. 7) LWGP <ref type="bibr" target="#b11">[12]</ref>: locally weighted graph partitioning.</p><p>There are several common parameters among the abovementioned algorithms. In our experiments, we comply with the following experimental settings:</p><p>• The SC and ESCG methods need to take the N × N affinity matrix as input. The affinity matrix is constructed using the same Gaussian kernel as Eq. <ref type="formula" target="#formula_10">(6)</ref> with K-nearest neighbors.</p><p>• The U-SPEC, U-SENC, Nyström, LSC-K, and LSC-R methods have a common parameter p. In the experiments, p = 1000 is used for these methods. Their performances with varying p will be further evaluated in Section 4.5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The U-SPEC, U-SENC, LSC-K, and LSC-R methods have a common parameter K. In the experiments, K = 5 is used. Their performances with varying K will be further evaluated in Section 4.5.2.</p><p>• For the seven ensemble clustering methods, the base clusterings are generated by k-means as suggested by their papers <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The number of clusters in each base clustering is randomly selected in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">60]</ref>. The number of base clusterings, i.e., m, is set to 20. Their performances with varying m will be further evaluated in Section 4.5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The true number of classes on each dataset is used as the number of clusters for all the test methods.</p><p>• Besides these common parameters, the other parameters in the baseline methods will be set as suggested by the corresponding papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Spectral Clustering Methods</head><p>In this section, we compare our U-SPEC and U-SENC algorithms with several state-of-the-art large-scale spectral clustering algorithms.</p><p>As the data sizes range from ten thousand to twenty million, most of the baseline algorithms are not computationally feasible for ten-million-level datasets. Specifically, we use N/A to indicate the out-of-memory error in the results. As shown in Tables 4 and 5, the SC and ESCG methods are not able to handle the datasets large than MNIST (which consists of 70,000 objects), due to the memory consumption of constructing and manipulating the N × N affinity matrix. The Nyström, LSC-K, LSC-R, and FastESC methods can at most partition a dataset with two million objects, and cannot deal with datasets larger than that. Out of the total of nine spectral clustering methods, only three methods (i.e., U-SPEC, U-SENC, and EulerSC) can deal with all of the benchmark datasets. As shown in Tables 4 and 5, our U-SENC and U-SPEC methods achieve the best and the second best scores, respectively, on most of the ten benchmark datasets.</p><p>In <ref type="table" target="#tab_4">Tables 4 and 5</ref>, we also provide the average score, normalized average score (N-Avg. score), and average rank of each method across the ten datasets. To obtain the normalized average score, the scores in each row will first be divided by the maximum score in this row, where it is obvious that the maximum score will become 100%. Then we take the average of these normalized rows as the normalized average score. Note that if a baseline method cannot process all the datasets, it will not have the average score and normalized average score information, but it will still have the average rank information. For example, if only three methods are efficient enough to process the CC-5M dataset, then all the other infeasible methods will be treated as equally ranked in the fourth position on this dataset. As shown in Tables 4 and 5, our U-SENC method ranks in the first position on nine out of the ten datasets, and achieves an average rank of 1.10 w.r.t. both NMI and CA. Our U-SPEC method achieves an average rank of 2.40 w.r.t. NMI and 2.00 w.r.t. CA. In terms of average score and normalized average score, our U-SENC and U-SPEC methods also significantly outperform the other methods. <ref type="table" target="#tab_5">Table 6</ref> reports the time costs of different methods on the benchmark datasets. The U-SPEC shows superior efficiency on most of the datasets, especially on the datasets larger than one million. The U-SENC requires a larger time cost than U-SPEC, but it still provides better scalability than most of the baseline methods and scales well for ten-million-level datasets due to its memory efficiency. As U-SENC is a spectral clustering algorithm and also an ensemble clustering  algorithm, in the following, we will further compare it with other state-of-the-art ensemble clustering algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Ensemble Clustering Methods</head><p>In this section, we compare our algorithms with several state-of-the-art ensemble clustering algorithms. Note that U-SPEC is not an ensemble clustering algorithm; its clustering results are provided in <ref type="table" target="#tab_6">Tables 7, 8</ref>, and 9 for reference only. As shown in Tables 7 and 8, our U-SENC algorithm obtains the highest NMI and CA scores on all of the ten datasets. In terms of average score across the ten datasets, U-SENC achieves the best average NMI(%) and CA(%) scores of 74.57 and 81.68, respectively while the second best ensemble clustering method (i.e., LWGP) only achieves average NMI(%) and CA(%) scores of 66.62 and 74.49, respectively. Similar advantages of U-SENC can also be observed in the normalized average scores. In terms of average rank, U-SENC obtains an average rank of 1.00 w.r.t. both NMI and CA, while the second best method obtains an average rank of 2.80 w.r.t. NMI and 2.90 w.r.t. CA.</p><p>In <ref type="table" target="#tab_8">Table 9</ref>, the time costs of different ensemble clustering methods are provided. As can be seen in <ref type="table" target="#tab_8">Table 9</ref>, the proposed U-SENC method has shown its advantage in efficiency over the other ensemble clustering methods, especially on the large-scale datasets whose data sizes go beyond millions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameters Analysis</head><p>In this section, we evaluate the performances of our algorithms and several baseline algorithms with varying parameters. Because some important baseline methods (such as Nyström, LSC-K, and LSC-R) can not go beyond twomillion-level datasets, in order to fairly test the influence of some common parameters among them, we perform the parameter analysis on four benchmark datasets, namely, MNIST, Covertype, TB-1M, and SF-2M, which are the largest four datasets whose sizes are no larger than two million.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Number of Representatives p</head><p>The parameter p denotes the number of representatives (or landmarks), which is a common parameter in the sub-matrix based spectral clustering methods, such as Nyström, LSC-K, LSC-R, and our U-SPEC and U-SENC methods. As can be seen in <ref type="table" target="#tab_0">Table 10</ref>, a larger p generally leads to better performance, but also brings in an increasing time cost. In terms of NMI and CA, our U-SENC method consistently outperforms the other methods with varying parameter p on all of the four datasets. The LSC-K outperforms U-SPEC on the MNIST dataset. But on all the other three datasets, U-SPEC achieves better or significantly better NMI and CA scores than LSC-K. In terms of computational cost, the LSC-K and Nyström methods cannot deal with p ≥ 1, 400 representatives on the SF-2M dataset with two million objects. On the benchmark datasets, U-SPEC is overall the fastest method with varying parameter p (as shown in <ref type="table" target="#tab_0">Table 10</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Number of Nearest Representatives K</head><p>The parameter K denotes the number of nearest representatives (or landmarks), which is a common parameter in LSC-K, LSC-R, and our U-SPEC and U-SENC methods. Note that the Nyström method doesn't have such a parameter K, but we still illustrate the performance of Nyström in <ref type="table" target="#tab_0">Table 11</ref> just to use Nyström as a benchmark here. As illustrated in <ref type="table" target="#tab_0">Table 11</ref>, on the MNIST dataset, U-SENC and LSC-K are respectively the best and the second best methods w.r.t. NMI and CA, while U-SPEC is the third best method. On all of the other three benchmark datasets, U-SENC and U-SPEC are overall the best two methods w.r.t. both NMI and CA with varying parameter K (as shown in <ref type="table" target="#tab_0">Table 11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Ensemble Size m</head><p>The parameter m denotes the number of base clusterings, which is a common parameter in all of the ensemble clustering methods, including U-SENC as well as the baseline ensemble clustering methods. Note that U-SPEC is not an ensemble clustering method and doesn't have the parameter m, but we still illustrate the performance of U-SPEC in <ref type="table" target="#tab_0">Table 12</ref> for reference only. As shown in <ref type="table" target="#tab_0">Table 12</ref>, U-SENC outperforms, or even significantly outperforms, the other ensemble clustering methods w.r.t. both NMI and CA on the benchmark datasets with varying ensemble size m. Meanwhile, U-SENC consistently requires a lower computational cost than the other ensemble clustering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Influence of Representative Selection Strategies</head><p>In this section, we compare the performances of our algorithms using different representative selection strategies. Specifically, <ref type="table" target="#tab_0">Table 13</ref> illustrates the performances of U-SPEC using hybrid selection (U-SPEC-H), U-SPEC using random selection (U-SPEC-R), and U-SPEC using k-means based selection (U-SPEC-K), whereas <ref type="table" target="#tab_0">Table 14</ref> illustrates the performances of U-SENC using hybrid selection (U-SENC-H), U-SENC using random selection (U-SENC-R), and U-SENC using k-means based selection (U-SENC-K). As shown in <ref type="table" target="#tab_0">Tables 13 and 14</ref>, the random representative selection is very efficient compared to k-means based selection, but may degrade the clustering quality due to its inherent instability. The k-means based selection generally leads to better clustering quality than random selection, but brings in a much larger computational cost. Compared to random selection and k-means based selection, our hybrid selection strategy strikes a balance between efficiency and clustering robustness. It achieves comparable efficiency to the random selection and significantly better efficiency than the k-means based selection, and also yields competitive clustering quality as compared to the k-means based selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Influence of Approximate K-Nearest Neighbors</head><p>In this section, we compare our algorithms using Approximate K-nearest representatives against using Exact K-nearest representatives, where four variants are evaluated, i.e., U-SPEC(A), U-SPEC(E), U-SENC(A), and U-SENC(E). The purpose of using approximate K-nearest representatives (see Section 3.1.2) is to alleviate the time and memory cost of the affinity sub-matrix construction while  , the improvement in efficiency is more significant for high-dimensional datasets, such as the MNIST dataset, whose dimension is 784. Even for the lowdimensional datasets, such as TB-1M and SF-2M, the use of approximate K-nearest representatives can still consistently reduce the time cost. Besides the time efficiency, the approximate K-nearest representatives also alleviate the memory burden. Specifically, on a machine with 64GB memory, the computation of conventional K-nearest representatives can hardly go beyond five million objects, whereas the proposed approximation method for K-nearest representatives can scale well for even ten-million-level datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper proposes two large-scale clustering algorithms, termed ultra-scalable spectral clustering (U-SPEC) and <ref type="bibr">TABLE 15</ref> The NMI(%), CA(%), and time costs(s) by U-SPEC using Approximate K-nearest representatives against Exact K-nearest representatives.   ultra-scalable ensemble clustering (U-SENC), respectively. In U-SPEC, a new hybrid representative selection strategy is designed to strike a balance between the efficiency of random selection and the effectiveness of k-means based selection. Then a new approximation method for K-nearest representatives is presented to efficiently construct a bipartite graph between the original data objects and the set of representatives, upon which the transfer cut can be utilized to obtain the clustering result. Starting from the U-SPEC algorithm, we further integrate multiple U-SPEC clusterers into a unified ensemble clustering framework and propose the U-SENC algorithm. Specifically, multiple U-SPEC's are exploited in the ensemble generation phase to produce an ensemble of diverse and high-quality base clusterings. The multiple base clusterings are incorporated into a new bipartite graph, which treats both objects and base clusters as graph nodes and is then efficiently partitioned to achieve the final consensus clustering. Extensive experiments have been conducted on ten large-scale datasets, which demonstrate the scalability and robustness of our algorithms.  <ref type="table">journals and conferences such as IEEE TPAMI,  IEEE TKDE, IEEE TCYB, IEEE TSMC</ref> Chee-Keong Kwoh received the bachelor's degree in electrical engineering (first class) and the master's degree in industrial system engineering from the National University of Singapore in 1987 and 1991, respectively. He received the PhD degree from the Imperial College of Science, Technology and Medicine, University of London, in 1995. He has been with the School of Computer Science and Engineering, Nanyang Technological University (NTU) since 1993. He is the programme director of the MSc in Bioinformatics programme at NTU. His research interests include data mining, soft computing and graph-based inference; applications areas include bioinformatics and biomedical engineering. He has done significant research work in his research areas and has published many quality international conferences and journal papers. He is an editorial board member of the International Journal of Data Mining and Bioinformatics; the Scientific World Journal; Network Modeling and Analysis in Health Informatics and Bioinformatics; Theoretical Biology Insights; and Bioinformation. He has been a guest editor for many journals such as Journal of Mechanics in Medicine and Biology, the International Journal on Biomedical and Pharmaceutical Engineering and others. He has often been invited as an organizing member or referee and reviewer for a number of premier conferences and journals including GIW, IEEE BIBM, RECOMB, PRIB, BIBM, ICDM and iCBBE. He is a member of the Association for Medical and Bio-Informatics, Imperial College Alumni Association of Singapore. He has provided many services to professional bodies in Singapore and was conferred the Public Service Medal by the President of Singapore in 2008.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 2 d) and O(N p 1 2 ) respectively. 4 )</head><label>114</label><figDesc>By integrating multiple U-SPEC clusterers, a new large-scale ensemble clustering algorithm termed U-SENC is developed, which significantly enhances the robustness of U-SPEC while maintaining high scalability. Its time and space complexity are dominated by O(N mp 1 2 d) and O(N p 1 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of hybrid representative selection. (a) The dataset. (b) Randomly select p candidates (p &gt; p). (c) Obtain p representatives from p candidates via k-means.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•Pre-step 1 .</head><label>1</label><figDesc>The set of representatives are grouped into z 1 rep-clusters via k-means (with z 1 p). The time complexity is O(pz 1 dt).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•Pre-step 2 .</head><label>2</label><figDesc>For each representative in R, its Knearest neighbors are computed and stored (with K &gt; K). The time complexity is O(p 2 (d + K )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 2</head><label>1</label><figDesc>The time cost of step 2 is O(N z 2 d) = O(N (p/z 1 )d), where z 2 = p/z 1 denotes the average size of the rep-clusters. The time cost of step 3 is O(N K d + N K K). It is obvious that z 1 + p/z 1 reaches its minimum when z 1 = z 2 = p . Thus, to minimize the cost, z 1 = p 1 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Approximate K-nearest representatives. (a) The representative set R and an object x i ∈ X . (b) Partition the representatives into several rep-clusters. (c) Compute the distances between x i and all the rep-cluster centers. (d) Find the nearest rep-cluster rc j . (e) Compute the distances between x i and all the representatives in rc j . (f) Find the nearest r l ∈ rc j . (g) Compute the distances between x i and the representatives in the K -nearest neighborhood of r l (K &gt; K). (h) Obtain the approximate K-nearest representatives (K = 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of the bipartite graph G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 2 d 1 2</head><label>21</label><figDesc>+ K 2 + Kk + Kd + k 2 t)) time. The consensus function of U-SENC takes O(N (m 2 + mk + k 2 t) + k c 3 ) time. With consideration to m, k, K p N , the dominant term of the overall time complexity of U-SENC is O(N mp 1 2 d). Meanwhile, the memory costs of the ensemble generation and the consensus function of our U-SENC algorithm are respectively O(N p ) and O(N m).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Illustration of the five synthetic datasets. Note that only a 0.1% subset of each dataset is plotted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>*</head><label></label><figDesc>On the SF-2M dataset, LSC-K cannot handle ≥ 1400 representatives (or landmarks), while Nyström cannot handle ≥ 1200 representatives (or landmarks), due to the memory bottleneck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>NMI(%), CA(%), and time costs(s) over 20 runs by different methods with varying ensemble size m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 2</head><label>1</label><figDesc><ref type="bibr" target="#b12">13</ref> The NMI(%), CA(%), and time costs(s) by U-SPEC using different representative selection strategies (H: hybrid selection; R: random selection; K: K-means based selection). NMI(%), CA(%), and time costs(s) by U-SENC using different representative selection strategies (H: hybrid selection; R: random selection; K: K-means based selection).maintaining the overall clustering quality. As shown in Tables 15 and 16, using approximate K-nearest representatives can achieve comparable clustering quality (w.r.t. NMI and CA) with using exact K-nearest representatives while alleviating the computational cost. As our approximation of K-nearest representatives reduces the time complexity from O(N pd) to O(N p d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>-C, Pattern Recognition, KAIS, AAAI, ICDM, CIKM and SDM. His ICDM 2010 paper won the Honorable Mention for Best Research Paper Awards. He won 2012 Microsoft Research Fellowship Nomination Award. He was awarded 2015 Chinese Association for Artificial Intelligence (CAAI) Outstanding Dissertation. Jian-Sheng Wu received the B.S. and Ph.D. degrees from the School of Information Science and Technology from Sun Yat-sen University, Guangzhou, China, in 2009 and 2015, respectively. He joined the School of Information Engineering, Nanchang University, in 2015. His current research interests include machine learning and data mining, especially focusing on large scale data clustering. Jian-Huang Lai received the M.Sc. degree in applied mathematics in 1989 and the Ph.D. degree in mathematics in 1999 from Sun Yat-sen University, China. He joined Sun Yat-sen University in 1989 as an Assistant Professor, where he is currently a Professor with the School of Data and Computer Science. His current research interests include the areas of digital image processing, pattern recognition, multimedia communication, wavelet and its applications. He has published more than 200 scientific papers in the international journals and conferences on image processing and pattern recognition, such as IEEE TPAMI, IEEE TKDE, IEEE TNN, IEEE TIP, IEEE TSMC-B, Pattern Recognition, ICCV, CVPR, IJCAI, ICDM and SDM. Prof. Lai serves as a Standing Member of the Image and Graphics Association of China, and also serves as a Standing Director of the Image and Graphics Association of Guangdong.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Summary of notationsX A dataset of N objects x i The i-th object in X N Number of objects in X d Dimension tNumber of iterations in the k-means method kNumber of clusters in the clustering result</figDesc><table><row><cell>p</cell><cell>Number of candidate representatives</cell></row><row><cell>p</cell><cell>Number of representatives</cell></row><row><cell>R</cell><cell>The set of representatives</cell></row><row><cell>r i</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Comparison of the time complexity of several large-scale spectral clustering methods.</figDesc><table><row><cell>Method</cell><cell>Representative</cell><cell>Affinity</cell><cell>Eigen-</cell></row><row><cell></cell><cell>selection</cell><cell>construction</cell><cell>decomposition</cell></row><row><cell>Nyström [3]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Description of the real and synthetic datasets.</figDesc><table><row><cell cols="2">Dataset</cell><cell>#Object</cell><cell>Dimension</cell><cell>#Class</cell></row><row><cell></cell><cell>PenDigits</cell><cell>10,992</cell><cell>16</cell><cell>10</cell></row><row><cell></cell><cell>USPS</cell><cell>11,000</cell><cell>256</cell><cell>10</cell></row><row><cell>Real</cell><cell>Letters</cell><cell>20,000</cell><cell>16</cell><cell>26</cell></row><row><cell></cell><cell>MNIST</cell><cell>70,000</cell><cell>784</cell><cell>10</cell></row><row><cell></cell><cell>Covertype</cell><cell>581,012</cell><cell>54</cell><cell>7</cell></row><row><cell></cell><cell>TB-1M</cell><cell>1,000,000</cell><cell>2</cell><cell>2</cell></row><row><cell></cell><cell>SF-2M</cell><cell>2,000,000</cell><cell>2</cell><cell>4</cell></row><row><cell>Synthetic</cell><cell>CC-5M</cell><cell>5,000,000</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell>CG-10M</cell><cell>10,000,000</cell><cell>2</cell><cell>11</cell></row><row><cell></cell><cell>Flower-20M</cell><cell>20,000,000</cell><cell>2</cell><cell>13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>PenDigits 66.66 ±1.76 59.36 ±0.00 76.41 ±2.26 65.67 ±1.16 79.73 ±2.09 78.13 ±2.20 65.31 ±0.71 58.59 ±0.73 80.30 ±2.18 85.34 ±0.91 USPS 44.11 ±1.24 63.44 ±0.01 48.41 ±3.53 44.91 ±1.28 66.86 ±1.58 58.64 ±1.31 41.36 ±1.80 40.31 ±1.91 63.47 ±0.97 73.89 ±1.82 Letters 34.86 ±0.60 10.43 ±0.50 35.80 ±1.72 39.02 ±0.83 43.41 ±0.81 40.98 ±0.93 35.92 ±1.41 31.76 ±0.92 42.53 ±1.32 45.90 ±0.58 MNIST 48.91 ±2.00 74.07 ±0.00 55.75 ±4.62 47.78 ±1.17 73.97 ±1.46 62.16 ±2.22 43.44 ±1.85 8.93 ±1.22 67.43 ±1.55 75.02 ±0.81 Covertype 6.17 ±0.00 N/A N/A 6.93 ±0.07 6.75 ±0.10 6.69 ±0.12 9.15 ±1.00 0.01 ±0.00 6.97 ±0.16 9.13 ±1.21 ±0.01 0.10 ±0.11 0.20 ±0.24 24.01 ±2.72 25.94 ±0.01 95.86 ±0.48 97.48 ±0.05 ±0.02 66.45 ±6.15 58.34 ±6.92 52.03 ±0.95 47.35 ±2.19 75.59 ±2.12 77.02 ±2.32 ±0.21 78.82 ±1.61 89.57±3.96   Note that N/A indicates the out-of-memory error. The k-means method is listed for reference only; it doesn't participate in the comparison of the spectral methods.</figDesc><table><row><cell>Dataset</cell><cell>k-means</cell><cell>SC</cell><cell>ESCG</cell><cell>Nyström</cell><cell>LSC-K</cell><cell>LSC-R</cell><cell>FastESC</cell><cell>EulerSC</cell><cell>U-SPEC</cell><cell>U-SENC</cell></row><row><cell cols="5">TB-1M 24.06 SF-2M 25.71 ±0.00 N/A N/A 47.34 ±0.23 N/A N/A 46.66 CC-5M 0.00 ±0.00 N/A N/A N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>0.00 ±0.00</cell><cell cols="2">99.87 ±0.01 99.91 ±0.00</cell></row><row><cell cols="11">CG-10M 16.19 Flower-20M 63.20 ±1.59 N/A N/A N/A N/A N/A N/A 64.19 ±2.56 N/A N/A N/A N/A N/A N/A 26.61 ±0.86 86.86 ±2.05 92.47 ±2.45</cell></row><row><cell>Avg. score</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>25.57</cell><cell>69.77</cell><cell>74.57</cell></row><row><cell>N-Avg. score</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>33.94</cell><cell>91.71</cell><cell>99.98</cell></row><row><cell>Avg. rank</cell><cell>-</cell><cell>5.90</cell><cell>6.00</cell><cell>5.20</cell><cell>3.70</cell><cell>4.60</cell><cell>5.20</cell><cell>6.00</cell><cell>2.50</cell><cell>1.10</cell></row></table><note>***</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc>Average CA(%) scores (over 20 runs) by our methods and the baseline spectral clustering methods (The best score in each row is in bold).±3.12 56.44 ±0.00 77.21 ±3.81 71.13 ±2.07 83.07 ±3.21 81.82 ±3.17 69.97 ±1.15 65.85 ±1.87 84.17 ±3.26 88.56 ±0.61 USPS 47.25 ±2.57 62.74 ±0.02 53.47 ±3.94 51.09 ±1.93 68.42 ±2.39 60.78 ±2.18 48.80 ±1.76 47.79 ±2.41 63.76 ±1.35 78.17 ±3.05 Letters 28.15 ±0.97 12.42 ±0.46 30.37 ±1.75 32.05 ±0.91 35.45 ±1.34 33.86 ±1.13 29.32 ±1.51 28.08 ±1.44 35.71 ±1.47 37.74 ±1.06 MNIST 58.48 ±2.67 74.46 ±0.00 63.32 ±4.64 59.72 ±1.75 79.45 ±1.02 69.24 ±2.75 55.93 ±2.41 24.06 ±1.53 74.31 ±2.28 80.58 ±1.75 ±0.11 49.45 ±0.16 49.32 ±0.25 48.88 ±0.18 48.76 ±0.00 49.76 ±0.35 50.73 ±0.62 ±0.01 51.54 ±1.13 52.09 ±1.58 77.97 ±1.52 79.04 ±0.00 99.55 ±0.06 99.75 ±0.01 ±0.05 85.34 ±5.70 78.26 ±7.43 74.13 ±0.32 76.93 ±2.17 93.60 ±1.00 93.46 ±2.27 ±0.67 81.32 ±2.00 93.99 ±3.25 ±0.56 88.89 ±2.85 93.79 ±3.21</figDesc><table><row><cell>Dataset</cell><cell>k-means</cell><cell>SC</cell><cell>ESCG</cell><cell>Nyström</cell><cell>LSC-K</cell><cell>LSC-R</cell><cell>FastESC</cell><cell>EulerSC</cell><cell>U-SPEC</cell><cell>U-SENC</cell></row><row><cell cols="5">PenDigits 71.57 Covertype 49.05 ±0.00 49.21 TB-1M N/A N/A 78.93 ±0.00 N/A N/A 78.04 SF-2M 74.33 ±2.14 N/A N/A 69.58 CC-5M 52.96 ±0.00 N/A N/A N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="3">52.96 ±0.00 99.99 ±0.00 99.99 ±0.00</cell></row><row><cell cols="9">CG-10M 32.81 Flower-20M 63.14 ±2.42 N/A N/A N/A N/A N/A N/A 60.85 ±3.33 N/A N/A N/A N/A N/A N/A 33.75 Avg. score -N/A N/A N/A N/A N/A N/A 49.00</cell><cell>77.11</cell><cell>81.68</cell></row><row><cell>N-Avg. score</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>62.12</cell><cell>94.26</cell><cell>99.99</cell></row><row><cell>Avg. rank</cell><cell>-</cell><cell>6.10</cell><cell>5.90</cell><cell>5.30</cell><cell>3.50</cell><cell>4.40</cell><cell>5.90</cell><cell>5.80</cell><cell>2.10</cell><cell>1.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6</head><label>6</label><figDesc>Time costs(s) of our methods and the baseline spectral clustering methods.</figDesc><table><row><cell>Dataset</cell><cell>k-means</cell><cell>SC</cell><cell>ESCG</cell><cell>Nyström</cell><cell>LSC-K</cell><cell>LSC-R</cell><cell>FastESC</cell><cell>EulerSC</cell><cell>U-SPEC</cell><cell>U-SENC</cell></row><row><cell>PenDigits</cell><cell>0.06</cell><cell>7.37</cell><cell>1.63</cell><cell>1.98</cell><cell>1.25</cell><cell>0.49</cell><cell>0.73</cell><cell>1.47</cell><cell>1.01</cell><cell>19.13</cell></row><row><cell>USPS</cell><cell>0.32</cell><cell>9.56</cell><cell>9.63</cell><cell>1.92</cell><cell>1.70</cell><cell>0.75</cell><cell>0.94</cell><cell>8.20</cell><cell>1.59</cell><cell>29.17</cell></row><row><cell>Letters</cell><cell>0.72</cell><cell>3.85</cell><cell>7.74</cell><cell>2.69</cell><cell>3.89</cell><cell>2.88</cell><cell>1.86</cell><cell>23.39</cell><cell>1.44</cell><cell>21.44</cell></row><row><cell>MNIST</cell><cell>8.79</cell><cell>1,231.68</cell><cell>1,211.54</cell><cell>6.40</cell><cell>16.51</cell><cell>6.38</cell><cell>3.82</cell><cell>125.35</cell><cell>7.48</cell><cell>131.60</cell></row><row><cell>Covertype</cell><cell>13.19</cell><cell>N/A</cell><cell>N/A</cell><cell>33.11</cell><cell>101.12</cell><cell>53.46</cell><cell>19.55</cell><cell>116.96</cell><cell>14.08</cell><cell>174.49</cell></row><row><cell>TB-1M</cell><cell>3.25</cell><cell>N/A</cell><cell>N/A</cell><cell>105.15</cell><cell>109.23</cell><cell>35.92</cell><cell>21.79</cell><cell>6.27</cell><cell>10.47</cell><cell>318.29</cell></row><row><cell>SF-2M</cell><cell>31.26</cell><cell>N/A</cell><cell>N/A</cell><cell>226.77</cell><cell>254.98</cell><cell>102.55</cell><cell>51.07</cell><cell>80.44</cell><cell>27.06</cell><cell>658.82</cell></row><row><cell>CC-5M</cell><cell>94.76</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>132.35</cell><cell>46.65</cell><cell>1,726.40</cell></row><row><cell>CG-10M</cell><cell>281.84</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>963.29</cell><cell>318.93</cell><cell>3,603.08</cell></row><row><cell>Flower-20M</cell><cell>579.06</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>3,397.57</cell><cell>764.09</cell><cell>7,225.83</cell></row><row><cell cols="5">conducted 20 times and their average NMI, CA, and time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">costs will be reported. Note that larger values of NMI and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">CA indicate better clustering results.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7</head><label>7</label><figDesc>Average NMI(%) scores (over 20 runs) by our methods and the baseline ensemble clustering methods (The best score in each row is in bold). PenDigits 80.30 ±2.18 76.31 ±2.70 77.69 ±2.54 58.92 ±3.47 75.58 ±2.26 57.64 ±4.14 47.07 ±7.53 77.54 ±1.87 85.34 ±0.91 USPS 63.47 ±0.97 59.02 ±1.69 58.40 ±2.15 49.24 ±2.98 59.63 ±1.76 48.89 ±1.80 39.00 ±3.83 57.55 ±1.78 73.89 ±1.82 Letters 42.53 ±1.32 37.19 ±0.50 36.59 ±0.95 33.64 ±1.03 38.09 ±0.66 34.59 ±0.68 31.81 ±2.01 37.09 ±0.75 45.90 ±0.58 MNIST 67.43 ±1.55 66.19 ±1.49 65.60 ±0.96 54.34 ±3.38 59.93 ±2.23 56.01 ±2.25 34.19 ±4.61 65.06 ±0.95 75.02 ±0.81 Covertype 6.97 ±0.16 N/A N/A 5.86 ±1.84 6.42 ±0.44 5.70 ±0.77 5.26 ±2.82 7.44 ±0.31 9.13 ±1.21 ±1.62 34.20 ±2.51 26.91 ±2.13 10.62 ±4.64 96.80 ±1.90 ±7.11 45.17 ±2.66 41.61 ±6.01 27.05 ±7.73 69.88 ±4.45 ±12.65 0.41 ±0.86 31.62 ±14.99 17.05 ±6.90 98.18 ±7.75 ±5.08 63.75 ±0.61 62.79 ±4.91 49.70 ±6.08 78.08 ±2.43 ±2.43 67.92 ±1.99 60.61 ±2.37 50.37 ±6.32 78.55 ±2.31 92.47 ±2.45 The U-SPEC is listed for reference only; it doesn't participate in the comparison of the ensemble methods.</figDesc><table><row><cell>Dataset</cell><cell>U-SPEC</cell><cell>EAC</cell><cell>WCT</cell><cell>KCC</cell><cell>PTGP</cell><cell>ECC</cell><cell>SEC</cell><cell>LWGP</cell><cell>U-SENC</cell></row><row><cell>TB-1M</cell><cell>95.86 ±0.48</cell><cell>N/A</cell><cell>N/A</cell><cell cols="6">23.36 97.48 ±0.05</cell></row><row><cell>SF-2M</cell><cell>75.59 ±2.12</cell><cell>N/A</cell><cell>N/A</cell><cell cols="6">42.72 77.02 ±2.32</cell></row><row><cell>CC-5M</cell><cell>99.87 ±0.01</cell><cell>N/A</cell><cell>N/A</cell><cell cols="6">33.36 99.91 ±0.00</cell></row><row><cell>CG-10M</cell><cell>78.82 ±1.61</cell><cell>N/A</cell><cell>N/A</cell><cell cols="6">64.78 89.57 ±3.96</cell></row><row><cell cols="5">Flower-20M 61.18 Avg. score 86.86 ±2.05 N/A N/A -N/A N/A 42.74</cell><cell>45.11</cell><cell>42.64</cell><cell>31.21</cell><cell>66.62</cell><cell>74.57</cell></row><row><cell>N-Avg. score</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>59.69</cell><cell>64.12</cell><cell>59.51</cell><cell>45.35</cell><cell>87.82</cell><cell>100.00</cell></row><row><cell>Avg. rank</cell><cell>-</cell><cell>5.40</cell><cell>5.60</cell><cell>4.90</cell><cell>3.60</cell><cell>5.40</cell><cell>6.70</cell><cell>2.80</cell><cell>1.00</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8</head><label>8</label><figDesc>Average CA(%) scores (over 20 runs) by our methods and the baseline ensemble clustering methods (The best score in each row is in bold). ±3.26 81.04 ±4.02 82.97 ±3.17 63.33 ±4.06 78.33 ±2.91 62.36 ±4.12 51.60 ±5.93 81.96 ±2.77 88.56 ±0.61 USPS 63.76 ±1.35 63.39 ±2.76 62.72 ±3.14 53.46 ±3.51 62.68 ±1.92 53.67 ±2.21 45.38 ±3.20 59.73 ±3.30 78.17 ±3.05 Letters 35.71 ±1.47 30.28 ±0.58 30.17 ±1.01 26.90 ±1.23 31.50 ±0.89 27.53 ±0.72 26.12 ±1.93 30.76 ±0.84 37.74 ±1.06 MNIST 74.31 ±2.28 73.12 ±2.73 70.73 ±1.76 59.86 ±5.11 65.06 ±2.75 61.18 ±3.58 43.13 ±4.88 71.98 ±1.67 ±0.58 49.11 ±0.30 49.68 ±0.40 49.86 ±0.94 49.50 ±0.28 ±1.21 82.94 ±1.08 72.50 ±1.48 60.12 ±3.64 99.65 ±0.31 ±5.41 73.46 ±1.76 66.90 ±6.15 55.91 ±5.71 88.71 ±3.28 ±6.24 52.96 ±0.00 62.71 ±5.38 61.91 ±5.49 99.30 ±3.07 ±5.60 63.36 ±1.26 64.74 ±6.80 58.19 ±4.69 81.95 ±3.93 93.99 ±3.25 ±3.37 63.83 ±2.34 56.69 ±2.35 50.70 ±5.02 81.37 ±2.69 93.79 ±3.21</figDesc><table><row><cell>Dataset</cell><cell>U-SPEC</cell><cell>EAC</cell><cell>WCT</cell><cell>KCC</cell><cell>PTGP</cell><cell>ECC</cell><cell>SEC</cell><cell>LWGP</cell><cell>U-SENC</cell></row><row><cell>PenDigits</cell><cell cols="9">84.17 80.58 ±1.75</cell></row><row><cell>Covertype</cell><cell>49.76 ±0.35</cell><cell>N/A</cell><cell>N/A</cell><cell cols="6">49.54 50.73 ±0.62</cell></row><row><cell>TB-1M</cell><cell>99.55 ±0.06</cell><cell>N/A</cell><cell>N/A</cell><cell cols="6">70.05 99.75 ±0.01</cell></row><row><cell>SF-2M</cell><cell>93.60 ±1.00</cell><cell>N/A</cell><cell>N/A</cell><cell cols="6">67.12 93.46 ±2.27</cell></row><row><cell>CC-5M</cell><cell>99.99 ±0.00</cell><cell>N/A</cell><cell>N/A</cell><cell cols="6">66.76 99.99 ±0.00</cell></row><row><cell cols="5">CG-10M 66.96 Flower-20M 81.32 ±2.00 N/A N/A 88.89 ±2.85 N/A N/A 57.78 Avg. score -N/A N/A 58.18</cell><cell>62.32</cell><cell>57.80</cell><cell>50.29</cell><cell>74.49</cell><cell>81.68</cell></row><row><cell>N-Avg. score</cell><cell>-</cell><cell>N/A</cell><cell>N/A</cell><cell>72.48</cell><cell>77.98</cell><cell>72.22</cell><cell>63.53</cell><cell>90.54</cell><cell>100.00</cell></row><row><cell>Avg. rank</cell><cell>-</cell><cell>5.40</cell><cell>5.60</cell><cell>5.00</cell><cell>4.20</cell><cell>5.00</cell><cell>6.30</cell><cell>2.90</cell><cell>1.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 9</head><label>9</label><figDesc>Time costs(s) of our methods and the baseline ensemble clustering methods.</figDesc><table><row><cell>Dataset</cell><cell>U-SPEC</cell><cell>EAC</cell><cell>WCT</cell><cell>KCC</cell><cell>PTGP</cell><cell>ECC</cell><cell>SEC</cell><cell>LWGP</cell><cell>U-SENC</cell></row><row><cell>PenDigits</cell><cell>1.01</cell><cell>8.89</cell><cell>47.01</cell><cell>8.97</cell><cell>11.94</cell><cell>13.56</cell><cell>5.27</cell><cell>5.46</cell><cell>19.13</cell></row><row><cell>USPS</cell><cell>1.59</cell><cell>13.11</cell><cell>48.45</cell><cell>15.87</cell><cell>59.71</cell><cell>23.53</cell><cell>10.15</cell><cell>10.25</cell><cell>29.17</cell></row><row><cell>Letters</cell><cell>1.44</cell><cell>29.60</cell><cell>177.11</cell><cell>33.91</cell><cell>137.46</cell><cell>53.04</cell><cell>16.06</cell><cell>15.58</cell><cell>21.44</cell></row><row><cell>MNIST</cell><cell>7.48</cell><cell>576.71</cell><cell>3,435.19</cell><cell>315.58</cell><cell>2,205.18</cell><cell>417.10</cell><cell>260.96</cell><cell>259.91</cell><cell>131.60</cell></row><row><cell>Covertype</cell><cell>14.08</cell><cell>N/A</cell><cell>N/A</cell><cell>954.89</cell><cell>7,919.02</cell><cell>1,482.43</cell><cell>712.84</cell><cell>685.89</cell><cell>174.49</cell></row><row><cell>TB-1M</cell><cell>10.47</cell><cell>N/A</cell><cell>N/A</cell><cell>1,308.54</cell><cell>1,276.82</cell><cell>2,100.02</cell><cell>1,000.30</cell><cell>989.10</cell><cell>318.29</cell></row><row><cell>SF-2M</cell><cell>27.06</cell><cell>N/A</cell><cell>N/A</cell><cell>2,908.34</cell><cell>2,493.99</cell><cell>4,714.16</cell><cell>2,160.46</cell><cell>2,105.82</cell><cell>658.82</cell></row><row><cell>CC-5M</cell><cell>46.65</cell><cell>N/A</cell><cell>N/A</cell><cell>6,833.38</cell><cell>5,027.91</cell><cell>11,202.43</cell><cell>5,130.84</cell><cell>5,070.21</cell><cell>1,726.40</cell></row><row><cell>CG-10M</cell><cell>318.93</cell><cell>N/A</cell><cell>N/A</cell><cell>17,344.29</cell><cell>11,578.11</cell><cell>27,492.40</cell><cell>10,938.88</cell><cell>10,700.38</cell><cell>3,603.08</cell></row><row><cell>Flower-20M</cell><cell>764.09</cell><cell>N/A</cell><cell>N/A</cell><cell>34,869.83</cell><cell>21,198.87</cell><cell>54,913.10</cell><cell>21,696.29</cell><cell>21,378.63</cell><cell>7,225.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 10 Average</head><label>10</label><figDesc>NMI(%), CA(%), and time costs(s) over 20 runs by different methods with varying number of representatives p.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="3">MNIST</cell><cell></cell><cell cols="3">Covertype</cell><cell>TB-1M</cell><cell>SF-2M</cell></row><row><cell></cell><cell></cell><cell>80</cell><cell>D1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NMI</cell><cell>NMI (%)</cell><cell>50 60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>200</cell><cell>600</cell><cell>1000</cell><cell>1400</cell><cell>200</cell><cell>600</cell><cell>1000</cell><cell>1400</cell></row><row><cell></cell><cell></cell><cell cols="4"># of representatives</cell><cell cols="3"># of representatives</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 11 Average</head><label>11</label><figDesc>NMI(%), CA(%), and time costs(s) over 20 runs by different methods with varying number of nearest representatives K.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">MNIST</cell><cell></cell><cell></cell><cell cols="3">Covertype</cell><cell>TB-1M</cell><cell>SF-2M</cell></row><row><cell></cell><cell></cell><cell></cell><cell>D1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NMI</cell><cell>NMI (%)</cell><cell>50 60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>1 0</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>1 0</cell></row><row><cell></cell><cell></cell><cell cols="4"># of nearest representatives</cell><cell cols="3"># of nearest representatives</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 16 The</head><label>16</label><figDesc>NMI(%), CA(%), and time costs(s) by U-SENC using Approximate K-nearest representatives against Exact K-nearest representatives.</figDesc><table><row><cell>Dataset</cell><cell>MNIST</cell><cell>Covertype</cell><cell>TB-1M</cell><cell>SF-2M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Chang-Dong Wang received the Ph.D. degree in computer science in 2013 from Sun Yat-sen University, China. He is a visiting student at University of Illinois at Chicago from Jan. 2012 to Nov. 2012. He joined Sun Yat-sen University in 2013 as an assistant professor and now he is currently an associate professor with School of Data and Computer Science. His research interests include machine learning and data mining. He has published over 90 papers in international</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data clustering: 50 years beyond k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="651" to="666" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parallel spectral clustering in distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="568" to="586" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale spectral clustering via landmarkbased sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1669" to="1680" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast large-scale spectral clustering via explicit feature mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segmentation using superpixels: A bipartite graph partitioning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Euler clustering on large-scale dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining multiple clusterings using evidence accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L N</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="835" to="850" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A link-based approach to the cluster ensemble problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Iam-On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boongoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2396" to="2409" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">K-means-based consensus clustering: A unified view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="169" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust ensemble clustering using probability trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1312" to="1326" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Locally weighted ensemble clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1460" to="1473" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enhanced ensemble clustering via fast propagation of cluster-wise similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kwoh</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSMC.2018.2876202</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Entropybased consensus clustering for patient stratification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2691" to="2698" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spectral ensemble clustering via weighted k-means: Theoretical and practical evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1129" to="1143" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining multiple clusterings via crowd agreement estimation and multi-granularity link analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="240" to="250" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ensemble clustering using factor graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cluster ensembles: A knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A framework for hierarchical ensemble clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A clustering ensemble: Two-level-refined co-association matrix with path-based transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2699" to="2709" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust ensemble clustering by matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>of IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Solving cluster ensemble problems by bipartite graph partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning (ICML)</title>
		<meeting>of International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ensemble clustering by means of clustering embedding in vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Franek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="833" to="842" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the point for which the sum of the distances to n given points is minimum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Weiszfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Plastria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="41" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient k-nearest neighbor graph construction for generic similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on World Wide Web</title>
		<meeting>of International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="577" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast kNN graph construction with locality sensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)</title>
		<meeting>of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="660" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">RNN-DBSCAN: A density-based clustering algorithm using reverse nearest neighbor density estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1109" to="1121" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Matrix computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>JHU Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Incremental semi-supervised clustering ensemble for high dimensional data clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="701" to="714" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adaptive ensembling of semi-supervised clustering solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1577" to="1590" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<ptr target="http://www.cs.nyu.edu/%7eroweis/data.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Consensus clusterings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prof. of IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="607" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Litekmeans: the fastest matlab implementation of kmeans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<ptr target="http://www.zjucadcg.cn/dengcai/Data/Clustering.html" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large-scale spectral clustering on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danilevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>of International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1486" to="1492" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
