<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inferencing Based on Unsupervised Learning of Disentangled Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-03-07">7 Mar 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Hinz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Universität Hamburg</orgName>
								<address>
									<addrLine>Knowledge Technology Vogt-Koelln-Str. 30</addrLine>
									<postCode>22527</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Universität Hamburg</orgName>
								<address>
									<addrLine>Knowledge Technology Vogt-Koelln-Str. 30</addrLine>
									<postCode>22527</postCode>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inferencing Based on Unsupervised Learning of Disentangled Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-03-07">7 Mar 2018</date>
						</imprint>
					</monogr>
					<note>Accepted as a conference paper at the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN) 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Combining Generative Adversarial Networks (GANs) with encoders that learn to encode data points has shown promising results in learning data representations in an unsupervised way. We propose a framework that combines an encoder and a generator to learn disentangled representations which encode meaningful information about the data distribution without the need for any labels. While current approaches focus mostly on the generative aspects of GANs, our framework can be used to perform inference on both real and generated data points. Experiments on several data sets show that the encoder learns interpretable, disentangled representations which encode descriptive properties and can be used to sample images that exhibit specific characteristics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning meaningful representations of data is an important step for models to understand the world <ref type="bibr" target="#b0">[1]</ref>. Recently, the Generative Adversarial Network (GAN) <ref type="bibr" target="#b1">[2]</ref> has been proposed as a method that can learn characteristics of data distributions without the need for labels. GANs traditionally consist of a generator G, which generates data from randomly sampled vectors Z, and a discriminator D, which tries to distinguish generated data from real data x. During training, the generator learns to generate realistic data samples G(Z), while the discriminator becomes better at distinguishing between the generated and the real data x. As a result, both the generator and the discriminator learn characteristics about the underlying data distribution without the need for any labels <ref type="bibr" target="#b2">[3]</ref>. One desirable characteristic of learned representations is disentanglement <ref type="bibr" target="#b0">[1]</ref>, which means that different parts of the representation encode different factors of the data-generating distribution. This makes representations more interpretable, easier to modify, and is a useful property for many tasks such as classification, clustering, or image captioning.</p><p>To achieve this, Chen et al. <ref type="bibr" target="#b3">[4]</ref> introduced a GAN variant in which the generator's input is split into two parts z and c. Here, z encodes unstructured noise while c encodes meaningful, data-generating factors. Through enforcing high mutual information between c and and the generated images G(z, c) the generator is trained using the inputs c as meaningful encodings for certain image characteristics. For example, a ten-dimensional categorical code for c could represent the ten different digit classes in the MNIST data set. Since no labels are provided the generator has to learn by itself which image characteristics can be represented through c. One drawback of this model is that the only way to perform inference, i.e. map real data samples into a (disentangled) representation, is to use the discriminator. However, there is no guarantee that the discriminator learns good representations of the data in general, as it is trained to discriminate between real and generated data and may therefore focus only on features that are helpful for discriminating these two, but are not necessarily descriptive of the data distribution in general <ref type="bibr" target="#b4">[5]</ref>. Zhang et al. <ref type="bibr" target="#b5">[6]</ref> tried to enforce disentangled representations in order to improve the controllability of the generator. The latent representation is split up into two parts encoding meaningful information and unknown factors of variation. Two additional inference networks are introduced to enforce the disentanglement between the two parts of the latent representation. While this setup yields a better controllability over the generative process it depends on labeled samples for its training objective and can not discover unknown data-generating factors, but only encodes known factors of variation (obtained through labels) in its disentangled representation.</p><p>Donahue et al. <ref type="bibr" target="#b4">[5]</ref> and Dumoulin et al. <ref type="bibr" target="#b6">[7]</ref> introduced an extension which includes an encoder E that learns the encodings of real data samples. The discriminator gets as input both the data sample x (either real or generated) and the according representation (either Z or E(x)) and has to classify them as either coming from the generator or the encoder. The generator and the encoder try to fool the discriminator into misclassifying the samples. As a result, the encoder E learns to approximate the inverse of the generator G and can be used to map real data samples into representations for other applications. However, in these approaches the representations follow a simple prior, e.g. a Gaussian or uniform distribution, and do not exhibit any disentangled properties.</p><p>Our model, the Bidirectional-InfoGAN, integrates some of these approaches by extending traditional GANs with an encoder that learns disentangled representations in an unsupervised setting. After training, the encoder can map data points to meaningful, disentangled representations which can potentially be used for different tasks such as classification, clustering, or image captioning. Compared to the InfoGAN <ref type="bibr" target="#b3">[4]</ref> we introduce an encoder to mitigate the problems of using a discriminator for both the adversarial loss and the inference task. Unlike the Structured GAN <ref type="bibr" target="#b5">[6]</ref> our training procedure is completely unsupervised, can detect unknown data-generating factors, and only introduces one additional inference network (the encoder). In contrast to the Bidirectional GAN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref> we replace the simple prior on the latent representation with a distribution that is amenable to disentangled representations and introduce an additional loss for the encoder and the generator to achieve disentangled representations. On the MNIST, CelebA <ref type="bibr" target="#b7">[8]</ref>, and SVHN <ref type="bibr" target="#b8">[9]</ref> data sets we show that the encoder does learn interpretable representations which encode meaningful properties of the data distribution. Using these we can sample images that exhibit certain characteristics, e.g. digit identity and specific stroke widths for the MNIST data set, or different hair colors and clothing accessories in the CelebA data set. The encoder E encodes images into a representation and tries to fool the discriminator D into misclassifying them as fake if its input is a real image while trying to approximate P (c|x) if its input is a generated image.</p><formula xml:id="formula_0">features E(x) E(G(z, c)) (z, c) G(z, c), (z, c) data x G(z, c) G(z, c) if image is real if image is generated x, E(x) G(z, c), E(G(z, c)) L I (G, E) mutual information G E D V (D, G, E) adversarial cost</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Our model, shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, consists of a generator G, a discriminator D, and an encoder E, which are implemented as neural networks. The input vector Z that is given to the generator G is made up of two parts Z = (z, c). Here, z is sampled from a uniform distribution, z ∼ U (−1, 1), and is used to represent unstructured noise in the images. On the other hand, c is the part of the representation that encodes meaningful information in a disentangled manner and is made up of both categorical values c cat and continuous values c cont . G takes Z as input and transforms it into an image x, i.e. G : Z → x. E is a convolutional network that gets as input either real or fake images and encodes them into a latent representation E : x → Z. D gets as input an image x and the corresponding representation Z concatenated along the channel axis. It then tries to classify the pair as coming either from the generator G or the encoder E, i.e. D : Z × x → {0, 1}, while both G and E try to fool the discriminator into misclassifying its input. As a result the original GAN minimax game <ref type="bibr" target="#b1">[2]</ref> is extended and becomes:</p><formula xml:id="formula_1">min G,E max D V (D, G, E) = E x∼P data [logD(x, E(x))] + E Z∼PZ [log(1 − D(G(Z), Z))],</formula><p>where V (D, G, E) is the adversarial cost as depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>In order to force the generator to use the information provided in c we maximize the mutual information I between c and G(z, c). Maximizing the mutual information directly is hard, as it requires the posterior P (c|x) and we therefore follow the approach by Chen et al. <ref type="bibr" target="#b3">[4]</ref> and define an auxiliary distribution E(c|x) to approximate P (c|x). We then maximize the lower bound L I (G, E) = E c∼P (c),z∼P (z),x∼G(z,c) [log E(c|x)] + H(c) ≤ I(c; G(z, c)), where L I (G, E) is the mutual information depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>. For simplicity reasons we fix the distribution over c and, therefore, the entropy term H(c) is treated as a constant. In our case E is the encoder network which gets images generated by G as input and is trained to approximate the unknown posterior P (c|x). For categorical c cat we use the softmax nonlinearity to represent E(c cat |x) while we treat the posterior E(c cont |x) of continuous c cont as a factored Gaussian. Given this structure, the minimax game for the Bidirectional-InfoGAN (BInfoGAN) is then min</p><formula xml:id="formula_2">c 1 0 c 1 1 c 1 2 c 1 3 c 1 4 c 1 5 c 1 6 c 1 7 c 1 8 c 1 9 (a)</formula><formula xml:id="formula_3">G,E max D V BInfoGAN (D, G, E) = V (D, G, E) − λL I (G, E)</formula><p>where λ determines the strength of the impact of the mutual information criterion L I and is set to 1.0 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We perform experiments on the MNIST, the CelebA <ref type="bibr" target="#b7">[8]</ref>, and the SVHN <ref type="bibr" target="#b8">[9]</ref> data set. While the final performance of the model is likely influenced by choosing the "optimal" characteristics for c this is usually not possible, since we do not know all data-generating factors beforehand. When choosing the characteristics and dimensionality of the disentangled vector c we therefore mostly stick with the values previously chosen by Chen et al. <ref type="bibr" target="#b3">[4]</ref>. For further information on the network architectures and more examples of the learned characteristics on the different data sets see our Git: https://github.com/tohinz/Bidirectional-InfoGAN.</p><p>On the MNIST data set we model the latent code c with one categorical variable c 1 ∼ Cat(K = 10, p = 0.1) and two continuous variables c 2 , c 3 ∼ U <ref type="figure" target="#fig_0">(−1, 1)</ref>. During the optimization process and without the use of any labels the encoder learns to use c 1 to encode different digit classes, while c 2 and c 3 encode stroke width and digit rotation. <ref type="figure" target="#fig_1">Fig. 2a</ref> shows images randomly sampled from the test set according to the ten different categorical values. We can see that the encoder has learned to reliably assign a different categorical value for different digits. Indeed, by manually matching the different categories in c 1 to a digit type, we achieve a test set accuracy of 96.61% (±0.32%, averaged over 10 independent runs) without ever using labels during the training, compared to Chen et al. <ref type="bibr" target="#b3">[4]</ref> (unsupervised) with an accuracy of 95%, and Zhang et al. <ref type="bibr" target="#b5">[6]</ref> (semi-supervised, 20 labels) with an accuracy of 96%. <ref type="figure" target="#fig_1">Fig. 2b</ref> shows images sampled from the test set for different values of c 2 and c 3 . We see that we can use the encodings from E to now sample for digits with certain characteristics such as stroke width and rotation, even though this information was not explicitly provided during training.</p><p>On the CelebA data set the latent code is modeled with four categorical codes c 1 , c 2 , c 3 , c 4 ∼ Cat(K = 10, p = 0.1) and four continuous variables c 5 , c 6 , c 7 , c 8 ∼ U <ref type="figure" target="#fig_0">(−1, 1)</ref>. Again, the encoder learns to associate certain image characteristics with specific codes in c. This includes characteristics such as the presence of glasses, hair color, and background color and is visualized in <ref type="figure" target="#fig_2">Fig. 3a</ref>.</p><p>On the SVHN data set we use the same network architecture and latent code representations as for the CelebA data set. Again, the encoder learns interpretable, disentangled representations encoding characteristics such as image background, contrast and digit type. See <ref type="figure" target="#fig_2">Fig. 3b</ref> for examples sampled from the SVHN test set. These results indicate that the Bidirectional-InfoGAN is indeed capable of mapping data points into disentangled representations that encode meaningful characteristics in a completely unsupervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We showed that an encoder coupled with a generator in a Generative Adversarial Network can learn disentangled representations of the data without the need for any explicit labels. Using the encoder network we maximize the mutual information between certain parts of the generator's input and the images that are generated from it. Through this the generator learns to associate certain image characteristics with specific parts of its input. Additionally, the adversarial cost from the discriminator forces both the generator to generate realistic looking images and the encoder to approximate the inverse of the generator, leading to disentangled representations that can be used for inference.</p><p>The learned characteristics are often meaningful and humanly interpretable, and can potentially help with other tasks such as classification and clustering. Additionally, our method can be used as a pre-training step on unlabeled data sets, where it can lead to better representations for the final task. However, currently we have no influence over which characteristics are learned in the unsupervised setting which means that the model can also learn characteristics or features that are meaningless or not interpretable by humans. In the future, this can be mitigated by combining our approach with semi-supervised approaches, in which we can supply a limited amount of labels for the characteristics we are interested in to exert more control over which data-generating factors are learned while still being able to discover "new" generating factors which do not have to be known or specified beforehand.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>High-level overview of the Bidirectional-InfoGAN. The generator G generates images from the vector (z, c) and tries to fool the discriminator into classifying them as real.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Images sampled from the MNIST test set. (a) Each row represents one value of the ten-dimensional code c 1 , which encodes different digits despite never seeing labels during the training process. (b) Images with maximum and minimum values for c 2 and c 3 for each categorical value from c 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Images sampled from the (a) CelebA and (b) SVHN test sets. Each row shows images sampled according to one specific categorical variable c cat which represents a learned characteristic.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3900" to="3910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
