<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Task Augmentation by Rotating for Meta-Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Liu</surname></persName>
							<email>jialin@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cognitive Science Department</orgName>
								<orgName type="institution">Xiamen University Fujian</orgName>
								<address>
									<postCode>361005</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chao</surname></persName>
							<email>fchao@xmu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Cognitive Science Department Xiamen University Fujian</orgName>
								<address>
									<postCode>361005</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Min</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical Engineering Yuan</orgName>
								<orgName type="institution">ze University</orgName>
								<address>
									<addrLine>Chung-Li, Tao-Yuan 320</addrLine>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Task Augmentation by Rotating for Meta-Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data augmentation is one of the most effective approaches for improving the accuracy of modern machine learning models, and it is also indispensable to train a deep model for meta-learning. In this paper, we introduce a task augmentation method by rotating, which increases the number of classes by rotating the original images 90, 180 and 270 degrees, different from traditional augmentation methods which increase the number of images. With a larger amount of classes, we can sample more diverse task instances during training. Therefore, task augmentation by rotating allows us to train a deep network by metalearning methods with little over-fitting. Experimental results show that our approach is better than the rotation for increasing the number of images and achieves state-of-theart performance on miniImageNet, CIFAR-FS, and FC100 few-shot learning benchmarks. The code is available on</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Although the machine learning systems have achieved a human-level ability in many fields with a large amount of data, learning from a few examples is still a challenge for modern machine learning techniques. Recently, the machine learning community has paid significant attention to this problem, where few-shot learning is the common task for meta-learning (e.g., <ref type="bibr" target="#b36">[20,</ref><ref type="bibr" target="#b21">5,</ref><ref type="bibr" target="#b46">30,</ref><ref type="bibr" target="#b43">27]</ref>). The purpose of few-shot learning is to learn to maximize generalization accuracy across different tasks with few training examples. In a classification application of the few-shot learning, tasks are generated by sampling from a conventional classification dataset; then, training samples are randomly selected from several classes in the classification dataset. In addition, a part of the examples is used as training examples and testing examples. Thus, a tiny learning task is formed by these examples. The meta-learning methods are applied to control the learning process of a base learner, so as to correctly classify on testing examples.</p><p>Data augmentation is widely used to improve the training of deep learning models. Usually, data augmentation is regarded as an explicit form of regularization <ref type="bibr" target="#b25">[9,</ref><ref type="bibr" target="#b42">26,</ref><ref type="bibr" target="#b29">13]</ref>. Data augmentation aims at artificially generating the training data by using various translations on existing data, such as: adding noises, cropping, flipping, rotation, translation, etc. The general idea of data augmentations is increasing the number of images by change data slightly to be different from original data, but the data still can be recognized by human. The new images involved in the classes are identical to the original data, we call this as Image Aug.</p><p>However, the minimum units of meta-learning are tasks rather than data, so we should use rotation operation to augment the number of tasks, which is called as task augmentation (referred to Task Aug). Task Aug means increasing the types of task instances by increasing the data that can be clearly recognized as the different classes as the original data and associating them as the novel classes(we show examples in <ref type="figure" target="#fig_1">Figure 1</ref>). This is important for the metalearning, since meta-learning models require to predict unseen classes during the testing phase, increasing the diverseness of tasks would help models to generate to unseen classes.</p><p>In experiments, we compared two cases, 1) the new images are converted to the classes of original images and 2) the new images are associated to the novel classes with the method proposed in <ref type="bibr" target="#b19">[3]</ref> on CIFAR-FS, FC100, miniIma-geNet few-shot learning tasks, and showed the second case got better results. Then the proposed method is evaluated by experiments with the state of art meta-learning methods <ref type="bibr" target="#b43">[27,</ref><ref type="bibr" target="#b31">15,</ref><ref type="bibr" target="#b19">3]</ref> on CIFAR-FS, FC100, miniImageNet fewshot learning tasks, and compare with the results without the data augmentation by rotating. In the comparative experiments, Task Aug by rotating achieves the better accuracy than the original meta-learning methods. Besides, the best results of our experiments exceed the current state-of-art result over a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Meta-learning involves two hierarchies learning processes: low-level and high-level. The low-level learning process learns to deal with general tasks, often termed as the "inner loop"; and the high-level learning process learns to improve the performance of a low-level task, often termed as the "outer loop". Since models are required to handle sensory data like images, deep learning methods are often applied for the "outer loop". However, the machine learning methods applied for the "inner loop" are very diverse. Based on different methods in the "inner loop", meta-learning can be applied in image recognition <ref type="bibr" target="#b20">[4,</ref><ref type="bibr" target="#b40">24,</ref><ref type="bibr" target="#b21">5,</ref><ref type="bibr" target="#b46">30,</ref><ref type="bibr" target="#b36">20]</ref>, image generation <ref type="bibr" target="#b18">[2,</ref><ref type="bibr" target="#b47">31,</ref><ref type="bibr" target="#b37">21]</ref>, reinforce learning <ref type="bibr" target="#b21">[5,</ref><ref type="bibr" target="#b15">1]</ref>, and etc. This work focuses on few-shot learning image recognition based on meta-learning. Therefore, in the experiment, the methods applied in the "inner loop" are able to classify data, and they are K-nearest neighbor (KNN), Support Vector Machine (SVM) and ridge regression, respectively <ref type="bibr" target="#b43">[27,</ref><ref type="bibr" target="#b31">15,</ref><ref type="bibr" target="#b19">3]</ref>.</p><p>Previous studies have introduced many popular regularization techniques to few-shot learning from deep learning, such as weight decay, dropout, label smooth <ref type="bibr" target="#b19">[3]</ref>, and data augmentation. Common data augmentation techniques for image recognition are usually designed manually and the best augmentation strategies depend on dataset. In natural color image datasets, random cropping and random horizontal flipping are the most common. Since the few-shot learning tasks consist of natural color images, the random horizontal flipping and random cropping are applied in fewshot learning. In addition, color (brightness, contrast, and saturation) jitter is often applied in the works of few-shot learning <ref type="bibr" target="#b23">[7,</ref><ref type="bibr" target="#b35">19]</ref>.</p><p>Other data augmentation technologies related to fewshot learning include generating samples by few-shot learning and generating samples for few-shot learning. The former tried to synthesize additional examples via transferring, extracting, and encoding to create the data of the new classes, that are intra-class relationships between pairs of reference classes' data instances <ref type="bibr" target="#b24">[8,</ref><ref type="bibr" target="#b41">25]</ref>. The later tried to apply meta-learning in a few-shot generation to gener-ate samples from other models <ref type="bibr" target="#b18">[2]</ref>.In addition to these two types of studies, the data augmentation technology most closed to the new proposed approach is applied to Omniglot dataset, which consists of handwritten words <ref type="bibr" target="#b30">[14]</ref>. They created the novel classes by rotating the original images 90, 180 and 270 degrees <ref type="bibr" target="#b40">[24]</ref>. However, when this approach is applied for the natural color image, it would be slightly changed, and we will explain this in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>We adopt the formulation purposed by <ref type="bibr" target="#b46">[30]</ref> to describe the N -way K-shot task. A few-shot task contains many task instances (denoted by T i ), each instance is a classification problem consisting of the data sampled from N classes. The classes are randomly selected from a classes set. The classes set are split into M tr , M val and M test for a training class set C tr , a validation classes set C val , and a test classes set C test . In particular, each class does not overlap others (i.e., the classes used during testing are unseen classes during training). Data is randomly sampled from C tr , C val and C test , so as to create task instances for training meta-set S tr , validation meta-set S val , and test meta-set S test , respectively. The validation and testing meta-sets are used for model selection and final evaluation, respectively. The data in each task instance, T i , are divided into training examples D tr and validation examples D val . Both of them only contains the data from N classes which sampled from the appropriate classes set randomly (for a task instance applied during training, the classes form a subset of the training classes set C tr ). In most settings, the training set D tr = {(x k n , y k n )|n = 1 . . . N ; k = 1 . . . K} consists of K data instances from each class, this processing usually called as a "shot". The validation set, D val , consists of several other data instances from the same classes, this processing is usually called as a "query". An evaluation is provided for generalization performance on the N classification task instance D tr . Note that: the validation set of a task instance D val (for optimizing model during "outer loop") is different from the held-out validation classes set C val and meta-set S val (for model selection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Task Augmentation by Rotating</head><p>This work is to increase the size of the training classes set, M tr , by rotating all images within the training classes set with 90, 180, 270 degrees. The size, M tr , is increased for three times. In the Omniglot dataset consisting of handwritten words <ref type="bibr" target="#b40">[24]</ref>, this approach works well, since it can rotate a handwritten word multiple of 90 degrees and treat the new one as another word; in addition, it is really possible that the novel word is similar to some words, which are not included in the training classes but existed.   The number of ways, shots and queries N ,</p><formula xml:id="formula_0">K, H 1: t ← t + 1 2: p ← p max * min{1, t T } 3: n ∼ Binomial(N, p) 4: D tr , D val ← {}, {} 5: V ← Sample N − n from {1, 2, · · · , M } 6: for all v ∈ V do 7: D ← Sample K + H from c v 8: D tr ← D tr ∪ First K of D 9: D val ← D val ∪ Last H of D 10: end for 11: U ← Sample n from {M, M + 1, · · · , 4M } 12: for all u ∈ U do 13: v ← (u mod M ) + 1 14: D ← Sample K + H from c v 15: r ← u M</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>Rotate all x ∈ {x|(x, y) ∈ D} 90r degrees 17: For natural images, it is obvious that the images generated by rotating is real enough. But should the new generated images be classified as the novel classes or the original classes? It dependents on the similarity between the new images and the original classes. If the most of they are similar enough, the new images should be classified as the original classes, and vice versa. This logic shows that one of the two methods must be effective. Since there are almost not works merge the new images into the original classes which worked well, we assume that Task Aug by rotating is effective for meta-learning, and we will compare two methods in experiments.</p><formula xml:id="formula_1">D tr ← D tr ∪ First K of D 18: D val ← D val ∪ Last H of D 19: end for 20: return (D tr , D val ) c1 c2 cM p 1 -p pmax * min{1, t T } N -n</formula><p>Besides, it is different from in handwritten that we assign the new data smaller weights than the original data, so as to make models prioritize learning the features of the original classes, since the images generated by rotating rarely exist in the original data. This way makes the features of the novel classes as a supplement to prevent the augmented data from taking up large capacity in the model, which is same as other common data augment methods.</p><p>The smaller weights are implemented in two ways, 1) lower probability and 2) delaying the probability of selecting the novel classes. For a class in a task instance, the probability of the class coming from the novel classes is p, and the probability coming from the original classes is 1−p. Besides, the initial p is set to 0, then linearly rises from 0 to p max for the first T tasks. The max probability p max is set lower than the proportion of the novel classes in all classes to make each novel class have a lower probability than each original class. The whole process of Task Aug on a classes set is summarized in Algorithm 1 and <ref type="figure" target="#fig_3">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ensemble</head><p>In this work, we also compare the methods with the training protocol with ensemble method <ref type="bibr" target="#b27">[11]</ref> in addition to the standard training protocol, which choosing a model by the validation set. The training protocol with an ensemble method use the models with different training epoch to an ensemble model, in order to better use the models obtained in a single training process, and this approach has been proved to be valid for meta-learning by experiments <ref type="bibr" target="#b32">[16]</ref>. We adopt this ensemble method. However, unlike <ref type="bibr" target="#b27">[11]</ref> and <ref type="bibr" target="#b32">[16]</ref> that we did not use cyclic annealing for learning rate and any methods to select models. We directly took the average of the prediction of all models, which are saved according to an interval of 1 epoch. In Section 4, the methods with this ensemble approach are marked by "+ens".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the proposed method on few-shot learning tasks. In order to ensure fair, both the results of baseline and Task Aug were run in our own environment. The comparative experiment is designed to answer the following questions: (1) Image Aug and Task Aug by rotating which is able to improve the performance of meta-learning? (2) How much should the probably for the novel classes be set? (3) Is Task Aug by rotating able to improve the performance of the current popular meta-learning methods?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Configuration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Backbone</head><p>Following <ref type="bibr" target="#b31">[15,</ref><ref type="bibr" target="#b34">18,</ref><ref type="bibr" target="#b33">17]</ref>, we used ResNet-12 network in our experiments. The ResNet-12 network had four residual blocks which contains three 3 × 3 convolution, batch normalization and Leaky ReLU with 0.1 negative slope. One 2 × 2 max-pooling layer is used for reducing the size of the feature map. The numbers of the network channels were 64, 160, 320 and 640, respectively. DropBlock regularization <ref type="bibr" target="#b22">[6]</ref> is used in the last two residual blocks, the conventional dropout <ref type="bibr" target="#b26">[10]</ref> is used in the first two residual blocks.</p><p>The block sizes of DropBlock were set to 2 and 5 for CI-FAR derivatives and ImageNet derivatives, respectively. In all experiments, the dropout possibility was set to 0.1. The global average pooling was not used for the final output of the last residual block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Base Learners</head><p>We used ProtoNets <ref type="bibr" target="#b43">[27]</ref>, MetaOptNet-SVM <ref type="bibr" target="#b31">[15]</ref> (we write it as M-SVM) and Ridge Regression Differentiable Discriminator (R2-D2) <ref type="bibr" target="#b19">[3]</ref> as basic methods to verify the effective of Task Aug.</p><p>For ProtoNets, we did not use a higher way for training than testing like <ref type="bibr" target="#b43">[27]</ref>. Instead, the equal number of shot and way were used in both training and evaluation, and its output multiplied by a learnable scale before the softmax following <ref type="bibr" target="#b34">[18,</ref><ref type="bibr" target="#b31">15]</ref>.</p><p>For M-SVM, we set training shot to 5 for CIFAR-FS; 15 for FC100; and 15 for miniImageNet; regularization parameter of SVM was set to 0.1; and a learnable scale was used following <ref type="bibr" target="#b31">[15]</ref>. We did not use label smoothing like <ref type="bibr" target="#b31">[15]</ref>, because we did not find that label smoothing can improve the performance in our environment. This was also affirmed from the <ref type="bibr" target="#b31">[15]</ref> author's message on GitHub, that Program language packages and environment might affect results of the meta-learning method.</p><p>For R2-D2, we set the same training shot as for M-SVM, and used a learnable scale and bias following <ref type="bibr" target="#b19">[3]</ref>. It was different from <ref type="bibr" target="#b19">[3]</ref> we used a fixed regularization parameter of ridge regression which was set to 50 because <ref type="bibr" target="#b19">[3]</ref> has confirmed that making it learnable might not be helpful.</p><p>Last, for all methods, each class in a task instance contained 6 test (query) examples during training and 15 test (query) examples during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Training Configuration</head><p>Stochastic gradient descent (SGD) was used. Following <ref type="bibr" target="#b45">[29]</ref>, we set weight decay and Nesterov momentum to 0.0005 and 0.9, respectively. Each mini-batch contained 8 task instances. The meta-learning model was trained for 60 epochs, and 1000 mini-batchs for each epoch. We set the initial learning rate to 0.1, then multiplied it by 0.06, 0.012, and 0.0024 at epochs 20, 40 and 50, respectively, as in <ref type="bibr" target="#b23">[7]</ref>. The results, which are marked by "+ens" were used the 60 models saved after each epoch to become an ensemble model. For the final training, the training classes set was augmented by the validation classes set. When we only chose one model, we will chose the model at the epoch where we got the best model during training on the training classes set. The results of the final run are marked by "+val" in this subsection. Since the base idea of "+ens" was proposed by other works and "+val" is popular for metalearning, we do not explain more details about them. <ref type="figure">Figure 3</ref>: The accuracies (%) on meta-test sets with varying probability p max for the novel classes.The 95% confidence interval is denoted by the shaded region.</p><formula xml:id="formula_2">7KHPD[SUREDELOLW\RIURWDWLRQ $FFXUDF\ &amp;,)$5)6ZD\VKRW %DVHOLQHVKRW ,PDJH$XJVKRW 7DVN$XJVKRW 7KHPD[SUREDELOLW\RIURWDWLRQ $FFXUDF\ )&amp;ZD\VKRW 7KHPD[SUREDELOLW\RIURWDWLRQ $FFXUDF\ PLQL,PDJH1HWZD\VKRW 7KHPD[SUREDELOLW\RIURWDWLRQ $FFXUDF\ &amp;,)$5)6ZD\VKRW %DVHOLQHVKRW ,PDJH$XJVKRW 7DVN$XJVKRW 7KHPD[SUREDELOLW\RIURWDWLRQ $FFXUDF\ )&amp;ZD\VKRW 7KHPD[SUREDELOLW\RIURWDWLRQ $FFXUDF\ PLQL,PDJH1HWZD\VKRW 7KHPD[SUREDELOLW\RIURWDWLRQ $FFXUDF\ &amp;,)$5)6ZD\VKRWZLWKHQVHPEOH 7KHPD[SUREDELOLW\RIURWDWLRQ $FFXUDF\ )&amp;ZD\VKRWZLWKHQVHPEOH 7KHPD[SUREDELOLW\RIURWDWLRQ $FFXUDF\ PLQL,PDJH1HWZD\VKRWZLWKHQVHPEOH 7KHPD[SUREDELOLW\RIURWDWLRQ $FFXUDF\ &amp;,)$5)6ZD\VKRWZLWKHQVHPEOH 7KHPD[SUREDELOLW\RIURWDWLRQ $FFXUDF\ )&amp;ZD\VKRWZLWKHQVHPEOH 7KHPD[SUREDELOLW\RIURWDWLRQ $FFXUDF\ PLQL,PDJH1HWZD\VKRWZLWKHQVHPEOH</formula><p>For data augmentation, we adopted random crop, horizontal flip, and color (brightness, saturation, and contrast) jitter data augmentation following the work of <ref type="bibr" target="#b23">[7,</ref><ref type="bibr" target="#b35">19]</ref>. In the experiments of comparing Task Aug and Image Aug by rotating, R2-D2 was applied, and we set T to 80000. In the evaluation of Task Aug for ProtoNets and M-SVM, we set p max to the value getting the best results for R2-D2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Dataset</head><p>The CIFAR-FS <ref type="bibr" target="#b19">[3]</ref> containing all 100 classes from CIFAR-100 <ref type="bibr" target="#b28">[12]</ref> is proposed as few-shot classification benchmark recently. These classes are randomly divided into training classes, validation classes and test classes. The three types contain 64, 16 and 20 classes, respectively. There are 600 nature color images of size 32 × 32 in each class.</p><p>The FC100 <ref type="bibr" target="#b34">[18]</ref> are also derived from CIFAR-100 <ref type="bibr" target="#b28">[12]</ref>, and the 100 classes are grouped into 20 superclasses. The training, validation, and testing classes contain 60 classes from 12 superclasses, 20 classes from 4 superclasses, and 20 classes from 4 superclasses, respectively. The target is to minimize the information overlap between classes to make it more challenging than current few-shot classification tasks. Same as CIFAR-FS, there are 600 nature color images of size 32 × 32 in each class.</p><p>The miniImageNet <ref type="bibr" target="#b46">[30]</ref> is one of the most popular benchmark for few-shot classification, which contains 100 classes randomly selected from ILSVRC-2012 <ref type="bibr" target="#b38">[22]</ref>. The classes are randomly divided into training classes, validation classes and test classes, and them contain 64, 16 and 20 classes, respectively. There are 600 nature color images of size 84 × 84 in each class. Since <ref type="bibr" target="#b46">[30]</ref> did not release the class splits, we use the more common split proposed by <ref type="bibr" target="#b36">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison between Task Aug and Image Aug</head><p>To prove our assumption that rotation multi 90 degrees for Task Aug is better than that for Image Aug, we draw the accuracy curves depending on p max for both Task Aug and Image Aug (curves showed in <ref type="figure">Figure 3</ref>). The linear rising of p was also used for Image Aug, and T = 80000 for both Task Aug and Image Aug. In all the results showed in <ref type="figure">Figure 3</ref>, the training classes set was not augmented by the validation classes set.</p><p>As shown in <ref type="figure">Figure 3</ref>, the performance of Task Aug on most of the regimes is better than Image Aug and baseline in general. Besides, we observed that: with the increase of p max , the accuracy rises at first, reaches the peaks between 0.25 and 0.5, then declines and reaches baseline when p max = 0.75 at the end, which is the proportion of the novel classes in all classes. The accuracy of Task Aug on CIFAR-FS, FC100 and miniImagNet reach the peaks at 0.5, 0.25 and 0.25 respectively. At the same time, the rotation multi 90 degrees for Image Aug cannot improve or even cause </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of Task Aug</head><p>In order to further prove the proposed approach can improve the performance of meta-learning, we evaluate Task Aug by rotating on several meta-learning methods in this section.</p><p>We choose several currently the state of art base learners for experiments, we detail in Section 4.1.2. Besides, the training protocol with ensemble method can get better results than the standard training protocol, we involve it in the experiments. We think this is important, because the proposed method can only be a contribution if it can further improve performance based on the best method available at present.</p><p>Results. <ref type="table" target="#tab_0">Table 1-6</ref> show the average accuracies (%) with 95% confidence intervals of the methods with and without Task Aug, and the best results are highlighted. The tables show that the proposed method can improve the performance in most of cases.</p><p>We can observe that: some results without the ensemble approach <ref type="bibr" target="#b27">[11]</ref> of baseline and Task Aug are close, but the advantage of Task Aug is still obvious on the comparison results with the ensemble approach. We suspect that the scale of backbone limits the performance of the best model. A larger scale backbone is needed for the training process with Task Aug. For the results of ensemble approach, since Task Aug reduces the over-fitting, more models during the training process have good performance, which provide ensemble with models of higher quality.</p><p>Last we compare the results of this work with the results proposed by the prior works, in order to show how much  this work raises the baselines after combining several prior methods and the proposed method, and they are showed in <ref type="table">Table 7</ref>, 8 and 9. The tables show that the highest accuracies of our experiments exceeded the current state-of-art accuracies 2% to 5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a Task Level Data Augmentation (Task Aug), a data augmentation technique that increased the number of training classes to provide more diverse fewshow task instances for meta-learning. We proved that Task Aug was valid for CIFAR-FS, FC100, and miniImageNet, <ref type="table">Table 4</ref>: Comparison to the average accuracies (%) with 95% confidence intervals between the methods with and without Task Aug on FC100 5-way 5-shot. and exceeded the result of the previous works. Task Aug achieved the performance by rotating the images 90, 180 and 270 degrees. This method is simple and cost-effective. With the ensemble method, we exceeded the state-of-the-art result over a large margin.</p><p>Future work will focus on searching different network structures for meta-learning, since the training with Task Aug would require larger model. Besides, we will try to apply Task Aug to other few-shot learning tasks to verify its effectiveness. Another interesting topic is to build other approaches for Task Aug, such as swapping channel order, picture blend or even auto augmentation. <ref type="table">Table 6</ref>: Comparison to the average accuracies (%) with 95% confidence intervals between the methods with and without Task Aug on miniImageNet 5-way 5-shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Baseline Task Aug ProtoNets <ref type="bibr" target="#b43">[27]</ref> 75  <ref type="table">Table 7</ref>: The average accuracies (%) with 95% confidence intervals on CIFAR-FS. * CIFAR-FS results from <ref type="bibr" target="#b19">[3]</ref>. † Result from <ref type="bibr" target="#b31">[15]</ref>.</p><p>Method 1-shot 5-shot MAML * <ref type="bibr" target="#b21">[5]</ref> 58.9±1.9 71.5±1.0 R2-D2 <ref type="bibr" target="#b19">[3]</ref> 65.3±0.2 79.4±0.1 ProtoNets † <ref type="bibr" target="#b43">[27]</ref> 72.2±0.7 83.5±0.5 M-SVM <ref type="bibr" target="#b31">[15]</ref> 72.8±0.7 85.0±0.5 M-SVM (best) (our) 76.75±0.46 88.38±0.33 R2-D2 (best) (our) 77.66±0.46 88.33±0.33 <ref type="table">Table 8</ref>: The average accuracies (%) with 95% confidence intervals on FC100. † FC100 result from <ref type="bibr" target="#b31">[15]</ref>.</p><p>Method 1-shot 5-shot TADAM <ref type="bibr" target="#b34">[18]</ref> 40.1±0.4 56.1±0.4 ProtoNets † <ref type="bibr" target="#b43">[27]</ref> 37.5±0.6 52.5±0.6 MTL <ref type="bibr" target="#b44">[28]</ref> 45.1±1.8 57.6±0.9 M-SVM <ref type="bibr" target="#b31">[15]</ref> 47.2±0.6 62.5±0.6 M-SVM (best) (our) 49.77±0.45 67.17±0.41 R2-D2 (best) (our) 51.35±0.46 67.66±0.42 <ref type="table">Table 9</ref>: The average accuracies (%) with 95% confidence intervals on miniImageNet. * Result from <ref type="bibr" target="#b31">[15]</ref>. Here only list the best results of previous works due to the shortage of space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Examples of the novel created classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Task Augmentation by Rotating. Require: Classes set C = {c 1 , c 2 , . . . , c M }; Max possibility for Task Aug p max ; The delay to Task Aug T; The current count t;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>from original classes n from novel classes D tr D val The process of generating a task instance with Task Aug by rotating.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison to the average accuracies (%) with 95% confidence intervals between the methods with and without Task Aug on CIFAR-FS 5-way 1-shot.</figDesc><table><row><cell>Method</cell><cell>Baseline</cell><cell>Task Aug</cell></row><row><cell>ProtoNets [27]</cell><cell cols="2">71.88±0.52 74.15±0.50</cell></row><row><cell>ProtoNets (+ens)</cell><cell cols="2">73.95±0.51 75.89±0.48</cell></row><row><cell>ProtoNets (+val)</cell><cell cols="2">73.20±0.51 75.10±0.49</cell></row><row><cell cols="3">ProtoNets (+ens+val) 76.05±0.49 77.28±0.47</cell></row><row><cell>M-SVM [15]</cell><cell cols="2">71.52±0.51 72.95±0.48</cell></row><row><cell>M-SVM (+ens)</cell><cell cols="2">74.12±0.50 75.85±0.47</cell></row><row><cell>M-SVM (+val)</cell><cell cols="2">72.42±0.50 73.13±0.47</cell></row><row><cell>M-SVM (+ens+val)</cell><cell cols="2">75.91±0.48 76.75±0.46</cell></row><row><cell>R2-D2 [3]</cell><cell cols="2">72.27±0.51 74.42±0.48</cell></row><row><cell>R2-D2 (+ens)</cell><cell cols="2">75.06±0.50 76.51±0.47</cell></row><row><cell>R2-D2 (+val)</cell><cell cols="2">73.52±0.50 76.02±0.47</cell></row><row><cell>R2-D2 (+ens+val)</cell><cell cols="2">76.40±0.49 77.66±0.46</cell></row><row><cell>worse performance.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison to the average accuracies (%) with 95% confidence intervals between the methods with and without Task Aug on CIFAR-FS 5-way 5-shot.</figDesc><table><row><cell>Method</cell><cell>Baseline</cell><cell>Task Aug</cell></row><row><cell>ProtoNets [27]</cell><cell cols="2">84.14±0.36 85.37±0.35</cell></row><row><cell>ProtoNets (+ens)</cell><cell cols="2">85.72±0.35 87.33±0.33</cell></row><row><cell>ProtoNets (+val)</cell><cell cols="2">85.29±0.35 86.53±0.34</cell></row><row><cell cols="3">ProtoNets (+ens+val) 86.88±0.34 88.24±0.33</cell></row><row><cell>M-SVM [15]</cell><cell cols="2">84.01±0.36 85.91±0.36</cell></row><row><cell>M-SVM (+ens)</cell><cell cols="2">85.85±0.34 87.73±0.33</cell></row><row><cell>M-SVM (+val)</cell><cell cols="2">84.94±0.36 86.94±0.34</cell></row><row><cell>M-SVM (+ens+val)</cell><cell cols="2">87.15±0.34 88.38±0.33</cell></row><row><cell>R2-D2 [3]</cell><cell cols="2">84.60±0.36 86.02±0.35</cell></row><row><cell>R2-D2 (+ens)</cell><cell cols="2">86.11±0.34 87.63±0.34</cell></row><row><cell>R2-D2 (+val)</cell><cell cols="2">85.39±0.36 86.73±0.34</cell></row><row><cell>R2-D2 (+ens+val)</cell><cell cols="2">87.04±0.34 88.33±0.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison to the average accuracies (%) with 95% confidence intervals between the methods with and without Task Aug on FC100 5-way 1-shot.</figDesc><table><row><cell>Method</cell><cell>Baseline</cell><cell>Task Aug</cell></row><row><cell>ProtoNets [27]</cell><cell cols="2">37.53±0.40 38.89±0.40</cell></row><row><cell>ProtoNets (+ens)</cell><cell cols="2">40.04±0.41 42.00±0.43</cell></row><row><cell>ProtoNets (+val)</cell><cell cols="2">43.63±0.43 44.91±0.46</cell></row><row><cell cols="3">ProtoNets (+ens+val) 47.16±0.46 48.91±0.47</cell></row><row><cell>M-SVM [15]</cell><cell cols="2">40.50±0.39 41.17±0.40</cell></row><row><cell>M-SVM (+ens)</cell><cell cols="2">43.24±0.42 44.38±0.42</cell></row><row><cell>M-SVM (+val)</cell><cell cols="2">46.72±0.45 47.39±0.44</cell></row><row><cell>M-SVM (+ens+val)</cell><cell cols="2">49.50±0.46 49.77±0.45</cell></row><row><cell>R2-D2 [3]</cell><cell cols="2">40.66±0.41 41.47±0.40</cell></row><row><cell>R2-D2 (+ens)</cell><cell cols="2">43.27±0.42 44.75±0.43</cell></row><row><cell>R2-D2 (+val)</cell><cell cols="2">47.12±0.44 48.21±0.45</cell></row><row><cell>R2-D2 (+ens+val)</cell><cell cols="2">49.92±0.45 51.35±0.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.24±0.37 77.00±0.36 ProtoNets (+ens) 78.11±0.34 79.77±0.34 ProtoNets (+val) 76.98±0.36 77.59±0.37 ProtoNets (+ens+val) 79.54±0.35 80.60±0.34 M-SVM [15] 77.85±0.34 78.90±0.34</figDesc><table><row><cell>M-SVM (+ens)</cell><cell>80.18±0.32 81.35±0.32</cell></row><row><cell>M-SVM (+val)</cell><cell>78.65±0.34 79.97±0.33</cell></row><row><cell>M-SVM (+ens+val)</cell><cell>81.39±0.32 82.13±0.31</cell></row><row><cell>R2-D2 [3]</cell><cell>77.44±0.34 78.81±0.34</cell></row><row><cell>R2-D2 (+ens)</cell><cell>79.90±0.33 81.08±0.32</cell></row><row><cell>R2-D2 (+val)</cell><cell>78.61±0.35 79.58±0.33</cell></row><row><cell>R2-D2 (+ens+val)</cell><cell>81.34±0.32 81.96±0.32</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Protonets</surname></persName>
		</author>
		<idno>+ens) 54.24±0.40 56.55±0.40</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Protonets</surname></persName>
		</author>
		<idno>+val) 61.16±0.42 60.86±0.41</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Protonets</surname></persName>
		</author>
		<idno>+ens+val) 63.64±0.43 65.47±0.42</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-Svm</forename></persName>
		</author>
		<idno>15] 54.83±0.40 56.23±0.40</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-Svm</forename></persName>
		</author>
		<idno>+ens) 58.49±0.41 60.14±0.41</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-Svm</forename></persName>
		</author>
		<idno>+val) 62.99±0.42 63.64±0.42</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-Svm</forename></persName>
		</author>
		<idno>+ens+val) 66.37±0.42 67.17±0.41</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<idno>27] 58.67±0.48 60.52±0.48</idno>
		<title level="m">Table 5: Comparison to the average accuracies (%) with 95% confidence intervals between the methods with and without Task Aug on miniImageNet 5-way 1-shot. Method Baseline Task Aug ProtoNets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Protonets</surname></persName>
		</author>
		<idno>+ens) 62.12±0.48 63.69±0.47</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Protonets</surname></persName>
		</author>
		<idno>+val) 60.13±0.48 62.22±0.49</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Protonets</surname></persName>
		</author>
		<idno>+ens+val) 63.84±0.48 65.04±0.48</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-Svm</forename></persName>
		</author>
		<idno>15] 60.02±0.45 62.12±0.44</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-Svm</forename></persName>
		</author>
		<idno>+ens) 63.44±0.45 64.56±0.44</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-Svm</forename></persName>
		</author>
		<idno>+val) 61.58±0.45 63.14±0.45</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-Svm</forename></persName>
		</author>
		<idno>+ens+val) 64.74±0.45 65.38±0.45</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maruan</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03641</idno>
		<idno>1-shot 5-shot [7] 56.20±0.86 73.00±0.64</idno>
		<title level="m">Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environments</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-Svm</forename></persName>
		</author>
		<idno>15] 64.09±0.62 80.00±0.45</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-Svm</forename></persName>
		</author>
		<idno>65.38±0.45 82.13±0.31 R2-D2 (best) (our) 65.95±0.45 81.96±0.32</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04340</idno>
		<title level="m">Data augmentation generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08136</idno>
		<title level="m">Meta-learning with differentiable closed-form solvers</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10727" to="10737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3018" to="3027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00109</idno>
		<title level="m">Snapshot ensembles: Train 1, get m for free</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margarita</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Osadchy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07270</idno>
		<title level="m">Learning to support: Exploiting structure information in support sets for one-shot learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodríguez López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fewshot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">One-shot generalization in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sygnowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Delta-encoder: an effective sample synthesis method for few-shot object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2845" to="2855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Metagan: An adversarial approach to few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2365" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
