<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Video Representation Learning with Space-Time Cubic Puzzles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
							<email>iskweon77@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Video Representation Learning with Space-Time Cubic Puzzles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised tasks such as colorization, inpainting and zigsaw puzzle have been utilized for visual representation learning for still images, when the number of labeled images is limited or absent at all. Recently, this worthwhile stream of study extends to video domain where the cost of human labeling is even more expensive. However, the most of existing methods are still based on 2D CNN architectures that can not directly capture spatio-temporal information for video applications. In this paper, we introduce a new self-supervised task called as Space-Time Cubic Puzzles to train 3D CNNs using large scale video dataset. This task requires a network to arrange permuted 3D spatio-temporal crops. By completing Space-Time Cubic Puzzles, the network learns both spatial appearance and temporal relation of video frames, which is our final goal. In experiments, we demonstrate that our learned 3D representation is well transferred to action recognition tasks, and outperforms state-of-the-art 2D CNN-based competitors on UCF101 and HMDB51 datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recent progress in computer vision stems from a huge amount of labeled images as well as deep convolutional neural networks. Typically, a network trained with Ima-geNet <ref type="bibr" target="#b13">(Russakovsky et al. 2015)</ref> consisting of one million images and label pairs learns the general features of the image and has been used to initialize the network for various kinds of downstream tasks. In fact, there are much more than one million images in the web, however, building large-scale annotated datasets is extremely expensive and impractical. Therefore, many researches have been attempted to minimize human supervision in computer vision. For example, <ref type="bibr" target="#b11">(Oquab et al. 2015)</ref> and <ref type="bibr" target="#b6">(Kim et al. 2017)</ref> proposed to use weak image tag information for object localization without using bounding boxes or pixel-level masks. In the same vein, unsupervised representation learning, which learns generalpurpose semantic features without human annotation, has been regarded as a fundamental problem for years <ref type="bibr" target="#b0">(Bengio, Courville, and Vincent 2013)</ref>. Among them, a prominent paradigm is the so-called self-supervised representation learning that defines an annotation-free pretext task from the Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Given a pair of images on the left, it is ambiguous to determine the time direction in between. One can finally identify the direction (b' → a') and action (throwing) when given video sequences.</p><p>raw data in order to provide a free supervision signal for feature learning. For instance, a deep CNN is taught to complete zigsaw puzzles <ref type="bibr" target="#b10">(Noroozi and Favaro 2016)</ref> and fill in missing pixels <ref type="bibr" target="#b12">(Pathak et al. 2016</ref>). The rationale behind such self-supervised tasks is that solving them will force the CNN to learn semantic image features that can be useful for other vision tasks. In image domain, self-supervised learning is performed using only images from ImageNet <ref type="bibr" target="#b13">(Russakovsky et al. 2015)</ref> without labels and is transferred to downstream tasks such as Pascal <ref type="bibr" target="#b2">(Everingham et al. 2007;</ref><ref type="bibr" target="#b2">Everingham et al. 2012)</ref>. Recent methods have shown promising results, and significantly narrowed the gap with the fully supervised learning using ImageNet labels. More recently, this worthwhile stream of research has extended to video domain, where the burden of human annotation is even more severe. Compared to images, videos provide additional temporal information. To illustrate, we introduce a problem of guessing the direction of time in <ref type="figure" target="#fig_0">Fig. 1</ref>. Given a pair of image on the left, one runs into a problem of determining whether the action is catching (a → b) or throwing (b → a). One can finally clarify when the neighboring frames are given together that it is throwing (b' → a). In the past few years, various self-supervision signals using video frames have shown promising results that are better than random initialization in action recognition tasks. However, the scope of these approaches is still limited to using 2D CNN architectures which are appearance-based, leaving the ambiguity in the temporal dimension unsolved.</p><p>In this paper, we focus on 3D CNNs which can directly extract spatio-temporal features from raw videos. With the advent of large scale video datasets such as Kinetics <ref type="bibr" target="#b6">(Kay et al. 2017)</ref>, these 3D CNNs have recently begun to outperform 2D CNNs in action recognition as <ref type="bibr" target="#b4">(Hara, Kataoka, and Satoh 2018)</ref>. In the context of self-supervised learning, we propose a pretext task for 3D CNNs to close the gap with fully supervised Kinetics-pretraining in video domain. Given a randomly permuted 3D spatio-temporal crops extracted from each video clips, we train a network to predict their original spatio-temporal arrangement. We call this task as Space-Time Cubic Puzzles, and examples are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. By solving Space-Time Cubic Puzzles, the 3D CNN is forced to have an understanding of both spatial appearance and temporal relation in the video, which is our final goal.</p><p>We conduct extensive experimental validation to demonstrate the effectiveness of our self-supervised video feature learning. Fisrt, we compare the proposed method with baseline methods including the random initialization and fully supervised pretraining as well as alternative pretraining strategies. Also, we perform various ablation studies to provide deeper analysis on 3D spatio-temporal representation. Finally, we demonstrate that our learned 3D representation with comparable or fewer number of parameters outperforms state-of-the art 2D CNN competitors on action recognition tasks of using UCF101 <ref type="bibr" target="#b15">(Soomro, Zamir, and Shah 2012)</ref> and HMDB51 <ref type="bibr" target="#b7">(Kuehne et al. 2011</ref>) benchmark datasets.</p><p>Our contributions can be summarized as follows:</p><p>• We propose a novel pretext task of solving 3D video cubic puzzles for self-supervised video representation learning from unlabeled videos. To our best knowledge, this is the first work to focus on the spatio-temporal 3D CNNS in self-supervised representation learning in videos.</p><p>• We provide various ablation studies and analysis for deeper understanding of 3D spatio-temporal representation.</p><p>• Our learned 3D CNN representation outperforms other self-supervised approaches on two publicly available action recognition datasets (UCF101, HMDB51), while having fewer or comparable number of parameters.</p><p>• We significantly close the gap between unsupervised representation learning and Kinetics-pretraining for 3D CNNs. When transferred onto UCF101, our selfsupervised learning improves +23.4% over training from scratch, and shows comparable performances to the strong supervision that uses one eighth of the Kinetics labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>In this section, we review two categories of prior works: video recognition and self-supervised representation learning, which are the most revelvant to our wok.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Recognition and Kinetics Dataset</head><p>Recent progress in video recognition is rooted in the use of large-scale datasets that enable the pretraining of CNNs for a wide variaty of downstream tasks. To date, ImageNet <ref type="bibr" target="#b13">(Russakovsky et al. 2015)</ref> has contributed substantially to the pretraining of a generic feature representation in many video recognition algorithms. First of all, <ref type="bibr" target="#b5">(Karpathy et al. 2014)</ref> introduced multiresolution CNN architecture for large-scale video classification. They also provided several schemes for time information fusion. <ref type="bibr" target="#b14">(Simonyan and Zisserman 2014)</ref> proposed a two-stream architecture to capture spatial and motion information with a RGB stream and an optical flow stream respectively. <ref type="bibr" target="#b18">(Wang et al. 2016</ref>) further improved the results by using temporal segments. These approaches are based on 2D CNNs that are pretrained on ImageNet.</p><p>Recently, CNNs with spatio-temporal 3D convolutional kernels (3D CNNs) have been actively touched for video applications. The first 3D CNN was proposed several years ago by <ref type="bibr" target="#b5">(Ji et al. 2013</ref>). However, even the usage of wellorganized models such as <ref type="bibr" target="#b16">(Tran et al. 2015)</ref> has failed to outperform the advantages of 2D CNNs that combined both RGB and stacked flow <ref type="bibr" target="#b14">(Simonyan and Zisserman 2014)</ref>. The primary reason for this failure has been the relatively small data-scale of video datasets for optimizing the large number of parameters in 3D CNNs, which can only be trained on video datasets. More recently, however, (Carreira and Zisserman 2017) achieved a significant breakthrough using the Kinetics dataset <ref type="bibr" target="#b6">(Kay et al. 2017)</ref>, which includes more than 300K annotated videos. It was created with the aim of being positioned as standard video dataset roughly equivalent to the position held by ImageNet in image domain. Thus, we now have the benefit of a 3D convolution that can directly extract spatio-temporal features, by virtue of the Kinetics dataset.</p><p>However, most of the previous studies have trained 3D CNNs using all labels in the Kinectis dataset. Therefore, we argue that developing a self-supervised method to train 3D CNNs is a worthwhile pursuit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervised Representation Learning</head><p>To overcome the inherent thirst for data in fully supervised training, a large body of literature have studied unsupervised feature learning. A recently emerging line of research is selfsupervised feature learning where the supervision signal is obtained automatically from unlabeled images or videos.</p><p>Over the last few years, several self-supervised tasks have been introduced. For instance, methods that use context arrangement of image patches <ref type="bibr" target="#b1">(Doersch, Gupta, and Efros 2015;</ref><ref type="bibr" target="#b10">Noroozi and Favaro 2016)</ref>, image completion <ref type="bibr" target="#b12">(Pathak et al. 2016)</ref>, motion frame ordering <ref type="bibr" target="#b9">(Misra, Zitnick, and Hebert 2016;</ref><ref type="bibr" target="#b8">Lee et al. 2017)</ref> and multi-task of many models (Kim et al. 2018) have been proposed. Our work is closely related to the context-based methods <ref type="bibr" target="#b1">(Doersch, Gupta, and Efros 2015;</ref><ref type="bibr" target="#b10">Noroozi and Favaro 2016;</ref><ref type="bibr" target="#b9">Misra, Zitnick, and Hebert 2016;</ref><ref type="bibr" target="#b8">Lee et al. 2017;</ref><ref type="bibr" target="#b6">Kim et al. 2018</ref>). These are a popular approach, and work by creating an arrangement of image patches in either space or time. Each distinct arrangement is assigned a class label, and the network then predicts the correct arrangement of these patches by solving a supervised classification problem. Context-based methods have the advantage of being easy to understand, network architecture agnostic, and frequently straightforward to implement. They also tend to perform well on standard measures of transfer learning. For instance, (Noroozi and Favaro 2016) and <ref type="bibr" target="#b8">(Lee et al. 2017)</ref> are the top performers on PASCAL VOC 2007 detection and UCF101 action classification respectively, even among a large number of new arrivals.</p><p>However, these context-based methods only leverage either of spatial or temporal dimension. Furthermore, they are based on 2D CNNs which can only extract frame-level features that cannot detect scene dynamics by nature. Since the spatial appearances and temporal relations are both very important cues for video understanding, our work investigates the use of both spatial and temporal dimensions using 3D CNNs in videos. To our best knowledge, only few works <ref type="bibr" target="#b20">(Zhao et al. 2017;</ref><ref type="bibr" target="#b17">Vondrick, Pirsiavash, and Torralba 2016)</ref> exploit the 3D architectures in self-supervised feature learning. They use reconstruction/generation-based pretext tasks, and aim for a specific target task: anomaly detection and video generation respectively. In contrast, we mainly investigate the fine-tuning of the learned feature representations for the video action recognition tasks. Arguably, the action recognition is a hallmark problem in video understanding, so it can serve as a general task, similarly to object recognition in image understanding. In experiments, quantitative comparisons demonstrate our supervisory signals are able to generate much richer 3D feature representations than previous 3D CNN-based methods, as well as the 2D CNNbased competitors, even with fewer number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Approach Pretext Task: Space-Time Cubic Puzzles</head><p>Our goal is to learn spatio-temporal representations with 3D CNNs using unlabeled videos. We propose a 3D cubic puzzle problem called as Space-Time Cubic Puzzles; Given a randomly permuted sequence of 3D spatio-temporal pieces cropped from a video clip, we train a network to predict their original arrangement. Although this is a difficult task even for a human, it becomes easy once we identify the objects and their actions in the video crops. We hypothesize that the successful network in this task captures representative and discriminative features for each 3D crop by determining their spatio-temporal arrangement. Thus, our learned clip-level 3D representations are transferable to downstream tasks in videos as well.</p><p>To generate the puzzle pieces, we consider a spatiotemporal cuboid consisting of 2 × 2 × 4 grid cells for each video, as shown in <ref type="figure" target="#fig_1">Fig. 2 and Fig. 4</ref>. Given 16 crops, there are 16! possible permutations. However, these include very similar permutations which make the puzzle task very ambiguous. For example, if the difference between two permutations lies only in two crops that are similar-looking, it will be impossible for the network to predict the right solution (Noroozi and Favaro 2016). To avoid such ambiguity, we sample 4 crops instead of 16, in either spatial or tempo- ral dimension. More specifically, the 3D crops are extracted from a 4-cell grid of shape 2×2×1 (colored in blue in <ref type="figure" target="#fig_1">Fig. 2</ref>left) or 1 × 1 × 4 (colored in red in <ref type="figure" target="#fig_1">Fig. 2</ref>-left) along the spatial or temporal dimension respectively. Finally, we randomly permute them to make our input. The network must feed the 4 input crops through several convolutional layers, and produce an output probability to each of the possible permutations that might have been sampled. Note, however, that we ultimately wish to learn spatio-temporal features for the individual 3D crop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Architecture</head><p>To achieve this, we use a late-fusion architecture shown in <ref type="figure" target="#fig_2">Fig. 3-(a)</ref>. It is a 4-tower siamese network, where the towers share the same parameters, and follow the 3D ResNet <ref type="bibr" target="#b4">(Hara, Kataoka, and Satoh 2018)</ref> architecture to provide a comparison with the Kinetics-pretraining. Each 3D crops are processed separately until the fully-connected layer, so that the network cannot "cheat" by viewing lowlevel statistics such as edge boundaries without having to understand the global scene dynamics. Since only two last fully-connected layers receive input from all 4 crops, we expect the network to perform the most semantic reasoning for each crops separately. Furthermore, each towers are agnostic of whether it was spatial or temporal dimension the input crops had been sampled from. That is, each tower must encode the spatial and temporal structures in a given video crop simultaneously, because it does not know if the problem to solve in the last layers is a spatial puzzle or a temporal puzzle. Similar to the jigsaw puzzle problem (Noroozi and Favaro 2016), we formulate the rearrangement problem as a multi-class classification task. In practice, for each tuple of four crops, we flip all the frames upside-down with 50% probability, doubling the number of classes to 48 (that is, 2×4!) to further boost our performance, as suggested in (Nathan Mundhenk, Ho, and Chen 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avoiding Trivial Learning</head><p>When designing a pretext task, it is crucial to ensure that the task forces the network to learn the desired semantic structure, without bypassing the understanding by finding low-level clues that reveal the location of a video crop. As pointed out by <ref type="bibr" target="#b1">(Doersch, Gupta, and Efros 2015)</ref>, an example of this is chromatic aberration, which occurs naturally as a result of camera lensing. A common remedy for this is to partially drop color channels <ref type="bibr" target="#b1">(Doersch, Gupta, and Efros 2015)</ref>, replicate one channel <ref type="bibr" target="#b8">(Lee et al. 2017)</ref>, use grayscale inputs (Noroozi and Favaro 2016). We choose channel replication as our data preprocessing.</p><p>Another often-cited worry in all context-based works relates to trivial low-level boundary pattern completion <ref type="bibr" target="#b1">(Doersch, Gupta, and Efros 2015;</ref><ref type="bibr" target="#b10">Noroozi and Favaro 2016;</ref><ref type="bibr" target="#b8">Lee et al. 2017)</ref>. The network may learn the alignment between video crops not based on the semantics, but instead by matching the volume boundaries. Thus, we apply spatiotemporal jittering when extracting each video crops from the grid cells to avoid the trivial cases, as shown in the right side of <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Network and dataset.</p><p>We implement our method and conduct all experiments mostly using the 3D ResNet <ref type="bibr" target="#b4">(Hara, Kataoka, and Satoh 2018)</ref> as a backbone architecture, since its performances on the random initialization and Kineticspretraining are well studied in their work. We can immediately compare the performance of our pretraining method to those scores. The training uses Kinetics datasets, which includes 400 human action classes, and consists of more than 400 videos for each class. The videos were temporally trimmed and last around 10 seconds. During the pretraining, we use the training split which has total 240K videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretraining.</head><p>We use video clips with 224 × 224 pixel frames and convert every video file into PNG images in our experiments. We sample 128 consecutive frames from each clip, and split them into 2 × 2 × 4-cell grid; That is, one grid cell consists of 112 × 112 × 32 pixels, and for each cell, we sample 80 × 80 × 16 pixels with random jittering to generate a 3D video crop. We set the mini-batch size as 128 and the initial learning rate as 0.01. We use stochastic gradient descent with a momentum of 0.9 on two GTX-1080Ti GPUs. All the pre-trained models and the source codes will be available soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>In this section, we evaluate the effectiveness of our spacetime cubic puzzle as a pretext task for self-supervised pretraining of 3D CNNs. As in prior works on self-supervised learning, we use the learned 3D CNNs features as the initialization for a fine-tuning stage for video recognition tasks.</p><p>Better results indicate better qualities and generalization abilities of the learned video representations. We organize our experimental results as follows: 1) comparison with the random initialization and Kinetics-pretraining (supervised), 2) comparison with our alternative strategies, 3) ablation analysis, 4) comparison with the state-of-the-art methods, and 5) Visualization of the low-level filters and high-level activations. The followings are the datasets and fine-tuning details in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets.</head><p>We conduct video recognition experiments on two benchmark action recognition datasets, namely UCF101 (Soomro, Zamir, and Shah 2012) and HMDB51 <ref type="bibr" target="#b7">(Kuehne et al. 2011</ref>). UCF101 contains 101 actions classes, 13K videos, and 27 hours of video data in total. The HMDB51 dataset consists of realistic videos captured from movies and Web videos, and contains 6,766 videos from 51 action classes. To be noted, all the experiments follow the training/test splits of UCF101 and HMDB51, and we mostly report the average classification accuracy over the three splits for UCF101, as done in <ref type="bibr" target="#b4">(Hara, Kataoka, and Satoh 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning for action recognition.</head><p>Once we finish the pretraining stage, we use our learned parameters to initialize the 3D CNNs for action recognition, while the last fullyconnected layer is initialized randomly. During the finetuning and testing, we follow the same protocol in <ref type="bibr" target="#b4">(Hara, Kataoka, and Satoh 2018)</ref> to provide a fair comparison. Specifically, for each clip, we randomly sample 16 consecutive frames, and spatially resize the frames at 112 × 112 pixels. During the fine-tuning, we apply random spatial cropping, scaling and horizontal flipping to perform data augmentation. We start from a learning rate of 0.05, and assign a weight decay of 5e-4. In testing, we adopt the sliding window manner to generate input clips, so that each video is split into non-overlapped 16-frame clips. The clip class scores are averaged over all the clips of the video.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Random Initialization and Fully-Supervised Pretraining</head><p>In these experiments we study the advantage of our selfsupervised pretraining for action recognition in comparison to training from the scratch and several fully-supervised pretraining methods. We report the performances in <ref type="table">Table.</ref> 1.</p><p>Our self-supervised pretraining shows a dramatic improvement of +23.4% over training from scratch in UCF101 and a significant gain of +16.6% in HMDB51. This impressive gain demonstrates the effectiveness of our self-supervised cubic puzzle task. Also, to quantitatively assess the effectiveness of our method in comparison to fully supervised methods, we gradually reduce the number of class labels in Kinetics dataset (full, 1/2, 1/4, and 1/8), and evaluate the pretraining results. Still having gap with the full Kinetics-pretraining, our method performs slightly better than the pretraining with one eighth of the Kinetics labels (that is, 50 out of 400 classes). In addition, to provide a comparison with ImageNet-pretraining, we import the existing ImageNet supervised 2D filters and inflate them into 3D, as suggested in   benchmark datasets. This implies that our video representations learned from the spatio-temporal context reasoning can be more powerful than the massively supervised 2D imagebased representations in video recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternative Pretraining Strategies</head><p>Since there are few prior works on self-supervised representation learning using 3D CNNs, we enumerate several alternative self-supervision tasks to provide our own reference levels and validate the effectiveness of our method. While we mainly focus on the context-based approaches, we also explore the reconstruction-based methods: spatio-temporal autoencoders <ref type="bibr" target="#b20">(Zhao et al. 2017</ref>) and 3D inpainting <ref type="bibr" target="#b12">(Pathak et al. 2016)</ref> as well. All the methods and experiments use the same 3D ResNet-18 as a backbone architecture, and use Kinetics dataset (without labels). To itemize, they are:</p><p>Context-based methods. Refer to <ref type="figure" target="#fig_2">Fig. 3-(a)</ref> for network architecture. We use cross entropy loss to train the networks.</p><p>• 3D ST-puzzle (spatio-temporal, Our full method):</p><p>The Space-Time Cubic Puzzles, where the tuple of 4 video crops is sampled in the spatial dimension with 50% probability, and in the temporal dimension otherwise. Due to  <ref type="table">Table 3</ref>: Comparison with the state-of-the-art methods. Top-1 accuracies on UCF101 and HMDB51. All methods use 3D ResNet-18, and the accuracies are averaged over three splits.</p><p>this randomness, the network is forced to learn both spatial and temporal structures simultaneously.</p><p>• 3D S-puzzle (spatial only): 3D extention of <ref type="bibr" target="#b1">(Doersch, Gupta, and Efros 2015;</ref><ref type="bibr" target="#b10">Noroozi and Favaro 2016)</ref>. Same as above, with the input tuple always generated from the spatial dimension.</p><p>• 3D T-puzzle (temporal only): 3D extention of <ref type="bibr" target="#b9">(Misra, Zitnick, and Hebert 2016)</ref>. Same as above, with the input tuple always generated from the temporal dimension.</p><p>• 3D ST score ensemble: Score ensemble of the classification scores of the S-puzzle and T-puzzle tasks. We average the softmax probabilities from both puzzle tasks.</p><p>Reconstruction-based methods. Refer to <ref type="figure" target="#fig_2">Fig. 3-(b)</ref> for network architecture. We use MSE loss to train the networks • 3D AE (reconstruction): The network is trained to reconstruct input stack of 16 frames. We use four 3D deconvolution layers with stride 2 × 2 × 2 in the decoder. We followed the same decoder structure and the training protocol in <ref type="bibr" target="#b20">(Zhao et al. 2017</ref>).</p><p>• 3D AE + future (recon. + future prediction): Same as above, with one more decoder branch for joint future prediction of additional 16 frames, as in <ref type="bibr" target="#b20">(Zhao et al. 2017</ref>).</p><p>• 3D inpainting: 3D extension of <ref type="bibr" target="#b12">(Pathak et al. 2016</ref>). The network is trained to recover the missing center region (64 × 64 × 16) in the input 16-frame stack.</p><p>We compare these methods in <ref type="table">Table.</ref> 2. The context-based methods consistently outperform the reconstruction-based baselines. Also, we can see that the score ensemble gives better scores than the single-dimension baselines, implying that the knowledge from the spatial appearance are indeed complementary with those from the temporal relations. Our full method brings additional 3% performance gain on top of the score ensemble. This implies that our proposed method effectively aggregates spatio-temporal video features, and these features are much more discriminative and representative than those from the single-dimension baselines or their late fusion ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>UCF101(%) with no regularizations 58.7 + channel replication 61.5 + random jittering 63.9 + rotation with classification 65.8 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>In order to validate various regularization techniques in our pretraining method, we evaluate the effect of each design choices on the UCF-101 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Channel replication.</head><p>As mentioned earlier, chromatic aberration is one of the often-cited worries in context-based self-supervised learning because it leads to learning trivial color features. To prevent such issue, we first use grayscale images. We further experimented with channel replication where we randomly choose one representative channel and replicate its values to the other two channels. <ref type="table" target="#tab_4">Table. 4</ref> shows that channel replication improves the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatio-temporal jittering.</head><p>Analogous to the random gap used in the puzzle-solving task (Noroozi and Favaro 2016), we apply spatio-temporal jittering to each video crops to prevent the network from learning low-level statistics. In practice, we crop 80 × 80 × 16 pixels from a 112 × 112 × 32pixel cuboid with random shifts in all horizontal, vertical and temporal directions. <ref type="table">Table.</ref> 4 shows that applying random jittering does help the network to learn better video features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotation with classification.</head><p>Recently, (Nathan Mundhenk, Ho, and Chen 2018) developed a set of methods to improve on the results of self-supervised learning using context. To see if our model can benefit from these technologies, we apply one of their methods: rotation with classification (RWC) which encourages the network to identify if the inputs are right-side-up or upside-down. We do this by flip-(a) Self-supervised 3D filters (with RGB frames) (b) Kinetics-pretrained 3D filters (c) ImageNetpretrained 2D filters <ref type="figure">Figure 5</ref>: Learned filters with self-supervision vs. fully supervised-pretraining. Visualization of the learned 64 filters in conv1 layer: (a) the resulting 3D 7×7×7 filters of our self-supervised learning, (b) 3D 7×7×7 filters from the Kineticspretrained network, and (c) 2D 7×7 filters from ImageNet-pretrained network. Note that our representation incorporate temporal dynamics and have rich temporal structure, without requiring massive human labels.</p><p>ping all video crops in a tuple upside-down and doubling the number of classes (24 × 2 = 48 in our work). <ref type="table" target="#tab_4">Table. 4</ref> shows that RWC does prevent learning to bypass and improves over the baseline. This implies that other off-the-shelf techniques for context-based self-supervised learning would further boost the performance of our pretraining method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with the State-of-the-art Methods</head><p>We show a comparison of our results and the state-ofthe-art self-supervised methods in <ref type="table">Table.</ref> 3. In particular, we compare with <ref type="bibr" target="#b10">(Mobahi, Collobert, and Weston 2009;</ref><ref type="bibr" target="#b18">Wang and Gupta 2015;</ref><ref type="bibr" target="#b9">Misra, Zitnick, and Hebert 2016;</ref><ref type="bibr" target="#b8">Lee et al. 2017;</ref><ref type="bibr" target="#b3">Gan et al. 2018;</ref><ref type="bibr" target="#b19">Wei et al. 2018</ref>) using the RGB video data. We quote the numbers directly from the published papers. It should be noted that the direct comparison with these 2D CNN-based methods is difficult due to the fundamental architectural difference. To complement this discrepancy, we conduct experiments with 3D architectures with different number of parameters and layers: C3D, 3D ResNet-10 and 3D ResNet-18. These networks have fewer parameters (11M, 14M and 33M respectively) compared with the AlexNet (58M) which is the backbone architecture in the 2D CNN-based methods. However, our approach outperforms other recent self-supervised methods. <ref type="bibr" target="#b2">(Fernando et al. 2017</ref>) utilizes temporal order verification as a supervisory signal and can be used as a baseline as well. The minor difference is that this baseline uses stacks of frame differences (15 channels) as inputs. To use a similar setting, we use frame difference as inputs during finetuning and testing. With 3D ResNet-18, we achieved 75.3% on UCF101, that is outperforming Odd-One-Out method by a margin of +15.0%. <ref type="bibr" target="#b17">(Vondrick, Pirsiavash, and Torralba 2016)</ref> used C3D architecture for video generation and tested their learned representations on action recognition. Our results with the same C3D backbone network brings +8.5% performance gain over this, showing the informativeness of our self-supervised task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Learned Filters</head><p>All the learned conv1 filters from our self-supervised learning, Kinetics-pretraining, and ImageNet-pretraining are visualized in <ref type="figure">Fig. 5</ref>. We observe that: 1) All our filters change in the time dimension, meaning each encodes temporal information; 2) For most of the ImageNet-pretrained 2D filters, we can find a 3D filter with a similar appearance pattern mostly at the center slice, 4th out of 7, both in our filters and Kinetics-pretrained ones. These observations may imply that our learned 3D representations are able to not only cover the appearance information in 2D filters, but can also capture useful temporal motion simultaneously, like the Kinetics-pretrained representations do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this study, we examined the self-supervised feature learning for spatio-temporal 3D CNNs. We propose Space-Time Cubic Puzzles as our pretext task, and train with unlabeled Kinetics dataset. Our method enables learning both spatial appearances and temporal relations in video, which has been hardly achieved by previous 2D CNN-based self-supervisions. Our self-supervised pretraining performs slightly better than supervised pretraining on one eighth of the Kinetics labels on UCF101 and HMDB51 datasets. The visualization shows that our learned 3D representations indeed encode spatial and temporal information jointly. We believe that the results of this study will facilitate further advances in self-supervised representation learning for spatio-temporal 3D CNNs. In recent years, significant progress has been made in self-supervised learning has narrowed the gap with ImageNet-pretraining in image domain. Similar to these, our self-supervised learning with 3D CNNs also shows promising results towards our ultimate goal of reducing human supervision in video domain. In our future work, we will investigate transfer learning not only for action recognition but also for other such tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Ambiguity in time direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>space-time cuboid (left) and spatio-temporal jittering (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The overall architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Example spatial and temporal tuples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>comparison with alternative methods. Top-1 accuracies on UCF10. All methods use 3D ResNet-18, and the accuracies are averaged over three splits.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies. Top-1 accuracies on UCF101. Each methods are accumulated down from the top and use 3D ResNet-18. The accuracies are averaged over three splits.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courville</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
	<note>Representation learning: A review and new perspectives</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gupta</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Computer Vision (ICCV)</title>
		<meeting>of Int&apos;l Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<idno>Everingham et al. 2012</idno>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
	</analytic>
	<monogr>
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5729" to="5738" />
		</imprint>
	</monogr>
	<note>Proc. of Computer Vision and Pattern Recognition (CVPR)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometry guided convolutional neural networks for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5589" to="5597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kataoka</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
	<note>3d convolutional neural networks for human action recognition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning image representations by completing damaged jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kay</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>of Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="793" to="802" />
		</imprint>
	</monogr>
	<note>2017 IEEE International Conference on Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuehne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Computer Vision (ICCV)</title>
		<meeting>of Int&apos;l Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Computer Vision (ICCV)</title>
		<meeting>of Int&apos;l Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitnick</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hebert ;</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conf. on Computer Vision (ECCV)</title>
		<meeting>of European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collobert</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename><forename type="middle">;</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nathan Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen ; Nathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
	<note>Proc. of European Conf. on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weaklysupervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Oquab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Pathak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">115</biblScope>
		</imprint>
	</monogr>
	<note>Russakovsky et al. 2015</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Neural Information Processing Systems (NIPS)</title>
		<meeting>of Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
	<note>and Zisserman</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zamir</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shah ; Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<title level="m">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Computer Vision (ICCV)</title>
		<meeting>of Int&apos;l Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pirsiavash</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conf. on Computer Vision (ECCV)</title>
		<meeting>of European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
	<note>Proc. of Int&apos;l Conf. on Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8052" to="8060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatio-temporal autoencoder for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Multimedia Conference (MM)</title>
		<meeting>of Multimedia Conference (MM)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
