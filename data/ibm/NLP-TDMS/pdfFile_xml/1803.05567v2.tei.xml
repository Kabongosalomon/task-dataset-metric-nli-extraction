<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Achieving Human Parity on Automatic Chinese to English News Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Aue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Chowdhary</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lewis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft AI &amp; Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Achieving Human Parity on Automatic Chinese to English News Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine translation has made rapid advances in recent years. Millions of people are using it today in online translation systems and mobile applications in order to communicate across language barriers. The question naturally arises whether such systems can approach or achieve parity with human translations. In this paper, we first address the problem of how to define and accurately measure human parity in translation. We then describe Microsoft's machine translation system and measure the quality of its translations on the widely used WMT 2017 news translation task from Chinese to English. We find that our latest neural machine translation system has reached a new state-of-the-art, and that the translation quality is at human parity when compared to professional human translations. We also find that it significantly exceeds the quality of crowd-sourced non-professional translations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have seen human performance levels reached or surpassed in tasks ranging from games such as Go <ref type="bibr" target="#b42">[33]</ref> to classification of images in ImageNet <ref type="bibr" target="#b30">[21]</ref> to conversational speech recognition on the Switchboard task <ref type="bibr" target="#b59">[50]</ref>.</p><p>In the area of machine translation, we have seen dramatic improvements in quality with the advent of attentional encoder-decoder neural networks <ref type="bibr" target="#b44">[35,</ref><ref type="bibr" target="#b11">3,</ref><ref type="bibr" target="#b48">39]</ref>. However, translation quality continues to vary a great deal across language pairs, domains, and genres, more or less in direct relationship to the availability of training data. This paper summarizes how we achieved human parity in translating text in the news domain, from Chinese to English. While the techniques we used are not specific to the news domain or the Chinese-English language pair, we do not claim that this result necessarily generalizes to other language pairs and domains, especially where limited by the availability of data and resources.</p><p>Translation of news text has been an area of active interest in the Machine Translation community for over a decade, due to the practical and commercial importance of this domain, the availability of abundant parallel data on the web (at least in the most popular languages) and a long history of government-funded projects and evaluation campaigns, such as NIST-OpenMT 1 and GALE 2 . The annual evaluation campaign of the WMT (Conference on Machine Translation) <ref type="bibr" target="#b14">[6]</ref>, has also focused on news translation for more than a decade.</p><p>Defining and measuring human quality in translation is challenging for a number of reasons. Traditional metrics of translation quality, such as BLEU <ref type="bibr" target="#b38">[29]</ref>, TER <ref type="bibr" target="#b43">[34]</ref> and Meteor <ref type="bibr" target="#b19">[10]</ref> measure translation quality by comparison with one or more human reference translations. However, the same source sentence can be translated in sometimes substantially different but equally correct ways. This makes reference-based evaluation nearly useless in determining quality of human translations or near-human-quality machine translations.</p><p>Further complicating matters, we find that the quality of reference translations, long assumed to be "gold" annotations by professional translators, are sometimes of remarkably poor quality. This is because references are often crowd-sourced (either directly, or indirectly through translation vendors). We have observed that crowd workers often use on-line MT with or without post-editing, rather than translating from scratch. Furthermore, many crowd workers appear to have only a rudimentary grasp of one of the languages, which often leads to unacceptable translation quality.</p><p>In Section 2, we describe how we address these challenges in defining and measuring human quality. In Section 3, we describe our system architecture. Section 4 describes our data and experiments. Sections 5 and 6 present our evaluation results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Human Parity on Translation</head><p>Achieving human parity for machine translation is an important milestone of machine translation research. However, the idea of computers achieving human quality level is generally considered unattainable and triggers negative reactions from the research community and end users alike. This is understandable, as previous similar announcements have turned out to be overly optimistic.</p><p>Before any meaningful discussion of human parity can occur, we require a rigorous definition of the concept of human parity for translation. Based on this theoretical definition we can then investigate how close neural machine translation is to this goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Defining Human Parity</head><p>Intuitively, we can define human parity for translation as follows: Definition 1. If a bilingual human judges the quality of a candidate translation produced by a human to be equivalent to one produced by a machine, then the machine has achieved human parity.</p><p>Assuming that it is possible for humans to measure translation quality by assigning scores to translations of individual sentences of a test set, and generalizing from a single sentence to a set of test sentences, this effectively yields the following statistical definition: Definition 2. If there is no statistically significant difference between human quality scores for a test set of candidate translations from a machine translation system and the scores for the corresponding human translations then the machine has achieved human parity.</p><p>We choose definition 2 to address the question of human parity for machine translation in a fair and principled way. Given a reliable scoring metric to determine translation quality, based on direct human assessment, one can use a paired statistical significance test to decide whether a given machine translation system can be considered at parity with human translation quality for a test set and corresponding human references.</p><p>It is important to note that this definition of human parity does not imply that the machine translation system outperforms the human benchmark, but rather that its quality is statistically indistinguishable. It also does not imply that the translation is error-free. Machines, like humans, will continue to make mistakes.</p><p>Finally, achieving human parity on a given test set is measured with respect to a specific set of benchmark human translations and does not automatically generalize to other domains or language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Judging Human Parity</head><p>Our operational definition of human parity requires that human annotators be used to judge translation quality. While there exist various automated metrics to measure machine translation quality, these can only act as a (not necessarily correlated) proxy. Such metrics are typically reference-based and thus subject to reference bias. This can occur in the form of bad reference translations which result in bad segment scores. Also, due to the generative nature of translation, there often are multiple valid translations for a given input segment. Any translation which does not closely match the structure of the corresponding reference has a scoring disadvantage, even perfect human translations. While these effects can be lessened using multiple references, the underlying problem remains unsolved 3 .</p><p>Therefore, following the Conference on Machine Translation (WMT17) <ref type="bibr" target="#b14">[6]</ref>, we adopt direct assessment <ref type="bibr" target="#b26">[17]</ref> as our human evaluation method. To avoid reference bias-which can also happen for human evaluation 4 -we use the source-based evaluation methodology following IWSLT17 <ref type="bibr" target="#b15">[7]</ref>.</p><p>In source-based direct assessment, annotators are shown source text and a candidate translation and are asked the question "How accurately does the above candidate text convey the semantics of the source text?", answering this using a slider ranging from 0 (Not at all ) to 100 (Perfectly). <ref type="bibr" target="#b13">5</ref> As a side effect, we have to employ bilingual annotators for our human evaluation campaigns.</p><p>The raw human scores are then standardized to a z-score, defined as the signed number of standard deviations an observation is above the mean, relative to a sample.</p><p>The z-scores are then averaged at the segment and system level. Results with statistically insignificant differences are grouped into clusters (according to Wilcoxon rank sum test <ref type="bibr" target="#b54">[45]</ref> at p-level p ≤ 0.05). <ref type="bibr" target="#b14">6</ref> To identify unreliable crowd workers, direct assessment includes artificially degraded translation output, so called "bad references". Any large scale crowd annotation task requires such integrated quality controls to guarantee high quality results. In our evaluation campaigns for Chinese into English, we observed relatively few attempts of gaming or spamming compared to other languages for which we run similar annotation tasks (we do not report on those in the context of this paper). In the remainder of this paper, direct assessment ranking clusters are computed in the same way as they had been generated for the WMT17 conference, with minor modifications 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Machine Translation</head><p>Neural Machine Translation (NMT) <ref type="bibr" target="#b11">[3]</ref> represents the state-of-the-art for translation quality. This has been demonstrated in various research evaluation campaigns (e.g. WMT <ref type="bibr" target="#b14">[6]</ref>), and also for large scale production systems <ref type="bibr" target="#b55">[46,</ref><ref type="bibr" target="#b20">11]</ref>. NMT scales to train on parallel data on the order of tens of millions of sentences.</p><p>Currently, State-of-the-art NMT <ref type="bibr" target="#b11">[3,</ref><ref type="bibr" target="#b44">35]</ref> is generally based on a sequence-to-sequence encoderdecoder model with an attention mechanism <ref type="bibr" target="#b11">[3]</ref>. Attentional sequence-to-sequence NMT models the conditional probability p(y|x) of the translated sequence y given an input sequence x. In general, an attentional NMT system θ consists of two components: an encoder θ e which transforms the input sequence into a sequence or set of continuous representations, and a decoder θ d that dynamically reads out the encoder's output with an attention mechanism and predicts the conditional distribution of each target word. Generally, θ is trained to maximize the likelihood on a parallel training set consisting of N sentence pairs:</p><formula xml:id="formula_0">L(θ) = N n=1 log p y (n) x (n) ; θ = N n=1 T t=1 log p y (n) t y (n) &lt;t , h (n) t−1 , f att f enc x (n) , y (n) &lt;t , h (n) t−1 , ; θ<label>(1)</label></formula><p>where h (n) t−1 denotes an internal decoder state, and y &lt;t the words preceding step t. At each step t, the attention mechanism f att determines a context vector as a weighted sum over the outputs of the encoder f enc x (n) , where the weights are determined essentially by comparing each of the encoder's outputs against the decoder's internal state and output up to time t − 1. f enc is a sentence-level feature extractor and can be implemented as multi-layer bidirectional RNNs <ref type="bibr" target="#b11">[3,</ref><ref type="bibr" target="#b55">46]</ref>, a convolutional model (ConvS2S), <ref type="bibr" target="#b25">[16]</ref> or a Transformer <ref type="bibr" target="#b48">[39]</ref>.</p><p>Like RNN sequence-to-sequence models, ConvS2S and Transformer utilize an encoder-decoder architecture. However, both models aim to eliminate the internal decoder state h t−1 . This side steps the recurrent nature of RNN, in which each sentence is encoded word by word, which limits the parallelizability of the computation and makes the encoded representation sensitive to the sequence length.</p><p>ConvS2S utilizes a stacked convolutional representation that models the dependencies between nearby words on lower layers, while longer-range dependencies are handled in the upper layers of the stack. The decoder applies attention on each layers. ConvS2S also utilizes position sensitive embeddings along with residual connections to accommodate positional variance.</p><p>The Transformer model replaces the convolutions with self-attention, which also eliminates the recurrent processing and positional dependency in the encoder. It also utilizes multi-head attention, which allows to attend to multiple source positions at once, in order to model different types of dependencies regardless of position. Similar to ConvS2S, the Transformer model utilizes positional embeddings to compensate for the ordering information, though it proposes a non-parametric representation. While these models eliminate recurrence in the encoder, all models discussed above decode auto-regressively, where each output word's distribution is conditioned on previously generated outputs. The Transformer model has shown <ref type="bibr" target="#b48">[39]</ref> to yield significant improvement and therefore was choses as the base for our work in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reaching Human Parity</head><p>Despite immense progress on NMT in the research community over the past years, human parity has remained out of reach. In this paper, we describe our efforts to achieve human parity on largescale datasets for a Chinese-English news translation task. We address a number of limitations of the current NMT paradigm. Our contributions are:</p><p>• We utilize the duality of the translation problem to allow the model to learn from both source-to-target and target-to-source translations. Simultaneously this allows us to learn from both supervised and unsupervised source and target data. This will be described in Section 3.3. Specifically, we utilize a generic Dual Learning approach <ref type="bibr" target="#b29">[20,</ref><ref type="bibr" target="#b57">48,</ref><ref type="bibr" target="#b56">47]</ref> (Section 3.3.1), and introduce a joint training algorithm to enhance the effect of monolingual source and target data by iteratively boosting the source-to-target and target-to-source translation models in a unified framework (Section 3.3.2).</p><p>• NMT systems decode auto-regressively from left-to-right, which means that during sequential generation of the output, previous errors will be amplified and may mislead subsequent generation. This is only partially remedied by beam search. We propose two approaches to alleviate this problem: Deliberation Networks <ref type="bibr" target="#b58">[49]</ref> is a method to refine the translation based on two-pass decoding (Section 3.4.1); and a new training objective over two Kullback-Leibler (KL) divergence regularization terms encourages agreement between left-to-right and right-to-left decoding results (Section 3.4.2).</p><p>• Since NMT is very vulnerable to noisy training data, rare occurrences in the data, and the training data quality in general <ref type="bibr" target="#b12">[4]</ref>. We discuss our approaches for data selection and filtering, including a cross-lingual sentence representation, in Section 3.5.</p><p>• Finally, we find that our systems are quite complementary, and can therefore benefit greatly from system combination, ultimately attaining human parity. See section 3.6.</p><p>In this work, we interchangeably use source-to-target and (Zh→En) to denote Chinese-to-English; target-to-source and (En→Zh) to denote English-to-Chinese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Exploiting the Dual Nature of Translation</head><p>We leverage the duality of the translation problem to allow the model to learn from both source-totarget and target-to-source translations. We explore the translation duality using two approaches: Dual Learning 3.3.1 and Joint Training 3.3.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Dual Learning for NMT</head><p>Dual learning <ref type="bibr" target="#b29">[20,</ref><ref type="bibr" target="#b57">48,</ref><ref type="bibr" target="#b56">47]</ref>, a recently proposed learning paradigm, tries to achieve the co-growth of machine learning models in two dual tasks, such as image classification vs. image generation, speech recognition vs. text-to-speech, and Chinese to English vs. English to Chinese translation. In dual learning, the two parallel models (referred to as the primal model and the dual model ) enhance each other by leveraging primal-dual structure in order to learn from unlabeled data or regularize the learning from labeled data. Ever since dual learning was proposed, it has been successfully applied to various real-world problems such as question answering <ref type="bibr" target="#b45">[36]</ref>, image classification <ref type="bibr" target="#b57">[48]</ref>, image segmentation <ref type="bibr" target="#b35">[26]</ref>, image to image translation <ref type="bibr" target="#b60">[51,</ref><ref type="bibr" target="#b62">53,</ref><ref type="bibr" target="#b34">25]</ref>, face attribute manipulation <ref type="bibr" target="#b41">[32]</ref>, and machine translation <ref type="bibr" target="#b29">[20,</ref><ref type="bibr" target="#b53">44,</ref><ref type="bibr" target="#b33">24,</ref><ref type="bibr" target="#b9">1]</ref>.</p><p>In this work, to achieve strong machine translation performance, we combine two different dual learning methods that respectively enhance the usage of monolingual and bilingual training data. We set the Chinese to English (Zh→En) translation model as the primal model and the English to Chinese (En→Zh) model as the dual model, respectively denoted as p(y|x; θ x→y ) and p(x|y; θ y→x ).</p><p>• Dual unsupervised learning (DUL) <ref type="bibr" target="#b29">[20]</ref>. To enhance the Zh→En translation quality, DUL efficiently leverages a monolingual Chinese corpus based on additional supervision signals from the dual En→Zh model. Concretely speaking, for a monolingual Chinese sentence x, an English translation y is sampled using the primal model p(·|x; θ x→y ); starting from y, we use the dual model p(·|y; θ y→x ) to compute the log-likelihood log p(x|y; θ y→x ) of reconstructing x from y and treat it as the reward of taking action y at state x. We would like to maximize the expected reconstruction log-likelihood when iterating over all possible translation y for x, shown as:</p><formula xml:id="formula_1">L(x; θ x→y ) = E y∼p(·|x;θx→y) log p(x|y; θ y→x ) = y p(y|x; θ x→y ) log p(x|y; θ y→x )<label>(2)</label></formula><p>Taking the gradient of L(x; θ x→y ) with respect to θ x→y , we obtain: </p><formula xml:id="formula_2">∂L</formula><p>Since summing over all possible y in the above equation is computationally intractable, we use Monte Carlo sampling to approximate the above expectation:</p><formula xml:id="formula_4">∂L(x; θ x→y ) ∂θ x→y ≈ ∂ log p(y |x; θ x→y ) ∂θ x→y log p(x|y ; θ y→x ),<label>(4)</label></formula><p>where y is a sampled translation from the primal model p(·|x; θ x→y ).</p><p>The approximated gradient is used to update the primal model parameters θ x→y . Note that the parameters of the dual model θ y→x can be updated using a monolingual English corpus in a similar way by maximizing the reconstruction likelihood from possible Chinese translations.</p><p>• Dual supervised learning (DSL) <ref type="bibr" target="#b57">[48]</ref>. Unlike DUL, which aims to effectively leverage monolingual data, DSL is an approach to better utilize bilingual training data by enhancing probabilistic correlations within the two models. The idea of DSL is to force the joint probability consistency within primal model and dual model. Specifically, for a bilingual sentence pair (x, y), ideally we have p(x, y) = p(x)p(y|x) = p(y)p(x|y). However, if the two models are trained separately, it is hard for them to satisfy p(x)p(y|x) = p(y)p(x|y). Therefore, when applied in neural machine translation, DSL conducts joint training of the two models and introduces an additional loss term on the parallel data (x, y) for regularization:</p><formula xml:id="formula_5">L DSL = (logp(x) + log p(y|x; θ x→y ) − logp(y) − log p(x|y; θ y→x )) 2 ,<label>(5)</label></formula><p>wherep(x) andp(y) are empirical marginal distributions induced by the training data. In our experiments, they are the output scores of two language models respectively trained on Chinese and English corpus containing both bilingual and monolingual data.</p><p>In our architecture, both DUL and DSL are used in model training, both of which are applied to the monolingual and bilingual training corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Joint Training of Source-to-Target and Target-to-Source Models</head><p>Back translation <ref type="bibr" target="#b39">[30]</ref> augments relatively scarce parallel data with plentiful monolingual data, allowing us to train source-to-target (S2T) models with the help of target-to-source (T2S) models. Specifically, given a set of sentences {y (t) } in the target language, a pre-constructed T2S translation system is used to automatically generate translations {x (t) } in the source language. These synthetic sentence pairs {(x (t) , y (t) )} are combined with the original bilingual data when training the S2T NMT model. In order to leverage both source and target language monolingual data, and also let S2T and T2S models help each other, we leverage the joint training method described in <ref type="bibr" target="#b61">[52]</ref> to optimize them by extending the back-translation method. The joint training method uses the monolingual data and updates NMT models through several iterations.</p><p>Given parallel corpus D = {(x (n) , y (n) )} N n=1 and target monolingual corpus Y = {y (t) } T t=1 , a semi-supervised training objective is used to jointly maximize the likelihood of both bilingual data and monolingual data:</p><formula xml:id="formula_6">L * (θ x→y ) = N n=1 log p(y (n) |x (n) ) + T t=1 log p(y (t) )<label>(6)</label></formula><p>By introducing x as the latent variable representing the source translation of target sentence y (t) , Equation 6 can be optimized in an EM framework, with the help of a T2S translation model:</p><formula xml:id="formula_7">L(θ x→y ) = N n=1 log p(y (n) |x (n) ) + T t=1 x p(x|y (t) ) log p(y (t) |x)<label>(7)</label></formula><p>Similarly, we can optimize the T2S translation model with the help of S2T translation model as follows:</p><formula xml:id="formula_8">L(θ y→x ) = N n=1 log p(x (n) |y (n) ) + S s=1 y p(y|x (s) ) log p(x (s) |y)<label>(8)</label></formula><p>As we can find from Equation 7 and 8, model p(y|x) and p(x|y) serve as each other's pseudotraining data generator: p(x|y) is used to translate Y into X for p(y|x), while p(y|x) is used to translate X to Y for p(x|y). The joint training process is illustrated in <ref type="figure" target="#fig_2">Figure 1</ref>. Before the first iteration starts, two initial translation models p 0 (y|x) and p 0 (x|y) are pre-trained with parallel data D = {(x (n) , y (n) )}. This step is denoted as iteration 0 for sake of consistency. In iteration 1, two NMT systems p 0 (y|x) and p 0 (x|y) are used to translate monolingual data X = {x (s) } and Y = {y (t) }, which creates two synthetic training data sets X = {x (s) , y</p><formula xml:id="formula_9">(s) 0 } and Y = {y (t) , x (t) 0 }.</formula><p>Models p 1 (y|x) and p 1 (x|y) are then trained on this augmented training data by combining Y and X with parallel data D. It is worth noting that n-best translations are used, and the selected translations are weighted with the translation probabilities given by the NMT model, so that the negative impact of noisy translations can be minimized. In iteration 2, the above process is repeated, and the synthetic training data are re-generated with the updated NMT models p 1 (y|x) and p 1 (x|y), which are presumably more accurate. The learned NMT models p 2 (y|x) and p 2 (x|y) are also expected to improve with better pseudo-training data. The training process continues until the performance on a development data set is no longer improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Beyond the Left-to-Right Bias</head><p>Current NMT systems suffer from the exposure bias problem <ref type="bibr" target="#b13">[5]</ref>. Exposure bias refers to the problem that during sequential generation of output, previous errors will be amplified and mislead subsequent generation. We address this limitation in two ways: a two-pass decoding (Deliberation Networks) 3.4.1 and Agreement Regularization 3.4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Deliberation Networks</head><p>Classical neural machine translation models generate a translation word by word from left to right, all in one pass. This is very different from human behavior such as, for instance, while writing articles or papers. When writing papers, usually we create a first draft, then we revisit the draft in its full context, further polishing each word (or phrase/sentence/paragraph) based on both its left-side context and right-side context. In contrast, in neural machine translation, decoding in only one pass makes the output of the t-th word y t dependent on the source-side sentence x and its left context only (i.e., already generated tokens {y 1 , · · · , y t−1 }), without any opportunity to look into the future. Inspired by the human writing process, Deliberation Networks <ref type="bibr" target="#b58">[49]</ref> try to overcome this drawback by decoding using a two-pass process with two decoders as illustrated in <ref type="figure">Fig. 3</ref>. The first-pass decoder outputs an initial translation as a draft. The second-pass decoder polishes this draft into a final translation. The draft translation output from the first pass decoder contains global information that enlarges the receptive field of decoding each token y t in the second-pass decoding process, and thus breaks the limitation of only looking to the left-hand side.</p><formula xml:id="formula_10">( ( ) | ) D = , X = Y = X = , ( ) Y = , ( ) X = , ( ) Y = , ( ) ( | ) ( | ) Iteration 0 Iteration 1 Iteration 2 … ( | ) ( | ) ( | ) ( | ) ( ( ) | ) ( ( ) | ) … … … … ( ( ) | )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second-Pass Decoder</head><p>First-Pass Decoder Encoder  <ref type="figure">Figure 3</ref>: An example showing the decoding process of deliberation network.</p><p>The detailed model architecture, with a deliberation network built on top of Transformer, is shown in <ref type="figure" target="#fig_1">Fig. 4</ref>. As in standard Transformer, both the encoder E and the first-pass decoder D 1 contain several stacked layers connected via a self attention mechanism. Specifically, the encoder assigns to each of the T s source words a representation based on its original embedding and contextual information gathered from other positions. We denote this sequence of top-layer state vectors h 1:Ts as H. The encoder E reads the source sentence x and outputs a sequence of hidden states H = h 1:Ts via self attention. The first-pass decoder D 1 takes H as inputs, conducts the first round decoding and obtains the first-pass translation sentenceŷ as well as the hidden states before softmax denoted asŜ. The second-pass decoder D 2 also contains several stacked layers, but is significantly different from D 1 in that D 2 takes the hidden states output by both E and D 1 as inputs. Specifically, denoting the output of the ith layer in D 2 as s i , we have</p><formula xml:id="formula_11">s i = A e (H, s i−1 )+A c (Ŝ, s i−1 )+A s (s i−1 ),</formula><p>where A e and A c are the multi-head attention mechanism <ref type="bibr" target="#b48">[39]</ref> connecting D 2 respectively with E and D 1 , and A s is the self attention mechanism within D 2 operating on s i−1 . It is easily observed that the last translation result y is dependent on the first translation sentenceŷ, since we feed the outputs of the first-pass decoder D 1 into the second-pass decoder D 2 . In this way we obtain global information on the target side, thereby allowing us to look at right context in sentence generation. Policy gradient algorithms are used to jointly optimize the parameters of the three parts. The combination of dual learning and deliberation networks takes place as follows: First, we train the Zh→En and En→Zh Transformer models using both DUL and DSL. Then, for a target side monolingual sentence y, the existing En→Zh model is used to translate it into Chinese sentence x . Afterwards, we treat (X , Y ) as pseudo bilingual data and add it into the bilingual data corpus. The enlarged bilingual corpus is then used to train the deliberation network as described above. In deliberation network training, we use the Zh→En model obtained in the first step to initialize the encoder and first-pass decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First-Pass Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second-Pass Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Agreement Regularization of Left-to-Right and Right-to-Left Models</head><p>An alternative way of addressing exposure bias is to leverage the fact that unsatisfactory translations with bad suffixes generated by a left-to-right (L2R) model usually have low prediction scores under a right-to-left (R2L) model. In the R2L model, if bad suffixes are fed as inputs to the decoder first, this will lead to corrupted hidden states, therefore good prefixes reached later will be given considerably lower prediction probabilities. This signal given by the R2L model can be leveraged to alleviate the exposure bias problem of the L2R model and vice versa.</p><p>To train the L2R model, two Kullback-Leibler (KL) divergence regularization terms are introduced into the maximum-likelihood training objective, as shown in</p><formula xml:id="formula_12">L( − → θ ) = N n=1 log p(y (n) |x (n) ; − → θ ) − λ N n=1 KL(p(y|x (n) ; ← − θ )||p(y|x (n) ; − → θ )) − λ N n=1 KL(p(y|x (n) ; − → θ )||p(y|x (n) ; ← − θ ))<label>(9)</label></formula><p>With a simple mathematic calculation and proper approximation, we can get the parameter gra- Pre-train four models with maximum likelihood on parallel corpora D = {(x (n) , y (n) )} N n=1 ;</p><formula xml:id="formula_13">Algorithm 1 Unified Joint Training Algorithm Input: Bilingual Data D = {(x (n) , y (n) )} N n=1 , Source and Target Monolingual Corpora X = {x (s) } S s=1 and Y = {y (t) } T t=1 ; Output: S2T-L2R Model p( − → y |x), S2T-R2L Model p( ← − y |x),</formula><p>3:</p><p>while Not Converged do 4:</p><formula xml:id="formula_14">Build weighted pseudo-parallel corpora Y = {(x (t) , y (t) )} T t=1 with p( − → x |y) using mono- lingual data Y = {y (t) } T t=1</formula><p>as shown in <ref type="figure" target="#fig_2">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Update P ( ← − y |x) and p( − → y |x) as shown in <ref type="figure">Figure 2</ref>, with original data D =</p><p>{(x (n) , y (n) )} N n=1 and synthetic data Y = {(x (t) , y (t) )} T t=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Build weighted pseudo-parallel corpora x = {(x (s) , y (s) )} S s=1 with p( − → y |x) using monolingual data X = {x (s) } S s=1 as introduced in <ref type="figure" target="#fig_2">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Update p( ← − x |y) and P ( − → x |y) as shown in <ref type="figure">Figure 2</ref>, with original data D =</p><p>{(x (n) , y (n) )} N n=1 and synthetic data X = {(x (s) , y (s) )} S s=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>end while 9: end procedure dients for L2R model as follows:</p><formula xml:id="formula_15">∂L( − → θ ) ∂ − → θ = N n=1 ∂ log p(y (n) |x (n) ; − → θ ) ∂ − → θ + λ N n=1 y∼p(·|x (n) ; ← − θ ) ∂ log p(y|x (n) ; − → θ ) ∂ − → θ + λ N n=1 y∼p(·|x (n) ; − → θ ) log p(y|x (n) ; ← − θ ) p(y|x (n) ; − → θ ) ∂ log p(y|x (n) ; − → θ ) ∂ − → θ<label>(10)</label></formula><p>The first part tries to maximize the log likelihood of the bilingual training corpus. The second part maximizes the log likelihood of the "pseudo corpus" constructed by the R2L model. The third part maximizes a weighted log likelihood of another pseudo corpus generated by the L2R model itself with a weight of (log(p(y|x (n) ; ← − θ )/p(y|x (n) ; − → θ ))) which penalizes the samples where the L2R and R2L models do not agree. We find that the R2L model plays the role of an auxiliary system which provides a pseudo corpus in the second part and calculates the weight in the third part.</p><p>Similarly, we can get corresponding parameter gradients for the R2L model by introducing two KL divergence regularization terms, as follows:</p><formula xml:id="formula_16">∂L( ← − θ ) ∂ ← − θ = N n=1 ∂ log p(y (n) |x (n) ; ← − θ ) ∂ ← − θ + λ N n=1 y∼p(·|x (n) ; − → θ ) ∂ log p(y|x (n) ; ← − θ ) ∂ ← − θ + λ N n=1 y∼p(·|x (n) ; ← − θ ) log p(y|x (n) ; − → θ ) p(y|x (n) ; ← − θ ) ∂ log p(y|x (n) ; ← − θ ) ∂ ← − θ<label>(11)</label></formula><p>With the help of the R2L model, the L2R model can be enhanced using Equation <ref type="bibr" target="#b19">10</ref>. With the enhanced L2R model, a better pseudo corpus and more accurate weights can be leveraged to improve the performance of the R2L model with <ref type="bibr">Equation 11</ref>, while simultaneously this better R2L model can be reused to improve the L2R model. In such a way, L2R and R2L models can mutually boost each other as illustrated in <ref type="figure">Figure 2</ref>. The training process continues until the performance on a development data set is no further improving.</p><p>Since both the source and target sentences can be generated from left to right and from right to left, we can have a total of four systems, two source to target models: S2T-L2R (target sentence is generated from left to right), S2T-R2L (target sentence is generated from right to left), and two target to source models: T2S-L2R (source sentence is generated from left to right), T2S-R2L (source sentence is generated from right to left). Using the agreement regularization method described above, these four models can be optimized in a unified joint training framework, as shown in Algorithm 1. With the joint training method, a weighted pseudo corpus is generated by T2S-L2R model and used to train two S2T models (S2T-L2R and S2T-R2L) with the help of agreement regularization. The enhanced S2T-L2R model is then used to build another weighted pseudo corpus to train two T2S models. These four systems boost each other until convergence is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Data Selection and Filtering</head><p>Though NMT systems require huge amounts of training data, not all data are equally useful for training the systems. NMT systems are more vulnerable to noisy training data, rare occurrences in the data, and the training data quality in general. We are trying to tackle two different problems: selecting data relevant to the task and removing noisy data. Out-of-domain and noisy data are distinct problems and may harm the system in different ways. Many studies have highlighted the bad impact of noisy data on MT, such as <ref type="bibr" target="#b12">[4]</ref>. Even small amounts of noisy data can have very bad effects since NMT models tend to assign high probabilities to rare events. Noise in data can take several forms, including totally incorrect translations, partial translations, inaccurate or machine translated data, wrong source or target language, or source copied to the target. We use features from word alignment to filter out the very noisy data, similar to the approach in <ref type="bibr" target="#b28">[19]</ref>. However, data that is less egregiously noisy represents a bigger problem since it is harder to recognize.</p><p>The de-facto standard method for data selection for SMT is <ref type="bibr" target="#b37">[28]</ref> and <ref type="bibr" target="#b10">[2]</ref>. Unfortunately it has not proved as useful for NMT; while it reduces the training data it does not lead to improvements in system quality <ref type="bibr" target="#b47">[38]</ref>. We propose a new approach that tackles both problems at once: filtering noisy data and selecting relevant data. Our approach centers on first learning a bilingual sentence vector representation where sentences in both languages are mapped into the same space. After learning this representation, we use it for both filtering noisy data and selecting relevant data.</p><p>To learn our sentence representation we train a unified bilingual NMT system similar to <ref type="bibr" target="#b63">[54]</ref> that can translate between Chinese and English in both directions. We train this on a selected subset of the data that is known to be of good quality and in the relevant domain. Building the model with such relevant data has two advantages. First: it helps the representation to be similar to the cleaner data; second: relevant sentences would have better representation than irrelevant ones. Therefore we would achieve both data cleaning and relevant data selection objectives.</p><p>Recent progress in multi-lingual NMT i.e. <ref type="bibr" target="#b31">[22]</ref> and <ref type="bibr" target="#b27">[18]</ref> shows that these models are able to represent multiple languages in the same space. However, we don't use language markers because we want to force the model to learn similar representations for both Chinese and English. Given this bilingual system, for any sentence in Chinese or English we can run the encoder part of the system to get a contextual vector representation for each word of a sentence. This is the vector from the last encoder layer, normally used as input to the attention model. We represent each sentence vector as the mean of the word-level contextual vectors.</p><p>Specifically, the encoder assigns to each of the T s source words a representation based on its original embedding and contextual information gathered from other positions. We denote this set of top-layer state vectors as h 1:Ts :</p><formula xml:id="formula_17">h 1:Ts = f enc E(x 1 ), ..., E(x Ts ))<label>(12)</label></formula><p>where E I ∈ R V ×d is a look-up table of joint source and target embeddings, assigning each individual word a unique embedding vector. If h enc 1:Ts denotes the encoder's top layer's output sequence, the sentence-vector representation S sv of a given sentence S of length T s is:</p><formula xml:id="formula_18">S sv = Ts =1 h enc<label>(13)</label></formula><p>A similarity measure SIM ST between any two given sentences S and T , regardless of their languages, can be represented as the cosine similarity between their corresponding sentences vectors:</p><formula xml:id="formula_19">SIM ST = S sv · T sv |S sv ||T sv |<label>(14)</label></formula><p>We train an RNN encoder-decoder system similar to <ref type="bibr" target="#b55">[46]</ref> with 4 encoder layers with the first layer being bidirectional and 4 decoder layers and an attention model. After training the model, we run the encoder part only. Each resulting word context vector is composed of an 1024 dimension vector; therefore the sentence vector (S sv ) representation is of the same size.</p><p>For each sentence in the parallel training corpus, we measure the cross-lingual similarity between source and target sentences as in Equation <ref type="bibr" target="#b23">14</ref>. We reject sentences with similarity below a specified threshold. This approach enables us to drastically reduce the training data while significantly improving the accuracy. Since we use a model trained on relevant data, this data selection technique can serve a dual purpose by filtering noisy data as well as selecting relevant data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">System Combination and Re-ranking</head><p>In order to combine the systems described above, we combine n-best hypotheses from all systems and then train a re-ranker using k-best MIRA on the validation set. K-best MIRA <ref type="bibr" target="#b16">[8]</ref> is a version of MIRA (a margin-based classification algorithm) that works with a batch tuning to learn a re-ranker for the k-best hypothesis.</p><p>The features we use for re-ranking are:</p><p>• SY S Score : Original System Score and identifier.</p><p>• LM Score : 5-gram language model trained on English news crawled data of 2015 and 2016.</p><p>• R2L score : R2L system re-scoring. A system trained on Chinese source and reversed English target; the system is used to score each hypothesis.</p><p>• E2Z score : English-to-Chinese system re-scoring. A system trained on English to Chinese is used to score each hypothesis. .</p><p>• ST SV : Cross-lingual sentence similarity between source and the hypothesis as described in Section 3.5.</p><p>• R2L SV : R2L sentence vector similarity: the best hypothesis from the R2L system is compared to each n-best hypothesis and used to generate a sentence similarity score based on sentence vector as above.</p><p>• E2Z SV : Back Composition sentence vector similarity. A round trip translation is done for each n-best hypothesis to translate it back to Chinese. Then we use sentence vector similarity to measure the similarity between the original source and the recomposed source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first introduce the data and experimental setup used in our experiments, and then evaluate each of the systems introduced in Section 3, both independently and after system combination and re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Experimental Setup</head><p>We use all of the available parallel data for the WMT17 Chinese-English translation task. This consists of about 332K sentence pairs from the News Commentary corpus, 15.8M sentence pairs from the UN Parallel Corpus, and 9M sentence pairs from the CWMT Corpus. We further filter the bilingual corpus according to the following criteria:</p><p>• Both the source and target sentences should contain at least 3 words and at most 70 words.</p><p>• Pairs where (source length &lt; 1.3 * target length or target length &lt; 1.3 * source length) are removed.</p><p>• Sentences with illegal characters (such as URLs, characters of other languages) are removed.</p><p>• Chinese sentences without any Chinese characters are removed.</p><p>• Duplicated sentence pairs are removed.</p><p>After filtration, we are left with 18M bilingual sentence pairs. We use the Chinese and English language models trained on the 18M sentences of bilingual data to filter the monolingual sentences from "News Crawl: articles from 2016" and "Common Crawl" provided by WMT17 using CED <ref type="bibr" target="#b37">[28]</ref>. After filtering, we retain about 7M English and Chinese monolingual sentences. The monolingual data will be deployed in both dual learning and back-translation setups through the experiments.</p><p>Newsdev2017 is used as the development set and Newstest2017 as the test set. All the data (parallel and monolingual) have been tokenized and segmented into subword symbols using bytepair encoding (BPE) <ref type="bibr" target="#b40">[31]</ref>. The Chinese data has been tokenized using the Jieba tokenizer 7 . English sentences are tokenized using the scripts provided in Moses. We learn a BPE model with 32K merge operations, in which 44K and 33K sub-word tokens are adopted as source and target vocabularies separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>The Transformer model <ref type="bibr" target="#b49">[40]</ref> is adopted as our baseline. Unless otherwise mentioned, all translation experiments use the following hyper-parameter settings based on Tensor2Tensor Transformer-big settings v1.3.0 8 . This corresponds to a 6-layer transformer with a model size of 1024, a feed forward network size (d f f ) of 4096, and 16 heads. All models are trained on 8 Tesla M40 GPUs for a total of 200K steps using the Adam <ref type="bibr" target="#b32">[23]</ref> algorithm. The initial learning rate is set to 0.3 and decayed according to the "noam" schedule as described in <ref type="bibr" target="#b49">[40]</ref>.During training, the batch size is set to 5120 words per batch and checkpoints are created every 60 minutes. All results are reported on averaged parameters of the last 20 checkpoints. At test time, we use a beam of 8 and a length penalty of 1.0. All reported scores are computed using sacreBLEU v1.2.3, 9 which calculates tokenization-independent BLEU <ref type="bibr" target="#b38">[29]</ref>. <ref type="bibr" target="#b19">10</ref> The first section of <ref type="table" target="#tab_2">Table 1</ref> shows the results for the baselines. First we compare with the Sogou system <ref type="bibr" target="#b52">[43]</ref>, which was the best result reported at WMT 2017 evaluation campaign. Though Sogou is an ensemble of many systems, we reference it here for comparison. The rest of the systems reported in the table are single systems. Our baseline system, labeled Base, is trained on 18M sentences. BT is adding the back-translated data to the baseline.  <ref type="table" target="#tab_2">Table 1</ref>. Dual Learning makes more efficient use of the monolingual sentences and exploits the duality between Zh→En and En→Zh translation directions. Based on system BT, the Dual Learning system DL achieves 26.51 BLEU, a 0.94 point improvement over the BT system, and outperforms the best ensemble result of 26.40 in the WMT 2017 Chinese-English challenge . The Deliberation Network is further applied to the Dual Learning system, which is denoted as DLDN. The Deliberation Network aims to improve sentence generation quality by incorporating the global information provided by a first pass decoder. The DLDN system further achieves a BLEU score of 27.40, a 0.89 BLEU score improvement over the already strong DL system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SystemID Settings BLEU</head><p>We also explore some variants of our DL and DLDN systems, denoted as DLDN2/3/4 in the second section of <ref type="table" target="#tab_2">Table 1</ref>. In DLDN, we use both the first and second pass decoders to rerank the generated sentence and choose the top-1 result. In system DLDN2, we then remove this reranking to see how the performance changes, yielding a 27.20 BLEU score, a 0.2 point drop. In system DLDN3, we replace the Deliberation Network with R2L sampling. R2L sampling is a data augmentation technique where we first train a Zh→En model that generates sentences in a rightto-left(R2L) manner by reversing the target sentence in the training data, and use the R2L model to sample English sentences given monolingual Chinese sentences. We can see that adding R2L sampling to Dual Learning indeed brings BLEU score improvements, but performs worse than the Deliberation Network. In system DLDN4, we further add Bi-NMT, which bidirectionally generates candidate sentences in a single model, on the DL system and achieve 27.16 BLEU score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results of Agreement Regularization and Joint Training</head><p>Data enhancement has been shown to improve NMT performance. We proposed the agreement regularization approach to explore data enhancement by using a right to left model to encourage consensus translations. The existing back-translation method is also one of the data enhancement approaches that leverages monolingual target data to generate synthetic bilingual data. Extending the back-translation approach, our proposed joint-training approach interactively makes data enhancement by boosting source-to-target and target-to-source NMT systems. Eventually, the unified joint training framework, denoted as ARJT, is used to integrate the agreement regularization approach, the back translation approach, and the joint training approach to further improve the performance of NMT systems. The evaluation results of the agreement regularization and the unified joint training are listed in the third section of <ref type="table" target="#tab_2">Table 1</ref>. Compared to BT, our agreement regularization can achieve improvements of 1.34 BLEU points. Adding the joint training can bring this up to a 1.81 gain.</p><p>We also explore several variants of our ARJT system, denoted as ARJT2/3/4 in <ref type="table" target="#tab_2">Table 1</ref>. We vary the dropout probability in order to explore the interaction between dropout regularization and agreement regularization. Unlike ARJT, these variants don't use the validation set for early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results of Data Selection</head><p>In addition to our results using the WMT training data, we also explore training our system on a larger corpus. We experimented with 100M parallel sentences drawn from UN data, Open Subtitles and Web crawled data. It is worth noting that the experiments reported in <ref type="table" target="#tab_2">Table 1</ref> were constrained data experiments limited to WMT17 official data only. While the experiments reported in <ref type="table" target="#tab_4">Table 2</ref> are unconstrained systems using additional data.</p><p>First we apply word alignment heuristics to filter very noisy data. This filters out around 10% of the data. Then we apply Cross-Entropy data selection <ref type="bibr" target="#b37">[28]</ref> and <ref type="bibr" target="#b10">[2]</ref> to order the sentences based on their relevance to the CWMT part of the WMT data. We then select a specific number of sentences pairs by rank.</p><p>In a separate experiment, we also apply the SentVec similarity filtering, described in Section 3.5, to select the same amount of data and measure its effect. We use a cutoff threshold of the cosine similarity of 0.2. We train the unified bi-lingual encoder on a selected subset of the data that is known to be of good quality and in the relevant domain, specifically, the CWMT data of 9M sentence pairs. Since the system is trained to translate in both directions, it is effectively trained on on 18M sentence pairs. <ref type="table" target="#tab_4">Table 2</ref> shows the results of data selection. Base8K is using baseline data and back translated data, however it uses a larger model architecture that we found to work better with larger data sets. Base8K uses 6-layer transformer with a model size of 1024, a Feed Forward Network size (d f f ) of 8192, and 16 heads. All models reported in <ref type="table" target="#tab_4">Table 2</ref> are trained for 300K steps with minibatch of 3500 on 8 GPUs. We average the last 20 checkpoints as before and decode with beam size of 8 and length penalty of 1.0 similar to the setup above.</p><p>CED1 and CED2 add 35M sentences and 50M sentences respectively to Base8k. SV1 and SV2 added the same amount of data selected by SentVec similarity discussed in Section 3.5. SV3 and SV4 experimented with varying the dropout ratio to measure its impact with the larger training data and model architecture. Generally the systems using SentVec similarity filtering achieve improvements up to 1.5 BLEU points over Base8K and nearly 1 BLEU point as compared to systems using the same amount of CED-selected data. We conclude that SentVec similarity filtering is a helpful approach since it filters out noisy data which is hard to identify. Since SentVec prevents data with partial and low-quality translation from negatively impacting the system. Furthermore, the proposed approach helps select relevant data similar to CWMT data.  We experiment with system combination of n-best lists generated from various systems discussed above with 8 hypothesis from each system. We use various features to re-rank the systems hypothesis as described in Section 3.6. As shown in <ref type="table" target="#tab_5">Table 3</ref>, combining the set of heterogeneous systems are complementary and achieved the highest results. We have experimented with many configurations and features for systems combination, we found out that the most helpful scoring features are: SY S Score , LM Score , R2L score , R2L SV and E2Z SV . This is quite surprising since the combined systems were focusing on modeling similar features. This may be due to the fact that the models are learning complimentary features, so they have extra capacity for complementing each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SystemID Settings BLEU</head><p>We think it would be useful to combine all proposed approaches in a single system. However, we leave this as a future work item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SystemID Settings BLEU</head><p>Combo-1 SV1, SV2, SV3 27.84 Combo-2 DLDN2, DLDN3, DLDN4</p><p>27.92 Combo-3 ARJT2, ARJT3, ARJT4 + 3 identical systems with different initialization 27.82</p><p>Combo-4 SV1, SV2, SV3, ARJT1, ARJT2, ARJT3, DLDN2, DLDN3, DLDN4</p><p>28.46 Combo-5 SV1, SV2, SV3, ARJT2, DLDN2, DLDN4</p><p>28.32 Combo-6 SV1, SV2, SV4, ARJT2, ARJT3, ARJT4, DLDN2, DLDN3, DLDN4</p><p>28.42  All our research systems significantly outperform Reference-PE, which is based on human postediting of machine translation output, and the original Reference-WMT, which is again a human translation. # denotes the ranking cluster, Ave % the averaged raw score r ∈ [0, 100], and Ave z the standardized z score. n ≥ x denotes that we collected at least x assessments per system for the respective evaluation campaign. This is referred to as Meta-1 in <ref type="table">Table 5g</ref>. <ref type="table" target="#tab_6">Table 4</ref> presents the results from our large scale human evaluation campaign. Based on these results we claim that we have achieved human parity according to Definition 2, as our research systems are indistinguishable from human translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Human Evaluation Results</head><p>In the table, systems in higher clusters significantly outperform all systems in lower clusters according to Wilcoxon rank sum test at p-level p ≤ 0.05, following WMT17. Systems in the same cluster are ordered by z score-which is defined as the signed number of standard deviations an observation is above the mean, computed on the annotator level to address different annotation behavior-but considered tied w.r.t. quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Human Evaluation Setup</head><p>As discussed in Section 2 our evaluation methodology is based on source-based direct assessment as described in <ref type="bibr" target="#b15">[7]</ref>. We use an updated version of Appraise <ref type="bibr" target="#b23">[14]</ref>, the same tool which is used in the human evaluation campaign for the Conference on Machine Translation (WMT). 11 See <ref type="bibr" target="#b14">[6]</ref> for more details on last year's WMT17 results and evaluation.</p><p>The main differences to the WMT17 campaign are:</p><p>1. Our evaluation is based on quality assessment of translations with respect to the source text, not a reference translation. To do this, we hire bilingual crowd workers;</p><p>2. We enforce full system coverage for the evaluation samples. This means that for every segment we get human scores for all systems under investigation;</p><p>3. We require redundancy so that for every annotation task (also referred to as "HIT" in other direct assessment publications) we collect scores from three annotators.</p><p>The latter two changes have been introduced to strengthen our results, by adding additional redundancy. Direct assessment as an estimator of general system quality does not require these, but in the context of achieving human parity, extra layers of fully comparable segment scores enable more thorough external validation. We intend to release all data related to the final human parity evaluation campaigns, so this data will become available for independent inspection by the research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Benchmark Translations</head><p>We compare our research systems against the following sets of translations. These sets have been kept stable across all evaluation campaigns, allowing us to track research results over time.</p><p>Reference-HT vendor-created human translations of newstest2017. Translators were instructed to translate from scratch, i.e., without using any online translation engines; 12</p><p>Reference-PE vendor-created human post-editing output, based on Google Translate machine translation results;</p><p>Reference-WMT Original newstest2017 reference released after WMT17. The original WMT17 reference translation for newstest2017 is known to contain errors, so we decided to add it to the set of evaluated systems. This allows us to get external validation for the quality of our two human references;</p><p>Online-A-1710 Microsoft Translator production system, collected on October 16, 2017;</p><p>Online-B-1710 Google Translate production system, collected on October 16, 2017;</p><p>Sogou The Sogou Knowing NMT system, which performed best at last year's WMT17 Conference on Machine Translation (WMT) shared task on news translation <ref type="bibr" target="#b51">[42]</ref>.</p><p>Note that the benchmark human references were not available to the system developers. Also, the presented set of translation systems affects human-perceived quality (both based on the total number and distribution of quality across systems), so we do not expect scores to be comparable across campaigns. The question of comparability of raw direct assessment scores over time is an open research problem still, so we take a conservative approach and do not compare them. Scores within a single campaign are reliable. We also assume that standardized scores for the same set of translation systems should be fairly comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Guarding Against Confounds</head><p>Whenever trying to draw a conclusion based on a pair of different translations, we must avoid measuring the effects of extraneous variables that can confound the experimental variables we wish to measure <ref type="bibr" target="#b17">[9]</ref>. For example, when comparing the translation quality by varying how it is produced (human translation versus automatic translation), we do not wish our measurements of translation quality to be influenced by external factors, e.g., perhaps a human translator did a poor job when translating a few sentences or an automatic translation system happens to be exceptionally good at translating a particular subset of sentences.</p><p>In this work, we specifically control for the effects of several potential extraneous variables:</p><p>• Variability of quality measure How sensitive is our quality measure (direct assessment) to different subsets of the data? We answer this by running redundant evaluation campaigns across different subsets of the data. • Annotator inconsistency What if the annotators produce different scores given the same data? Would using different annotators still lead to the same conclusion? To control for this, our evaluation campaigns directly include multiple evaluators.</p><p>• Choice of systems Was this particular system combination somehow "lucky", or would similar combinations also lead to the same conclusion? To answer this question, we include multiple system combinations with varying sets of input systems. (Section 5.4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation Campaigns</head><p>We conduct the following evaluations:</p><p>Annotator variability study To measure this, we repeat the same evaluation campaign three times. All data is collected on the same subset. We allow annotator overlap but do not enforce it. In the end, we had a near complete annotator overlap, likely due to the timing of our campaigns. <ref type="bibr" target="#b22">13</ref> We refer to this as Eval Round 1, on evaluation sample Subset-1;</p><p>Data variability study Our data subsets are randomly selected from the source data. Still, the actual subset could affect results in our favor. To counter this, we conduct three additional evaluation campaigns on three completely different subsets of data. We refer to this as Eval Round 2, on evaluation samples Subset-2, Subset-3, and Subset-4.</p><p>As the set of systems for all these campaigns does not change, results are theoretically comparable, so we can also report synthesized, joint scores, for both dimensions in isolation and in combined form.</p><p>Evaluation campaign parameters are as follows: After completion of all six evaluation campaigns, we have collected at least 25,200 data points (i.e., segment scores) or at least 2,520 per system. This is comparable to the amount of annotations collected for last year's WMT17 evaluation campaign (2,421 assessments per system). We report results for individual campaigns and our final synthesized, joint meta-campaign:</p><p>Meta-1 We combine assessments from evaluation campaigns Eval Round 1a-c, on evaluation sample SubsetB, effectively increasing data points by a factor of 3x. Note that this is fair as result clusters are based on standardized scores which can fairly be computed if all annotators are exposed to exactly the same segments per system.</p><p>While it is also possible to combine data across subsets, we choose not to do this as this potentially affects standardization of annotator scores. For Meta-1, due to the identical assignment of annotators to segments, we have a guarantee that standardization is reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Annotator Variability Results</head><p>Subset-1, first iteration (g) Meta-1, n ≥ 1, 827 <ref type="table">Table 5</ref>: Complete results for our three iterations over Subset-1 (5a, 5b, 5c) and our evaluation campaigns for Subset-2 (5d), Subset-3 (5e), and Subset-4 (5f). We also show results for combined data for Meta-1 (5g) combining annotations from all iterations over Subset-1. # denotes the ranking cluster, Ave % the averaged raw score r ∈ [0, 100], and Ave z the standardized z score. n ≥ x denotes that we collected at least x assessments per system for the respective evaluation campaign. All campaigns involved a = 15 annotators. Systems in higher clusters significantly outperform all systems in lower clusters according to Wilcoxon rank sum test at p-level p ≤ 0.05, following WMT17. Systems in the same cluster are ordered by z score but considered tied w.r.t. quality.   <ref type="table">Table 5</ref>. This emphasizes the need for human evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Data Release</head><p>We have released 15 all data from the human evaluation campaigns to 1) allow external validation of our claim of having achieved human parity and 2) to foster future research by releasing two additional human references for the Reference-WMT test set. The release package contains the following items:</p><p>New references for newstest2017 Two new references for newstest2017, one based on human translation from scratch (Reference-HT), the other based on human post-editing (Reference-PE). <ref type="table" target="#tab_11">Table 6</ref> reports the BLEU scores for single and multi reference use with sacreBLEU;</p><p>Human parity translations Output generated by our research systems Combo-4, Combo-5, and Combo-6;</p><p>Online translations Output from online machine translation service Online-A-1710, collected on October 16, 2017;</p><p>Human evaluation data All data points collected in our human evaluation campaigns. This includes annotations for Subset-1, Subset-2, Subset-3, and Subset-4. We share the (anonymized) annotator IDs, segment IDs, system IDs, type ID (either TGT or CHK, the second being a repeated judgment for the first), raw scores r ∈ [0, 100], as well as annotation start and end times.</p><p>We do not redistribute the following items:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Human Analysis</head><p>Lastly, a preliminary human error analysis was conducted over the output of the Combo-6 system (the system that achieved the best results). We randomly sampled 500 sentences and annotated each translation with whether a specific error type was present. Following <ref type="bibr" target="#b50">[41]</ref>, we use 9 categories: Missing Words, Word Repetition, Named Entity, Word Order, Incorrect Words, Unknown Words, Collocation, Factoid, and Ungrammatical. The Named-Entity category is further subdivided into Person, Location, Organization, Event, and Other.  <ref type="table">Table 7</ref>: Error distribution, as fraction of sentences that contain specific error categories. <ref type="table">Table 7</ref> shows the distribution of the annotated errors as the fraction of sentences containing a specific error category. The four major error types are Missing words, Incorrect Words, Ungrammatical, and Named Entity. Each accounts for roughly 5% of errors. This indicates that there is still room to improve machine translation quality via various approaches, such as modeling Missing Words <ref type="bibr" target="#b46">[37,</ref><ref type="bibr" target="#b24">15]</ref>, integration of high quality data for named-entity translation, as well as domain and topic adaptation for the issues of incorrect words and ungrammaticality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Illustration of joint training: S2T p(y|x) and T2S p(x|y) Illustration of agreement regularization: L2R p(y|x; − → θ ) and R2L p(y|x; ← − θ )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Deliberation network: Blue, red and green parts indicate encoder E, first-pass decoder D 1 and second-pass decoder D 2 respectively. Solid lines represent the information flow via attention model. The self attention model within E and the E-to-D 1 attention model are omitted for readability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 :</head><label>1</label><figDesc>T2S-L2R Model p( − → x |y) and T2S-R2L Model p( ← − x |y); procedure training process 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Automatic (BLEU) evaluation results on the WMT 2017 Chinese-English test set Experimental Results of Dual Learning and Deliberation Networks Our Dual Learning system consists of a Zh→En model and an En→Zh model, each adopting the same model configuration as the baseline (Base). For the deliberation network, the encoder and the first-pass decoder are initialized from the Zh→En model in the Dual Learning system, 7 https://github.com/fxsjy/jieba 8 https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py 9 https://github.com/awslabs/sockeye/tree/master/contrib/sacrebleu 10 sacreBLEU signature: BLEU+case.mixed+lang.zh-en+numrefs.1+smooth.exp_+test.wmt17/improved+tok.13a+version.1.2.3 and the second pass decoder share the same model structures with the first-pass decoder. The evaluation results of the Dual Learning and Deliberation Network systems on WMT 2017 Chinese-English test set are listed in the second section of</figDesc><table><row><cell>Sogou</cell><cell>WMT 2017 best result [43]</cell><cell>26.40</cell></row><row><cell>Base</cell><cell>Transformer Baseline</cell><cell>24.2</cell></row><row><cell>BT</cell><cell>+Back Translation</cell><cell>25.57</cell></row><row><cell>DL</cell><cell>BT + Dual Learning</cell><cell>26.51</cell></row><row><cell>DLDN</cell><cell>BT + Dual Learning + Deliberation Nets</cell><cell>27.40</cell></row><row><cell>DLDN2</cell><cell>DLDN without first decoder reranking</cell><cell>27.20</cell></row><row><cell>DLDN3</cell><cell>BT+ Dual Learning + R2L sampling</cell><cell>26.88</cell></row><row><cell>DLDN4</cell><cell>BT+ Dual Learning + Bi-NMT</cell><cell>27.16</cell></row><row><cell>AR</cell><cell>BT + Agreement Regularization</cell><cell>26.91</cell></row><row><cell>ARJT</cell><cell>BT + Agreement Regularization + Joint Training</cell><cell>27.38</cell></row><row><cell>ARJT2</cell><cell>ARJT + dropout=0.1</cell><cell>27.19</cell></row><row><cell>ARJT3</cell><cell>ARJT + dropout=0.05</cell><cell>27.07</cell></row><row><cell>ARJT4</cell><cell>ARJT + dropout=0.01</cell><cell>26.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Evaluation Data selection results on the WMT 2017 Chinese-English test set</figDesc><table><row><cell>Experimental Results of Systems Combination</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>System combination results on the WMT 2017 Chinese-English test set</figDesc><table><row><cell cols="3"># Ave % Ave z System</cell></row><row><cell>1</cell><cell>69.0</cell><cell>0.237 Combo-6</cell></row><row><cell></cell><cell>68.5</cell><cell>0.220 Reference-HT</cell></row><row><cell></cell><cell>68.9</cell><cell>0.216 Combo-5</cell></row><row><cell></cell><cell>68.6</cell><cell>0.211 Combo-4</cell></row><row><cell>2</cell><cell>67.3</cell><cell>0.141 Reference-PE</cell></row><row><cell>3</cell><cell cols="2">62.3 -0.094 Sogou</cell></row><row><cell></cell><cell cols="2">62.1 -0.115 Reference-WMT</cell></row><row><cell>4</cell><cell cols="2">56.0 -0.398 Online-A-1710</cell></row><row><cell></cell><cell cols="2">54.1 -0.468 Online-B-1710</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Human Evaluation Results for at least n ≥ 1, 827 assessments per system show that our research systems Combo-4, Combo-5, and Combo-6 achieve human parity according to definition 2 as they are not distinguishable from Reference-HT, which is a human translation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Test set selection Would we likely obtain the same result on slightly different test data? We control for this by running redundant large-scale human evaluation campaigns under several configurations to replicate results (Section 5.4). Annotator errors What if some annotators become inattentive, unfairly improving or damaging the score of one system over the other? To control for this effect, we use rejection sampling when gathering human assessments by occasionally showing annotators examples where one sentence is intentionally and noticeably worse; annotators that fail to detect these are excluded from the data, ensuring that human judgments are high quality.</figDesc><table /><note>••</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Data points: 4,200 (at least 14 466 per system) The set of systems for the final evaluation campaigns consists of the following systems:</figDesc><table /><note>• Annotators: 15• Tasks: 20• Redundancy: 3• Tasks per annotator: 4 (about 2 hours of work)• Systems: 9•• References: Reference-HT, Reference-PE, Reference-WMT• Production: Online-A-1710, Online-B-1710• WMT17: Sogou• Candidates: Combo-4, Combo-5, Combo-6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Table 5ashows the results of our first evaluation round on Subset-1. Note how our research systems outperform Sogou and both Reference-WMT and Reference-PE. Based on this clustering it becomes clear that there must be quality issues with the original Reference-WMT reference. All three systems Combo-4, Combo-5, and Combo-6 achieve human parity with Reference-HT. We collected at least n ≥ 609 assessments per system.</figDesc><table><row><cell>#</cell><cell>Ave %</cell><cell>Ave z</cell><cell>System</cell><cell>#</cell><cell>Ave %</cell><cell>Ave z</cell><cell>System</cell><cell>#</cell><cell>Ave %</cell><cell>Ave z</cell><cell>System</cell></row><row><cell>1</cell><cell>69.9</cell><cell>0.256</cell><cell>Combo-6</cell><cell>1</cell><cell>68.6</cell><cell>0.233</cell><cell>Reference-HT</cell><cell>1</cell><cell>68.5</cell><cell>0.240</cell><cell>Reference-HT</cell></row><row><cell></cell><cell>69.8</cell><cell>0.233</cell><cell>Combo-4</cell><cell></cell><cell>68.6</cell><cell>0.225</cell><cell>Combo-6</cell><cell></cell><cell>68.4</cell><cell>0.229</cell><cell>Combo-6</cell></row><row><cell></cell><cell>69.9</cell><cell>0.230</cell><cell>Combo-5</cell><cell></cell><cell>68.6</cell><cell>0.217</cell><cell>Combo-5</cell><cell></cell><cell>68.1</cell><cell>0.201</cell><cell>Combo-5</cell></row><row><cell></cell><cell>68.6</cell><cell>0.186</cell><cell>Reference-HT</cell><cell></cell><cell>68.3</cell><cell>0.207</cell><cell>Combo-4</cell><cell></cell><cell>67.7</cell><cell>0.194</cell><cell>Combo-4</cell></row><row><cell></cell><cell>67.6</cell><cell>0.129</cell><cell>Reference-PE</cell><cell></cell><cell>67.4</cell><cell>0.154</cell><cell>Reference-PE</cell><cell></cell><cell>66.8</cell><cell>0.141</cell><cell>Reference-PE</cell></row><row><cell>2</cell><cell cols="3">63.3 -0.095 Sogou</cell><cell>2</cell><cell cols="3">61.9 -0.105 Sogou</cell><cell>2</cell><cell cols="2">61.8 -0.083</cell><cell>Sogou</cell></row><row><cell></cell><cell cols="2">62.1 -0.132</cell><cell>Reference-WMT</cell><cell></cell><cell cols="2">62.1 -0.113</cell><cell>Reference-WMT</cell><cell></cell><cell cols="2">62.0 -0.100</cell><cell>Reference-WMT</cell></row><row><cell>3</cell><cell cols="3">57.0 -0.383 Online-A-1710</cell><cell>3</cell><cell cols="3">55.7 -0.399 Online-A-1710</cell><cell>3</cell><cell cols="2">55.2 -0.413</cell><cell>Online-A-1710</cell></row><row><cell></cell><cell cols="2">54.1 -0.494</cell><cell>Online-B-1710</cell><cell></cell><cell cols="2">53.9 -0.468</cell><cell>Online-B-1710</cell><cell></cell><cell cols="2">54.3 -0.442</cell><cell>Online-B-1710</cell></row><row><cell></cell><cell cols="3">(a) Subset-1, n ≥ 609</cell><cell></cell><cell cols="3">(b) Subset-1, second iteration</cell><cell></cell><cell cols="3">(c) Subset-1, third iteration</cell></row><row><cell>#</cell><cell>Ave %</cell><cell>Ave z</cell><cell>System</cell><cell>#</cell><cell>Ave %</cell><cell>Ave z</cell><cell>System</cell><cell>#</cell><cell>Ave %</cell><cell>Ave z</cell><cell>System</cell></row><row><cell>1</cell><cell>68.6</cell><cell>0.212</cell><cell>Reference-HT</cell><cell>1</cell><cell>67.4</cell><cell>0.251</cell><cell>Reference-HT</cell><cell>1</cell><cell>66.6</cell><cell>0.254</cell><cell>Reference-HT</cell></row><row><cell></cell><cell>68.2</cell><cell>0.200</cell><cell>Combo-5</cell><cell></cell><cell>67.1</cell><cell>0.247</cell><cell>Reference-PE</cell><cell></cell><cell>65.2</cell><cell>0.179</cell><cell>Combo-6</cell></row><row><cell></cell><cell>67.9</cell><cell>0.182</cell><cell>Combo-4</cell><cell></cell><cell>65.3</cell><cell>0.147</cell><cell>Combo-6</cell><cell></cell><cell>64.4</cell><cell>0.151</cell><cell>Combo-5</cell></row><row><cell></cell><cell>67.9</cell><cell>0.177</cell><cell>Combo-6</cell><cell></cell><cell>64.9</cell><cell>0.106</cell><cell>Combo-4</cell><cell></cell><cell>64.2</cell><cell>0.147</cell><cell>Combo-4</cell></row><row><cell>2</cell><cell>64.8</cell><cell>0.044</cell><cell>Reference-PE</cell><cell></cell><cell>64.3</cell><cell>0.091</cell><cell>Combo-5</cell><cell></cell><cell>63.4</cell><cell>0.127</cell><cell>Reference-PE</cell></row><row><cell></cell><cell cols="2">62.5 -0.061</cell><cell>Sogou</cell><cell>2</cell><cell cols="3">61.1 -0.065 Sogou</cell><cell>2</cell><cell cols="2">60.5 -0.030</cell><cell>Sogou</cell></row><row><cell>3</cell><cell cols="3">59.6 -0.200 Reference-WMT</cell><cell></cell><cell cols="2">59.6 -0.119</cell><cell>Reference-WMT</cell><cell></cell><cell cols="2">60.1 -0.074</cell><cell>Reference-WMT</cell></row><row><cell></cell><cell cols="2">58.4 -0.277</cell><cell>Online-A-1710</cell><cell>3</cell><cell cols="3">55.3 -0.351 Online-A-1710</cell><cell>3</cell><cell cols="2">53.4 -0.367</cell><cell>Online-A-1710</cell></row><row><cell></cell><cell cols="2">55.7 -0.353</cell><cell>Online-B-1710</cell><cell></cell><cell cols="2">54.4 -0.377</cell><cell>Online-B-1710</cell><cell></cell><cell cols="2">51.7 -0.455</cell><cell>Online-B-1710</cell></row><row><cell></cell><cell cols="3">(d) Subset-2, n ≥ 607</cell><cell></cell><cell cols="3">(e) Subset-3, n ≥ 650</cell><cell></cell><cell cols="3">(f) Subset-4, n ≥ 649</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>#</cell><cell>Ave %</cell><cell>Ave z</cell><cell>System</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>69.0</cell><cell>0.237</cell><cell>Combo-6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68.5</cell><cell>0.220</cell><cell>Reference-HT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68.9</cell><cell>0.216</cell><cell>Combo-5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68.6</cell><cell>0.211</cell><cell>Combo-4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>67.3</cell><cell>0.141</cell><cell>Reference-PE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell cols="3">62.3 -0.094 Sogou</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">62.1 -0.115</cell><cell>Reference-WMT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell cols="3">56.0 -0.398 Online-A-1710</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">54.1 -0.468</cell><cell>Online-B-1710</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>BLEU scores against single or multiple references. WMT is Reference-WMT, PE is Reference-PE, HT is Reference-HT. Scoring based on sacreBLEU v1.2.3, with signature BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.3 for refs=1. Signature changes to numrefs.2 and numrefs.3 for refs=2 and refs=3, respectively. Note how different scores for Reference-WMT and Reference-PE are compared to Reference-HT and how these compare to our findings reported in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">HyTER<ref type="bibr" target="#b21">[12]</ref> attempted to solve this but did not achieve mainstream success.<ref type="bibr" target="#b12">4</ref> Results from both source-based and reference-based direct assessment collected for IWSLT17<ref type="bibr" target="#b15">[7]</ref> show that annotators assign higher scores in the source-based scenario and that they are more strict with their scoring in the reference-based scenario. This indicates that references do in fact influence human scoring behavior. Consequently, bad references will affect human evaluation in a reference-based direct assessment.<ref type="bibr" target="#b13">5</ref> Co-author Christian Federmann, in his role as co-organizer of the annual WMT evaluation campaign, was instrumental in developing the Appraise evaluation system used by WMT and also in this paper. He was not involved in developing the systems being evaluated here, nor were the human benchmark references available to the system developers. Hence, our evaluation was implemented in a double-blind manner. 6 WMT17 implemented this using R's wilcox.test(). Our implementation differs from this as the clustering has been integrated into Appraise and uses the Mann-Whitney rank test<ref type="bibr" target="#b36">[27]</ref> at the same p-level p ≤ 0.05, based on Python's scipy.mannwhitneyu(). For the purpose of determining if the difference between scores for two candidate systems is statistically significant, both implementations are equivalent.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">This version of Appraise will also be used to run the WMT18 evaluation campaigns. Source code will be released to the public in time for WMT18, as in previous years.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Of course, there are sentences for which the human translation matches Google Translate or Microsoft Translator machine translation output. Relative to the overlap for the post-editing-based reference, this is negligible.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">To complete so many campaigns in such a short time, it was easier to attract crowd workers when they knew they could earn more by completing several campaigns. Combined with our reliability testing, this motivation likely had a positive impact on annotation fidelity and quality.14 Note that as we annotate on unique translation output only, there is a chance that more data points are collected.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Future Work</head><p>In this paper, we described the techniques used in the latest Microsoft machine translation system to reach a new state-of-the-art. Our evaluation found that our system has reached parity with professional human translations on the WMT 2017 Chinese to English news task, and exceeds the quality of crowd-sourced references.</p><p>We exploited the dual nature of the translation problem to better utilize parallel data as well as monolingual data in a more principled way. We utilized joint training of source-to-target, and target-to-source systems to further improve on the duality of the translation task. We addressed the exposure bias problem in two ways: by two-pass decoding using Deliberation networks, as well as by agreement regularization and joint training of left-to-right, right-to-left systems. We trained a bilingual encoder to obtain bilingual sentence representations used to filter noisy data and select relevant data. We also found significant gains from combining multiple heterogeneous systems.</p><p>We addressed the problem of defining and measuring the quality of human translations and near-human machine translations. We found that as translation quality has dramatically improved, automatic reference-based evaluation metrics have become increasingly problematic. We used direct human annotation to measure the quality of both human and machine translations.</p><p>We wish to acknowledge the tremendous progress in sequence-to-sequence modeling made by the entire research community that paved the road for this achievement. We have introduced a few new approaches that helped us to reach human parity for WMT2017 Chinese to English news translation task. At the same time, much work remains to be done, especially in domains and language-pairs that do not benefit from huge amounts of available data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Subset-1, second iteration Table 5b shows the results for our second evaluation round on Subset-1. This time, annotators do not see a significant difference between our research systems and Reference-PE. Consequently, Reference-HTand all three systems Combo-4, Combo-5, and Combo-6 end up in the same cluster as Reference-PE. All these systems outperform Sogou and Reference-WMT. As in the previous round</title>
		<imprint/>
	</monogr>
	<note>online systems Online-A-1710 and Online-B-1710 perform worst</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Similar to the second round, we do not observe a significant difference between Reference-PE and our research systems. Again, Reference-HT, all three systems Combo-4, Combo-5, and Combo-6, and Reference-PE end up in the top cluster. Sogou and Reference-WMT end in the third cluster</title>
	</analytic>
	<monogr>
		<title level="m">Subset-1, third iteration Table 5c shows the results for our third evaluation round on Subset-1</title>
		<imprint/>
	</monogr>
	<note>outperforming Online-A-1710 and Online-B-1710</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Again, the latter are not significantly different w.r.t human perceived quality</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Annotators seem to have a preference for Reference-HT over Combo-4, Combo-5, and Combo-6, but not significantly so. All four systems outperform Reference-PE, which itself outperforms all other systems. Sogou ends up in its own cluster, significantly better than Reference-WMT and the two online systems Online-A-1710 and Online-B-1710</title>
	</analytic>
	<monogr>
		<title level="m">Data Variability Results Subset-2 Table 5d shows the results for our evaluation on Subset-2</title>
		<imprint/>
	</monogr>
	<note>We collected at least n ≥ 607 assessments per system</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">This one is interesting as it is the only evaluation round which shows Reference-PE on top, based on its z score. Otherwise, we continue to see Reference-HT, Combo-4, Combo-5, and Combo-6 in the top cluster. Sogou and Reference-WMT are indistinguishable for this subset and both outperform the two online systems, Online-A-1710 and Online-B-1710</title>
	</analytic>
	<monogr>
		<title level="m">Subset-3 Table 5e shows the results for our evaluation on Subset-3</title>
		<imprint/>
	</monogr>
	<note>We collected at least n ≥ 610 assessments per system</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combo-5, and Combo-6 are indistinguishable from Reference-HT and Reference-PE. There is no significant difference in quality between these five systems. Sogou and Reference-WMT outperform the online systems Online-A-1710 and Online-B-1710</title>
	</analytic>
	<monogr>
		<title level="m">Subset-4 Table 5f shows the results for our evaluation on Subset-4. Again, our research systems Combo-4</title>
		<imprint/>
	</monogr>
	<note>We collected at least n ≥ 649 assessments per system</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">Reference-WMT test data This is publicly available from the WMT17 website 16 . In this work, we used the source newstest2017-zhen-src.zh and the reference (as Reference-WMT) newstest2017-zhen-ref</title>
		<imprint/>
	</monogr>
	<note>en</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sogou translation This is publicly available from the WMT17 website as well 17 . We used newstest2017.SogouKnowing-nmt</title>
		<imprint>
			<biblScope unit="volume">5171</biblScope>
		</imprint>
	</monogr>
	<note>zh-en (as Sogou</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Appraise repository on GitHub 18 contains code to recompute result clusters. We share this data in the hope that the research community might find it useful and also to ensure greatest possible transparency regarding the generation of the results</title>
		<imprint/>
	</monogr>
	<note>presented in this paper</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain adaptation via pseudo in-domain data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<idno>abs/1711.02173</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Findings of the 2017 conference on machine translation (wmt17)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="169" to="214" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overview of the iwslt 2017 evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoshino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Spoken Language Translation (IWSLT)</title>
		<meeting>the 14th International Workshop on Spoken Language Translation (IWSLT)<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch tuning strategies for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
	<note>NAACL HLT &apos;12</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th</title>
		<meeting>the 49th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation</title>
		<meeting>the EMNLP 2011 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sharp models on dull hardware: Fast and accurate neural machine translation decoding on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="2810" to="2815" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">HyTER: Meaning-Equivalent Semantics for Translation Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Validity of Randomization Tests for One-subject Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Edgington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="235" to="251" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Appraise: An open-source toolkit for manual evaluation of machine translation output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving attention modeling with implicit distortion and fertility for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3082" to="3092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Can machine translation systems be evaluated by the crowd alone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324915000339</idno>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="30" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Universal neural machine translation for extremely low resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Synthetic data for neural machine translation of spoken-dialects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elaraby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Tawfik</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04558</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conditional image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep dual learning for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2737" to="2745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">On a test of whether one of two random variables is stochastically larger than the other. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Whitney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1947" />
			<biblScope unit="page" from="50" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Intelligent selection of language model training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
	<note>ACLShort &apos;10</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning residual images for face attribute manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1225" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V D</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page">484489</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of association for machine translation in the Americas</title>
		<meeting>association for machine translation in the Americas</meeting>
		<imprint>
			<date type="published" when="0200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Question answering and question generation as dual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02027</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.04811</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Dynamic data selection for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Wees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<idno>abs/1708.00712</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Error analysis of statistical machine translation output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Language Resources and Evaluation</title>
		<meeting>the Fifth International Conference on Language Resources and Evaluation<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05-22" />
			<biblScope unit="page" from="697" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sogou neural machine translation systems for WMT17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-07" />
			<biblScope unit="page" from="410" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Sogou neural machine translation systems for wmt17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="410" to="415" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dual transfer learning for neural machine translation with marginal distribution regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Individual Comparisons by Ranking Methods. Biometrics Bulletin</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dual inference for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3112" to="3118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dual supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="page" from="3789" to="3798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1782" to="1792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Toward human parity in conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2410" to="2423" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dualgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Joint training for neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Transfer learning for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02201</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
