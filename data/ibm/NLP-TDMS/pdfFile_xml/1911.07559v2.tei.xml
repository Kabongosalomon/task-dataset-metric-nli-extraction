<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FFA-Net: Feature Fusion Attention Network for Single Image Dehazing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Wang</surname></persName>
							<email>007wangzhilin@buaa.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanchao</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">Huizhu</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FFA-Net: Feature Fusion Attention Network for Single Image Dehazing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose an end-to-end feature fusion attention network (FFA-Net) to directly restore the haze-free image. The FFA-Net architecture consists of three key components: 1) A novel Feature Attention (FA) module combines Channel Attention with Pixel Attention mechanism, considering that different channel-wise features contain totally different weighted information and haze distribution is uneven on the different image pixels. FA treats different features and pixels unequally, which provides additional flexibility in dealing with different types of information, expanding the representational ability of CNNs. 2) A basic block structure consists of Local Residual Learning and Feature Attention, Local Residual Learning allowing the less important information such as thin haze region or low-frequency to be bypassed through multiple local residual connections, let main network architecture focus on more effective information. 3) An Attentionbased different levels Feature Fusion (FFA) structure, the feature weights are adaptively learned from the Feature Attention (FA) module, giving more weight to important features. This structure can also retain the information of shallow layers and pass it into deep layers. The experimental results demonstrate that our proposed FFA-Net surpasses previous state-of-the-art single image dehazing methods by a very large margin both quantitatively and qualitatively, boosting the best published PSNR metric from 30.23 dB to 36.39 dB on the SOTS indoor test dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Single image dehazing as a fundamental low-level vision task, has attracted increasing attention in the computer vision community and artificial intelligence companies over the past few decades.</p><p>Due to the existence of smoke, dust, fumes, mist and other floating particles in the atmosphere, images taken in such atmosphere are often subject to color distortion, blurring, low contrast and other visible quality degradation, and the hazy image input will make it difficult to solve the other visual tasks such as classification, tracking, person re-identification and object detection. In view of this, image dehazing aims to recover the clean image from the corrupted input, this will be the preprocessing step of the high-level vision tasks. The atmosphere scattering model <ref type="bibr" target="#b2">(Cartney 1976</ref>)(Narasimhan and Nayar 2000)(Narasimhan and Nayar 2002) provides a simple approximation of the haze effect, it is formulated as:</p><formula xml:id="formula_0">I(z) = J (z)t(z) + A(1 − t(z))<label>(1)</label></formula><p>Where I(z) is the observed hazy image, A is the global atmosphere light, and t(z) is the medium transmission map, J (z) is the haze-free image. Moreover, we have t(z) = e −βd(z) with β and d(z) being the atmosphere scattering parameter and the scene depth, respectively. The atmosphere scattering model shows that image dehazing is an underdetermined problem without the knowledge of A and t(z). Formulation (1) can also be formulated as:</p><formula xml:id="formula_1">J (z) = (I(z) − A) t(z) + A<label>(2)</label></formula><p>From the formulation 1 and 2, we can notice that if we estimate the global atmosphere and transmission map properly for a captured hazy image, we can restore a clear haze-free image.</p><p>Based on the atmosphere scattering model, early dehazing methods did a series of works <ref type="bibr" target="#b0">(Berman, Avidan, and others 2016)</ref> <ref type="bibr" target="#b3">(Fattal 2014)</ref> <ref type="bibr" target="#b5">(He, Sun, and Tang 2010)</ref> <ref type="bibr" target="#b6">(Jiang et al. 2017</ref>) <ref type="bibr" target="#b7">(Ju, Gu, and Zhang 2017)</ref> <ref type="bibr" target="#b11">(Meng et al. 2013</ref>) <ref type="bibr" target="#b18">(Zhu, Mai, and Shao 2015)</ref>. DCP is one of the outstanding prior-based methods, they propose the dark channel prior based on the assumption that image patches of outdoor haze-free images often have low-intensity values in at least one channel. However, the prior-based methods may lead to an inaccu-rate estimation of transmission map because of the prior may be easily violated in practice, so the prior-based meth-ods may not work well in certain real cases.</p><p>With the rising-up of deep learning, many neural network approaches have also been proposed to estimate the haze effect, including the pioneering work of DehazeNet <ref type="bibr" target="#b1">(Cai et al. 2016)</ref>, the multi-scale CNN(MSCNN) , the residual learning technique <ref type="bibr" target="#b3">(He et al. 2016)</ref>, the quadtree CNN <ref type="bibr" target="#b8">(Kim, Ha, and Kwon 2018)</ref>, and the densely connected pyramid dehazing network <ref type="bibr" target="#b16">(Zhang and Patel 2018)</ref>. Compared to traditional methods, deep learning methods try to directly regress the intermediate transmission map or the final haze-free image. With the big data being applied, they achieve superior performance with robustness.</p><p>In this paper, we propose a novel end-to-end feature fusion network(denoted as FFA-Net) for single image dehazing.</p><p>Previous CNN-based image dehazing networks treat the channel-wise and pixel-wise feature equally, but haze is unevenly distributed across an image, the weight of the very thin haze should be significantly different from that of the thick haze region pixels. Furthermore, DCP also finds that it is very common that some pixels have very low intensity in at least one color(RGB) channel, this further illustrates that different channel features have totally different weighted information. If we treat it equally, it will spend plenty of resources on unnecessary computations for less important information, the network will lack the ability to cover all the pixels and channels. Finally, it will greatly limit the representation of the network.</p><p>Since the attention mechanism(Xu et al. 2015)(Vaswani et al. 2017)(Wang et al. 2018) has been widely used in the design of neural networks, it has played an important role in the performance of networks. Inspired by the work , we further design a novel feature attention (FA) module. The FA module combines the channel attention and pixel attention in channel-wise and pixel-wise features, respectively. FA treats different features and pixels unequally, which can provide additional flexibility in dealing with different types of information.</p><p>The emergence of ResNet <ref type="bibr" target="#b3">(He et al. 2016</ref>) has made it possible to train a very deep network. We adopt the idea of skip connection and the attention mechanism and design a basic block consisting of multiple local residual learning skip connections and feature attention. For one thing, the local residual learning allows the information of the thin haze region and low-frequency information to be bypassed through multiple local residual learning, making the main network learn more useful information. And channel attention further improves the capability of FFA-Net.</p><p>As the network goes deeper and deeper, shallow feature information is often difficult to preserve. In order to identify and fuse different level features, U-Net <ref type="bibr" target="#b14">(Ronneberger, Fischer, and Brox 2015)</ref> and other networks strive to integrate the shallow and deep information. Similarly, we propose an attention-based feature fusion structure (FFA), this structure can retain shallow information and pass it into deep layers. Most importantly, the FFA-Net gives different weights to different level features before feeding all features to feature fusion module, the weight is obtained by adaptive learning of the FA module. It is much better than those that directly specified the weight.</p><p>To evaluate the performance of different image dehazing networks, peak-signal-to-noise-ratio (PSNR) and structure similarity index (SSIM) are commonly used to quantify dehazed image restoration quality. For human subjective assessment, we also provide plenty of network outputs from corrupted inputs. We validate the effectiveness of the FFA-Net on the widely used dehazing benchmark dataset RESIDE . Compared the PSNR and SSIM metrics with previous state-of-the-art methods. Experiments demonstrate that FFA-Net surpasses all the previous methods both qualitatively and quantitatively by a very large margin. Moreover, we conduct many ablation experiments to prove that our key components of FFA-Net have an excellent performance.</p><p>Overall, our contributions are four-folds as below: • We propose a novel end-to-end feature fusion attention network FFA-Net for single image dehazing. FFA-Net surpasses previous state-of-the-art image dehazing methods by a very large margin, the FFA-Net performs especially outstanding in region with thick haze and rich texture details. We also have a powerful advantage in the restoration of image detail and color fidelity, as seen in <ref type="figure" target="#fig_0">Fig.1</ref> and <ref type="figure" target="#fig_7">Fig.8</ref>. • We propose a novel feature attention (FA) module, which combines the channel attention and pixel attention mechanism. This module provides additional flexibility in dealing with different types of information, focusing more attention on the thick haze pixels and more important channel information. • We propose a basic block consisting of local residual learning and feature attention (FA), local residual learning allows the information of the thin haze region and low-frequency information to be bypassed through multiple skip connections, feature attention (FA) further improves the capacity of FFA-Net. • We propose an attention-based feature fusion (FFA) structure, this structure can retain shallow layers' information and pass it into deep layers. Besides, it can not only fuse all features but also adaptively learn the different weights of different level feature information. Finally, it achieves a much better performance than other feature fusion methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Previously, most of the existing image dehazing methods depend on the formulation of physical scattering model equa-tion1, which is a highly ill-posed problem because of the unknown transmission map and global atmospheric light. These methods can be roughly divided into two classes: traditional prior-based methods and modern learning-based methods. No matter which method is used, the key is to solve the transmission map and the atmosphere light. For traditional methods, based on the different image statistics prior, they leverage it as extra constraints to compensate for the information loss during the corruption procedure. DCP <ref type="bibr" target="#b5">(He, Sun, and Tang 2010)</ref> proposed a dark channel prior for the estimation of the transmission map. However, the priors are found to be unreliable when the scene objects are similar to the atmospheric light. <ref type="bibr" target="#b18">(Zhu, Mai, and Shao 2015)</ref> propose a simple but powerful color attenuation prior by creating a linear model for modeling the scene depth of the hazy image. (Fattal 2008) present a new method for estimating the optical transmission in hazy scenes, the scattered light is eliminated to increase scene visibility and recover haze-free scene contrasts., (Berman, Avidan, and others 2016) proposed a non-local prior to characterize the clean image, the algorithm relies on the assumption that colors of a haze-free image are well approximated by a few hundred distinct colors, which forms tight clusters in RGB space. Although these methods have made a series of success, the prior is not robust to handle all the cases, such as the unconstraint environment in the wild.</p><p>In view of the prevailing success of deep learning in image processing tasks and the availability of large image datasets, <ref type="bibr" target="#b1">(Cai et al. 2016)</ref> proposed an end-to-end dehazing model based on convolution neural network DehazeNet, it takes a hazy image as input, and outputs its medium transmission map, which is subsequently used to recover a hazefree image via atmospheric scattering model. ) employed a Multi-Scale MSCNN that is able to perform a refined transmission map from the hazy image. <ref type="bibr" target="#b15">(Yang and Sun 2018)</ref> combines the advantages of traditional prior-based dehazing methods and deep learning methods by incorporating haze-related prior learning into deep network.. <ref type="bibr" target="#b8">(Li et al. 2017</ref>) AOD-Net directly generates a clean image through a light-weight CNN. Such a novel end-to-end design makes it easy to embed AOD-Net into other deep models. The gated fusion network(GFN) ) leverages hand-selected pre processing methods and multi-scale estimation, which are generic in nature and are subject to improvement.  proposed an end-to-end gated context aggregation network to directly restore the final haze-free image, which adopted the latest smoothed dilation technique to help remove the gridding artifacts caused by the widely used dilated convolution with negligible extra parameters. <ref type="bibr">EPDN (Qu et al. 2019</ref>) is embedded by a generative adversarial network, which is followed by a welldesigned enhancer without relying on the physical scattering model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion Feature Attention Network (FFA-Net)</head><p>In this section, we mainly introduce our feature fusion attention network FFA-Net. As shown in <ref type="figure" target="#fig_1">Fig.2</ref>, the input of FFA-Net is a hazy image, it is passed into a shallow feature extraction part, then is fed into N Group Architectures with multiple skip connections, the output features of N Group Architectures are fused together through our proposed Feature Attention module, after that, the features will be finally passed to the reconstruction part and global residual learning structure, thereby getting a haze-free output.</p><p>Furthermore, every Group Architecture combines B Basic Blocks Architecture with local residual learning, every Basic Block combines the skip connection and Feature Attention (FA) module. FA is an attention mechanism structure consisting of Channel-wise Attention and Pixel-wise Attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Attention (FA)</head><p>Most image dehazing networks treat the channel-wise and pixel-wise features equally, which can not handle the image with uneven haze distribution and weighted channel-wise FA treats different features and pixels region unequally, which can provide additional flexibility in dealing with different types of information, and can expand the representational ability of CNNs. The crucial step is how to generate different weights for each channel-wise and pixel-wise feature. Our solution is below.</p><p>Channel Attention (CA) Our channel attention mainly concerns that different channel features have totally different weighted information with regards to DCP <ref type="bibr" target="#b5">(He, Sun, and Tang 2010)</ref>. Firstly, we take the channel-wise global spatial information into a channel descriptor by using global average pooling.</p><formula xml:id="formula_2">g c = H p (F c ) = 1 H × W H i=1 W j=1 X c (i, j)<label>(3)</label></formula><p>Where X c (i, j) stands for the value of c-th channel X c at position(i, j), H p is the global pooling function. The shape of the feature map changes from C × H × W to C × 1 × 1.</p><p>To get the weights of the different channels, features pass through two convolution layers and sigmoid, ReLu activation function latter.</p><formula xml:id="formula_3">CA c = σ(Conv(δ(Conv(g c ))))<label>(4)</label></formula><p>Where the σ is the sigmoid function, δ is the ReLu function.</p><p>Finally, we element-wise multiply the input F c and the weights of the channel CA c .</p><formula xml:id="formula_4">F * c = CA c ⊗ F c<label>(5)</label></formula><p>Pixel Attention (PA) Considering that the haze distribution is uneven on the different image pixels, we propose a pixel attention (PA) module to make the network pay more attention to informative features, such as thick-hazed pixels and high-frequency image region. Similar to CA, We directly feed the input F * (the output of the CA) into two convolution layers with ReLu and sigmoid activation function. The shape changes from</p><formula xml:id="formula_5">C × H × W to 1 × H × W . P A = σ(Conv(δ(Conv(F * )))) (6)</formula><p>Finally we utilize element-wise multiplication for input F * and P A, F is the output of the Future Attention (FA) module. F = F * ⊗ P A (7)</p><p>To visually illustrate the effectiveness of the feature attention (FA) mechanism, we print the channel-wise and pixelwise feature weights map of the Group Structure output. We can clearly see that different feature maps are adaptively learned with different weights. <ref type="figure">Fig.4</ref> shows that thick hazy image pixel region and the edges, textures of objects having a larger weight. Pixel Attention (PA) mechanism makes the FFA-Net focus more attention on high frequency and thickhazed pixels region. <ref type="figure" target="#fig_3">Fig.5</ref> shows a 3 × 64 sized graph, and three rows correspond to the feature map weights of the three Group Architectures output in the channel direction, illustration shows that different features adaptively learn completely different weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic Block Structure</head><p>As is shown in <ref type="figure" target="#fig_4">Fig.6</ref>, a basic block structure consists of local residual learning and feature attention (FA) module, local residual learning allows the less important information such as thin haze or low-frequency region to be bypassed through multiple local residual connection, and main network focus on effective information.</p><p>Experiment results show that its structure can further improve network performance and training stabilization, the effect of local residual learning can be seen in <ref type="figure" target="#fig_5">Fig.7</ref>, specific details can be seen in the ablation study section </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group Architecture and Global Residual Learning</head><p>Our Group Architecture combines B Basic Block structures with skip connections module. Continuous B blocks increase the depth and expressiveness of the FFA-Net. And skip connections make FFA-Net get around training difficulty. At the end of the FFA-Net, we add a recovery part using a two-layer convolutional network implementation and a long shortcut global residual learning module. Finally, we restore our desired haze-free image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Fusion Attention</head><p>As discussed above, firstly we concatenate all feature maps output by G Group Architectures in the channel direction. Furthermore, We fuse features by multiplying the adaptive learning weights which are obtained by Feature Attention (FA) mechanism. From this, we can retain the low-level information and pass it into deep layers, we let FFA-Net pay more attention to effective information such as thick haze region, high-frequency texture and color fidelity because of the weight mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>Mean squared error (MSE) or L2 loss is the most widely used loss function for single image dehazing. However <ref type="bibr" target="#b10">(Lim et al. 2017)</ref> pointed out that many image restoration tasks training with L1 loss achieved a better performance than L2 loss in terms of PSNR and SSIM metrics. Following the same strategy, we adopt the simple L1 loss by default. Although many dehazing algorithms also use the perceptual loss and GAN loss, we choose to optimize the L1 loss.</p><formula xml:id="formula_6">L(Θ) = 1 N N i=1 I i gt − F F A(I i haze )<label>(8)</label></formula><p>Where Θ denotes the parameters of FFA-Net, I gt stands for ground truth, and I haze stands for input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>In this section, we specify the implementation details of our proposed FFA-Net. The number of Group Structure G is 3.</p><p>In each Group Structure, we set the Basic Block Structure number as B = 19. Except for the Channel Attention whose kernel size is 1 × 1, we set all convolution layers filter size is 3 × 3. All feature maps keep size fixed except for Channel Attention module. Every Group Structure outputs 64 filters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Datasets and Metrics   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Settings</head><p>We train the FFA-Net in RGB channels and augment the training dataset with randomly rotated by 90,180,270 degrees and horizontal flip. The 2 hazy-image patches with the size 240 × 240 are extracted as FFA-Nets input. The whole network is trained for 5 × 10 5 , 1 × 10 6 steps on indoor and outdoor images respectively. We use Adam optimizer, where β1 and β2 take the default values of 0.9 and 0.999, respectively. The initial learning rate is set to 1×10 −4 , we adopt the cosine annealing strategy  to adjust the learning rate from the initial value to 0 by following the cosine function. Assume the total number of batches is T ,η is the initial earning rate, then at batch t, the learning rate η t is computed as:</p><formula xml:id="formula_7">η t = 1 2 (1 + cos( tπ T ))η<label>(9)</label></formula><p>PytTorch <ref type="bibr" target="#b12">(Paszke et al. 2017</ref>) was used to implement our models with a RTX 2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on RESIDE Dataset</head><p>In this section, we will compare FFA-Net with previous state-of-the-art image dehazing algorithms both quantitatively and qualitatively.</p><p>We compared with four different state-of-the-art dehazing algorithms which are the DCP, AOD-Net, DehazeNet, GCANet.The comparison results are shown in <ref type="table">Table 1</ref> .  For convenience, the metrics are cited from  and <ref type="bibr" target="#b12">(Qu et al. 2019</ref> art methods by a very large margin in terms of PSNR and SSIM. Moreover, we give the comparison of the visual effect in <ref type="figure" target="#fig_7">Fig.8</ref> for qualitative comparisons. From the indoor and outdoor results, three upper rows are indoor results, and outdoor results are shown in the bottom three rows. we can observe that DCP suffers from severe color distortion because of their underlying prior assumptions, consequently, it loses the details in the depth of image. AOD-Net can not remove the haze completely and tends to output low-brightness images. In contrast, Dehazenet recovers images with excessive brightness relative to ground truth. The processing power of GCANet at high-frequency detail information performance such as textures, edges and the blue sky in row 5 is always unsatisfactory.</p><p>For real hazy image results, our network can magically discover the towers that are looming in the depths of the image in row 1. More importantly, the results of our network are almost entirely in line with real scene information, such as the wet road with texture and raindrops in row 2. However, it is found that there are nonexistent spots on the building surface of the GCANet result in row 2. The images recovered from other networks are not satisfactory. Our network is clearly superior in the realistic performance of image details and color fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Analysis</head><p>To further demonstrate the superiority of FFA-Net architecture, we conduct an ablation study by considering the different modules of our proposed FFA-Net. We mainly concern these factors: 1) The FA (Feature Attention) module. 2) The combination of local residual learning (LRL) and FA. 3) The Feature Fusion Attention (FFA) structure. We crop the image to 48 × 48 as input with training of 3 × 10 5 steps, other configuration is the same as our implementation details. The results are shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>And If we fully use the implementation details in our paper, then PSNR will achieve 35.77db. The results show that every factor we consider plays an important role in the network performance, especially the FFA structure. We can also clearly see that even if we only use the FA structure, our network can be very competitive compared with previous FA LRL FFA PSNR 30.28db 31.16db 32.44db state-of-the-art methods. LRL makes network training stable while improving the network performance. The combination of the FA mechanism and feature fusion (FFA) has brought our results to a very high level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose an end-to-end Feature Fusion Attention Network and demonstrated its strong power in single image dehazing. Although Our FFA-Net structure is simple, it is better than the previous state-of-the-art methods with a very large margin. Our network has a powerful advantage in the restoration of image detail and color fidelity, it is expected to solve other low-level vision tasks such as deraining, super-resolution, denoising. FFA and other effective modules in our FFA-Net play an important role in the image restoration algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of image dehazing. (a) Input hazy image. (b) Ground Truth. (c) Result of GCANet. (d) Our result. Our FFA-Net outperforms GCANet in image detail and color fidelity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The feature fusion attention network (FFA-Net) architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Feature Attention module feature properly. Our Feature Attention (see Fig.3) consists of channel attention and pixel attention, which can provide additional flexibility in dealing with different types of information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Channel Attention weight map</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Basic Block Structure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>The effect of local residual learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Indoor and outdoor results (b) Real hazy image results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative comparisons on SOTS and Realistic Hazy Images testset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). It can be seen that our proposed FFA-Net outperforms all four different state-of-the-</figDesc><table><row><cell>Methods</cell><cell>Indoor</cell><cell>Outdoor</cell><cell></cell></row><row><cell></cell><cell cols="3">PSNR SSIM PSNR SSIM</cell></row><row><cell>DCP</cell><cell cols="3">16.62 0.8179 19.13 0.8148</cell></row><row><cell>AOD-Net</cell><cell cols="3">19.06 0.8504 20.29 0.8765</cell></row><row><cell cols="4">DehazeNet 21.14 0.8472 22.46 0.8514</cell></row><row><cell>GFN</cell><cell cols="3">22.30 0.8800 21.55 0.8444</cell></row><row><cell>GCANet</cell><cell>30.23 0.9800</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="3">36.39 0.9886 33.57 0.9840</cell></row><row><cell cols="4">Table 1: Quantitative comparisons on SOTS for dif-</cell></row><row><cell>ferent methods.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons on SOTS indoor testset for different configurations.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avidan</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1674" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Optics of the atmosphere: scattering by molecules and particles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Cartney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<publisher>John Wiley and Sons, Inc</publisher>
			<biblScope unit="volume">421</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gated context aggregation network for image dehazing and deraining</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Tang ; He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image dehazing using adaptive bi-channel priors on superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="17" to="32" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single image haze removal based on the improved atmospheric scattering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Zhang ; Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="page" from="180" to="191" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive patch based convolutional neural network for robust dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ha</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4770" to="4778" />
		</imprint>
	</monogr>
	<note>25th IEEE International Conference on Image Processing</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Benchmarking single-image dehazing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="492" to="505" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="233" to="254" />
		</imprint>
	</monogr>
	<note>Vision and the atmosphere</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Paszke</surname></persName>
		</author>
		<idno>Qu et al. 2019</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="8160" to="8168" />
		</imprint>
	</monogr>
	<note>Enhanced pix2pix dehazing network</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single image dehazing via multiscale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
	<note>Gated fusion network for single image dehazing</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fischer</forename><surname>Brox ; Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>; I-I</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ieee ; Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>International conference on machine learning</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Proximal dehaze-net: a prior learning-based deep network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="702" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A fast single image haze removal algorithm using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><surname>Shao ; Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3522" to="3533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
