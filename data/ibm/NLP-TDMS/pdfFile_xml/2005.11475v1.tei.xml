<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-guided Context Feature Pyramid Network for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxu</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichao</forename><surname>Shi</surname></persName>
						</author>
						<title level="a" type="main">Attention-guided Context Feature Pyramid Network for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Receptive fields</term>
					<term>object detection</term>
					<term>instance seg- mentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For object detection, how to address the contradictory requirement between feature map resolution and receptive field on high-resolution inputs still remains an open question. In this paper, to tackle this issue, we build a novel architecture, called Attention-guided Context Feature Pyramid Network (AC-FPN), that exploits discriminative information from various large receptive fields via integrating attention-guided multi-path features. The model contains two modules. The first one is Context Extraction Module (CEM) that explores large contextual information from multiple receptive fields. As redundant contextual relations may mislead localization and recognition, we also design the second module named Attention-guided Module (AM), which can adaptively capture the salient dependencies over objects by using the attention mechanism. AM consists of two sub-modules, i.e., Context Attention Module (CxAM) and Content Attention Module (CnAM), which focus on capturing discriminative semantics and locating precise positions, respectively. Most importantly, our AC-FPN can be readily plugged into existing FPN-based models. Extensive experiments on object detection and instance segmentation show that existing models with our proposed CEM and AM significantly surpass their counterparts without them, and our model successfully obtains state-of-the-art results. We have released the source code at: https://github.com/Caojunxu/AC-FPN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>O BJECT detection is a fundamental but non-trivial problem in computer vision ( <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). The study of this task can be applied to various applications, such as face detection <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, people counting <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, pedestrian detection <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, object tracking <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, etc. However, how to perform the task effectively still remains an open question.</p><p>Nowadays, to accurately locate objects, representative object detectors, e.g., Faster R-CNN <ref type="bibr" target="#b13">[14]</ref>, RetinaNet <ref type="bibr" target="#b14">[15]</ref>, and DetNet <ref type="bibr" target="#b15">[16]</ref>, use high-resolution images (with the shorter edge being 800) as inputs, which contain much more detailed information and improve the performance in object detection (See AP in <ref type="table" target="#tab_0">Table I</ref>). However, unfortunately, images with higher resolution require neurons to have larger receptive fields to obtain effective semantics ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). Otherwise, it will deteriorate the performance when capturing large objects in the higher resolution images. (See AP L in <ref type="table" target="#tab_0">Table I)</ref>.</p><p>Intuitively, to obtain a larger receptive field, we can design a deeper network model by increasing the convolutional and * Authors contributed equally.</p><p>Junxu Cao, Jun Guo, and Ruichao Shi are with the Tencent. E-mail: {qibaicao, garryshi}@tencent.com, artanis.protoss@outlook.com Qi Chen is with the School of Software Engineering, South China University of Technology. E-mail: sechenqi@mail.scut.edu.cn downsampling layers, where the downsampling layer includes the pooling layer and the convolutional layer with a stride larger than 1. However, simply increasing the number of convolutional layers is rather inefficient. It leads to much more parameters and thereby causes higher computational and memory costs. What's worse, aggressively deep networks are hard to optimize due to the overfitting problem <ref type="bibr" target="#b16">[17]</ref>. On the other hand, the increased number of downsampling layers results in reduced feature map sizes, which causes more challenging issues in localization. Thus, how to build a model that can achieve large receptive fields while maintaining highresolution feature maps remains a key issue in object detection. Recently, FPN <ref type="bibr" target="#b17">[18]</ref> is proposed to exploit the inherent multiscale feature representation of deep convolutional networks. More specifically, by introducing a top-down pathway, FPN combines low-resolution, large-receptive-field features with high-resolution, small-receptive-field features to detect objects at different scales, and thus alleviates the aforementioned contradictory requirement between the feature map resolution and receptive fields. To further increase feature map resolution while keeping the receptive field, DetNet <ref type="bibr" target="#b15">[16]</ref> employs dilated convolutions and adds an extra stage. Until now, FPN-based approaches (e.g., FPN and DetNet) have reached the stateof-the-art performance in object detection. Nevertheless, the receptive field of these models is still much smaller than their input size.</p><p>In addition, due to the limitation of the network architecture, FPN-based approaches cannot make good use of the receptive fields of different sizes. Specifically, the bottom-up pathway simply stacks layers to enlarge the receptive field without arXiv:2005.11475v1 [cs.CV] 23 May 2020 encouraging information propagation, and the feature maps corresponding to different receptive fields are just merged by element-wise addition in the top-down pathway. Therefore, semantic information captured by different receptive fields does not well in communicating with each other, leading to the limited performance. In short, there exist two main problems in current FPNbased approaches: 1) the contradictory requirement between feature map resolution and receptive field on high-resolution inputs, and 2) the lack of effective communication among multi-size receptive fields. To effectively tackle these two problems, we propose a module, called Context Extraction Module (CEM). Without significantly increasing the computational overhead, CEM can capture rich context information from different large receptive fields by using multi-path dilated convolutional layers with different dilation rates ( <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>). Furthermore, to merge multi-receptive-field information elaborately, we introduce dense connections between the layers with different receptive fields in CEM.</p><p>Nevertheless, although the feature from CEM contains rich context information and substantially helps to detect objects of different scales, we found that it is somewhat miscellaneous and thereby might confuse the localization and recognition tasks. Thus, as shown in <ref type="figure" target="#fig_0">Fig. 1 (d)</ref>, to reduce the misleading of redundant context and further enhance the discriminative ability of feature, we design another module named Attention-guided Module (AM), which introduces a self-attention mechanism to capture effective contextual dependencies. Specifically, it consists of two parts: 1) Context Attention Module (CxAM) which aims at capturing semantic relationship between any two positions of the feature maps, and 2) Content Attention Module (CnAM) which focuses on discovering spatial dependencies.</p><p>In this paper, we name our whole model, which consists of CEM and AM, as Attention-guided Context Feature Pyramid Network (AC-FPN). Our proposed AC-FPN can readily be plugged into existing FPN-based model and be easily trained end-to-end without additional supervision.</p><p>We compare our AC-FPN with several state-of-the-art baseline methods on the COCO dataset. Extensive experiments demonstrate that, without any bells and whistles, our model achieves the best performance. Embedding the baselines with our modules (CEM and AM) significantly improves the performance on object detection. Furthermore, we also validate the proposed method on the more challenging instance segmentation task, and the experimental results show that the models integrated with CEM and AM substantially outperform the counterparts (i.e., without them). The source code will be made publicly available.</p><p>We highlight our principal contributions as follows:</p><p>• To address the contradictory requirement between feature map resolution and receptive fields in high-resolution images, we design a module named CEM to leverage features from multiple large contexts. • In addition to producing more salient context information and further enhance the discriminative ability of feature representations, we introduce an attention-guided module named AM. • Our two modules (CEM and AM, named together as AC-FPN) can readily be plugged into existing FPNbased models, e.g., PANet <ref type="bibr" target="#b18">[19]</ref>, and significantly boost the performance in both object detection and instance segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Object Detection</head><p>The frameworks of object detection in deep learning can be mainly divided into two categories: 1) two-stage detectors and 2) one-stage detectors. The two-stage detectors generate thousands of region proposals and then classifies each proposal into different object categories. For example, R-CNN <ref type="bibr" target="#b19">[20]</ref> generates 2, 000 candidate proposals by Selective Search <ref type="bibr" target="#b20">[21]</ref> while filtering out the majority of negative positions in the first stage, and classifies the previous proposals into different object categories in the second stage. Afterward, Ross Girshick proposes Fast R-CNN <ref type="bibr" target="#b21">[22]</ref>, which shares the convolution operations and thus enables end-to-end training of classifiers and regressors. Moreover, for Faster R-CNN <ref type="bibr" target="#b13">[14]</ref>, Region Proposal Networks (RPN) integrates proposal generation with the classifier into a single convolutional network. Numerous extensions of this framework have been proposed, such as R-FCN <ref type="bibr" target="#b22">[23]</ref>, FPN <ref type="bibr" target="#b17">[18]</ref>, Mask R-CNN <ref type="bibr" target="#b23">[24]</ref>, Cascade R-CNN <ref type="bibr" target="#b24">[25]</ref>, CBNet <ref type="bibr" target="#b25">[26]</ref> and DetNet <ref type="bibr" target="#b15">[16]</ref>.</p><p>The other regards object detection as a regression or classification problem, adopting a unified network to achieve final results (locations and categories) directly. OverFeat <ref type="bibr" target="#b26">[27]</ref> is one of the first models with the one-stage framework on deep networks. Afterward, Redmon et al. propose YOLO <ref type="bibr" target="#b27">[28]</ref> to make use of the whole topmost feature map to predict both classification confidences and bounding boxes. In addition, Liu et al. devise SSD <ref type="bibr" target="#b28">[29]</ref> to handle objects of various sizes using multi-scale bounding boxes on multiple feature maps. Besides, there are extensive other one-stage models enhancing the detection process in the prediction objectives or the network architectures, such as YOLOv2 <ref type="bibr" target="#b29">[30]</ref>, DSSD <ref type="bibr" target="#b30">[31]</ref> and DSOD <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Context Information</head><p>Context information can facilitate the performance of localizing the region proposals and thereby improve the final results of detection and classification. According to that, Bell et al. present Inside-Outside Net (ION) <ref type="bibr" target="#b32">[33]</ref> that exploits information both inside and outside the regions of interest. Chen et al. propose a context refinement algorithm <ref type="bibr" target="#b33">[34]</ref>, which explores rich contextual information to better refine each region proposals. Besides, Hu et al. design a relation model <ref type="bibr" target="#b34">[35]</ref>  that focuses on the interactions between each object that can be regarded as a kind of contextual cues. Unlike the plain architecture in <ref type="bibr" target="#b34">[35]</ref>, the models in <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b4">[5]</ref> consider the relations with a sequential fashion. Moreover, for reasoning the obstructed objects, Chen et al. present a framework <ref type="bibr" target="#b37">[38]</ref> to exploit the relational and contextual information by knowledge graphs.</p><formula xml:id="formula_0">P 5 P 4 P 3 P 2 F 5 F 4 F 3 F 2 Conv (a) FPN (b) AC-FPN ⊕ ⨀ ⨀ Conv ⊕ Rate=3 Rate=12 Rate=24 Rate=6 Rate=18 Concat Upsampling P 5 P 4 P 3 P 2 F 5 F 4 F 3 F 2 Conv</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention Modules</head><p>Attention modules can model long-range dependencies and become the workhorse of many challenging tasks, including image classification <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, semantic and instance segmentation <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, image captioning <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, natural language processing <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, etc. For object detection, Li et al. propose a MAD unit <ref type="bibr" target="#b49">[50]</ref> to aggressively search for neuron activations among feature maps from both low-level and high-level streams. Likewise, to improve the detection performance, Zhu et al. design an Attention CoupleNet <ref type="bibr" target="#b50">[51]</ref> that incorporates the attention-related information with global and local properties of the objects. Moreover, Pirinen et al. present a drl-RPN <ref type="bibr" target="#b51">[52]</ref> that replaces the typical RoI selection process of RPN <ref type="bibr" target="#b13">[14]</ref> with a sequential attention mechanism, which is optimized via deep reinforcement learning (RL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>Recently, the hierarchical detection approaches like FPN <ref type="bibr" target="#b17">[18]</ref> and DetNet <ref type="bibr" target="#b15">[16]</ref> have achieved promising perfor-mance. However, for larger input images, these models have to stack more convolutional layers to ensure the appropriateness of receptive fields. Otherwise, they will be in a dilemma between feature map resolution and receptive fields. Besides, for these models, the representation ability of generated features are limited due to the lack of effective communication between receptive fields with different sizes.</p><p>To alleviate these limitations, we propose a novel Attentionguided Context Feature Pyramid Network (AC-FCN) that captures context information from receptive fields with different sizes and produces the objective features with stronger discriminative ability. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, built upon the basic FPN architecture <ref type="bibr" target="#b17">[18]</ref>, our proposed model has two novel components: 1) Context Extraction Module (CEM) that exploits rich context information from receptive fields with various sizes; 2) Attention-guided Module (AM) that enhances salient context dependencies. We will depict each part of our model in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Context Extraction Module</head><p>To integrate the contextual information from different receptive fields, we build the Context Extraction Module (CEM), which only contains several additional layers. To be specific, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, for the bottom-up pathway, we denote the output of the convolutional layer in each scale as</p><formula xml:id="formula_1">{F 2 , F 3 , F 4 , F 5 } according to the settings in [18]. Likewise, Conv 1×1 Conv 1×1 (C, H, W) Q K (N, C′) (C′, N) Reshape&amp;Transpose Reshape (N, H, W) Reshape Sigmoid &amp; Average pooling (1, H, W) (C, H, W) F Conv 1×1 (C, H, W) V R′ ⨂ ⨀ E R Fig. 3. Architecture of Context Attention Module (CxAM).</formula><p>both the top-down pathway and lateral connections follow the official settings in the original paper <ref type="bibr" target="#b17">[18]</ref>.</p><p>After obtaining the feature maps from preceding layers (i.e., F 5 ), to exploit rich contextual information, we feed it into our CEM, which consists of multi-path dilated convolutional layers <ref type="bibr" target="#b52">[53]</ref> with different rates, e.g., rate = 3, 6, 12. These separated convolutional layers can harvest multiple feature maps in various receptive fields. Besides, to enhance the capacity of modeling geometric transformations, we introduce deformable convolutional layers <ref type="bibr" target="#b53">[54]</ref> in each path. It ensures our CEM can learn transformation-invariant features from the given data.</p><p>In addition, to merge multi-scale information elaborately, we employ dense connections in our CEM, where the output of each dilated layer is concatenated with the input feature maps and then fed into the next dilated layer. DenseNet <ref type="bibr" target="#b54">[55]</ref> employs the dense connection to tackle the issues of vanishing gradients and strengthens feature propagation when the CNN model is increasingly deep. By contrast, we use the dense fashion to achieve better scale diversities of the features with various receptive fields. Finally, in order to maintain the coarse-grained information of the initial inputs, we concatenate the outputs of the dilated layers with the up-sampled inputs and feed them into a 1 × 1 convolutional layer to fuse the coarse-and-finegrained features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention-guided Module</head><p>Although the features from CEM contain rich receptive field information, not all of them are useful to facilitate the performance of object detection. The accuracy may reduce since the bounding boxes or region proposals are mislead by redundant information. Thus, to remove the negative impacts of the redundancy and further enhance the representation ability of feature maps, we propose an Attention-guided Module (AM), which is able to capture salient dependencies with strong semantics and precise locations. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the attention module consists of two parts: 1) Context Attention Module (CxAM) and 2) Content Attention Module (CnAM).</p><p>More specifically, CxAM focuses on the semantics between subregions of given feature maps (i.e., the features from CEM). However, due to the effects of the deformable convolution, the location of each object has been destroyed dramatically. To alleviate this issue, we introduce CnAM, which pays more attention to ensure the spatial information but sacrifices some semantics due to the attention from the shallower layer (i.e., F 5 ). Finally, the features refined by CxAM and CnAM are merged with the input features to obtain more comprehensive representations.</p><p>1) Context Attention Modules: To actively capture the semantic dependencies between subregions, we introduce a Context Attention Module (CxAM) based on the self-attention mechanism. Unlike <ref type="bibr" target="#b55">[56]</ref>, we feed the preceding features, which are produced by CEM and contain multi-scale receptive field information, into CxAM module. Based on these informative features, CxAM adaptively pays more attention to the relations between subregions which are more relevant. Thus, the output features from CxAM will have clear semantics and contain contextual dependencies within surrounding objects.</p><p>As can be seen in <ref type="figure">Fig. 3</ref>, given discriminative feature maps F ∈ R C×H×W , we transform them into a latent space by using the convolutional layers W q and W k , respectively. The converted feature maps are calculated by</p><formula xml:id="formula_2">Q = W q F and K = W k F,<label>(1)</label></formula><p>where {Q, K} ∈ R C ×H×W . Then, we reshape Q and K to R C ×N , where N = H × W . To capture the relationship between each subregion, we calculate a correlation matrix as</p><formula xml:id="formula_3">R = Q K,<label>(2)</label></formula><p>where R ∈ R N ×N and then be reshaped to R ∈ R N ×H×W . After normalizing R via sigmoid activation function and average pooling, we build an attention matrix R , where R ∈ R 1×H×W . Meanwhile, we transform the feature map F to another representation V by using the convolutional layer W v :</p><formula xml:id="formula_4">V = W v F,<label>(3)</label></formula><p>where V ∈ R C×H×W . Finally, an element-wise multiplication is performed on R and the feature V to get the attentional representation E. We formulate the function as</p><formula xml:id="formula_5">E i = R V i ,<label>(4)</label></formula><p>where E i refers to the i th feature map along with the channel dimension C.</p><p>2) Content Attention Module: Due to the effects of deformable convolutions in CEM, the geometric properties of the given images have been destroyed drastically, leading to the location offsets. To solve this problem, we design a new attention module, called Content Attention Module (CnAM), to maintain precise position information of each object.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, similar to CxAM, we use convolutional layers to transform the given feature maps. However, instead of using the feature maps F to produce the attention matrix, we adopt the feature maps F 5 ∈ R C ×H×W , which can capture the more precise location of each object.</p><p>To get the attention matrix, at first we apply two convolutional layers W p and W z , to convert F 5 into the latent space, respectively:</p><formula xml:id="formula_6">P = W p F 5 and Z = W z F 5 ,<label>(5)</label></formula><p>where {P, Z} ∈ R C ×H×W . Then, we reshape the dimension of P and Z to R C ×N , and produce the correlation matrix similar to Eq. (2) as:</p><formula xml:id="formula_7">S = P Z,<label>(6)</label></formula><p>where S ∈ R N ×N . After reshaping S to R N ×H×W , we employ sigmoid function and average pooling to produce an attention matrix S ∈ R 1×H×W . To obtain a prominent representation, we combine the extracted features V (see Section III-B1) with S by element-wise multiplication:</p><formula xml:id="formula_8">D i = S V i ,<label>(7)</label></formula><p>where D ∈ R C×H×W and D i indicates the i th output feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS ON OBJECT DETECTION</head><p>In this section, we evaluate the performance of our AC-FPN compared to the baseline methods including Cascade R-CNN <ref type="bibr" target="#b24">[25]</ref>, Faster R-CNN with FPN <ref type="bibr" target="#b17">[18]</ref>, PANet <ref type="bibr" target="#b18">[19]</ref> and DetNet <ref type="bibr" target="#b15">[16]</ref>. Following the settings in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b18">[19]</ref>, we train our model on MS-COCO 2017 <ref type="bibr" target="#b56">[57]</ref>, which consists of 115k training images and 5k validation images (minival). We also report the final results on a set of 20k test images (testdev). For quantitative comparison, we use the COCO-style Average Precision (AP) and PASCAL-style AP (i.e. averaged over IoU thresholds). Following the definitions in <ref type="bibr" target="#b56">[57]</ref>, on COCO, we denote the objects with small, medium and large sizes as AP S , AP M and AP L , respectively. For PASCAL-style AP, we use AP 50 and AP 75 , which are defined at a single IoU of 0.5 and 0.75.</p><p>A. Implementation Details.</p><p>Following the settings in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b18">[19]</ref>, we resize the input images with the shorter side of 800 pixels and initialize the feature extractor with a model pretrained on ImageNet. Specifically, we train our model with learning rate 0.02 for 60k iterations and reduce to 0.002 for another 20k iterations. For a fair comparison, we train our model without any data augmentations except the horizontal image flipping.</p><p>For our AC-FPN, different from the original FPN, we use dilated convolution on F 5 and subsample P 5 via max pooling to keep the same stride with FPN. More specifically, in CEM, we first reduce F 5 to 512 channels as input, followed by several 3 × 3 deformable convolutional layers with different dilated rates, e.g., 3, 6, 9. Then we reduce the output to 256 channels for reusing the top-down structure of FPN. More details are shown in Tab. II. In AM, as shown in Figs. 3 and 4, we use 1 × 1 convolution to reduce the input to 256 channels in CnAM and 128 channels in CxAM, respectively. For compared methods, we use the reimplementations and settings in Detectron <ref type="bibr" target="#b57">[58]</ref>.</p><p>B. Comparisons with State-of-the-arts 1) Quantitative Evaluation: Tab. III shows the detection performance of state-of-the-art methods on COCO testdev. Compared to FPN (ResNet-50) and FPN (DetNet-59), our AC-FPN (ResNet-50) consistently achieves the best performance. To be specific, compared to FPN (ResNet-50), the promotions of AP S , AP M and AP L are 2.4, 3.7 and 4.1, respectively. Likewise, for FPN (DetNet-59), we also obtain the biggest improvement in AP L , which demonstrates that our model is able to capture much more effective information from large receptive fields.</p><p>On the other hand, compared to FPN (ResNet-101), although it contains more layers and establishes a deeper network, the performance even can not surpass our AC-FPN's built upon ResNet-50. Hence, we can conjecture that even if both FPN (ResNet-101) and AC-FPN (ResNet-50) have large receptive fields, our improvements are still relatively large since the proposed method extracts much stronger semantics and context information.</p><p>2) Adaptation to Existing Methods: To investigate the effects of our proposed modules, we embed our CEM and AM into some existing models. For a fair comparison, we use the same hyper-parameters for both baseline models and ours. Besides, we train two models for each baseline with different backbones, i.e., ResNet-50 and ResNet-101. Tab. IV shows that the models with CEM and AM consistently outperform their counterparts without them. And we argue that the reasons lie in that our modules can capture much richer context information from various receptive fields.</p><p>3) Qualitative Evaluation: Moreover, we show the visual results of FPN with or without our CEM and AM. For a fair comparison, we build the models upon ResNet-50 and test on COCO minival. Besides, for convenient, we compare the detection performance of the same images with threshold = 0.7. From <ref type="figure" target="#fig_3">Fig. 5 (a)</ref>, (b) and (c), the typical FPN model misses some large objects since these objects may be out of the receptive fields and FPN can not capture. By contrast, with our modules, the integrated FPN overcomes this limitation by accessing richer receptive fields. As shown in <ref type="figure" target="#fig_3">Figs. 5 (d)</ref> and (e), our models also perform much better on the ambiguous objects by exploring the context information from various receptive fields.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments of Context Extraction Module 1) Effectiveness of CEM:</head><p>To discuss the effects of CEM, we plug CEM into the existing models and compare the performance with that without it. As shown in Tab. V, compared to the baselines, the models with CEM achieves more compelling performance. It demonstrates that our proposed CEM is able to facilitate the object detection task by capturing richer context information from receptive fields with different sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Impacts of Deformable Convolutions:</head><p>To evaluate the impact of deformable convolutions in our framework, we conduct an ablation study, where we compare the results of object detection using our proposed CEM module with and without deformable convolutions. Note that in order to avoid interference from other modules, we remove the proposed AM module from the whole framework. From Tab. VI, the results show that with deformable convolution, the AP L improves while the AP S goes down slightly. The reason may lie on that the deformable convolutions destroy the location information when seeking more powerful contextual information, especially for small objects, which are more sensitive to the locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Effects of Dense Connections:</head><p>To investigate the impact of dense connections in CEM, we report the detection results on COCO minival with or without the dense connections. Similarly, to reduce the influence of other factors, we remove the proposed AM module and deformable convolutions from the whole framework. As shown in Tab. VII, when using dense connection method, the performance increases consistently in all metrics, which demonstrates that the dense connections are effective and an elaborate fusion method has a positive impact in the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments of Attention-guided Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Effectiveness of CxAM and CnAM:</head><p>We conduct an ablation study to investigate the effects of our CxAM and CnAM. For FPN, we introduce our modules gradually and test the AP values of each combined models. As shown in Tab. VIII, the model incorporated with both CxAM and CnAM achieves the best performance in all metrics. Moreover, if we only embed with one of them, the results also increase to some extent. These results demonstrate that our modules improve the performance consistently for the objects of all sizes by capturing much richer multi-scale information.   2) Visualization of Attention: To further verify the effects of our attention modules, we pose the visual results of attention maps in <ref type="figure" target="#fig_4">Fig. 6</ref>. Compared to ground-truth bounding boxes ( <ref type="figure" target="#fig_4">Fig. 6 (b)</ref>), when only employing the CEM module ( <ref type="figure" target="#fig_4">Fig. 6  (c)</ref>), the results contain some redundant region proposals owing to the miscellaneous context information captured by CEM. After incorporating CnAM, as shown in <ref type="figure" target="#fig_4">Fig. 6 (d)</ref>, the attention model locates the object much more accurately. Nonetheless, the attention map also captures some unnecessary regions since the context information is insufficient to distinguish them. To handle this issue, we integrate the CxAM module, which contains stronger semantics, to further filter the needless dependencies and then obtain a clearer attention map ( <ref type="figure" target="#fig_4">Fig. 6 (e)</ref>). Therefore, in contrast to <ref type="figure" target="#fig_4">Fig. 6</ref> (c), our model ( <ref type="figure" target="#fig_4">Fig. 6 (f)</ref>) gets rid of the redundant regions and then achieves better performance.</p><p>3) Impact of Pooling Method: Moreover, we also investigate the impact of pooling method in our Attention-guided Module (AM). From Tab. IX, the results show that compared with max pooling, average pooling obtains a similar but slightly better performance with both ResNet-50 and ResNet-101 backbones.  E. More Discussions 1) Complexity Analysis: To analyze the model complexity, we compare the number of model parameters and floatingpoint operations per second (FLOPS). First, we compare the number of parameters with or without the proposed modules (i.e., CEM and AM). Tab. X shows that AC-FCN improves the performance in a large margin while only introducing a few extra parameters. Besides, our proposed AC-FPN (ResNet-50), which contains fewer parameters than FPN (ResNet-101), can also obtain better performance. The results demonstrate that the improvement brought by our methods mainly comes from the elaborate design rather than the additional parameters.</p><p>Furthermore, to evaluate the efficiency of our modules, we report the FLOPS of FPN and AC-FPN in Tab. XI. Specifically, for feature maps C 5 , we set dilation rate = 1 and stride = 32 in both FPN and AC-FPN. The results show that our modules improve performance significantly while increasing the computational cost slightly. Thus, we can draw a conclusion that the improvement mostly comes from our well-designed modules (i.e., CEM and AM) instead of the additional computations.</p><p>In addition, we evaluate the training time of all the models using an NVIDIA Tesla P40 GPU. For Tab. X, our ACFPN achieves more promising performance in both ResNet-50 and ResNet-101, and only requires 1.18 seconds and 1.56 seconds for each iteration, respectively. Moreover, we also investigate the change of training time when increasing the number of paths and dilation rate of ACFPN. As shown in Tab. XII, even with more paths and dilation rates, the training cost grows very slightly.</p><p>2) Influence of Number of Path in CEM: To further evaluate the impacts of multi-scale context information, we adopt a different number of paths and dilation rates for our CEM module. From Tab. XII, the model with too many (e.g., 7) or too few (e.g., 1) paths always achieves sub-optimal results. This situation demonstrates that the features produced by fewer paths cannot sufficiently capture the information from different receptive fields very well. On the contrary, too many paths in CEM will cause the extracted features more complicated, which may contain much redundant information and thereby confuse the detector. Notably, compared to the results of path = 1, using more paths can achieve better performance, e.g., the results of path = 3, 5, 7. Thus, we can draw the conclusion that it is necessary and vital to increase the receptive field for larger input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS ON INSTANCE SEGMENTATION</head><p>To further verify the adaptability of our CEM and AM, we extend them to some instance segmentation models, including Mask R-CNN <ref type="bibr" target="#b23">[24]</ref>, PANet <ref type="bibr" target="#b18">[19]</ref> and Cascade R-CNN <ref type="bibr" target="#b24">[25]</ref>. Specifically, we simply extend the Cascade R-CNN model to instance segmentation by adding the mask branch following <ref type="bibr" target="#b23">[24]</ref>. Moreover, we use the official implementations and evaluate the models with the aforementioned metrics, including AP, AP 50 , AP 75 , AP S , AP M and AP L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Instance Segmentation Results</head><p>We investigate the performance of CEM and AM in terms of instance segmentation by plugging them into existing segmentation networks. Then we test these incremental models on minival and test-dev, and report the results in Tabs. XIII and XIV, respectively. As shown in Tabs. XIII and XIV, the models with the proposed CEM and AM achieve more promising performance compared to the original ones. To be specific, due to the lack of rich context information, Cascade R-CNN gets a limited performance in detection and thereby obtains the inferior results in segmentation. Thus, the rich context information from various receptive fields is critical for both object detection and instance segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object Detection Results</head><p>To further validate the effects of our modules, we report the intermediate results of instance segmentation, i.e., APs of object detection. As shown in Tabs. XIII and XIV, the models integrated with our modules achieve much better performance in all metrics, which demonstrates that our proposed modules capture more effective information and thereby greatly facilitate the bounding box regression and object recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization Results</head><p>We further show the visual results of Mask R-CNN with or without our CEM and AM. For a fair comparison, we build the models upon ResNet-50 and test on COCO minival. The results with threshold = 0.7 are exhibited in <ref type="figure" target="#fig_5">Fig. 7. Fig. 7</ref> (a) and (b) show that the model with CEM and AM is able to capture the large objects, which are always neglected by the original Mask R-CNN. Moreover, as shown in <ref type="figure" target="#fig_5">Fig. 7</ref> (c) and (d), although Mask R-CNN successfully detects the large objects, the bounding boxes are always imprecise due to the limitation of receptive fields. Furthermore, from <ref type="figure" target="#fig_5">Fig. 7 (e)</ref>, based on the more discriminative features, Mask R-CNN with our modules distinguishes the instances accurately while the counterpart fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we build a novel architecture, named AC-FPN, containing two main sub-modules (CEM and AM), to solve the contradictory requirements between feature map resolution and receptive fields in high-resolution images, and enhances the discriminative ability of feature representations. Moreover, our proposed modules (i.e., CEM and AM) can be readily plugged into the existing object detection and segmentation networks, and be easily trained end-to-end. The extensive experiments on object detection and instance segmentation tasks demonstrate the superiority and adaptability of our CEM and AM modules.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Detected objects. (b) The receptive fields of the same model on images of different sizes. (c) Captured context information from various receptive fields. (d) Identified salient relations. The dashed lines indicate the context dependencies over images and the line weight refers to the degree of correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The overall architecture of our proposed modules. Based on the structure of FPN, Context Extraction Module (CEM) is trained to capture the rich context information for various receptive fields and then produces an integrated representation. The Context Attention Module (CxAM) and Content Attention Module (CnAM) are devised to identify the salient dependencies among the extracted context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Architecture of Content Attention Module (CnAM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of object detection. Both models are built upon ResNet-50 on COCO minival.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Discussion for the impacts of our CxAM and CnAM modules via visualizing attention map on COCO minival.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Results of Mask R-CNN with (w) and without (w/o) our modules built upon ResNet-50 on COCO minival.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DETECTION</head><label>I</label><figDesc>RESULTS USING RESNET-101 FPN [18] WITH DIFFERENT INPUT IMAGE SIZES ON COCO .</figDesc><table><row><cell>Image size</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>600 × 1000</cell><cell>37.9</cell><cell>59.5</cell><cell>41.2</cell><cell>19.8</cell><cell>41.3</cell><cell>51.6</cell></row><row><cell>800 × 1333</cell><cell>39.4</cell><cell>61.2</cell><cell>43.4</cell><cell>22.5</cell><cell>42.9</cell><cell>51.3</cell></row><row><cell>1000 × 1433</cell><cell>39.5</cell><cell>61.6</cell><cell>43.0</cell><cell>24.0</cell><cell>43.0</cell><cell>49.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DETAILED</head><label>II</label><figDesc>MODEL DESIGN OF THE PROPOSED CEM.</figDesc><table><row><cell>Module</cell><cell>Module details</cell><cell>Input shape</cell><cell>Output shape</cell></row><row><cell>CEM 3 1x1</cell><cell>Conv</cell><cell>(2048, w, h)</cell><cell>(512, w, h)</cell></row><row><cell>CEM 3 3x3</cell><cell>DeformConv(dilate=3)</cell><cell>(512, w, h)</cell><cell>(256, w, h)</cell></row><row><cell>CEM concat 1</cell><cell>Concatenation(C5, CEM 3 3x3)</cell><cell>(2048, w, h)⊕(256, w, h)</cell><cell>(2304, w, h)</cell></row><row><cell>CEM 6 1x1</cell><cell>Conv</cell><cell>(2304, w, h)</cell><cell>(512, w, h)</cell></row><row><cell>CEM 6 3x3</cell><cell>DeformConv(dilate=6)</cell><cell>(512, w, h)</cell><cell>(256, w, h)</cell></row><row><cell>CEM concat 2</cell><cell>Concatenation(CEM concat 1, CEM 6 3x3)</cell><cell>(2304, w, h)⊕(256, w, h)</cell><cell>(2560, w, h)</cell></row><row><cell>CEM 12 1x1</cell><cell>Conv</cell><cell>(2560, w, h)</cell><cell>(512, w, h)</cell></row><row><cell>CEM 12 3x3</cell><cell>DeformConv(dilate=12)</cell><cell>(512, w, h)</cell><cell>(256, w, h)</cell></row><row><cell>CEM concat 3</cell><cell>Concatenation(CEM concat 2, CEM 12 3x3)</cell><cell>(2560, w, h)⊕(256, w, h)</cell><cell>(2816, w, h)</cell></row><row><cell>CEM 18 1x1</cell><cell>Conv</cell><cell>(2816, w, h)</cell><cell>(512, w, h)</cell></row><row><cell>CEM 18 3x3</cell><cell>DeformConv(dilate=18)</cell><cell>(512, w, h)</cell><cell>(256, w, h)</cell></row><row><cell>CEM concat 4</cell><cell>Concatenation(CEM concat 3, CEM 18 3x3)</cell><cell>(2816, w, h)⊕(256, w, h)</cell><cell>(3072, w, h)</cell></row><row><cell>CEM 24 1x1</cell><cell>Conv</cell><cell>(3072, w, h)</cell><cell>(512, w, h)</cell></row><row><cell>CEM 24 3x3</cell><cell>DeformConv(dilate=24)</cell><cell>(512, w, h)</cell><cell>(256, w, h)</cell></row><row><cell>CEM global context</cell><cell>Global Averge Pooling</cell><cell>(2048, w, h)</cell><cell>(2048, 1, 1)</cell></row><row><cell>CEM gc reduce 1x1</cell><cell>Conv</cell><cell>(2048, 1, 1)</cell><cell>(256, 1, 1)</cell></row><row><cell>CEM gc upsample</cell><cell>Bilinear Interpolation</cell><cell>(256, 1, 1)</cell><cell>(256, w, h)</cell></row><row><cell>CEM concat 5</cell><cell>Concatenation(CEM 3 3x3, CEM 6 3x3, CEM 12 3x3, CEM 18 3x3, CEM 24 3x3, CEM gc upsample)</cell><cell>(256, w, h)⊕(256, w, h)⊕(256, w, h)⊕ (256, w, h)⊕(256, w, h)⊕(256, w, h)</cell><cell>(1536, w, h)</cell></row><row><cell>CEM reduce 1x1</cell><cell>Conv</cell><cell>(1536, w, h)</cell><cell>(256, w, h)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III THE</head><label>III</label><figDesc>STATE-OF-THE-ART DETECTORS ON COCO -. THE ENTRIES DENOTED BY "*" USE THE IMPLEMENTATIONS OF DETECTRON[58]. "AC-CASCADE" MEANS CASCADE R-CNN EMBEDDED WITH AC-FPN.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>FPN* [18]</cell><cell>ResNet-50</cell><cell>37.2</cell><cell>59.3</cell><cell>40.2</cell><cell>20.9</cell><cell>39.4</cell><cell>46.9</cell></row><row><cell>FPN</cell><cell>DetNet-59 [16]</cell><cell>40.3</cell><cell>62.1</cell><cell>43.8</cell><cell>23.6</cell><cell>42.6</cell><cell>50.0</cell></row><row><cell>FPN* [18]</cell><cell>ResNet-101</cell><cell>39.4</cell><cell>61.5</cell><cell>42.8</cell><cell>22.7</cell><cell>42.1</cell><cell>49.9</cell></row><row><cell>DRFCN [54]</cell><cell>ResNet-101</cell><cell>37.1</cell><cell>58.9</cell><cell>39.8</cell><cell>17.1</cell><cell>40.3</cell><cell>51.3</cell></row><row><cell>Mask R-CNN* [24]</cell><cell>ResNet-101</cell><cell>40.2</cell><cell>62.0</cell><cell>43.9</cell><cell>22.8</cell><cell>43.0</cell><cell>51.1</cell></row><row><cell>Cascade R-CNN* [25]</cell><cell>ResNet-101</cell><cell>42.9</cell><cell>61.5</cell><cell>46.6</cell><cell>23.7</cell><cell>45.3</cell><cell>55.2</cell></row><row><cell>C-Mask R-CNN [34]</cell><cell>ResNet-101</cell><cell>42.0</cell><cell>62.9</cell><cell>46.4</cell><cell>23.4</cell><cell>44.7</cell><cell>53.8</cell></row><row><cell>AC-FPN*</cell><cell>ResNet-50</cell><cell>40.4</cell><cell>63.0</cell><cell>44.0</cell><cell>23.5</cell><cell>43.0</cell><cell>50.9</cell></row><row><cell>AC-FPN*</cell><cell>ResNet-101</cell><cell>42.4</cell><cell>65.1</cell><cell>46.2</cell><cell>25.0</cell><cell>45.2</cell><cell>53.2</cell></row><row><cell>AC-Cascade*</cell><cell>ResNet-101</cell><cell>45.0</cell><cell>64.4</cell><cell>49.0</cell><cell>26.9</cell><cell>47.7</cell><cell>56.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV DETAILED</head><label>IV</label><figDesc>COMPARISONS ON MULTIPLE POPULAR BASELINE OBJECT DETECTORS ON THE COCO DATASET. WE PLUG OUR MODULES (CEM AND AM) INTO THE EXISTING METHODS AND TEST THEIR RESULTS BASED ON BOTH RESNET-50 AND RESNET-101.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>+ Our modules</cell><cell>AP</cell><cell>AP 50</cell><cell cols="2">minival AP 75 AP S</cell><cell>AP M</cell><cell>AP L</cell><cell>AP</cell><cell>AP 50</cell><cell cols="2">test-dev AP 75 AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>FPN [18]</cell><cell>ResNet-50 ResNet-101</cell><cell>√ √</cell><cell>36.7 40.1 39.4 42.0</cell><cell>58.4 62.5 61.2 64.7</cell><cell>39.6 43.2 43.4 45.6</cell><cell>21.1 23.9 22.6 25.1</cell><cell>39.8 43.6 42.9 45.7</cell><cell>48.1 52.4 51.4 53.4</cell><cell>37.2 40.4 39.4 42.4</cell><cell>59.3 63.0 61.5 65.1</cell><cell>40.2 44.0 42.8 46.2</cell><cell>20.9 23.5 22.7 25.0</cell><cell>39.4 43.0 42.1 45.2</cell><cell>46.9 50.9 49.9 53.2</cell></row><row><cell>PANet [19]</cell><cell>ResNet-50 ResNet-101</cell><cell>√ √</cell><cell>38.7 40.8 40.5 42.7</cell><cell>60.4 62.4 62.0 64.4</cell><cell>41.7 44.3 43.8 46.5</cell><cell>22.6 24.1 23.0 25.5</cell><cell>42.4 44.7 44.8 46.7</cell><cell>50.3 53.0 53.2 54.9</cell><cell>39.0 40.9 40.8 43.0</cell><cell>60.8 62.8 62.7 65.1</cell><cell>42.1 44.3 44.2 46.8</cell><cell>22.2 23.6 23.2 25.6</cell><cell>41.7 43.6 43.9 46.1</cell><cell>48.7 51.6 51.7 53.6</cell></row><row><cell>Cascade R-CNN [25]</cell><cell>ResNet-50 ResNet-101</cell><cell>√ √</cell><cell>40.9 43.0 42.8 44.9</cell><cell>59.0 62.4 61.4 64.3</cell><cell>44.6 46.7 46.8 48.7</cell><cell>22.5 25.3 24.1 27.5</cell><cell>43.6 46.4 45.8 48.7</cell><cell>55.3 56.4 57.4 57.8</cell><cell>41.1 43.3 42.9 45.0</cell><cell>59.6 62.7 61.5 64.4</cell><cell>44.6 47.4 46.6 49.0</cell><cell>22.8 25.0 23.7 26.9</cell><cell>43.0 45.7 45.3 47.7</cell><cell>53.2 55.1 55.2 56.6</cell></row><row><cell>FPN w/o</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CEM and AM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FPN with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CEM and AM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a)</cell><cell>(b)</cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell><cell>(d)</cell><cell></cell><cell></cell><cell></cell><cell>(e)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V EFFECTS</head><label>V</label><figDesc>OF OUR CEM WITH RESNET-50 ON COCO .</figDesc><table><row><cell>Methods</cell><cell>+ CEM</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>FPN</cell><cell>√</cell><cell>36.7 39.3</cell><cell>58.4 61.9</cell><cell>39.6 42.6</cell><cell>21.1 23.6</cell><cell>39.8 42.9</cell><cell>48.1 50.4</cell></row><row><cell>PANet</cell><cell>√</cell><cell>38.7 39.9</cell><cell>60.4 62.3</cell><cell>41.7 43.3</cell><cell>22.6 23.6</cell><cell>42.4 43.6</cell><cell>50.3 52.0</cell></row><row><cell>Cascade R-CNN</cell><cell>√</cell><cell>40.9 42.1</cell><cell>59.0 61.1</cell><cell>44.6 45.8</cell><cell>22.5 22.7</cell><cell>43.6 45.8</cell><cell>55.3 57.1</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">RESULTS OF DETECTION USING RESNET-50 BACKBONE WITH AND W/O</cell></row><row><cell cols="5">DEFORMABLE CONVOLUTIONS ON COCO</cell><cell></cell><cell>.</cell><cell></cell></row><row><cell cols="2">Deformable Rest of CEM</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>√</cell><cell>√ √</cell><cell>36.7 38.8 39.2</cell><cell>58.4 61.8 61.9</cell><cell>39.6 41.8 42.5</cell><cell>21.1 23.7 23.5</cell><cell>39.8 42.5 42.9</cell><cell>48.1 49.7 50.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII DETECTION</head><label>VII</label><figDesc></figDesc><table><row><cell></cell><cell cols="9">RESULTS WITH AND W/O DENSE CONNECTIONS OF CEM ON</cell></row><row><cell></cell><cell>COCO</cell><cell></cell><cell cols="5">BASED ON RESNET-50.</cell><cell></cell></row><row><cell cols="2">Dense connections</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell cols="2">AP S</cell><cell cols="2">AP M</cell><cell>AP L</cell></row><row><cell>√</cell><cell></cell><cell>38.3 38.8</cell><cell>61.1 61.8</cell><cell>41.5 41.8</cell><cell cols="2">22.4 23.7</cell><cell></cell><cell>41.7 42.5</cell><cell>49.5 49.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VIII</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">EFFECTS OF OUR ATTENTION MODULES CXAM AND CNAM WITH</cell></row><row><cell></cell><cell cols="4">RESNET-50 ON COCO</cell><cell></cell><cell>.</cell><cell></cell><cell></cell></row><row><cell cols="3">+ CxAM + CnAM</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell cols="2">AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>FPN</cell><cell>√ √</cell><cell>√ √</cell><cell>39.3 39.6 39.8 40.1</cell><cell>61.9 62.0 62.4 62.5</cell><cell>42.6 42.8 43.2 43.2</cell><cell cols="2">23.6 23.5 23.8 23.9</cell><cell>42.9 43.4 43.3 43.6</cell><cell>50.4 50.6 51.6 52.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IX IMPACT</head><label>IX</label><figDesc>OF POOLING METHOD IN ATTENTION-GUIDED MODULE (AM).</figDesc><table><row><cell>Backbone ResNet-50 ResNet-101</cell><cell>Attention-guided Module (AM) max pooling avg pooling √ √ √ √</cell><cell>AP 39.9 40.1 41.6 42.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE X DETECTION</head><label>X</label><figDesc>RESULTS ON COCO -AND THE CORRESPONDING NUMBER OF PARAMETERS AND TRAINING TIME FOR EACH ITERATION.</figDesc><table><row><cell>Backbone</cell><cell>#Params</cell><cell>FPN Time</cell><cell>AP</cell><cell>#Params</cell><cell cols="2">ACFPN Time</cell><cell>AP</cell></row><row><cell>ResNet-50</cell><cell>39.82M</cell><cell>0.92s</cell><cell>37.2</cell><cell>54.58M</cell><cell cols="3">1.18s 40.4</cell></row><row><cell>ResNet-101</cell><cell>57.94M</cell><cell>1.24s</cell><cell>39.4</cell><cell>72.69M</cell><cell cols="3">1.56s 42.4</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE XI</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">FLOPS OF DETECTION RESULTS ON COCO.</cell><cell></cell></row><row><cell></cell><cell>Backbone</cell><cell cols="4">minival test-dev</cell><cell>FLOPS</cell></row><row><cell>FPN ACFPN</cell><cell>resnet50</cell><cell>36.7 39.1</cell><cell></cell><cell>37.2 39.4</cell><cell cols="3">97.63G 102.78G</cell></row><row><cell>FPN ACFPN</cell><cell>resnet101</cell><cell>39.4 41.5</cell><cell></cell><cell>39.4 41.7</cell><cell cols="3">143.73G 148.89G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE XII INFLUENCES</head><label>XII</label><figDesc>OF THE PATH NUMBER AND DILATION RATE IN OUR CEM. WE EVALUATE THEM WITH RESNET-50 ON COCO AND REPORT THE CORRESPONDING TRAINING TIME FOR EACH ITERATION.</figDesc><table><row><cell>Path</cell><cell>Rate</cell><cell>AP</cell><cell>AP50</cell><cell>AP75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell><cell>Time</cell></row><row><cell>1</cell><cell>(1)</cell><cell>38.3</cell><cell>61.1</cell><cell>41.5</cell><cell>22.4</cell><cell>41.7</cell><cell>49.5</cell><cell>0.95s</cell></row><row><cell>3</cell><cell>(3,12,24)</cell><cell>39.6</cell><cell>62.1</cell><cell>43.1</cell><cell>23.5</cell><cell>43.0</cell><cell>51.5</cell><cell>1.03s</cell></row><row><cell>5</cell><cell>(3,6,12,18,24)</cell><cell>40.1</cell><cell>62.5</cell><cell>43.2</cell><cell>23.9</cell><cell>43.6</cell><cell>52.4</cell><cell>1.18s</cell></row><row><cell>7</cell><cell>(3,6,9,12,18,24,32)</cell><cell>39.9</cell><cell>62.1</cell><cell>43.4</cell><cell>23.6</cell><cell>43.1</cell><cell>52.0</cell><cell>1.38s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE XIII OBJECT</head><label>XIII</label><figDesc>DETECTION RESULTS (BOUNDING BOX AP) AND INSTANCE SEGMENTATION MASK AP ON COCO . BOUNDING BOX AP) AND INSTANCE SEGMENTATION MASK AP ON COCO -.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Object Detection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Instance Sementation</cell><cell></cell></row><row><cell>Methods</cell><cell>Backbone</cell><cell>+ Our modules</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>√</cell><cell>37.7 40.7</cell><cell>59.2 62.8</cell><cell>40.9 44.0</cell><cell>21.4 24.2</cell><cell>40.8 44.1</cell><cell>49.7 53.0</cell><cell>33.9 36.0</cell><cell>55.8 59.4</cell><cell>35.8 38.1</cell><cell>14.9 17.0</cell><cell>36.3 39.2</cell><cell>50.9 53.2</cell></row><row><cell>Mask R-CNN</cell><cell>ResNet-101</cell><cell>√</cell><cell>40.0 42.8</cell><cell>61.8 65.2</cell><cell>43.7 47.0</cell><cell>22.6 26.2</cell><cell>43.4 46.6</cell><cell>52.7 54.2</cell><cell>35.9 38.0</cell><cell>58.3 61.9</cell><cell>38.0 40.1</cell><cell>15.9 18.0</cell><cell>38.9 41.5</cell><cell>53.2 54.6</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>√</cell><cell>39.5 41.4</cell><cell>60.4 62.2</cell><cell>42.6 45.2</cell><cell>23.9 24.1</cell><cell>43.3 44.9</cell><cell>49.6 54.3</cell><cell>35.2 36.5</cell><cell>57.4 59.1</cell><cell>37.2 38.6</cell><cell>16.6 17.0</cell><cell>38.6 39.8</cell><cell>50.8 54.0</cell></row><row><cell>PANet</cell><cell>ResNet-101</cell><cell>√</cell><cell>41.5 43.5</cell><cell>62.1 64.9</cell><cell>45.1 47.6</cell><cell>24.1 26.1</cell><cell>45.2 47.6</cell><cell>54.4 55.8</cell><cell>36.7 38.2</cell><cell>59.1 61.6</cell><cell>38.8 40.4</cell><cell>16.5 18.2</cell><cell>40.1 42.2</cell><cell>54.9 56.0</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>√</cell><cell>41.3 43.6</cell><cell>59.6 62.7</cell><cell>44.9 47.8</cell><cell>23.1 25.4</cell><cell>44.2 47.0</cell><cell>55.4 57.5</cell><cell>35.4 37.2</cell><cell>56.2 59.5</cell><cell>37.8 39.7</cell><cell>15.7 17.4</cell><cell>37.6 40.2</cell><cell>53.4 54.7</cell></row><row><cell>Cascade R-CNN</cell><cell>ResNet-101</cell><cell>√</cell><cell>43.3 45.4</cell><cell>61.7 64.5</cell><cell>47.3 49.3</cell><cell>24.2 27.6</cell><cell>46.3 49.1</cell><cell>58.2 58.6</cell><cell>37.1 38.5</cell><cell>58.6 61.2</cell><cell>39.8 41.0</cell><cell>16.7 18.3</cell><cell>39.7 41.8</cell><cell>55.7 56.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE XIV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">OBJECT DETECTION RESULTS (Methods Backbone + Our modules</cell><cell>AP</cell><cell>AP 50</cell><cell cols="2">Object Detection AP 75 AP S</cell><cell>AP M</cell><cell>AP L</cell><cell>AP</cell><cell cols="4">Instance Segmentation AP 50 AP 75 AP S AP M</cell><cell>AP L</cell></row><row><cell>Mask R-CNN</cell><cell>ResNet-50 ResNet-101</cell><cell>√ √</cell><cell>38.0 41.2 40.2 43.1</cell><cell>59.7 63.5 62.0 65.5</cell><cell>41.3 44.9 43.9 47.2</cell><cell>21.2 23.8 22.8 25.5</cell><cell>40.2 43.6 43.0 45.9</cell><cell>48.1 52.2 51.1 54.3</cell><cell>34.2 36.4 35.9 38.3</cell><cell>56.4 59.9 58.5 62.0</cell><cell>36.0 38.6 38.0 40.7</cell><cell>14.8 16.7 15.9 18.1</cell><cell>36.0 38.8 38.2 41.0</cell><cell>49.7 52.5 51.8 54.6</cell></row><row><cell>PANet</cell><cell>ResNet-50 ResNet-101</cell><cell>√ √</cell><cell>40.0 41.8 41.8 43.6</cell><cell>60.7 62.9 62.7 64.7</cell><cell>43.6 45.8 45.7 47.7</cell><cell>22.6 24.0 23.6 25.7</cell><cell>42.7 44.4 44.7 46.8</cell><cell>50.3 52.7 52.7 54.5</cell><cell>35.5 36.9 37.0 38.3</cell><cell>57.6 59.8 59.7 61.7</cell><cell>37.6 39.2 39.3 40.7</cell><cell>15.6 16.8 16.5 18.0</cell><cell>37.9 392 39.7 41.1</cell><cell>51.3 53.3 53.4 54.7</cell></row><row><cell>Cascade R-CNN</cell><cell>ResNet-50 ResNet-101</cell><cell>√ √</cell><cell>41.7 43.8 43.4 45.6</cell><cell>60.0 62.9 61.9 64.9</cell><cell>45.4 47.8 47.2 49.8</cell><cell>23.1 25.2 23.9 27.2</cell><cell>43.6 46.2 45.9 48.6</cell><cell>54.2 55.9 56.2 57.5</cell><cell>35.6 37.4 37.1 38.8</cell><cell>57.0 59.9 58.9 61.7</cell><cell>38.0 40.0 39.7 41.7</cell><cell>15.5 17.2 16.2 18.8</cell><cell>37.1 39.6 39.1 41.4</cell><cell>52.0 53.6 54.1 55.1</cell></row><row><cell>Mask R-CNN w/o</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CEM and AM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask R-CNN w/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CEM and AM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell></cell><cell>(d)</cell><cell></cell><cell>(e)</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face detection with the faster r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="650" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Facenessnet: Face detection through deep facial part responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1845" to="1859" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">People counting in dense crowd images using sparse head detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoona</forename><surname>Shami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Maqbool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasan</forename><surname>Sajid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasar</forename><surname>Ayaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen-Ching Samson</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2325" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pedestrian detection via body part semantic and contextual information with dnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3148" to="3159" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scale-aware fast r-cnn for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengmei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="985" to="996" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Oriented spatial transformer network for pedestrian detection using fisheye camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqiang</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3038" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An incremental framework for video-based traffic sign detection, tracking, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitong</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1918" to="1929" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning linear regression via singleconvolutional layer for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust object tracking using manifold regularized convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="510" to="521" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning attentional recurrent neural network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiurui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="930" to="942" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detnet: Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cbnet: A novel composite backbone network architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03625</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rob Fergus, and Yann LeCun</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dsod: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1919" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2874" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Context refinement for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="71" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatial memory for context reasoning in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4086" to="4096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attentive contexts for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="944" to="954" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Iterative visual reasoning beyond convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7239" to="7248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-end instance segmentation with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6656" to="6664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="375" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Zoom outand-in network with map attention decision for region proposal and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention couplenet: Fully convolutional attention coupling network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="126" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning of region proposal networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksis</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6945" to="6954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02983</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Piotr Dollár, and Kaiming He. Detectron</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
