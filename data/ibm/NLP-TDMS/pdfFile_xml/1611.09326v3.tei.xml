<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jégou</surname></persName>
							<email>simon.jegou@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Montreal Institute for Learning Algorithms 2É cole Polytechnique de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
							<email>michal@imagia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Imagia Inc</orgName>
								<address>
									<settlement>Montréal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
							<email>dvazquez@cvc.uab.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Montreal Institute for Learning Algorithms 2É cole Polytechnique de Montréal</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Computer Vision Center</orgName>
								<address>
									<settlement>Barcelona</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
							<email>adriana.romero.soriano@umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Montreal Institute for Learning Algorithms 2É cole Polytechnique de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Montreal Institute for Learning Algorithms 2É cole Polytechnique de Montréal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions.</p><p>Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train.</p><p>In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is publicly available here : https://github.com/SimJeg/FC-DenseNet</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) are driving major advances in many computer vision tasks, such as image classification <ref type="bibr" target="#b31">[29]</ref>, object detection <ref type="bibr" target="#b27">[25,</ref><ref type="bibr" target="#b25">24]</ref> and semantic image segmentation <ref type="bibr" target="#b21">[20]</ref>. The last few years have witnessed outstanding improvements on CNN-based models. Very deep architectures <ref type="bibr" target="#b31">[29,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">31]</ref> have shown impressive results on standard benchmarks such as ImageNet <ref type="bibr" target="#b5">[6]</ref> or MSCOCO <ref type="bibr" target="#b20">[19]</ref>. State-of-the-art CNNs heavily reduce the input resolution through successive pooling layers and, <ref type="bibr">Figure 1</ref>. Diagram of our architecture for semantic segmentation. Our architecture is built from dense blocks. The diagram is composed of a downsampling path with 2 Transitions Down (TD) and an upsampling path with 2 Transitions Up (TU). A circle represents concatenation and arrows represent connectivity patterns in the network. Gray horizontal arrows represent skip connections, the feature maps from the downsampling path are concatenated with the corresponding feature maps in the upsampling path. Note that the connectivity pattern in the upsampling and the downsampling paths are different. In the downsampling path, the input to a dense block is concatenated with its output, leading to a linear growth of the number of feature maps, whereas in the upsampling path, it is not. thus, are well suited for applications where a single prediction per input image is expected (e.g. image classification task).</p><p>Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b29">27]</ref> were introduced in the literature as a natural extension of CNNs to tackle per pixel prediction problems such as semantic image segmentation. FCNs add upsampling layers to standard CNNs to recover the spatial resolution of the input at the output layer. As a consequence, FCNs can process images of arbitrary size. In order to compensate for the resolution loss induced by pooling layers, FCNs introduce skip connections between their downsampling and upsampling paths. Skip connections help the upsampling path recover fine-grained information from the downsampling layers.</p><p>Among CNN architectures extended as FCNs for semantic segmentation purposes, Residual Networks (ResNets) <ref type="bibr" target="#b10">[11]</ref> make an interesting case. ResNets are designed to ease the training of very deep networks (of hundreds of layers) by introducing a residual block that sums two signals: a non-linear transformation of the input and its identity mapping. The identity mapping is implemented by means of a shortcut connection. ResNets have been extended to work as FCNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref> yielding very good results in different segmentation benchmarks. ResNets incorporate additional paths to FCN (shortcut paths) and, thus, increase the number of connections within a segmentation network. This additional shortcut paths have been shown not only to improve the segmentation accuracy but also to help the network optimization process, resulting in faster convergence of the training <ref type="bibr" target="#b7">[8]</ref>.</p><p>Recently, a new CNN architecture, called DenseNet, was introduced in <ref type="bibr" target="#b12">[13]</ref>. DenseNets are built from dense blocks and pooling operations, where each dense block is an iterative concatenation of previous feature maps. This architecture can be seen as an extension of ResNets <ref type="bibr" target="#b10">[11]</ref>, which performs iterative summation of previous feature maps. However, this small modification has some interesting implications: (1) parameter efficiency, DenseNets are more efficient in the parameter usage; (2) implicit deep supervision, DenseNets perform deep supervision thanks to short paths to all feature maps in the architecture (similar to Deeply Supervised Networks <ref type="bibr" target="#b19">[18]</ref>); and (3) feature reuse, all layers can easily access their preceding layers making it easy to reuse the information from previously computed feature maps. The characteristics of DenseNets make them a very good fit for semantic segmentation as they naturally induce skip connections and multi-scale supervision.</p><p>In this paper, we extend DenseNets to work as FCNs by adding an upsampling path to recover the full input resolution. Naively building an upsampling path would result in a computationally intractable number of feature maps with very high resolution prior to the softmax layer. This is because one would multiply the high resolution feature maps with a large number of input filters (from all the layers below), resulting in both very large amount of computation and number of parameters. In order to mitigate this effect, we only upsample the feature maps created by the preceding dense block. Doing so allows to have a number of dense blocks at each resolution of the upsampling path independent of the number of pooling layers. Moreover, given the network architecture, the upsampled dense block combines the information contained in the other dense blocks of the same resolution. The higher resolution information is passed by means of a standard skip connection between the downsampling and the upsampling paths. The details of the proposed architecture are shown in <ref type="figure">Figure 1</ref>. We evaluate our model on two challenging benchmarks for urban scene understanding, Camvid <ref type="bibr" target="#b1">[2]</ref> and Gatech <ref type="bibr" target="#b23">[22]</ref>, and confirm the potential of DenseNets for semantic segmentation by improving the state-of-the-art.</p><p>Thus, the contributions of the paper can be summarized as follows:</p><p>• We carefully extend the DenseNet architecture <ref type="bibr" target="#b12">[13]</ref> to fully convolutional networks for semantic segmentation, while mitigating the feature map explosion.</p><p>• We highlight that the proposed upsampling path, built from dense blocks, performs better than upsampling path with more standard operations, such as the ones in <ref type="bibr" target="#b29">[27]</ref>.</p><p>• We show that such a network can outperform current state-of-the-art results on standard benchmarks for urban scene understanding without neither using pretrained parameters nor any further post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent advances in semantic segmentation have been devoted to improve architectural designs by (1) improving the upsampling path and increasing the connectivity within FCNs <ref type="bibr" target="#b29">[27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b7">8]</ref>; <ref type="bibr" target="#b1">(2)</ref> introducing modules to account for broader context understanding <ref type="bibr" target="#b39">[36,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">37]</ref>; and/or (3) endowing FCN architectures with the ability to provide structured outputs <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">38]</ref>.</p><p>First, different alternatives have been proposed in the literature to address the resolution recovery in FCN's upsampling path; from simple bilinear interpolation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b0">1]</ref> to more sophisticated operators such as unpooling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">21]</ref> or transposed convolutions <ref type="bibr" target="#b21">[20]</ref>. Skip connections from the downsampling to the upsampling path have also been adopted to allow for a finer information recovery <ref type="bibr" target="#b29">[27]</ref>. More recently, <ref type="bibr" target="#b7">[8]</ref> presented a thorough analysis on the combination of identity mapping <ref type="bibr" target="#b10">[11]</ref> and long skip connections <ref type="bibr" target="#b29">[27]</ref> for semantic segmentation.</p><p>Second, approaches that introduce larger context to semantic segmentation networks include <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">36,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">37]</ref>. In <ref type="figure">Figure 2</ref>. Diagram of a dense block of 4 layers. A first layer is applied to the input to create k feature maps, which are concatenated to the input. A second layer is then applied to create another k features maps, which are again concatenated to the previous feature maps. The operation is repeated 4 times. The output of the block is the concatenation of the outputs of the 4 layers, and thus contains 4 * k feature maps <ref type="bibr" target="#b9">[10]</ref>, an unsupervised global image descriptor is computed added to the feature maps for each pixel. In <ref type="bibr" target="#b39">[36]</ref>, Recurrent Neural Networks (RNNs) are used to retrieve contextual information by sweeping the image horizontally and vertically in both directions. In <ref type="bibr" target="#b4">[5]</ref>, dilated convolutions are introduced as an alternative to late CNN pooling layers to capture larger context without reducing the image resolution. Following the same spirit, <ref type="bibr" target="#b40">[37]</ref> propose to provide FCNs with a context module built as a stack of dilated convolutional layers to enlarge the field of view of the network.</p><p>Third, Conditional Random Fields (CRF) have long been a popular choice to enforce structure consistency to segmentation outputs. More recently, fully connected CRFs <ref type="bibr" target="#b16">[16]</ref> have been used to include structural properties of the output of FCNs <ref type="bibr" target="#b4">[5]</ref>. Interestingly, in <ref type="bibr" target="#b41">[38]</ref>, RNN have been introduced to approximate mean-field iterations of CRF optimization, allowing for an end-to-end training of both the FCN and the RNN.</p><p>Finally, it is worth noting that current state-of-the-art FCN architectures for semantic segmentation often rely on pre-trained models (e.g. VGG <ref type="bibr" target="#b31">[29]</ref> or ResNet101 <ref type="bibr" target="#b10">[11]</ref>) to improve their segmentation results <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fully Convolutional DenseNets</head><p>As mentioned in Section 1, FCNs are built from a downsampling path, an upsampling path and skip connections. Skip connections help the upsampling path recover spatially detailed information from the downsampling path, by reusing features maps. The goal of our model is to further exploit the feature reuse by extending the more sophisticated DenseNet architecture, while avoiding the feature explosion at the upsampling path of the network.</p><p>In this section, we detail the proposed model for semantic segmentation. First, we review the recently proposed DenseNet architecture. Second, we introduce the construction of the novel upsampling path and discuss its advantages w.r.t. a naive DenseNet extension and more classical architectures. Finally, we wrap up with the details of the main architecture used in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Review of DenseNets</head><p>Let x be the output of the th layer. In a standard CNN, x is computed by applying a non-linear transformation H to the output of the previous layer x −1</p><formula xml:id="formula_0">x = H (x −1 ),<label>(1)</label></formula><p>where H is commonly defined as a convolution followed by a rectifier non-linearity (ReLU) and often dropout <ref type="bibr" target="#b32">[30]</ref>.</p><p>In order to ease the training of very deep networks, ResNets <ref type="bibr" target="#b10">[11]</ref> introduce a residual block that sums the identity mapping of the input to the output of a layer. The resulting output x becomes</p><formula xml:id="formula_1">x = H (x −1 ) + x −1 ,<label>(2)</label></formula><p>allowing for the reuse of features and permitting the gradient to flow directly to earlier layers. In this case, H is defined as the repetition (2 or 3 times) of a block composed of Batch Normalization (BN) <ref type="bibr" target="#b13">[14]</ref>, followed by ReLU and a convolution. Pushing this idea further, DenseNets <ref type="bibr" target="#b12">[13]</ref> design a more sophisticated connectivity pattern that iteratively concatenates all feature outputs in a feedforward fashion. Thus, the output of the th layer is defined as</p><formula xml:id="formula_2">x = H ([x −1 , x −2 , ..., x 0 ]),<label>(3)</label></formula><p>where [ ... ] represents the concatenation operation. In this case, H is defined as BN, followed by ReLU, a convolution and dropout. Such connectivity pattern strongly encourages the reuse of features and makes all layers in the architecture receive direct supervision signal. The output dimension of each layer has k feature maps where k, hereafter referred as to growth rate parameter, is typically set to a small value (e.g. k = 12). Thus, the number of feature maps in DenseNets grows linearly with the depth (e.g. after layers, the input [x −1 , x −2 , ..., x 0 ] will have × k feature maps).</p><p>A transition down is introduced to reduce the spatial dimensionality of the feature maps. Such transformation is composed of a 1×1 convolution (which conserves the number of feature maps) followed by a 2 × 2 pooling operation.</p><p>In the remainder of the article, we will call dense block the concatenation of the new feature maps created at a given resolution. <ref type="figure">Figure 2</ref> shows an example of dense block construction. Starting from an input x 0 (input image or output of a transition down) with m feature maps, the first layer of the block generates an output x 1 of dimension k by applying H 1 (x 0 ). These k feature maps are then stacked to the previous m feature maps by concatenation ([x 1 , x 0 ]) and used as input to the second layer. The same operation is repeated n times, leading to a new dense block with n × k feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">From DenseNets to Fully Convolutional DenseNets</head><p>The DenseNet architecture described in Subsection 3.1 constitutes the downsampling path of our Fully Convolutional DenseNet (FC-DenseNet). Note that, in the downsampling path, the linear growth in the number of features is compensated by the reduction in spatial resolution of each feature map after the pooling operation. The last layer of the downsampling path is referred to as bottleneck.</p><p>In order to recover the input spatial resolution, FCNs introduce an upsampling path composed of convolution, upsampling operations (transposed convolutions or unpooling operations) and skip connections. In FC-DenseNets, we substitute the convolution operation by a dense block and an upsampling operation referred to as transition up. Transition up modules consist of a transposed convolution that upsamples the previous feature maps. The upsampled feature maps are then concatenated to the ones coming from the skip connection to form the input of a new dense block. Since the upsampling path increases the feature maps spatial resolution, the linear growth in the number of features would be too memory demanding, especially for the full resolution features in the pre-softmax layer.</p><p>In order to overcome this limitation, the input of a dense block is not concatenated with its output. Thus, the transposed convolution is applied only to the feature maps obtained by the last dense block and not to all feature maps concatenated so far. The last dense block summarizes the information contained in all the previous dense blocks at the same resolution. Note that some information from earlier dense blocks is lost in the transition down due to the pooling operation. Nevertheless, this information is available in the downsampling path of the network and can be passed via skip connections. Hence, the dense blocks of the upsampling path are computed using all the available feature maps at a given resolution. <ref type="figure">Figure 1</ref> illustrates this idea in detail. Therefore, our upsampling path approach allows us to build very deep FC-DenseNets without a feature map explosion. An alternative way of implementing the upsampling path would be to perform consecutive transposed convolutions and complement them with skip connections from the downsampling path in a U-Net <ref type="bibr" target="#b29">[27]</ref> or FCN-like <ref type="bibr" target="#b21">[20]</ref> fashion. This will be further discussed in Section 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semantic Segmentation Architecture</head><p>In this subsection, we detail the main architecture, FC-DenseNet103, used in Section 4.</p><p>First, in <ref type="table" target="#tab_0">Table 1</ref>, we define the dense block layer, transition down and transition up of the architecture. Dense block layers are composed of BN, followed by ReLU, a 3 × 3 same convolution (no resolution loss) and dropout with probability p = 0.2. The growth rate of the layer is set to k = 16. Transition down is composed of BN, followed by ReLU, a 1 × 1 convolution, dropout with p = 0.2 and a non-overlapping max pooling of size 2 × 2. Transition up is composed of a 3 × 3 transposed convolution with stride 2 to compensate for the pooling operation.</p><p>Second, in <ref type="table" target="#tab_1">Table 2</ref>, we summarize all Dense103 layers. This architecture is built from 103 convolutional layers : a first one on the input, 38 in the downsampling path, 15 in the bottleneck and 38 in the upsampling path. We use 5 Transition Down (TD), each one containing one extra convolution, and 5 Transition Up (TU), each one containing a transposed convolution. The final layer in the network is a 1 × 1 convolution followed by a softmax non-linearity to provide the per class distribution at each pixel.</p><p>It is worth noting that, as discussed in Subsection 3.2, the proposed upsampling path properly mitigates the DenseNet feature map explosion, leading to reasonable pre-softmax feature map number of 256. Finally, the model is trained by minimizing the pixelwise cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on two urban scene understanding datasets: CamVid <ref type="bibr" target="#b1">[2]</ref>, and Gatech <ref type="bibr" target="#b23">[22]</ref>. We trained our models from scratch without using any extra-data nor postprocessing module. We report the results using the Intersection over Union (IoU) metric and the global accuracy (pixel-wise accuracy on the dataset). For a given class c, predictions (o i ) and targets (y i ), the IoU is defined by</p><formula xml:id="formula_3">IoU (c) = i (o i == c ∧ y i == c) i (o i == c ∨ y i == c) ,<label>(4)</label></formula><p>where ∧ is a logical and operation, while ∨ is a logical or operation. We compute IoU by summing over all the pixels i of the dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture and training details</head><p>We initialize our models using HeUniform <ref type="bibr" target="#b11">[12]</ref> and train them with RMSprop <ref type="bibr" target="#b35">[33]</ref>, with an initial learning rate of 1e − 3 and an exponential decay of 0.995 after each epoch. All models are trained on data augmented with random crops and vertical flips. For all experiments, we finetune our models with full size images and learning rate of 1e − 4. We use validation set to earlystop the training and the finetuning. We monitor mean IoU or mean accuracy and use patience of 100 (50 during finetuning).</p><p>We regularized our models with a weight decay of 1e−4 and a dropout rate of 0.2. For batch normalization, we use current batch statistics at training, validation and test time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CamVid dataset</head><p>CamVid 1 <ref type="bibr" target="#b1">[2]</ref> is a dataset of fully segmented videos for urban scene understanding. We used the split and image resolution from <ref type="bibr" target="#b0">[1]</ref>, which consists of 367 frames for training, 101 frames for validation and 233 frames for test. Each frame has a size 360 × 480 and its pixels are labeled with 11 semantic classes. Our models were trained with crops of 224 × 224 and batch size 3. At the end, the model is finetuned with full size images.</p><p>In <ref type="table">Table 3</ref>, we report our results for three networks with respectively (1) 56 layers (FC-DenseNet56), with 4 layers per dense block and a growth rate of 12; (2) 67 layers (FC-DenseNet67) with 5 layers per dense block and a growth rate of 16; and (3) 103 layers (FC-DenseNet103) with a growth rate k = 16 (see <ref type="table" target="#tab_1">Table 2</ref> for details). We also trained an architecture using standard convolutions in the upsampling path instead of dense blocks (Classic Upsampling). In the latter architecture, we used 3 convolutions per resolution level with respectively 512, 256, 128, 128 and 64 filters, as in <ref type="bibr" target="#b29">[27]</ref>. Results show clear superiority of the proposed upsampling path w.r.t. the classic one, consistently improving the IoU significantly for all classes. Particularly, we observe that unrepresented classes benefit notably from the FC-DenseNet architecture, namely sign, pedestrian, fence, cyclist experience a crucial boost in performance (ranging from 15% to 25%).</p><p>As expected, when comparing FC-DenseNet56 or FC-DenseNet67 to FC-DenseNet103, we see that the model benefits from having more depth as well as more parameters.</p><p>When compared to other methods, we show that FC-DenseNet architectures achieve state-of-the-art, improving upon models with 10 times more parameters. It is worth mentioning that our small model FC-DenseNet56 already outperforms popular architectures with at least 100 times more parameters.</p><p>It is worth noting that images in CamVid correspond to video frames and, thus, the dataset contains temporal information. Some state-of-the-art methods such as <ref type="bibr" target="#b18">[17]</ref> incorporate long range spatio-temporal regularization to the out-put of a FCN to boost their performance. Our model is able to outperform such state-of-the-art model, without requiring any temporal smoothing. However, any post-processing temporal regularization is complementary to our approach and could bring additional improvements.</p><p>Unlike most of the current state-of-the-art methods, FC-DenseNets have not been pretrained on large datasets such as ImageNet <ref type="bibr" target="#b5">[6]</ref> and could most likely benefit from such pretraining. More recently, it has been shown that deep networks can also boost their performance when pretrained on data other than natural images, such as video games <ref type="bibr" target="#b28">[26,</ref><ref type="bibr" target="#b30">28]</ref> or clipart <ref type="bibr" target="#b2">[3]</ref>, and this an interesting direction to explore. <ref type="figure">Figure 3</ref> shows some qualitative segmentation results on the CamVid dataset. Qualitative results are well aligned with the quantitative ones, showing sharp segmentations that account for a lot of details. For example, trees, column poles, sidewalk and pedestrians appear very well sketched. Among common errors, we find that thin details found in trees can be confused with column poles (see fifth row), buses and trucks can be confused with buildings (fourth row), and shop signs can be confused with road signs (second row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Gatech dataset</head><p>Gatech 2 <ref type="bibr" target="#b24">[23]</ref> is a geometric scene understanding dataset, which consists of 63 videos for training/validation and 38 for testing. Each video has between 50 and 300 frames (with an average of 190). A pixel-wise segmentation map is provided for each frame. There are 8 classes in the dataset: sky, ground, buildings, porous (mainly trees), humans, cars, vertical mix and main mix. The dataset was originally built to learn 3D geometric structure of outdoor video scenes and the standard metric for this dataset is mean global accuracy.</p><p>We used the FC-DenseNet103 model pretrained on CamVid, removed the softmax layer, and finetuned it for 10 epochs with crops of 224 × 224 and batch size 5. Given the high redundancy in Gatech frames, we used only one out of 10 frames to train the model and tested it on all full resolution test set frames.</p><p>In <ref type="table">Table 4</ref>, we report the obtained results. We compare the results to the recently proposed method for video segmentation of <ref type="bibr" target="#b36">[34]</ref>, which reports results of their architecture with 2D and 3D convolutions. Frame-based 2D convolutions do not have temporal information. As it can be seen in <ref type="table">Table 4</ref>, our method gives an impressive improvement of 23.7% in global accuracy with respect to previously published state-of-the-art with 2D convolutions. Moreover, our model (trained with only 2D convolutions) also achieves a significant improvement over state-of-the-art models based on spatio-temporal 3D convolutions (3.4% improvement). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Our fully convolutional DenseNet implicitly inherits the advantages of DenseNets, namely: (1) parameter efficiency, as our network has substantially less parameters than other segmentation architectures published for the Camvid dataset; (2) implicit deep supervision, we tried including additional levels of supervision to different layers of our network without noticeable change in performance; and (3) feature reuse, as all layers can easily access their preceding layers not only due to the iterative concatenation of feature maps in a dense block but also thanks to skip connections that enforce connectivity between downsampling and upsampling path.</p><p>Recent evidence suggest that ResNets behave like ensemble of relatively shallow networks <ref type="bibr" target="#b37">[35]</ref>: "Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks". It would be interesting to revisit this finding in the context of fully convolutional DenseNets. Due to iterative feature map concatenation in the dense block, the gradients are forced to be passed through networks of different depth (with different numbers of nonlinearities). Thus, thanks to the smart connectivity patterns, FC-DenseNets might represent an ensemble of variable depth networks. This particular ensemble behavior would be very interesting for semantic segmentation models, where the ensemble of different paths throughout the model would capture the multi-scale appearance of objects in urban scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have extended DenseNets and made them fully convolutional to tackle the problem semantic image segmentation. The main idea behind DenseNets is captured in dense blocks that perform iterative concatenation of feature maps. We designed an upsampling path mitigating the linear growth of feature maps that would appear in a naive extension of DenseNets.</p><p>The resulting network is very deep (from 56 to 103 layers) and has very few parameters, about 10 fold reduction w.r.t. state-of-the-art models. Moreover, it improves stateof-the-art performance on challenging urban scene understanding datasets (CamVid and Gatech), without neither additional post-processing, pretraining, nor including temporal information.</p><p>Aknowledgements</p><p>The authors would like to thank the developers of Theano <ref type="bibr" target="#b34">[32]</ref> and Lasagne <ref type="bibr" target="#b6">[7]</ref>. Special thanks to Frédéric Bastien for his work assessing the compilation issues. Thanks to Francesco Visin for his well designed dataloader <ref type="bibr" target="#b8">[9]</ref>, as well as Harm de Vries for his support  <ref type="table">Table 3</ref>. Results on CamVid dataset. Note that we trained our own pretrained FCN8 model Model Acc. 2D models (no time) 2D-V2V-from scratch <ref type="bibr" target="#b36">[34]</ref> 55.7 FC-DenseNet103 79.4 3D models (incorporate time) 3D-V2V-from scratch <ref type="bibr" target="#b36">[34]</ref> 66.7 3D-V2V-pretrained <ref type="bibr" target="#b36">[34]</ref> 76.0 <ref type="table">Table 4</ref>. Results on Gatech dataset in network parallelization, and Tristan Sylvain. We acknowledge the support of the following agencies for research funding and computing support: Imagia Inc., Spanish projects TRA2014-57088-C2-1-R &amp; 2014-SGR-1506, TECNIOspring-FP7-ACCI grant.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Architecture Input, m = 3 3 × 3</head><label>33</label><figDesc>Convolution, m = 48 DB (4 layers) + TD, m = 112 DB (5 layers) + TD, m = 192 DB (7 layers) + TD, m = 304 DB (10 layers) + TD, m = 464 DB (12 layers) + TD, m = 656 DB (15 layers), m = 896 TU + DB (12 layers), m = 1088 TU + DB (10 layers), m = 816 TU + DB (7 layers), m = 578 TU + DB (5 layers), m = 384 TU + DB (4 layers), m = 256 1 × 1 Convolution, m = c Softmax</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2 http://www.cc.gatech.edu/cpl/projects/videogeometriccontext/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Building blocks of fully convolutional DenseNets. From left to right: layer used in the model, Transition Down (TD) and Transition Up (TU). See text for details.</figDesc><table><row><cell>Layer</cell><cell>Transition Down (TD)</cell><cell></cell></row><row><cell>Batch Normalization ReLU 3 × 3 Convolution Dropout p = 0.2</cell><cell>2 × 2 Max Pooling Batch Normalization ReLU 1 × 1 Convolution Dropout p = 0.2</cell><cell>Transition Up (TU) 3 × 3 Transposed Convolution stride = 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Architecture details of FC-DenseNet103 model used in our experiments. This model is built from 103 convolutional lay-</figDesc><table /><note>ers. In the Table we use following notations: DB stands for Dense Block, TD stands for Transition Down, TU stands for Transition Up, BN stands for Batch Normalization and m corresponds to the total number of feature maps at the end of a block. c stands for the number of classes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>! 29.5 68.7 52.0 87.0 58.5 13.4 86.2 25.3 17.9 16.0 60.5 24.8 46.4 62.5 ! 134.5 77.8 71.0 88.7 76.1 32.7 91.2 41.7 24.4 19.9 72.7 31.0 57.0 88.0 DeepLab-LFOV [5] ! 37.3 81.5 74.6 89.0 82.2 42.3 92.2 48.4 27.2 14.3 75.4 50.1 61.6 − Dilation8 [37] ! 140.8 82.6 76.2 89.0 84.0 46.9 92.2 56.3 35.8 23.4 75.3 55.5 65.3 79.0 Dilation8 + FSO [17] ! 140.8 84.0 77.2 91.3 85.6 49.9 92.5 59.1 37.6 16.9 76.0 57.2 66.1 88.3</figDesc><table><row><cell>Model</cell><cell>Pretrained</cell><cell># parameters (M)</cell><cell>Building</cell><cell>Tree</cell><cell>Sky</cell><cell>Car</cell><cell>Sign</cell><cell>Road</cell><cell>Pedestrian</cell><cell>Fence</cell><cell>Pole</cell><cell>Sidewalk</cell><cell>Cyclist</cell><cell>Mean IoU</cell><cell>Global accuracy</cell></row><row><cell>SegNet [1] Bayesian SegNet [15] DeconvNet [21] Visin et al. [36]</cell><cell cols="2">! 29.5 ! 252 ! 32.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>n/a n/a n/a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">63.1 86.9 48.9 85.9 58.8 88.7</cell></row><row><cell>FCN8 [20]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Classic Upsampling</cell><cell></cell><cell>20</cell><cell cols="11">73.5 72.2 92.4 66.2 26.9 90.0 37.7 22.7 30.8 69.6 25.1</cell><cell cols="2">55.2 86.8</cell></row><row><cell>FC-DenseNet56 (k=12)</cell><cell></cell><cell>1.5</cell><cell cols="11">77.6 72.0 92.4 73.2 31.8 92.8 37.9 26.2 32.6 79.9 31.1</cell><cell cols="2">58.9 88.9</cell></row><row><cell>FC-DenseNet67 (k=16)</cell><cell></cell><cell>3.5</cell><cell cols="11">80.2 75.4 93.0 78.2 40.9 94.7 58.4 30.7 38.4 81.9 52.1</cell><cell cols="2">65.8 90.8</cell></row><row><cell>FC-DenseNet103 (k=16)</cell><cell></cell><cell>9.4</cell><cell cols="2">83.0 77.3</cell><cell cols="9">93.0 77.3 43.9 94.5 59.6 37.1 37.8 82.2 50.5</cell><cell>66.9</cell><cell>91.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning aligned cross-modal representations from weakly aligned data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1607.07295</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lasagne: First release</title>
		<imprint>
			<date type="published" when="2015-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno>abs/1608.04117</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dataset loaders: a python library to load and preprocess datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R F</forename><surname>Visin</surname></persName>
		</author>
		<ptr target="https://github.com/fvisin/dataset_loaders" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unrolling loopy top-down semantic feedback in convolutional deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno>abs/1608.06993</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoderdecoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Qualitative results on the CamVid test set. Pixels labeled in yellow are void class. Each row represents (from left to right): original image, original annotation (ground truth) and prediction of our model</title>
		<imprint/>
	</monogr>
	<note>Figure 3</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature space optimization for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04366</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geometric context from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geometric context from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICAI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<idno>abs/1605.02688</idno>
		<title level="m">Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv eprints</title>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">rmsprop adaptive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1511.06681</idno>
		<title level="m">Deep end2end voxel2voxel prediction. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Residual networks are exponential ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1605.06431</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reseg: A recurrent neural network-based model for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
