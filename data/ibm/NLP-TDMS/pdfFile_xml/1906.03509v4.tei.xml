<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OUTLIER EXPOSURE WITH CONFIDENCE CONTROL FOR OUT-OF-DISTRIBUTION DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristotelis-Angelos</forename><surname>Papadopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Mohammad</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Rajati</surname></persName>
							<email>rajati@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazim</forename><surname>Shaikh</surname></persName>
							<email>nshaikh@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiamian</forename><surname>Wang</surname></persName>
							<email>jiamianw@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OUTLIER EXPOSURE WITH CONFIDENCE CONTROL FOR OUT-OF-DISTRIBUTION DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks have achieved great success in classification tasks during the last years. However, one major problem to the path towards artificial intelligence is the inability of neural networks to accurately detect samples from novel class distributions and therefore, most of the existent classification algorithms assume that all classes are known prior to the training stage. In this work, we propose a methodology for training a neural network that allows it to efficiently detect outof-distribution (OOD) examples without compromising much of its classification accuracy on the test examples from known classes. We propose a novel loss function that gives rise to a novel method, Outlier Exposure with Confidence Control (OECC), which achieves superior results in OOD detection with OE both on image and text classification tasks without requiring access to OOD samples. Additionally, we experimentally show that the combination of OECC with state-of-the-art post-training OOD detection methods, like the Mahalanobis Detector (MD) and the Gramian Matrices (GM) methods, further improves their performance in the OOD detection task, demonstrating the potential of combining training and posttraining methods for OOD detection. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Modern neural networks have recently achieved superior results in classification problems <ref type="bibr" target="#b31">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b19">He et al., 2016)</ref>. However, most classification algorithms proposed so far assume that samples from all class conditional distributions are available during training time i.e., they make the closed-world assumption. In an open world environment <ref type="bibr" target="#b1">(Bendale &amp; Boult, 2015)</ref>, where examples from novel class distributions might appear during test time, it is necessary to build classifiers that are able to detect OOD examples while having high classification accuracy on known class distributions.</p><p>It is generally known that deep neural networks can make predictions for out-of-distribution (OOD) examples with high confidence <ref type="bibr" target="#b47">(Nguyen et al., 2015)</ref>. High confidence predictions are undesirable since they consist a symptom of overfitting <ref type="bibr" target="#b52">(Szegedy et al., 2015)</ref>. They also make the calibration of neural networks difficult.  observed that modern neural networks are miscalibrated since their average confidence is usually much higher than their accuracy.</p><p>A simple yet effective method to address the problem of the inability of neural networks to detect OOD examples is to train them so that they make highly uncertain predictions for examples generated by novel class distributions. In order to achieve that, <ref type="bibr" target="#b34">Lee et al. (2018a)</ref> defined a loss function based on the Kullback-Leibler (KL) divergence to minimize the distance between the output distribution given by softmax and the uniform distribution for samples generated by a GAN <ref type="bibr" target="#b16">(Goodfellow et al., 2014)</ref>. Using a similar loss function, <ref type="bibr" target="#b23">Hendrycks et al. (2019)</ref> showed that the technique of Outlier Exposure (OE) that draws anomalies from a real and diverse dataset can outperform the GAN framework for OOD detection.</p><p>In this paper, based on the idea of <ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref>, our main contribution is threefold: 1 Our code is publicly available at https://github.com/nazim1021/OOD-detection-using-OECC.</p><p>• We propose a novel method, Outlier Exposure with Confidence Control (OECC), consisting of two regularization terms. The first regularization term minimizes the total variation distance between the output distribution given by softmax for an auxiliary dataset and the uniform distribution, which constitutes a distance metric between the two distributions <ref type="bibr" target="#b15">(Gibbs &amp; Su, 2002;</ref><ref type="bibr" target="#b11">Deza &amp; Deza, 2009</ref>). The second regularization term minimizes the Euclidean distance between the training accuracy of a DNN and the average confidence in its predictions on the training set.</p><p>• We experimentally show that OECC achieves superior results in OOD detection with OE without requiring access to OOD samples. Additionally, similar to <ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref> and in contrast with many other state-of-the-art OOD detection methods such as the Mahalanobis Detector (MD) <ref type="bibr" target="#b35">(Lee et al., 2018b)</ref>, the Gramian Matrices (GM) method (Sastry &amp; Oore, 2020), ODIN <ref type="bibr" target="#b36">(Liang et al., 2018)</ref> and Res-Flow <ref type="bibr" target="#b59">(Zisselman &amp; Tamar, 2020)</ref>, we show that OECC can be applied to both image and text classification tasks. Furthermore, the experimental results demonstrate that OECC can successfully detect OOD samples both when in-and out-of-distribution examples are far from each other as well as in the more challenging case when in-and out-of-distribution examples are close.</p><p>• We experimentally show that OECC can be combined with state-of-the-art post-training methods for OOD detection like the Mahalanobis Detector (MD) <ref type="bibr" target="#b35">(Lee et al., 2018b)</ref> and the Gramian Matrices method (GM) <ref type="bibr">(Sastry &amp; Oore, 2020)</ref>. The experimental results demonstrate that the resulting combination achieves superior results in OOD detection, demonstrating the potential of combining training and post-training methods for OOD detection in the future research efforts.</p><p>2 RELATED WORK  used the GAN framework <ref type="bibr" target="#b16">(Goodfellow et al., 2014)</ref> to generate negative instances of seen classes by finding data points that are close to the training instances but are classified as fake by the discriminator. Then, they used those samples in order to train SVM classifiers to detect examples from unseen classes. Similarly, <ref type="bibr" target="#b29">Kliger &amp; Fleishman (2018)</ref> used a multi-class GAN framework in order to produce a generator that generates a mixture of nominal data and novel data and a discriminator that performs simultaneous classification and novelty detection. <ref type="bibr" target="#b22">Hendrycks &amp; Gimpel (2017)</ref> proposed a baseline for detecting misclassified and out-of-distibution examples based on their observation that the prediction probability of out-of-distribution examples tends to be lower than the prediction probability for correct examples. <ref type="bibr" target="#b34">Lee et al. (2018a)</ref> generated GAN examples and forced the DNN to have lower confidence in predicting the classes for those examples while in <ref type="bibr" target="#b23">Hendrycks et al. (2019)</ref>, the GAN samples were substituted with a real and diverse dataset using the technique of OE. Similar works <ref type="bibr" target="#b39">(Malinin &amp; Gales, 2018;</ref><ref type="bibr" target="#b2">Bevandić et al., 2018</ref>) also force the model to make uncertain predictions for OOD examples. Using an ensemble of classifiers, <ref type="bibr" target="#b33">Lakshminarayanan et al. (2017)</ref> showed that their method was able to express higher uncertainty in OOD examples. <ref type="bibr" target="#b20">Hein et al. (2019)</ref> showed that RELU networks might produce high confidence predictions far away from the training data while in <ref type="bibr">Meinke &amp; Hein (2020)</ref>, the authors proposed to modify the network architecture by integrating a generative model and they showed that the resulting architecture produces close to uniform predictions far away from the training data. <ref type="bibr" target="#b37">Liu et al. (2018)</ref> provided theoretical guarantees for detecting OOD examples under the assumption that an upper bound of the fraction of OOD examples is available. Recently, an adversarial training approach that can significantly increase the robustness of OOD detectors was proposed <ref type="bibr" target="#b5">(Chen et al., 2020a)</ref>.</p><p>For image data, based on the idea of <ref type="bibr" target="#b22">Hendrycks &amp; Gimpel (2017)</ref>, it was observed that simultaneous use of temperature scaling  and small perturbations at the input, a method called ODIN <ref type="bibr" target="#b36">(Liang et al., 2018)</ref>, can push the softmax scores of in-and out-of-distribution images further apart from each other, making the OOD images distinguishable. Recently, the ODIN method was generalized to not require access to OOD examples to tune its parameters <ref type="bibr">(Hsu et al., 2020)</ref>. Under the assumption that the pre-trained features of a softmax neural classifier can be fitted well by a class-conditional Gaussian distribution, one can define a confidence score using the Mahalanobis distance that can efficiently detect abnormal test samples <ref type="bibr" target="#b35">(Lee et al., 2018b)</ref>. Sastry &amp; Oore (2020) proposed the use of higher order Gram matrices to compute pairwise feature correlations between the channels of each layer of a DNN. The methods proposed by <ref type="bibr" target="#b35">Lee et al. (2018b)</ref>, Sastry &amp; Oore (2020) and <ref type="bibr" target="#b36">Liang et al. (2018)</ref> are post-training methods for OOD detection.</p><p>Recently, there is also a growing interest in applying machine learning in a self-supervised manner for OOD detection. <ref type="bibr" target="#b24">Hendrycks et al. (2019b)</ref> combined different self-supervised geometric translation prediction tasks in one model, using multiple auxiliary heads. They showed that their method performs well on detecting outliers which are close to the in-distribution data. <ref type="bibr" target="#b42">Mohseni et al. (2020)</ref> proposed using one auxiliary head in a self-supervised manner to learn generalizable OOD features.</p><p>Unsupervised methods have also been studied for OOD detection. <ref type="bibr" target="#b8">Choi et al. (2018)</ref> showed that in high dimensions, likelihood models might assign high likelihoods to OOD inputs. <ref type="bibr" target="#b45">Nalisnick et al. (2019)</ref> experimentally showed that deep generative models assign higher likelihood to OOD data compared to in-distribution data. To mitigate this problem, <ref type="bibr" target="#b49">Ren et al. (2019)</ref> proposed a likelihood ratio method that improves the OOD detection capabilities of deep generative models of images.</p><p>Recently, <ref type="bibr" target="#b43">Morningstar et al. (2020)</ref> and <ref type="bibr" target="#b14">Erdil et al. (2020)</ref> proposed unsupervised methods for OOD detection based on kernel density estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUTLIER EXPOSURE WITH CONFIDENCE CONTROL (OECC)</head><p>We consider the multi-class classification problem under the open-world assumption <ref type="bibr" target="#b1">(Bendale &amp; Boult, 2015)</ref>  <ref type="bibr" target="#b34">(Lee et al., 2018a;</ref><ref type="bibr" target="#b23">Hendrycks et al., 2019)</ref>, the KL divergence metric was used in order to minimize the distance between the output distribution produced by softmax for the OOD examples and the uniform distribution. However, it is generally known that KL divergence does not satisfy the symmetry and the triangle inequality properties as required by a distance metric <ref type="bibr" target="#b15">(Gibbs &amp; Su, 2002;</ref><ref type="bibr" target="#b11">Deza &amp; Deza, 2009</ref>). In our work, we choose to minimize the total variation distance <ref type="bibr" target="#b15">(Gibbs &amp; Su, 2002)</ref> between the two distributions. There are several reasons for this choice. First, the total variation distance satisfies all the properties required by a distance metric. Second, it is one of the most commonly used probability metrics since it admits natural interpretations. For instance, in Bayesian statistics, the error in a bounded expected loss function due to the approximation of a probability measure by another is given by the total variation distance <ref type="bibr" target="#b15">(Gibbs &amp; Su, 2002)</ref>. Moreover, as we also mention later, the total variation distance has the unique property to uniformly attract all the prediction probabilities produced by softmax for data sampled from D OE out towards the uniform distribution, making the neural network better detect in-and out-of-distribution examples.</p><p>Viewing the knowledge of a model as the class conditional distribution it produces over outputs given an input <ref type="bibr" target="#b25">(Hinton et al., 2015)</ref>, the entropy of this conditional distribution can be used as a regularization method that penalizes confident predictions of a neural network <ref type="bibr" target="#b48">(Pereyra et al., 2017)</ref>. In our approach, instead of penalizing the confident predictions of posterior probabilities yielded by a neural network, we force it to make predictions for examples generated by D in with an average confidence close to its training accuracy. In such a manner, not only do we make the neural network avoid making overconfident predictions, but we also take into consideration its calibration .</p><p>Let us consider a classification model that can be represented by a parametrized function f θ , where θ stands for the vector of parameters in f θ . Without loss of generality, assume that the cross-entropy loss function is used during training. We propose the following constrained optimization problem for finding θ:</p><formula xml:id="formula_0">minimize θ E (x,y)∼Din [L CE (f θ (x), y)] subject to E x∼Din max l=1,...,K e z l K j=1 e zj = A tr max l=1,...,K e z l K j=1 e zj = 1 K , ∀x (i) ∼ D OE out (1)</formula><p>where L CE is the cross-entropy loss function and K is the number of classes available in D in . Even though the constrained optimization problem (1) can be used for training various classification models, for clarity we limit our discussion to deep neural networks. Let z denote the vector representation of the example x (i) in the feature space produced by the last layer of the deep neural network (DNN) and let A tr be the training accuracy of the DNN. Observe that the optimization problem (1) minimizes the cross entropy loss function subject to two additional constraints. The first constraint forces the average maximum prediction probabilities calculated by the softmax layer towards the training accuracy of the DNN for examples sampled from D in , while the second constraint forces the maximum probability calculated by the softmax layer towards 1 K for all examples sampled from the probability distribution D OE out . In other words, the first constraint makes the DNN predict examples from known classes with an average confidence close to its training accuracy, while the second constraint forces the DNN to be highly uncertain for examples of classes it has never seen before by producing a uniform distribution at the output for examples sampled from the probability distribution D OE out . It is also worth noting that the first constraint of (1) uses the training accuracy of the neural network A tr which is not available in general. To handle this issue, one can train a neural network by only minimizing the cross-entropy loss function for a few number of epochs in order to estimate A tr and then fine-tune it using (1).</p><p>Because solving the nonconvex constrained optimization problem described by (1) is extremely difficult, let us introduce Lagrange multipliers <ref type="bibr" target="#b4">(Boyd &amp; Vandenberghe, 2004)</ref> and convert (1) into the following unconstrained optimization problem:</p><formula xml:id="formula_1">minimize θ E (x,y)∼Din [L CE (f θ (x), y)] + λ 1 A tr − E x∼Din max l=1,...,K e z l K j=1 e zj + λ 2 x (i) ∼D OE out 1 K − max l=1,...,K e z l K j=1 e zj<label>(2)</label></formula><p>where it is worth mentioning that in (2), we used only one Lagrange multiplier for the second set of constraints in (1) instead of using one for each constraint in order to avoid introducing a large number of hyperparameters to our loss function. This modification is a special case where we consider the Lagrange multiplier λ 2 to be common for each individual constraint involving a different x (i) ∼ D OE out . Note also that according to the original Lagrangian theory, one should optimize the objective function of (2) both with respect to θ, λ 1 and λ 2 but as it is common in machine learning applications, we approximate the original problem by calculating appropriate values for λ 1 and λ 2 through a validation technique <ref type="bibr" target="#b18">(Hastie et al., 2001)</ref>.</p><p>After converting the constrained optimization problem (1) into an unconstrained optimization problem described by (2), we observed in the simulation experiments that at each training epoch, the maximum prediction probability produced by softmax for each example drawn from D OE out changes, introducing difficulties in making the DNN produce a uniform distribution at the output for those examples. For instance, assume that we have a K-class classifier with K = 3 and at epoch t n , the maximum prediction probability produced by softmax for an example x (i) ∼ D OE out corresponds to the second class. Then, the last term of (2) will push the prediction probability of example x (i) for the second class towards 1 3 while concurrently increasing the prediction probabilities for either the first class or the third class or both. At the next epoch t n+1 , it is possible that the prediction probability for either the first class or the third class becomes the maximum among the three and hence, the last term of (2) will push that one towards 1 3 by possibly increasing again the prediction probability for the second class. It becomes obvious that this process introduces difficulties in making the DNN produce a uniform distribution at the output for examples sampled from D OE out . Fortunately, this issue can be resolved by concurrently pushing all the prediction probabilities produced by the softmax layer for examples drawn from D OE out towards 1 K . Additionally, in order to prevent the second and the third term of (2) from taking negative values during training, let us convert (2) into the following:</p><formula xml:id="formula_2">minimize θ E (x,y)∼Din [L CE (f θ (x), y)] + λ 1 A tr − E x∼Din max l=1,...,K e z l K j=1 e zj 2 + λ 2 x (i) ∼D OE out K l=1 1 K − e z l K j=1 e zj<label>(3)</label></formula><p>The second term of the the loss function described by (3) minimizes the squared distance between the training accuracy of the DNN and the average confidence in its predictions for examples drawn from D in . Additionally, the third term of (3) minimizes the l 1 norm between the uniform distribution and the distribution produced by the softmax layer for the examples drawn from D OE out . At this point, let Ω be a countable sample space and let P, Q denote two probability measures on Ω. Then, the total variation distance between P and Q is defined as:</p><formula xml:id="formula_3">d T V (P, Q) = 1 2 x∈Ω |P (x) − Q(x)|<label>(4)</label></formula><p>Let Ω be the space of D OE out , P be the discrete uniform distribution and Q be the probability distribution produced by the softmax layer for the examples sampled from D OE out . Then, the third term of (3) is equivalent to the total variation distance described by (4). We call the methodology of training a DNN with the loss function described by (3) Outlier Exposure with Confidence Control (OECC).</p><p>While converting the unconstrained optimization problem (2) into (3), one could use several combinations of norms for regularization. However, we found that minimizing the squared distance between the training accuracy of the DNN and the average confidence in its predictions for examples drawn from D in and the total variation distance (or equivalently, l 1 norm) between the uniform distribution and the distribution produced by the softmax layer for the examples drawn from D OE out works best. This is because l 1 norm uniformly attracts all the prediction probabilities produced by softmax to the desired value 1 K , better contributing to producing a uniform distribution at the output of the DNN for the examples drawn from D OE out . On the other hand, minimizing the squared distance between the training accuracy of the DNN and the average confidence in its predictions for examples drawn from D in emphasizes more on attracting the maximum softmax probabilities that are further away from the training accuracy of the DNN, making the neural network better detect inand out-of-distribution examples at the low softmax probability levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>During the experiments, we observed that if we start training the DNN with a relatively high value of λ 1 , the learning process might slow down since we constantly force the neural network to make predictions with an average confidence close to its training accuracy, which is initially low mainly due to the random weight initialization. Therefore, it is recommended to split the training of the algorithm into two stages where in the first stage, we train the DNN using only the cross-entropy loss function until it reaches the desired level of accuracy A tr and then using a fixed A tr , we finetune it using the OECC method. 2 Many of the proposed methods for OOD detection can be classified into two categories. In the first category of methods, the DNN is trained in a manner that improves its OOD detection capability. This category of methods includes both supervised <ref type="bibr" target="#b34">(Lee et al., 2018a;</ref><ref type="bibr" target="#b23">Hendrycks et al., 2019)</ref> and self-supervised approaches <ref type="bibr" target="#b24">(Hendrycks et al., 2019b;</ref><ref type="bibr" target="#b42">Mohseni et al., 2020)</ref>. These methods can be called training methods for OOD detection. OECC belongs to the category of training methods. The second category are post-training methods for OOD detection <ref type="bibr" target="#b35">(Lee et al., 2018b;</ref><ref type="bibr">Sastry &amp; Oore, 2020;</ref><ref type="bibr" target="#b36">Liang et al., 2018;</ref><ref type="bibr" target="#b59">Zisselman &amp; Tamar, 2020)</ref> where the OOD detection method is applied after training a neural network.</p><p>We conducted two types of experiments that are presented in Sections 4.1 and 4.2. In Section 4.1, we compare our method with the Outlier Exposure (OE) method <ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref>, which is a state-of-the-art method for OOD detection. Since OE is a training method for OOD detection, we adopt the experimental setup that is commonly used in the training methods for OOD detection. On the other hand, in Section 4.2, we show how OECC can be effectively combined with post-training methods for OOD detection such as the Mahalanobis Detector (MD) <ref type="bibr" target="#b35">(Lee et al., 2018b)</ref> and the Gramian Matrices (GM) method <ref type="bibr">(Sastry &amp; Oore, 2020)</ref>. Consequently in Section 4.2, we adopt the experimental setup that is commonly used by the post-training methods for OOD detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">COMPARISON WITH STATE-OF-THE-ART IN OE</head><p>The experimental setting at this section is as follows. We draw samples from D in and we train the DNN using only the cross-entropy loss function until it reaches the desired level of accuracy A tr . Then, drawing samples from D OE out , we fine-tune it using the OECC method given by <ref type="formula" target="#formula_2">(3)</ref>. During the test phase, we evaluate the OOD detection capability of the DNN using examples from D test out which is disjoint from D OE out . We demonstrate the effectiveness of our method in both image and text classification tasks by comparing it with the previous OOD detection with <ref type="bibr">OE (Hendrycks et al., 2019)</ref>, which is a state-of-the-art method in OE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">EVALUATION METRICS</head><p>Our method belongs to the class of training methods for OOD detection. Therefore, for the comparison with the OE method, we adopt the evaluation metrics that were also used in other training methods for OOD detection <ref type="bibr" target="#b23">(Hendrycks et al., 2019;</ref><ref type="bibr" target="#b58">b;</ref><ref type="bibr" target="#b42">Mohseni et al., 2020)</ref>. Defining the OOD examples as the positive class and the in-distribution examples as the negative class, the performance metrics associated with OOD detection are the following:</p><p>• False Positive Rate at N % True Positive Rate (FPRN): This performance metric <ref type="bibr" target="#b0">(Balntas et al., 2016;</ref><ref type="bibr" target="#b32">Kumar et al., 2016)</ref> measures the capability of an OOD detector when the maximum softmax probability threshold is set to a predefined value. More specifically, assuming N % of OOD examples need to be detected during the test phase, we calculate a threshold in the softmax probability space and given that threshold, we measure the false positive rate, i.e. the ratio of indistribution examples that are incorrectly classified as OOD.</p><p>• Area Under the Receiver Operating Characteristic curve (AUROC): In the out-of-distribution detection task, the ROC curve <ref type="bibr" target="#b10">(Davis &amp; Goadrich, 2006)</ref> summarizes the performance of an OOD detection method for varying threshold values.</p><p>• Area Under the Precision-Recall curve (AUPR): The AUPR (Manning &amp; Schütze, 1999) is an important measure when there exists a class-imbalance between OOD and in-distribution examples in a dataset. Similar to <ref type="bibr" target="#b23">Hendrycks et al. (2019)</ref>, in our experiments, the ratio of OOD and in-distribution test examples is 1:5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">IMAGE CLASSIFICATION EXPERIMENTS</head><p>Results. The results of the image classification experiments are shown in <ref type="table">Table 1</ref>. In <ref type="figure">Figure 1</ref>, as an example, we plot the histogram of softmax probabilities using CIFAR-10 as D in and Places365 as D test out . It can be easily observed that a Maximum Softmax Probability (MSP) detector makes predictions for Places365 samples with very high probability, making it impossible for a thresholdbased detector to separate in-and out-of-distribution data. On the other hand, after fine-tuning with the OECC method given by <ref type="formula" target="#formula_2">(3)</ref>, it can be observed that the DNN is making predictions for Places365 samples with very low confidence, forming a uniform distribution at the output for these samples. Therefore, in this case, one can easily design a threshold-based detector that can separate in-and out-of-distribution data.  <ref type="bibr" target="#b23">Hendrycks et al. (2019)</ref>, CIFAR-10, CIFAR-100 and SVHN datasets were used as D in . For CIFAR-10 and CIFAR-100 experiments, we used 40-2 Wide Residual Networks (WRNs) <ref type="bibr" target="#b57">(Zagoruyko &amp; Komodakis, 2016)</ref>. We initially trained the WRN for 100 epochs using a cosine learning rate <ref type="bibr" target="#b38">(Loshchilov &amp; Hutter, 2017)</ref> with an initial value 0.1, a dropout rate of 0.3 and a batch size of 128. As in <ref type="bibr" target="#b23">Hendrycks et al. (2019)</ref>, we also used Nesterov momentum and l 2 weight regularization with a decay factor of 0.0005. For CIFAR-10, we fine-tuned the network for 15 epochs with the OECC method described by (3) using a learning rate of 0.001, while for the CIFAR-100 the corresponding number of epochs was 30. For the SVHN experiments, we trained 16-4 WRNs using a cosine learning rate with an initial value 0.01, a dropout rate of 0.4 and a batch size of 128. We then fine-tuned the network for 5 epochs using a learning rate of 0.001. During fine-tuning, the 80 Million Tiny Images dataset was used as D OE out . The values of the hyperparameters λ 1 and λ 2 were chosen in the range [0.03, 0.09] using a separate validation dataset D val out similar to <ref type="bibr" target="#b23">Hendrycks et al. (2019)</ref>. Note that D val out and D test out are disjoint. The data used for validation are presented in A.3.</p><p>Contribution of each regularization term. To demonstrate the effect of each regularization term of the OECC method described by (3) in the OOD detection task, we ran some additional image classification experiments that are presented in <ref type="table" target="#tab_2">Table 2</ref>. For these experiments, we incrementally added each regularization term to the loss function described by (3) and we measured its effect both in the OOD detection evaluation metrics as well as in the accuracy of the DNN on the test images of D in . The values of the hyperparameters λ 1 and λ 2 were chosen in the range [0.03, 0.09] using a separate validation dataset D val out presented in A.3. The results of these experiments validate that the combination of the two regularization terms of (3) not only improves the OOD detection performance of the DNN but also improves its accuracy on the test examples of D in compared to the case where λ 1 = 0.    <ref type="bibr" target="#b7">(Cho et al., 2014)</ref> for 5 epochs with learning rate 0.01 and a batch size of 64 and then we fine-tune them for 2 epochs using the OECC method described by (3) with a batch size 64 and learning rate 0.01. During fine-tuning, the WikiText-2 dataset was used as D OE out . The values of the hyperparameters λ 1 and λ 2 were chosen in the range [0.04, 0.1] using a separate validation dataset as described in B.2.</p><p>Discussion. As can be observed from the results of <ref type="table" target="#tab_4">Table 1 and Table 3</ref>, OECC achieves superior results in OOD detection compared to the previously proposed OE method <ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref> for both image and text classification tasks. Additionally, the experimental results of <ref type="table" target="#tab_2">Table 2</ref> suggest that the combination of the two regularization terms of the OECC method is needed in order to maximize the OOD detection capability of the DNN. At this point, it is worth mentioning that the experimental results of <ref type="table" target="#tab_4">Table 1 and Table 3</ref> show that OECC can successfully detect OOD samples in case where in-and out-of-distribution examples are far from each other as well as in the more challenging case where in-and out-of-distribution examples are close. More specifically, <ref type="bibr" target="#b54">Winkens et al. (2020)</ref> proposed the confusion log probability (CLP) metric to measure how close (or far) in-distribution and out-of-distribution examples are. In the case where CIFAR-10 is considered as D in and SVHN as D test out , CLP = [−12.1, −7.6] denoting that in-and out-of-distribution data are far from each other. In this case, OECC achieves AUROC 99.6 (see A.1) which is higher than the one achieved by the joint contrastive <ref type="bibr" target="#b6">(Chen et al., 2020b)</ref> and supervised learning scheme proposed in <ref type="bibr" target="#b54">Winkens et al. (2020)</ref>. On the other hand, in the case where CIFAR-100 is considered as D in and CIFAR-10 as D test out , CLP = [−4.5, −2.6] denoting that in-and out-of-distribution examples are close. In this more challenging case, OECC achieves AUROC 78.7 (see A.1) which is higher than the one achieved by the joint contrastive and supervised learning, which has been specifically designed for this setting. Note that <ref type="bibr" target="#b54">Winkens et al. (2020)</ref> only use the AUROC metric in their results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COMBINATION OF OECC AND POST-TRAINING METHODS FOR OOD DETECTION</head><p>The experimental setting at this section is as follows. We draw samples from D in and train the DNN until it reaches the desired level of accuracy A tr . Then, drawing samples from D OE out , we fine-tune it using the OECC method given by (3). Finally, we apply the post-training method to the fine-tuned model. During the test phase, we evaluate the OOD detection capability of the post-processed model using examples from D test out , which is disjoint from D OE out . The detailed description of the image datasets used as D in , D OE out and D test out in the image OOD detection experiments is presented in C.1. The validation data D val out used to tune the hyperparameters λ 1 and λ 2 for image classification experiments are presented in C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">EVALUATION METRICS</head><p>To follow the convention in the literature of post-training methods for OOD detection <ref type="bibr" target="#b35">(Lee et al., 2018b;</ref><ref type="bibr">Sastry &amp; Oore, 2020;</ref><ref type="bibr" target="#b36">Liang et al., 2018;</ref><ref type="bibr" target="#b59">Zisselman &amp; Tamar, 2020)</ref> and to further demonstrate the adaptability of our method, in the experiments where we combine OECC with post-training methods, we adopt the following OOD detection evaluation metrics. In order to calculate these metrics, we consider in-distribution as positive examples and OOD as negative examples. </p><formula xml:id="formula_4">1 − min {P in (q(x) ≤ )P (x is from D in ) + P out (q(x) &gt; )P (x is from D out )}, where q(x)</formula><p>is a confidence score. Similar to <ref type="bibr" target="#b35">Lee et al. (2018b)</ref>, we assume that:</p><formula xml:id="formula_5">P (x is from D in ) = P (x is from D out )</formula><p>• Area Under the Precision Recall curve (AUPR): The PR curve plots the precision against the recall for a varying threshold. In our experiments, we denote by AUPRin (or AUPRout) the area under the PR curve when in-(or out-of-) distribution examples are considered as the positive class. <ref type="bibr" target="#b35">Lee et al. (2018b)</ref> proposed a post-training method for OOD detection that can be applied to any pre-trained softmax neural classifier. Under the assumption that the pre-trained features of a DNN can be fitted well by a class-conditional Gaussian distribution, they defined a confidence score using the Mahalanobis distance with respect to the closest class-conditional probability distribution, where its parameters are chosen as empirical class means and tied empirical covariance of training samples <ref type="bibr" target="#b35">(Lee et al., 2018b)</ref>. This confidence score can be used as a threshold to determine whether an example is in-or out-of-distribution. To further distinguish in-and out-of-distribution examples, they proposed two additional techniques. In the first technique, they added a small perturbation before processing each input example to increase the confidence score of their method. In the second technique, they proposed a feature ensemble method in order to obtain a better calibrated score. The feature ensemble method extracts all the hidden features of the DNN and computes their empirical class mean and tied covariances. Subsequently, it calculates the Mahalanobis distancebased confidence score for each layer and finally calculates the weighted average of these scores by training a logistic regression detector using validation samples in order to calculate the weight of each layer at the final confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">A COMBINATION OF OECC AND MAHALANOBIS DETECTION METHOD FOR OOD DETECTION</head><p>Since the Mahalanobis distance-based detector proposed by <ref type="bibr" target="#b35">Lee et al. (2018b)</ref> is a post-training method, it can be combined with the proposed OECC method described by (3). More specifically, in our experiments, we initially trained a DNN using the standard cross-entropy loss function and then we fine-tuned it with the OECC method given by (3). After fine-tuning, we applied the MD method and we compared the results obtained with the results presented in <ref type="bibr" target="#b35">Lee et al. (2018b)</ref>. The simulation experiments on image classification tasks show that the combination of our method which is a training method, with the MD method which is a post-training method achieves state-of-the-art results in the OOD detection task. A part of our experiments was based on the publicly available code of <ref type="bibr" target="#b35">Lee et al. (2018b)</ref>.</p><p>Experimental Setup. To demonstrate the adaptability and the effectiveness of our method, we adopt the experimental setup of <ref type="bibr" target="#b35">Lee et al. (2018b)</ref>. We train ResNet <ref type="bibr" target="#b19">(He et al., 2016)</ref>  Similar to <ref type="bibr" target="#b35">Lee et al. (2018b)</ref>, for the MD method we train the ResNet model for 200 epochs with batch size 128 by minimizing the cross-entropy loss using the SGD algorithm with momentum 0.9. The learning rate starts at 0.1 and is dropped by a factor of 10 at 50% and 75% of the training progress, respectively. Subsequently, we compute the Mahalanobis distance-based confidence score using both the input pre-processing and the feature ensemble techniques. The hyper-parameters that need to be tuned are the magnitude of the noise added at each test input example as well as the layer indexes for feature ensemble. Similar to <ref type="bibr" target="#b35">Lee et al. (2018b)</ref>, both of them are tuned using a separate validation dataset consisting of both in-and out-of-distribution data since the MD method originally requires access to OOD samples.</p><p>As mentioned earlier, since the Mahalanobis Detector (MD) is a post-training method for OOD detection, it can be combined with our proposed method. More specifically, we initially train the ResNet model with 34 layers for 200 epochs using exactly the same training setup as mentioned above. Subsequently, we fine-tune the network with the OECC method described by (3) using the 80 Million Tiny Images as D OE out . During fine-tuning, we use the SGD algorithm with momentum 0.9 and a cosine learning rate <ref type="bibr" target="#b38">(Loshchilov &amp; Hutter, 2017)</ref> with an initial value 0.001 using a batch size of 128 for data sampled from D in and a batch size of 256 for data sampled from D OE out . For CIFAR-10 and CIFAR-100 experiments, we fine-tune the network for 30 and 20 epochs respectively, while for SVHN the corresponding number of epochs was 5. The values of the hyper-parameters λ 1 and λ 2 were chosen in the range [0.03, 0.12] using a separate validation dataset consisting of both in-and out-of-distribution images similar to <ref type="bibr" target="#b35">Lee et al. (2018b)</ref>. The validation dataset is described in C.2. The results are shown in   <ref type="bibr" target="#b35">(Lee et al., 2018b)</ref> and the combination of OECC with the MD method using a ResNet-34 architecture. Similar to <ref type="bibr" target="#b35">Lee et al. (2018b)</ref>, the hyper-parameters are tuned using a validation dataset of in-and out-of-distribution data as described in C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">A COMBINATION OF OECC AND GRAM MATRICES METHOD FOR OOD DETECTION</head><p>Recently, Sastry &amp; Oore (2020) proposed a post-training method for OOD detection that does not require access to OOD data for hyper-parameter tuning, unlike the MD method <ref type="bibr" target="#b35">(Lee et al., 2018b)</ref>. More specifically, they proposed the use of higher order Gram matrices to compute pairwise feature correlations between the channels of each layer of a DNN. Subsequently, after computing the minimum and maximum values of the correlations for every class c in which an example generated by D in is classified, they used those values to calculate the layerwise deviation of each test sample, i.e. the deviation of test sample from the images seen during training with respect to each of the layers. Finally, they calculated the total deviation by taking a normalized sum of the layerwise deviations and using a threshold τ , they classified a sample as OOD if its corresponding total deviation was above the threshold. The experimental results presented in Sastry &amp; Oore (2020) showed that GM method outperforms MD method in most of the experiments without requiring access to OOD samples to tune its parameters. However, it should be noted that GM, in its current form, does not perform equally well when the samples from D test out are close to D in , as it happens for instance in the case where CIFAR-10 is used as D in and CIFAR-100 is used as D test out .</p><p>ResNet experiments. For the results related to the GM method, we initially trained the ResNet model using exactly the same training details presented in Section 4.2.2 and then we applied the GM method where the tuning of the normalizing factor used to calculate the total deviation of a test image is done using a randomly selected validation partition from D test in , as described in Sastry &amp; Oore (2020). For the combined OECC+GM method, we initially trained the ResNet model as described above, then we fine-tuned it using the loss function described by (3) and finally, we applied the GM method. During fine-tuning, we used the SGD algorithm with momentum 0.9 and a cosine learning rate <ref type="bibr" target="#b38">(Loshchilov &amp; Hutter, 2017)</ref> with an initial value 0.001 using a batch size of 128 for data sampled from D in and a batch size of 256 for data sampled from D OE out . In our experiments, the 80 Million Tiny Images dataset <ref type="bibr" target="#b53">(Torralba et al., 2008)</ref> was considered as D OE out . For CIFAR-10 experiments, we fine-tuned the network for 30 epochs, for CIFAR-100 we fine-tuned it for 10, while for SVHN the corresponding number of epochs was 5. Note that in the previous experiment, in which we combined the OECC method with the MD method, the hyper-parameters λ 1 and λ 2 of (3) were tuned using a validation dataset of in-and out-of-distribution data as described in C.2 since this is required by the MD method. On the contrary, in this experiment, the hyper-parameters λ 1 and λ 2 of (3) were tuned using a separate validation dataset D val out described in C.2. Note that D val out and D test out are disjoint. Therefore, for these experiments, no access to D test out was assumed. The results of the experiments are shown in <ref type="table">Table 5</ref>. The values of the hyper-parameters λ 1 and λ 2 were chosen in the range [0.03, 0.12].  <ref type="table">Table 5</ref>: Comparison between the Gramian Matrices (GM) method (Sastry &amp; Oore, 2020) versus the combination of OECC method and the GM method using a ResNet-34 architecture. The tuning of the hyperparameters λ 1 and λ 2 of (3) is done using a separate validation dataset D val out described in C.2. Note that D val out and D test out are disjoint.</p><p>DenseNet experiments For the results related to the GM method, we used the pre-trained DenseNet <ref type="bibr" target="#b27">(Huang et al., 2017)</ref> model provided by <ref type="bibr" target="#b36">Liang et al. (2018)</ref>. The network has depth L = 100, growth rate m = 12 and dropout rate 0. It has been trained using the stochastic gradient descent algorithm with Nesterov momentum <ref type="bibr" target="#b12">(Duchi et al., 2011;</ref><ref type="bibr" target="#b28">Kingma &amp; Ba, 2014)</ref> for 300 epochs with batch size 64 and momentum 0.9. The learning rate started at 0.1 and was dropped by a factor of 10 at 50% and 75% of the training progress, respectively. Subsequently, we applied the GM method (Sastry &amp; Oore, 2020) where the tuning of the normalizing factor used to calculate the total deviation of a test image was done using a randomly selected validation partition from D test in as described in Sastry &amp; Oore (2020). For the combined OECC+GM method, we fine-tuned the pre-trained DenseNet network model provided by <ref type="bibr" target="#b36">Liang et al. (2018)</ref> using the OECC loss function described by (3) and then we applied the GM method. During fine-tuning, we used the SGD algorithm with momentum 0.9 and a cosine learning rate <ref type="bibr" target="#b38">(Loshchilov &amp; Hutter, 2017)</ref> with an initial value 0.001 for CIFAR-10 and SVHN experiments and 0.01 for the CIFAR-100 experiments using a batch size of 128 for data sampled from D in and a batch size of 256 for data sampled from D OE out . In our experiments, the 80 Million Tiny Images dataset <ref type="bibr" target="#b53">(Torralba et al., 2008)</ref> was considered as D OE out . The DenseNet model was fine-tuned for 15 epochs for the CIFAR-10 experiments, for 10 epochs for the CIFAR-100 experiments, while for SVHN the corresponding number of epochs was 5. The hyperparameters λ 1 and λ 2 of the OECC method were tuned using a separate validation dataset D val out described in C.2. Note that D val out and D test out are disjoint. The experimental results are presented in <ref type="table" target="#tab_10">Table 6</ref>. The values of the hyper-parameters λ 1 and λ 2 were chosen in the range [0.03, 0.12].</p><p>Discussion. The results presented in <ref type="table" target="#tab_6">Table 4</ref>, <ref type="table">Table 5</ref>, and <ref type="table" target="#tab_10">Table 6</ref> demonstrate the superior performance that can be achieved when combining training and post-training methods for OOD detection. More specifically, the MD method <ref type="bibr" target="#b35">(Lee et al., 2018b)</ref>   <ref type="table" target="#tab_10">Table 6</ref>: Comparison between the Gramian Matrices (GM) method (Sastry &amp; Oore, 2020) versus the combination of OECC method and the GM method using a DenseNet-100 architecture. The tuning of the hyperparameters λ 1 and λ 2 of (3) is done using a separate validation dataset D val out described in C.2. Note that D val out and D test out are disjoint.</p><p>score. The GM method (Sastry &amp; Oore, 2020) also extracts the features from a pre-trained softmax neural classifier and then computes higher order Gram matrices to subsequently calculate pairwise feature correlations between the channels of each layer of a DNN. As also mentioned earlier, both of these methods are post-training methods for OOD detection. On the other hand, the simulation results presented in <ref type="table">Table 1</ref> and <ref type="table" target="#tab_4">Table 3</ref> showed that OECC, which belongs to the category of training methods for OOD detection, can teach the DNN to learn feature representations that can better distinguish in-and out-of-distribution data compared to the baseline method <ref type="bibr" target="#b22">(Hendrycks &amp; Gimpel, 2017)</ref> and the OE method <ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref>. Therefore, by feeding a post-training method like the MD method <ref type="bibr" target="#b35">(Lee et al., 2018b)</ref> and the GM method (Sastry &amp; Oore, 2020) with better feature representations, it is expected that one can achieve superior results in the OOD detection task as it is also validated by the experimental results in <ref type="table" target="#tab_6">Table 4</ref>, <ref type="table">Table 5</ref>, and  <ref type="bibr" target="#b19">(He et al., 2016)</ref> has an average confidence on its predictions for CIFAR-100 images that is much higher than its accuracy.</p><p>As discussed earlier in Section 3 and as was also shown experimentally, the purpose of the second term of the loss function described by <ref type="formula" target="#formula_2">(3)</ref> is to further distinguish in-and out-of-distribution examples and enhance the OOD detection capability of a neural network by pushing the maximum prediction probabilities produced by the softmax layer for the in-distribution examples towards the training accuracy of the DNN. Motivated by the fact that overconfident predictions constitute a symptom of overfitting <ref type="bibr" target="#b52">(Szegedy et al., 2015)</ref> and also by the results of the experiments of , we expect that by minimizing the squared distance between the training accuracy of the DNN and the average confidence in its predictions for examples drawn from D in , not only will the neural network have a higher OOD detection capability, but it will also be more calibrated.</p><p>To validate our hypothesis, we use two miscalibration measures, namely the Expected Calibration Error (ECE) and the Maximum Calibration Error (MCE) <ref type="bibr" target="#b44">(Naeini et al., 2015)</ref>. ECE measures the difference in expectation between confidence and accuracy while MCE measures the worst-case deviation between confidence and accuracy. </p><formula xml:id="formula_6">M CE = max m∈{1,...,M } acc(B m ) − conf (B m )<label>(6)</label></formula><p>To evaluate our method, we draw n = 1000 test samples from D in and we compare the ECE and MCE miscalibration errors for the MSP baseline detector <ref type="bibr" target="#b22">(Hendrycks &amp; Gimpel, 2017)</ref> and the MSP detector fine-tuned with the OECC method described by <ref type="formula" target="#formula_2">(3)</ref>. We partition the data into M = 15 bins. In <ref type="figure">Figure 2</ref>, we plot the miscalibration errors considering as D in the CIFAR-100 and the SST datasets, respectively. For the CIFAR-100 experiments, we train a 40-2 WRN with exactly the same training details as in Section 4.1.2. For the SST experiments, we train a 2-layer GRU with exactly the same training details as in Section 4.1.3. As can be observed from the results of <ref type="figure">Figure 2</ref>, the minimization of the squared distance between the average confidence of the neural network and its training accuracy through the second term of (3) also generalizes to the test set by reducing the miscalibration errors for both image and text datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we proposed a novel method for OOD detection, called Outlier Exposure with Confidence Control (OECC). OECC includes two regularization terms the first of which minimizes the total variation distance between the output distribution of the softmax layer of a DNN and the uniform distribution, while the second minimizes the Euclidean distance between the training accuracy of a DNN and the average confidence in its predictions on the training set. Experimental results showed that the proposed method achieves superior results in OOD detection with OE <ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref> in both image and text classification tasks. Additionally, we experimentally showed that our method can be combined with state-of-the-art post-training methods for OOD detection like the Mahalanobis Detector (MD) <ref type="bibr" target="#b35">(Lee et al., 2018b)</ref>    <ref type="bibr" target="#b51">(Socher et al., 2013</ref>) is a binary classification dataset for sentiment prediction of movie reviews containing around 10,000 examples. WikiText-2: This dataset contains over 2 million articles from Wikipedia and is exclusively used as D OE out in our experiments. We used the same preprocessing as in <ref type="bibr" target="#b23">Hendrycks et al. (2019)</ref> in order to have a valid comparison. SNLI: The Stanford Natural Language Inference (SNLI) corpus is a collection of 570,000 humanwritten English sentence pairs <ref type="bibr" target="#b3">(Bowman et al., 2015)</ref>. IMDB: A sentiment classification dataset containing movies reviews. Multi30K: A dataset of English and German descriptions of images <ref type="bibr" target="#b13">(Elliott et al., 2016)</ref>. For our experiments, only the English descriptions were used. WMT16: A dataset used for machine translation tasks. For our experiments, only the English part of the test set was used. Yelp: A dataset containing reviews of users for businesses on Yelp.</p><p>EWT: The English Web Treebank (EWT) consists of 5 different datasets: weblogs (EWT-W), newsgroups (EWT-N), emails (EWT-E), reviews (EWT-R) and questions-answers (EWT-A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 VALIDATION DATA FOR NLP EXPERIMENTS</head><p>The validation dataset D val out used for the NLP OOD detection experiments was constructed as follows. For each D in dataset used, we used the rest two in-distribution datasets as D  TinyImageNet: This dataset consists of 10,000 test images with 200 image classes from a subset of ImageNet images. Similar to <ref type="bibr" target="#b35">Lee et al. (2018b)</ref> and Sastry &amp; Oore (2020), we downsized the data to 32 × 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSUN:</head><p>In these experiments, we only used 10,000 test images of 10 different scenes. Similar to <ref type="bibr" target="#b35">Lee et al. (2018b)</ref> and Sastry &amp; Oore (2020), we downsized the data to 32 × 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 VALIDATION DATA FOR IMAGE EXPERIMENTS</head><p>For the results of <ref type="table" target="#tab_6">Table 4</ref>, similar to <ref type="bibr" target="#b35">Lee et al. (2018b)</ref>, the validation set consists of 1,000 images from each in-and out-of-distribution pair.</p><p>For the results of <ref type="table">Table 5</ref> and <ref type="table" target="#tab_10">Table 6</ref>, the validation data used were similar to the ones described in A.3. Note that D val out and D test out are disjoint.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>True Negative Rate at N % True Positive Rate (TNRN): This performance metric measures the capability of an OOD detector to detect true negative examples when the true positive rate is set to 95%. • Area Under the Receiver Operating Characteristic curve (AUROC): Similar to Section 4.1.1. • Detection Accuracy (DAcc): This evaluation metric corresponds to the maximum classification accuracy that we can achieve between in-and out-of-distribution examples over all possible thresholds :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>OE out . During the test phase, we evaluate the OOD detection capability of the neural network using examples sampled from D test out , where D OE out and D test out are disjoint.</figDesc><table /><note>, where samples from some classes are not available during training. Our task is to design deep neural network classifiers that can achieve high accuracy on examples generated by a learned probability distribution called D in while at the same time, they can effectively detect examples generated by a different probability distribution called D out during the test phase. The examples generated by D in are called in-distribution while the examples generated by D out are called out-of-distribution (OOD). Adopting the idea of Outlier Exposure (OE) (Hendrycks et al., 2019), we train the neural network using training examples sampled from D in and DIn previous works</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>97.81 98.40 90.48 93.08 CIFAR-100 38.50 28.89 87.89 91.80 58.15 71.50 Table 1: Image OOD example detection for the maximum softmax probability (MSP) baseline detector after fine-tuning with OE (Hendrycks et al., 2019) versus fine-tuning with OECC given by (3). All results are percentages and averaged over 10 runs and over 8 OOD datasets. Detailed experimental results are shown in Appendix A.1. The detailed description of the image datasets used as D in , D OE out and D test out in the image OOD detection experiments is presented in A.2. The validation data D val out used to tune the hyperparameters λ 1 and λ 2 for image classification experiments are presented in A.3. Note that D val out</figDesc><table><row><cell></cell><cell cols="2">FPR95↓</cell><cell>AUROC↑</cell><cell>AUPR↑</cell></row><row><cell>D in</cell><cell cols="4">+OE OECC +OE OECC +OE OECC</cell></row><row><cell>SVHN</cell><cell>0.10</cell><cell>0.03</cell><cell cols="2">99.98 99.99 99.83 99.55</cell></row><row><cell>CIFAR-10</cell><cell>9.50</cell><cell>6.56</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>and D test out are</cell></row><row><cell>disjoint.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Figure 1: Histogram of soft-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>max probabilities with CIFAR-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>10 as D in and Places365 as D test out (1,000 samples from</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>each dataset). Top: MSP base-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>line detector. Bottom: MSP de-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>tector fine-tuned with (3).</cell></row></table><note>Network Architecture and Training Details. Similar to</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>also demonstrates that our method can significantly improve the OOD detection performance of the DNN compared to the case where only the crossentropy loss is minimized at the expense of only an insignificant degradation in the test accuracy of the DNN on examples generated by D in . 4.1.3 TEXT CLASSIFICATION EXPERIMENTS Results. The results of the text classification experiments are shown in Table 3. The detailed description of the text datasets used as D in , D OE out and D test out in the NLP OOD detection experiments D in λ 1 λ 2 FPR95↓ AUROC↑ AUPR↑ Test Accuracy(D in )</figDesc><table><row><cell></cell><cell>-</cell><cell>-</cell><cell>34.94</cell><cell>89.27</cell><cell>59.16</cell><cell>94.65</cell></row><row><cell>CIFAR-10</cell><cell>-</cell><cell></cell><cell>8.87</cell><cell>96.72</cell><cell>77.65</cell><cell>92.72</cell></row><row><cell></cell><cell></cell><cell></cell><cell>6.56</cell><cell>98.40</cell><cell>93.08</cell><cell>93.86</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>62.66</cell><cell>73.11</cell><cell>30.05</cell><cell>75.73</cell></row><row><cell>CIFAR-100</cell><cell>-</cell><cell></cell><cell>26.75</cell><cell>91.59</cell><cell>68.27</cell><cell>71.29</cell></row><row><cell></cell><cell></cell><cell></cell><cell>28.89</cell><cell>91.80</cell><cell>71.50</cell><cell>73.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Contribution of each regularization term of (3) on the OOD detection performance and the test accuracy of the DNN. Results are averaged over 10 runs and over 8 OOD datasets.is presented in B.1. The validation data D val out used to tune the hyperparameters λ 1 and λ 2 for image classification experiments are presented in B.2. Note that D val out and D test out are disjoint.</figDesc><table><row><cell></cell><cell cols="2">FPR90↓</cell><cell>AUROC↑</cell><cell>AUPR↑</cell></row><row><cell>D in</cell><cell cols="4">+OE OECC +OE OECC +OE OECC</cell></row><row><cell cols="2">20 Newsgroups 4.86</cell><cell>0.63</cell><cell cols="2">97.71 99.18 91.91 97.02</cell></row><row><cell>TREC</cell><cell>0.78</cell><cell>0.75</cell><cell cols="2">99.28 99.32 97.64 97.52</cell></row><row><cell>SST</cell><cell>27.33</cell><cell></cell><cell></cell></row></table><note>17.91 89.27 93.79 59.23 74.10</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>NLP OOD example detection for the maximum softmax probability (MSP) baseline de- tector after fine-tuning with OE (Hendrycks et al., 2019) versus fine-tuning with OECC given by (3). All results are percentages and averaged over 10 runs and over 10 OOD datasets. Detailed experimental results are shown in Appendix B.3. Network Architecture and Training Details. For all text classification experiments, similar to Hendrycks et al. (2019), we train 2-layer GRUs</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>with 34 layers using CIFAR-10, CIFAR-100, and SVHN datasets as D in . For the CIFAR experiments, SVHN, TinyImageNet (a sample of 10,000 images drawn from the ImageNet dataset) and LSUN are used as D test out . For the SVHN experiments, CIFAR-10, TinyImageNet, and LSUN are used as D test out . Both TinyImageNet and LSUN images are downsampled to 32 × 32.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>TNR95↑</cell><cell></cell><cell>AUROC↑</cell><cell></cell><cell>DAcc↑</cell><cell></cell><cell>AUPRin↑</cell><cell cols="2">AUPRout↑</cell></row><row><cell>D in</cell><cell>D test out</cell><cell cols="10">MD OECC+MD MD OECC+MD MD OECC+MD MD OECC+MD MD OECC+MD</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>98.4</cell><cell>99.9</cell><cell>99.3</cell><cell>99.9</cell><cell>96.9</cell><cell>99.2</cell><cell>99.7</cell><cell>100.0</cell><cell>97.0</cell><cell>99.6</cell></row><row><cell>SVHN</cell><cell cols="2">TinyImageNet 99.9</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0</cell><cell>99.1</cell><cell>99.9</cell><cell>99.9</cell><cell>100.0</cell><cell>99.1</cell><cell>100.0</cell></row><row><cell></cell><cell>LSUN</cell><cell>99.9</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0</cell><cell>99.5</cell><cell>100.0</cell><cell>99.9</cell><cell>100.0</cell><cell>99.1</cell><cell>100.0</cell></row><row><cell></cell><cell>SVHN</cell><cell>96.4</cell><cell>97.3</cell><cell>99.1</cell><cell>99.2</cell><cell>95.8</cell><cell>96.3</cell><cell>98.3</cell><cell>98.4</cell><cell>99.6</cell><cell>99.6</cell></row><row><cell>CIFAR-10</cell><cell cols="2">TinyImageNet 97.1</cell><cell>98.8</cell><cell>99.5</cell><cell>99.6</cell><cell>96.3</cell><cell>97.3</cell><cell>99.5</cell><cell>99.4</cell><cell>99.5</cell><cell>99.6</cell></row><row><cell></cell><cell>LSUN</cell><cell>98.9</cell><cell>99.7</cell><cell>99.7</cell><cell>99.8</cell><cell>97.7</cell><cell>98.5</cell><cell>99.7</cell><cell>99.5</cell><cell>99.7</cell><cell>99.8</cell></row><row><cell></cell><cell>SVHN</cell><cell>91.9</cell><cell>93.0</cell><cell>98.4</cell><cell>98.7</cell><cell>93.7</cell><cell>94.2</cell><cell>96.4</cell><cell>97.1</cell><cell>99.3</cell><cell>99.5</cell></row><row><cell>CIFAR-100</cell><cell cols="2">TinyImageNet 90.9</cell><cell>92.3</cell><cell>98.2</cell><cell>98.3</cell><cell>93.3</cell><cell>93.9</cell><cell>98.2</cell><cell>98.3</cell><cell>98.2</cell><cell>98.3</cell></row><row><cell></cell><cell>LSUN</cell><cell>90.9</cell><cell>95.6</cell><cell>98.2</cell><cell>98.6</cell><cell>93.5</cell><cell>95.4</cell><cell>98.4</cell><cell>98.4</cell><cell>97.8</cell><cell>98.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison between the Mahalanobis Detector (MD)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>extracts the features from all layers of a pre-trained softmax neural classifier and then calculates the Mahalanobis distance-based confidence</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>TNR95↑</cell><cell></cell><cell>AUROC↑</cell><cell></cell><cell>DAcc↑</cell><cell></cell><cell>AUPRin↑</cell><cell cols="2">AUPRout↑</cell></row><row><cell>D in</cell><cell>D test out</cell><cell cols="10">GM OECC+GM GM OECC+GM GM OECC+GM GM OECC+GM GM OECC+GM</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>80.4</cell><cell>98.5</cell><cell>95.5</cell><cell>99.6</cell><cell>89.1</cell><cell>97.5</cell><cell>89.6</cell><cell>98.6</cell><cell>97.8</cell><cell>99.8</cell></row><row><cell>SVHN</cell><cell cols="2">TinyImageNet 99.1</cell><cell>99.9</cell><cell>99.7</cell><cell>100.0</cell><cell>97.9</cell><cell>99.7</cell><cell>99.3</cell><cell>99.9</cell><cell>99.9</cell><cell>100.0</cell></row><row><cell></cell><cell>LSUN</cell><cell>99.5</cell><cell>100.0</cell><cell>99.8</cell><cell>100.0</cell><cell>98.6</cell><cell>99.9</cell><cell>99.5</cell><cell>99.9</cell><cell>99.9</cell><cell>100.0</cell></row><row><cell></cell><cell>SVHN</cell><cell>96.1</cell><cell>98.5</cell><cell>99.1</cell><cell>99.6</cell><cell>95.9</cell><cell>97.4</cell><cell>96.8</cell><cell>98.6</cell><cell>99.7</cell><cell>99.9</cell></row><row><cell>CIFAR-10</cell><cell cols="2">TinyImageNet 98.8</cell><cell>99.3</cell><cell>99.7</cell><cell>99.8</cell><cell>97.9</cell><cell>98.3</cell><cell>99.6</cell><cell>99.7</cell><cell>99.8</cell><cell>99.8</cell></row><row><cell></cell><cell>LSUN</cell><cell>99.5</cell><cell>99.8</cell><cell>99.9</cell><cell>99.9</cell><cell>98.6</cell><cell>99.0</cell><cell>99.9</cell><cell>99.9</cell><cell>99.9</cell><cell>99.9</cell></row><row><cell></cell><cell>SVHN</cell><cell>89.3</cell><cell>88.3</cell><cell>97.3</cell><cell>96.9</cell><cell>92.4</cell><cell>91.9</cell><cell>91.7</cell><cell>91.1</cell><cell>99.1</cell><cell>98.5</cell></row><row><cell>CIFAR-100</cell><cell cols="2">TinyImageNet 95.7</cell><cell>96.1</cell><cell>99.0</cell><cell>99.0</cell><cell>95.5</cell><cell>95.7</cell><cell>98.8</cell><cell>98.9</cell><cell>99.1</cell><cell>98.4</cell></row><row><cell></cell><cell>LSUN</cell><cell>97.2</cell><cell>98.1</cell><cell>99.3</cell><cell>99.3</cell><cell>96.4</cell><cell>96.9</cell><cell>99.3</cell><cell>99.4</cell><cell>99.4</cell><cell>98.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>To calculate ECE, we first calculate the average confidence of the DNN for a selected number of examples x i sampled from D test in and then we partition the examples into equally-spaced bins {B m } M m=1 based on the output confidence of the DNN. Then, ECE is given by the following equation: | is the number of examples in that bin, n is the total number of examples, acc(B m ) is the average classification accuracy of the DNN on the examples in B m and conf (B m ) is the average confidence of the predictions made by the DNN for the examples in B m . Using the same definitions, MCE is given by the following equation:</figDesc><table><row><cell>ECE =</cell><cell>M m=1</cell><cell>|B m | n</cell><cell>acc(B m ) − conf (B m )</cell><cell>(5)</cell></row><row><cell>where |B m</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>and the Gramian Matrices method (GM) (Sastry &amp; Oore, 2020) demonstrating the desirability of combination of training and post-training methods for OOD detection in the future research efforts. A EXPANDED IMAGE OOD DETECTION RESULTS AND DATASETS USED FOR COMPARISON WITH STATE-OF-THE-ART IN OE</figDesc><table><row><cell cols="3">A.1 IMAGE OOD DETECTION RESULTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">FPR95↓</cell><cell cols="2">AUROC↑</cell><cell cols="2">AUPR↑</cell></row><row><cell cols="2">D in D test out</cell><cell cols="6">+OE OECC +OE OECC +OE OECC</cell></row><row><cell></cell><cell>Gaussian</cell><cell>0.0</cell><cell>0.0</cell><cell>100.</cell><cell>100.</cell><cell>100.</cell><cell>99.4</cell></row><row><cell></cell><cell>Bernulli</cell><cell>0.0</cell><cell>0.0</cell><cell>100.</cell><cell>100.</cell><cell>100.</cell><cell>99.2</cell></row><row><cell>SVHN</cell><cell>Blobs Icons-50 Textures Places365</cell><cell>0.0 0.3 0.2 0.1</cell><cell>0.0 0.1 0.1 0.0</cell><cell>100. 99.8 100. 100.</cell><cell>100. 99.9 100. 100.</cell><cell>100. 99.2 99.7 99.9</cell><cell>99.6 99.5 99.6 99.7</cell></row><row><cell></cell><cell>LSUN</cell><cell>0.1</cell><cell>0.0</cell><cell>100.</cell><cell>100.</cell><cell>99.9</cell><cell>99.7</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>0.1</cell><cell>0.0</cell><cell>100.</cell><cell>100.</cell><cell>99.9</cell><cell>99.7</cell></row><row><cell></cell><cell>Mean</cell><cell>0.10</cell><cell>0.03</cell><cell cols="4">99.98 99.99 99.83 99.55</cell></row><row><cell></cell><cell>Gaussian</cell><cell>0.7</cell><cell>0.7</cell><cell>99.6</cell><cell>99.8</cell><cell>94.3</cell><cell>99.0</cell></row><row><cell></cell><cell>Rademacher</cell><cell>0.5</cell><cell>1.1</cell><cell>99.8</cell><cell>99.6</cell><cell>97.4</cell><cell>97.6</cell></row><row><cell>CIFAR-10</cell><cell>Blobs Textures SVHN Places365</cell><cell>0.6 12.2 4.8 17.3</cell><cell>1.5 4.0 1.4 13.3</cell><cell>99.8 97.7 98.4 96.2</cell><cell>99.1 98.9 99.6 96.9</cell><cell>98.9 91.0 89.4 87.3</cell><cell>91.7 95.0 97.9 89.5</cell></row><row><cell></cell><cell>LSUN</cell><cell>12.1</cell><cell>6.7</cell><cell>97.6</cell><cell>98.4</cell><cell>89.4</cell><cell>91.9</cell></row><row><cell></cell><cell>CIFAR-100</cell><cell>28.0</cell><cell>23.8</cell><cell>93.3</cell><cell>94.9</cell><cell>76.2</cell><cell>82.0</cell></row><row><cell></cell><cell>Mean</cell><cell>9.50</cell><cell>6.56</cell><cell cols="4">97.81 98.40 90.48 93.08</cell></row><row><cell></cell><cell>Gaussian</cell><cell>12.1</cell><cell>0.7</cell><cell>95.7</cell><cell>99.7</cell><cell>71.1</cell><cell>97.2</cell></row><row><cell>CIFAR-100</cell><cell cols="2">Rademacher 17.1 Blobs 12.1 Textures 54.4 SVHN 42.9 Places365 49.8 LSUN 57.5</cell><cell>0.7 1.3 50.1 16.7 47.8 56.6</cell><cell>93.0 97.2 84.8 86.9 86.5 83.4</cell><cell>99.7 99.6 87.8 94.9 88.1 85.9</cell><cell>56.9 86.2 56.3 52.9 57.9 51.4</cell><cell>96.2 96.3 61.5 74.1 58.5 53.0</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>62.1</cell><cell>57.2</cell><cell>75.7</cell><cell>78.7</cell><cell>32.6</cell><cell>35.2</cell></row><row><cell></cell><cell>Mean</cell><cell cols="6">38.50 28.89 87.89 91.80 58.15 71.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Image OOD example detection for the maximum softmax probability (MSP) baseline detector after fine-tuning with OE<ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref> versus fine-tuning with OECC given by (3). All results are percentages and averaged over 10 runs. Values are rounded to the first decimal digit. As also mentioned before, for these results, no access to OOD samples was assumed.A.2 D in , D OEout AND D test out FOR IMAGE EXPERIMENTS SVHN: The Street View House Number (SVHN) dataset (Netzer et al., 2011) consists of 32 × 32 color images out of which 604,388 are used for training and 26,032 are used for testing. The dataset has 10 classes and was collected from real Google Street View images. Similar to Hendrycks et al. (2019), we rescale the pixels of the images to be in [0, 1]. CIFAR 10: This dataset (Krizhevsky &amp; Hinton, 2009) contains 10 classes and consists of 60,000 32 × 32 color images out of which 50,000 belong to the training and 10,000 belong to the test set. Before training, we standardize the images per channel similar to Hendrycks et al. (2019). CIFAR 100: This dataset<ref type="bibr" target="#b30">(Krizhevsky &amp; Hinton, 2009</ref>) consists of 20 distinct superclasses each of which contains 5 different classes giving us a total of 100 classes. The total number of images in the dataset are 60,000 and we use the standard 50,000/10,000 train/test split. Before training, we standardize the images per channel similar to<ref type="bibr" target="#b23">Hendrycks et al. (2019)</ref>. 80 Million Tiny Images: The 80 Million Tiny Images dataset<ref type="bibr" target="#b53">(Torralba et al., 2008)</ref> was exclusively used in our experiments in order to represent D OE out . It consists of 32 × 32 color images collected from the Internet. Similar to<ref type="bibr" target="#b23">Hendrycks et al. (2019)</ref>, in order to make sure that D OE out and D test out are B EXPANDED TEXT OOD DETECTION RESULTS AND DATASETS USED FOR COMPARISON WITH STATE-OF-THE-ART IN OE B.1 D in , D OE out AND D test out FOR NLP EXPERIMENTS 20 Newsgroups: This dataset contains 20 different newsgroups, each corresponding to a specific topic. It contains around 19,000 examples and we used the standard 60/40 train/test split. TREC: A question classification dataset containing around 6,000 examples from 50 different classes. Similar to Hendrycks et al. (2019), we used 500 examples for the test phase and the rest for training.</figDesc><table /><note>SST: The Stanford Sentiment Treebank</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>val out . For instance, during the experiments where 20 Newsgroups represented D in , we used TREC and SST as D val out making sure that D val out and D test out are disjoint. Mean 27.33 17.91 89.27 93.79 59.23 74.10</figDesc><table><row><cell cols="3">B.3 TEXT OOD DETECTION RESULTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">FPR90↓</cell><cell cols="2">AUROC↑</cell><cell cols="2">AUPR↑</cell></row><row><cell cols="2">D in D test out</cell><cell cols="6">+OE OECC +OE OECC +OE OECC</cell></row><row><cell></cell><cell>SNLI</cell><cell>12.5</cell><cell>2.1</cell><cell>95.1</cell><cell>97.1</cell><cell>86.3</cell><cell>93.0</cell></row><row><cell>20 Newsgroups</cell><cell>IMDB Multi30K WMT16 Yelp EWT-A EWT-E</cell><cell>18.6 3.2 2.0 3.9 1.2 1.4</cell><cell>2.5 0.1 0.2 0.4 0.2 0.1</cell><cell>93.5 97.3 98.8 97.8 99.2 99.2</cell><cell>98.2 99.4 99.8 99.6 99.8 99.9</cell><cell>74.5 93.7 96.1 87.9 97.3 97.2</cell><cell>92.9 98.6 99.4 97.9 98.4 98.9</cell></row><row><cell></cell><cell>EWT-N</cell><cell>1.8</cell><cell>0.5</cell><cell>98.7</cell><cell>99.2</cell><cell>95.7</cell><cell>94.5</cell></row><row><cell></cell><cell>EWT-R</cell><cell>1.7</cell><cell>0.1</cell><cell>98.9</cell><cell>99.4</cell><cell>96.6</cell><cell>98.3</cell></row><row><cell></cell><cell>EWT-W</cell><cell>2.4</cell><cell>0.1</cell><cell>98.5</cell><cell>99.4</cell><cell>93.8</cell><cell>98.3</cell></row><row><cell></cell><cell>Mean</cell><cell>4.86</cell><cell>0.63</cell><cell cols="4">97.71 99.18 91.91 97.02</cell></row><row><cell></cell><cell>SNLI</cell><cell>4.2</cell><cell>0.8</cell><cell>98.1</cell><cell>99.1</cell><cell>91.6</cell><cell>94.9</cell></row><row><cell></cell><cell>IMDB</cell><cell>0.6</cell><cell>0.6</cell><cell>99.4</cell><cell>98.9</cell><cell>97.8</cell><cell>97.1</cell></row><row><cell>TREC</cell><cell>Multi30K WMT16 Yelp EWT-A</cell><cell>0.3 0.2 0.4 0.9</cell><cell>0.2 0.2 0.8 4.0</cell><cell>99.7 99.8 99.7 97.7</cell><cell>99.9 99.9 99.1 98.0</cell><cell>99.0 99.4 96.1 96.1</cell><cell>99.6 99.6 92.9 95.6</cell></row><row><cell></cell><cell>EWT-E</cell><cell>0.4</cell><cell>0.3</cell><cell>99.5</cell><cell>99.2</cell><cell>99.1</cell><cell>98.1</cell></row><row><cell></cell><cell>EWT-N</cell><cell>0.3</cell><cell>0.2</cell><cell>99.6</cell><cell>99.9</cell><cell>99.2</cell><cell>99.6</cell></row><row><cell></cell><cell>EWT-R</cell><cell>0.4</cell><cell>0.2</cell><cell>99.5</cell><cell>99.6</cell><cell>98.8</cell><cell>98.9</cell></row><row><cell></cell><cell>EWT-W</cell><cell>0.2</cell><cell>0.2</cell><cell>99.7</cell><cell>99.6</cell><cell>99.4</cell><cell>98.9</cell></row><row><cell></cell><cell>Mean</cell><cell>0.78</cell><cell>0.75</cell><cell cols="4">99.28 99.32 97.64 97.52</cell></row><row><cell></cell><cell>SNLI</cell><cell>33.4</cell><cell>7.4</cell><cell>86.8</cell><cell>95.8</cell><cell>52.0</cell><cell>76.4</cell></row><row><cell></cell><cell>IMDB</cell><cell>32.6</cell><cell>10.8</cell><cell>85.9</cell><cell>95.8</cell><cell>51.5</cell><cell>77.6</cell></row><row><cell></cell><cell cols="2">Multi30K 33.0</cell><cell>5.1</cell><cell>88.3</cell><cell>97.9</cell><cell>58.9</cell><cell>86.9</cell></row><row><cell>SST</cell><cell>WMT16 Yelp</cell><cell>17.1 11.3</cell><cell>3.6 15.6</cell><cell>92.9 92.7</cell><cell>98.3 95.2</cell><cell>68.8 60.0</cell><cell>88.1 81.1</cell></row><row><cell></cell><cell>EWT-A</cell><cell>33.6</cell><cell>21.4</cell><cell>87.2</cell><cell>92.7</cell><cell>53.8</cell><cell>70.8</cell></row><row><cell></cell><cell>EWT-E</cell><cell>26.5</cell><cell>22.6</cell><cell>90.4</cell><cell>92.4</cell><cell>63.7</cell><cell>67.7</cell></row><row><cell></cell><cell>EWT-N</cell><cell>27.2</cell><cell>19.2</cell><cell>90.1</cell><cell>93.6</cell><cell>62.0</cell><cell>67.4</cell></row><row><cell></cell><cell>EWT-R</cell><cell>41.4</cell><cell>36.7</cell><cell>85.6</cell><cell>88.1</cell><cell>54.7</cell><cell>62.5</cell></row><row><cell></cell><cell>EWT-W</cell><cell>17.2</cell><cell>36.7</cell><cell>92.8</cell><cell>88.1</cell><cell>66.9</cell><cell>62.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>NLP OOD example detection for the maximum softmax probability (MSP) baseline detector after fine-tuning with OE<ref type="bibr" target="#b23">(Hendrycks et al., 2019)</ref> versus fine-tuning with OECC given by (3). All results are percentages and the result of 10 runs. Values are rounded to the first decimal digit. As also mentioned before, for these results, no access to OOD samples was assumed.C IMAGE DATASETS USED FOR COMBINATION OF OECC WITH POST-TRAINING METHODS C.1 D in , D OE out AND D test out FOR IMAGE EXPERIMENTS SVHN: In these experiments, we only used 73,257 training and 26,032 test images, i.e. in contrast with the experiments where we compared with the OE method, we did not use the extra SVHN dataset for training. CIFAR-10: Similar to the description in A.2. CIFAR-100: Similar to the description in A.2. 80 Million Tiny Images: Similar to the description in A.2.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our code is publicly available at https://github.com/nazim1021/OOD-detection-using-OECC.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Google for donating Google Cloud Platform research credits used in this research.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors with triplets and shallow convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards open world recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Discriminative out-of-distribution detection for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krešo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Oršić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinišašegvić</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07703</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Convex Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Robust out-of-distribution detection via informative outlier mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15207</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01392</idno>
		<title level="m">Waic, but why? generative ensembles for robust anomaly detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Describing textures in the wild</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The relationship between precision-recall and roc curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Goadrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Encyclopedia of Distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Deza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.00459</idno>
		<title level="m">Multi30K: Multilingual English-German Image Descriptions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised out-of-distribution detection using kernel density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ertunc</forename><surname>Erdil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ender</forename><surname>Konukoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10712</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On choosing and bounding probability metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><forename type="middle">L</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">Edward</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="419" to="435" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Elements of Statistical Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer Series in Statistics</title>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer New York Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Why relu networks yield highconfidence predictions far away from the training data and how to mitigate the problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Bitterwolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01697</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generalized odin: Detecting out-ofdistribution image without learning from out-of-distribution data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE COnference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE COnference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Kliger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shachar</forename><surname>Fleishman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10560</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Novelty Detection with GAN. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning Local Image Descriptors with Deep Siamese and Triplet Convolutional Networks by Minimising Global Loss Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enhancing The Reliability of Out-of-distribution Image Detection in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Open Category Detection with PAC Guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risheek</forename><surname>Garrepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00529</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Foundations of Statistical Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards neural networks that provably know when they don&apos;t know</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Meinke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-supervised learning for generalizable out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Mohseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Pitale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Yadawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Density of states estimation for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><forename type="middle">R</forename><surname>Morningstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cusuh</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09273</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Obtaining well calibrated probabilities using bayesian binning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Mahdi Pakdaman Naeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milos</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilan</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Mai Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conferece on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Likelihood Ratios for Out-of-Distribution Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02845</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Detecting out-of-distribution examples with gram matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shama</forename><surname>Chandramouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sageev</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on empirical methods in natural language processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudy</forename><surname>Bunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit Guha</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Macwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylan</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05566</idno>
		<title level="m">Contrastive training for improved out-of-distribution detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Open-Category Classification by Adversarial Sample Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yang</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimin</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08722</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">06</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep residual flow for out of distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ev</forename><surname>Zisselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">) was exclusively used in our experiments in order to represent D test out</title>
	</analytic>
	<monogr>
		<title level="m">disjoint, we removed all the images of the dataset that appear on CIFAR 10 and CIFAR 100 datasets. Places365: Places365 dataset introduced by</title>
		<editor>Zhou et al.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>It consists of millions of photographs of scenes</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Gaussian: A synthetic image dataset created by i.i.d. sampling from an isotropic Gaussian distribution. Bernoulli: A synthetic image dataset created by sampling from a Bernoulli distribution. Blobs: A synthetic dataset of images with definite edges</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">consists of 10,000 images belonging to 50 classes of icons. As part of preprocessing, we removed the class &quot;Number&quot; in order to make it disjoint from the SVHN dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cimpoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Textures: This dataset contains 5,640 textural images</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>This dataset intoduced by Hendrycks &amp; Dietterich</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">LSUN: It consists of around 1 million large-scale images of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Rademacher: A synthetic image dataset created by sampling from a symmetric Rademacher distribution</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">VALIDATION DATA FOR IMAGE EXPERIMENTS Uniform Noise: A synthetic image dataset where each pixel is sampled from</title>
		<imprint/>
	</monogr>
	<note>0, 1] or U[−1, 1] depending on the input space of the classifier</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Geometric Mean: A synthetic image dataset created by randomly sampling a pair of in-distribution images and subsequently taking their pixelwise geometric mean. Jigsaw: A synthetic image dataset created by partitioning an image sampled from D in into 16 equally sized patches and by subsequently permuting those patches</title>
		<imprint/>
	</monogr>
	<note>Arithmetic Mean: A synthetic image dataset created by randomly sampling a pair of in-distribution images and subsequently taking their pixelwise arithmetic mean</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Speckle Noised: A synthetic image dataset created by applying speckle noise to images sampled from D in . Inverted Images: A synthetic image dataset created by shifting and reordering the color channels of images sampled from D in</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">A synthetic image dataset created by inverting the color channels of images sampled from D in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rgb Ghosted</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
