<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Task Graph Autoencoders 1 Autoencoder Architecture for Link Prediction and Node Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-11-07">7 Nov 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phi</forename><surname>Vu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Strategic Innovation Group Booz</orgName>
								<address>
									<settlement>Allen | Hamilton San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Strategic Innovation Group Booz</orgName>
								<address>
									<settlement>Allen | Hamilton San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Task Graph Autoencoders 1 Autoencoder Architecture for Link Prediction and Node Classification</title>
					</analytic>
					<monogr>
						<title level="m">Workshop on Relational Representation Learning, NIPS 2018</title>
						<meeting> <address><addrLine>Montréal, Canada</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2018-11-07">7 Nov 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the world is becoming increasingly interconnected, relational data are also growing in ubiquity. In this work, we examine the task of learning to make predictions on graphs for a broad range of real-world applications. Specifically, we study two canonical subtasks associated with relational, graph-structured datasets: link prediction and node classification (LPNC). A graph is a partially observed set of edges and nodes (or vertices), and the learning task is to predict the labels for edges and nodes. In real-world applications, the input graph is a network with nodes representing unique entities and edges representing relationships (or links) between entities. Further, the labels of nodes and edges in a graph are often correlated, exhibiting complex relational structures that violate the general assumption of independent and identical distribution fundamental in traditional machine learning <ref type="bibr" target="#b4">[5]</ref>. Therefore, models capable of exploiting topological structures of graphs have been shown to achieve superior predictive performances on many LPNC tasks <ref type="bibr" target="#b9">[10]</ref>.</p><p>We introduce the Multi-Task Graph Autoencoder (MTGAE) architecture, schematically depicted in <ref type="figure">Figure 1</ref>, capable of learning a shared representation of latent node embeddings from local graph topology and available explicit node features for LPNC. Our simple, yet effective and versatile model is efficiently trained end-to-end for the joint, simultaneous multi-task learning (MTL) of unsupervised link prediction and semi-supervised node classification in a single stage, whereas previous related deep graph embedding methods require multiple training steps that are difficult to optimize. We present an empirical evaluation of the MTGAE model on five challenging benchmark graph-structured datasets and show significant improvement in predictive performance over three strong baselines designed specifically for LPNC. Reference code and data are available at https://github.com/vuptran/graph-representation-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adjacency Vector</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Sharing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Sharing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Graph Symmetrical Autoencoder</head><p>Last layer reconstructs neighborhood for unsupervised link prediction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Penultimate layer decodes node label for semi-supervised classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Prediction</head><p>In [9]:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Link Prediction</head><p>Link prediction attempts to answer the principal question: given two entities, should there be a link between them? One can view link prediction as a graph/matrix completion problem, where the goal is to predict missing links using data from known, observed positive and negative links. We approach the task of link prediction through two stages of supervised machine learning: matrix factorization and linear (multiclass) classification. Matrix factorization learns and extracts low dimensional latent features from the global topology of the graph. A linear classifier can combine latent features with observed features on graph nodes and edges to learn a decision function that can predict link propensity for any pair of nodes in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Out[9]:</head><p># compute 10 community clusters using the Girvan-Newman edge betweenness community detection algorithm community = g.community_edge_betweenness(clusters=10).as_clustering() ig.plot(community, layout=layout, vertex_size=20, edge_arrow_width=0.75, edge_arrow_size=0.75) <ref type="figure">Figure 1</ref>: Schematic depiction of the Multi-Task Graph Autoencoder (MTGAE) architecture. Left: A partially observed graph with positive links (solid lines) and negative links (dashed lines) between two nodes; pairs of nodes not yet connected have unknown status links. Middle: A symmetrical, densely connected autoencoder with parameter sharing is trained end-to-end to learn node embeddings from the adjacency vector. Right: Exemplar multi-task output for link prediction and node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Learning</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Autoencoder Architecture for Link Prediction and Node Classification</head><p>As the world is becoming increasingly interconnected, relational data are also growing in ubiquity. In this work, we examine the task of learning to make predictions on graphs for a broad range of real-world applications. Specifically, we study two canonical subtasks associated with relational, graph-structured datasets: link prediction and node classification (LPNC). A graph is a partially observed set of edges and nodes (or vertices), and the learning task is to predict the labels for edges and nodes. In real-world applications, the input graph is a network with nodes representing unique entities and edges representing relationships (or links) between entities. Further, the labels of nodes and edges in a graph are often correlated, exhibiting complex relational structures that violate the general assumption of independent and identical distribution fundamental in traditional machine learning <ref type="bibr" target="#b4">[5]</ref>. Therefore, models capable of exploiting topological structures of graphs have been shown to achieve superior predictive performances on many LPNC tasks <ref type="bibr" target="#b9">[10]</ref>.</p><p>We introduce the Multi-Task Graph Autoencoder (MTGAE) architecture, schematically depicted in <ref type="figure">Figure 1</ref>, capable of learning a shared representation of latent node embeddings from local graph topology and available explicit node features for LPNC. Our simple, yet effective and versatile model is efficiently trained end-to-end for the joint, simultaneous multi-task learning (MTL) of unsupervised link prediction and semi-supervised node classification in a single stage, whereas previous related deep graph embedding methods require multiple training steps that are difficult to optimize. We present an empirical evaluation of the MTGAE model on five challenging benchmark graph-structured datasets and show significant improvement in predictive performance over three strong baselines designed specifically for LPNC. Reference code and data are available at https://github.com/vuptran/graph-representation-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adjacency Vector</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Sharing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Sharing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Graph Symmetrical Autoencoder</head><p>Last layer reconstructs neighborhood for unsupervised link prediction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Penultimate layer decodes node label for semi-supervised classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Prediction</head><p>In <ref type="bibr" target="#b8">[9]</ref>:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Link Prediction</head><p>Link prediction attempts to answer the principal question: given two entities, should there be a link between them? One can view link prediction as a graph/matrix completion problem, where the goal is to predict missing links using data from known, observed positive and negative links. We approach the task of link prediction through two stages of supervised machine learning: matrix factorization and linear (multiclass) classification. Matrix factorization learns and extracts low dimensional latent features from the global topology of the graph. A linear classifier can combine latent features with observed features on graph nodes and edges to learn a decision function that can predict link propensity for any pair of nodes in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Out[9]:</head><p># compute 10 community clusters using the Girvan-Newman edge betweenness community detection algorithm community = g.community_edge_betweenness(clusters=10).as_clustering() ig.plot(community, layout=layout, vertex_size=20, edge_arrow_width=0.75, edge_arrow_size=0.75) <ref type="figure">Figure 1</ref>: Schematic depiction of the Multi-Task Graph Autoencoder (MTGAE) architecture. Left: A partially observed graph with positive links (solid lines) and negative links (dashed lines) between two nodes; pairs of nodes not yet connected have unknown status links. Middle: A symmetrical, densely connected autoencoder with parameter sharing is trained end-to-end to learn node embeddings from the adjacency vector. Right: Exemplar multi-task output for link prediction and node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Formulation and Notation</head><p>The input to the MTGAE model is a graph G = (V, E) of N = |V| nodes. Graph G is represented by its adjacency matrix A ∈ R N ×N paired with a unique ordering of vertices. For a partially observed graph, A ∈ {1, 0, UNK} N ×N , where 1 denotes a known positive edge, 0 denotes a known negative edge, and UNK denotes an unknown status (missing or unobserved) edge. In general, the input to the model can be directed or undirected, weighted or unweighted, and/or bipartite graphs.</p><p>Optionally, we are given a matrix of available node features, i.e. side information X ∈ R N ×F . The aim of the MTGAE model h(A, X) is to learn a set of low-dimensional latent variables for the nodes Z ∈ R N ×D that can produce an approximate reconstruction outputÂ such that the empirical error between A andÂ is minimized, thereby preserving the global graph structure. In this paper, we use capital variables (e.g., A) to denote matrices and lower-case variables (e.g., a) to denote row vectors. For example, we use a i to mean the ith row of the matrix A.</p><p>Unsupervised Link Prediction Let a i ∈ R N be an adjacency vector of A that contains the local neighborhood of the ith node. Our proposed MTGAE architecture comprises a set of non-linear transformations on a i summarized in two component parts: encoder g(a i ) :</p><formula xml:id="formula_0">R N → R D and decoder f (g (a i )) : R D → R N .</formula><p>We stack two layers of the encoder part to derive D-dimensional latent feature representation of the ith node z i ∈ R D , and then stack two layers of the decoder part to obtain an approximate reconstruction outputâ i ∈ R N , resulting in a four-layer autoencoder architecture. Note that a i is highly sparse, with up to 80 percent of the edges missing at random in some of our experiments, and the dense reconstructed outputâ i contains the predictions for the missing edges. The hidden representations for the encoder and decoder parts are computed as follows:</p><p>Encoder <ref type="bibr" target="#b1">(2)</ref> . <ref type="bibr" target="#b3">(4)</ref> .</p><formula xml:id="formula_1">z i = g (a i ) = ReLU W · ReLU Va i + b (1) + b</formula><formula xml:id="formula_2">Decoderâ i = f (z i ) = V T · ReLU W T z i + b (3) + b</formula><formula xml:id="formula_3">Autoencoderâ i = h (a i ) = f (g (a i )</formula><p>) . The choice of non-linear, element-wise activation function is the rectified linear unit ReLU(x) = max(0, x). The last decoder layer computes a linear transformation to score the missing links as part of the reconstruction. We constrain the MTGAE architecture to be symmetrical with shared parameters for {W, V} between the encoder and decoder parts, resulting in almost 2× fewer parameters than an unconstrained architecture. Parameter sharing is a powerful form of regularization that helps improve learning and generalization, and is also the main motivation behind MTL <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>. Notice the bias units b do not share parameters, and W T , V T are transposed copies of {W, V}. For brevity of notation, we summarize the parameters to be learned in θ = W, V, b (k) , k = 1, ..., 4.</p><p>Optionally, if a matrix of node features X ∈ R N ×F is available, then we concatenate (A, X) to obtain an augmented adjacency matrixĀ ∈ R N ×(N +F ) and perform the above encoder-decoder transformations onā i for unsupervised link prediction. The intuition behind the concatenation of node features is to enable a shared representation of both graph and node features throughout the autoencoding transformations by way of the tied parameters {W, V}.</p><p>During the forward pass, or inference, the model takes as input an adjacency vector a i and computes its reconstructed outputâ i = h(a i ) for unsupervised link prediction. During the backward pass, we learn θ by minimizing the Masked Balanced Cross-Entropy (MBCE) loss, which allows only the contributions of those parameters associated with observed edges, as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>. We handle class imbalance in link prediction by defining a weighting factor ζ ∈ [0, 1] to be used as a multiplier for the positive class in the cross-entropy loss formulation. For a single example a i and its reconstructed outputâ i , we compute the MBCE loss as follows:</p><formula xml:id="formula_4">L BCE = −a i log (σ (â i )) · ζ − (1 − a i ) log (1 − σ (â i )) , L MBCE = i m i L BCE i m i .</formula><p>Here, L BCE is the balanced binary cross-entropy loss with weighting factor ζ = 1 − # positive links # negative links , σ(·) is the sigmoid function, is the Hadamard (element-wise) product, and m i is the Boolean mask:</p><formula xml:id="formula_5">m i = 1 if a i = UNK, else m i = 0.</formula><p>Semi-Supervised Node Classification The MTGAE model can also be used to perform efficient information propagation on graphs for the task of semi-supervised node classification. For a given augmented adjacency vectorā i , the model learns the corresponding node embedding z i to obtain an optimal reconstruction. Intuitively, z i encodes a vector of latent features derived from the concatenation of both graph and node features, and can be used to predict the label of the ith node. For multi-class classification, we decode z i using the softmax activation function to produce a probability distribution over node labels. More precisely, we predict node labels via the following transformation: <ref type="bibr" target="#b4">(5)</ref> . Multi-Task Learning In many applications, such as knowledge base completion and network analysis, the input graph is partially observed with an incomplete set of edges and a small fraction of labeled nodes. Thus, it is desirable for a model to predict the labels of missing links and nodes simultaneously in a multi-task learning setting. We achieve multi-task learning on graphs by training the MTGAE model using a joint loss function that combines the masked categorical cross-entropy loss for semi-supervised node classification with the MBCE loss for unsupervised link prediction:</p><formula xml:id="formula_6">y i = softmax(z i ) = 1 Z exp(z i ), where Z = i exp(z i ) andz i = U · ReLU W T z i + b (3) + b</formula><formula xml:id="formula_7">L MULTI-TASK = semi-supervised classification −MASK i c∈C y ic log(ŷ ic ) +L MBCE ,</formula><p>where C is the set of node labels, y ic is the binary indicator if node i belongs to class c,ŷ ic is the softmax probability that node i belongs to class c, L MBCE is the loss defined for unsupervised link prediction, and MASK i is the Boolean variable: MASK i = 1 if node i has a label, else MASK i = 0.</p><p>The training complexity of the MTGAE model is O((N + F )DI), where N is the number of nodes, F is the dimensionality of node features, D is the size of the hidden layer, and I is the number of iterations. In practice, F , D N , and I are independent of N . Thus, the overall complexity of MTGAE is O(N ), linear in the number of nodes. <ref type="table">Table 1</ref>: Summary of datasets (left) and baselines (right) used in empirical evaluation. See <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> for dataset details. The notation |O + |:|O − | denotes the ratio of positive to negative edges and is a measure of class imbalance. Label rate is defined as the number of nodes labeled for training divided by the total number of nodes. Acronyms: AUC -Area Under ROC Curve; AP -Average Precision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Empirical Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head><p>Evaluation Task Metric SDNE <ref type="bibr" target="#b13">[14]</ref> Reconstruction Precision@k VGAE <ref type="bibr" target="#b7">[8]</ref> Link Prediction AUC, AP GCN <ref type="bibr" target="#b6">[7]</ref> Node Classification Accuracy Implementation Details We closely follow the experimental protocols described in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> to train and evaluate our MTGAE model for LPNC. For link prediction, we form disjoint test, and validation, sets containing 10, and 5, percent of randomly sampled positive links and the same number of negative links, respectively, while utilizing all node features. For node classification, we split the data into disjoint test, and validation, sets of 1,000, and 500, examples, respectively and use only 20 examples per class for semi-supervised learning. In comparison to the baselines, we evaluate our MTGAE model on the same data splits over 10 runs with random weight initialization and report mean AUC/AP scores for link prediction and accuracy scores for node classification. We also compare the representation capacity of our MTGAE model against the related autoencoder-based SDNE model on the network reconstruction task. We use the ranking metric precision@k to evaluate the model's ability to retrieve positive edges as part of the reconstruction.</p><p>Hyper-parameter tuning is performed on the validation set. Key hyper-parameters include mini-batch size, dimensionality of the hidden layers, and the percentage of dropout regularization <ref type="bibr" target="#b12">[13]</ref>. In all experiments, the dimensionality of the hidden layers in the MTGAE architecture is fixed at N -256-128-256-N . We train for 100 epochs using Adam <ref type="bibr" target="#b5">[6]</ref> gradient descent with a fixed learning rate of 0.001 on mini-batches of 64 examples.</p><p>We implement the MTGAE architecture using Keras <ref type="bibr" target="#b2">[3]</ref> on top of the GPU-enabled TensorFlow <ref type="bibr" target="#b0">[1]</ref> backend. The diagonal elements of the adjacency matrix are set to 1 with the interpretation that every node is connected to itself. We apply mean-variance normalization after each ReLU activation layer to help improve link prediction performance, where it compensates for noise between train and test instances by normalizing the activations to have zero mean and unit variance. During training, we implement several regularization techniques to mitigate overfitting, including dropout for highly sparse graphs and early stopping as a form of regularization in time when the model shows signs of overfitting on the validation set. We initialize weights according to the Glorot scheme described in <ref type="bibr" target="#b3">[4]</ref>. We do not apply weight decay regularization.</p><p>Results and Analysis Results of the reconstruction task for the Arxiv-GRQC and BlogCatalog network datasets are illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. In comparison to SDNE, we show that our MTGAE model achieves better precision@k performance for all k values, up to k = 10, 000 for Arxiv-GRQC and k = 100, 000 for BlogCatalog, when trained on the complete datasets. We also systematically test the capacity of the MTGAE model to reconstruct the original networks when up to 80 percent of the edges are randomly removed, akin to the link prediction task. We show that the MTGAE model only gets worse precision@k performance than SDNE on the Arxiv-GRQC dataset when more than 40 percent of the edges are missing.  Lastly, we report LPNC results obtained by our MTGAE model in the MTL scenario. The model takes as input an incomplete graph with 10 percent of the positive edges, and the same number of negative edges, missing at random and all available node features to simultaneously predict labels for the nodes and missing edges. <ref type="table" target="#tab_2">Table 2</ref> shows the efficacy of the MTGAE model for MTL when compared against recent state-of-the-art task-specific link prediction and node classification models, which require the complete adjacency matrix as input. For link prediction, MTGAE significantly outperforms the best VGAE model on Cora and Citeseer. For node classification, MTGAE is the best performing model on the Citeseer and Pubmed datasets, which have very low node label rates. Future Work Further research will explore inductive reasoning on out-of-network nodes and mitigate O(N ) complexity for improved scalability on large, dynamic graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of precision@k performance between our MTGAE model and the related autoencoder-based SDNE model for the reconstruction task on the Arxiv-GRQC and BlogCatalog network datasets. The parameter k indicates the total number of retrieved edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>On the BlogCatalog dataset, the MTGAE model achieves better precision@k performance than SDNE for large k values even when 80 percent of the edges are missing at random. This experiment demonstrates the superior representation capacity of our MTGAE model when compared to SDNE, which is attributed to parameter sharing in the architecture.</figDesc><table><row><cell></cell><cell>1.0</cell><cell></cell><cell>Arxiv − GRQC</cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell>BlogCatalog</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision@k</cell><cell>0.8 0.9</cell><cell>0. 2</cell><cell>0. 4 SDNE [14] MTGAE (this work) 0. 6 k MTGAE -20% Edges Missing MTGAE -40% Edges Missing MTGAE -60% Edges Missing MTGAE -80% Edges Missing</cell><cell>0. 8</cell><cell>1. 0 ×10 4</cell><cell>Precision@k</cell><cell>0. 0 0.0 0.2 0.4 0.6</cell><cell>0. 2</cell><cell>0. 4 SDNE [14] MTGAE (this work) 0. 6 k MTGAE -20% Edges Missing MTGAE -40% Edges Missing MTGAE -60% Edges Missing MTGAE -80% Edges Missing</cell><cell>0. 8</cell><cell>×10 5 1. 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of LPNC performances between our MTGAE model and recent state-of-the-art graph embedding methods. Link prediction performance is reported as the combined average of AUC and AP scores. Accuracy is used for node classification performance.</figDesc><table><row><cell>Method</cell><cell cols="3">Cora Citeseer Pubmed</cell></row><row><cell></cell><cell cols="2">Link Prediction</cell><cell></cell></row><row><cell>MTGAE</cell><cell>0.946</cell><cell>0.949</cell><cell>0.944</cell></row><row><cell>VGAE [8]</cell><cell>0.920</cell><cell>0.914</cell><cell>0.965</cell></row><row><cell></cell><cell cols="2">Node Classification</cell><cell></cell></row><row><cell>MTGAE</cell><cell>0.790</cell><cell>0.718</cell><cell>0.804</cell></row><row><cell>GCN [7]</cell><cell>0.815</cell><cell>0.703</cell><cell>0.790</cell></row><row><cell cols="2">Planetoid [15] 0.757</cell><cell>0.647</cell><cell>0.772</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/tensorflow" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multitask learning: a knowledge-based source of inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/keras-team/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS 9</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bahareh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Canadian Conference on Advances in Artificial Intelligence</title>
		<meeting>the 23rd Canadian Conference on Advances in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="256" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph auto-encoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01715</idno>
		<title level="m">Training deep autoencoders for collaborative filtering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transforming graph data for statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="363" to="441" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">AutoRec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04038</idno>
		<title level="m">Trace norm regularised deep multi-task learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
