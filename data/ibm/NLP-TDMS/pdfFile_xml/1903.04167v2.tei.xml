<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Partially Shuffling the Training Data to Improve Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-03-12">12 Mar 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
							<email>ofirp@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Partially Shuffling the Training Data to Improve Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-03-12">12 Mar 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although SGD requires shuffling the training data between epochs, currently none of the word-level language modeling systems do this. Naively shuffling all sentences in the training data would not permit the model to learn inter-sentence dependencies. Here we present a method that partially shuffles the training data between epochs. This method makes each batch random, while keeping most sentence ordering intact. It achieves new state of the art results on word-level language modeling on both the Penn Treebank and WikiText-2 datasets. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Background</head><p>A language model is trained to predict word n + 1 given all previous n words. A recurrent language model receives at timestep n the nth word and the previous hidden state and outputs a prediction of the next word and the next hidden state.</p><p>The training data for word-level language modeling consists of a series of concatenated documents. The sentences from these documents are unshuffled. This lets the model learn long term, multi-sentence dependencies between words.</p><p>The concatenation operation results in a single long sequence of words. The naive way to train a language model would be to, at every epoch, use the entire training sequence as the input, and use the same sequence shifted one word to the left as target output. Since the training sequence is too long, this solution is infeasible.</p><p>To solve this, we set a back propagation through-time length (b), and split the training sequence into sub-sequences of length b. In this case, in each epoch the model is first trained on the first sub-sequence, and then on the second one, [A B C D E F G H I J K L] for b = 3, the resulting four sub-sequences are:</p><p>[</p><formula xml:id="formula_0">A B C] [D E F] [G H I] [J K L]</formula><p>Note that we only present the input subsequences, as the target output sub-sequences are simply the input sub-sequences shifted one word to the left 2 . This method works, but it does not utilize current GPUs to their full potential.</p><p>In order to speed up training, we batch our training data. We set a batch size s, and at every training step we train the model on s sub-sequences in parallel.</p><p>To do this, we first split the training sequence into s parts. Continuing the example from above, for s = 2, this results in:</p><formula xml:id="formula_1">[A B C D E F] [G H I J K L]</formula><p>Then, as before, we split each part into subsequences of length b:</p><p>[ </p><formula xml:id="formula_2">A B C] [D E F] [G H I] [J K L]</formula><formula xml:id="formula_3">[D E F] [J K L]</formula><p>Note that at every step, all sub-sequences in the batch are processed in parallel.</p><p>Before we introduced batching, in each epoch the output for each word in the training sequence was dependant on all previous words. With batching, the output of the model for each word is only dependant on the previous words in that batch element (or equivalently, row in our example), and the other words are ignored.</p><p>In our example, the hidden state that is given when inputting G is the default initial hidden state, and not the one that resulted after the input of F. This is not optimal, but since batching reduces the training time by a significant amount, all current models use this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Partial Shuffle Method</head><p>While SGD calls for random batches in each epoch, in existing language models, the data is not shuffled between epochs during training. This means that batch i in every epoch is made up of the same sub-sequences.</p><p>The straightforward way to shuffle the data would be to shuffle all sentences in the training sequence between each epoch. This hurts the language model's performance, since it does not learn inter-sentence dependencies.</p><p>Here we present the Partial Shuffle method, which improves the performance of the model.</p><p>Like before, we first separate the sequence of words into s rows. Using the example sequence from above, this would result in (for s = 2):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[A B C D E F] [G H I J K L]</head><p>Then, for each row, we pick a random index between zero and the length of the row and we take the words that are located before this index and move them to the end of the row. So in our example, if the random index for row one was 2 and for row two was 5 this would result in (red marks the words which were moved):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[C D E F A B] [L G H I J K]</head><p>Finally, as before, each row (or equivalently, batch element) is divided into back-propagation through time segments. For b = 3, this will result in:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[C D E] [F A B] [L G H] [I J K]</head><p>This method randomizes the batches while still keeping most of the word ordering intact.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We evaluate our method on the current state of the art model, <ref type="bibr">DOC (Takase et al., 2018)</ref>, and the previous state of the art model, MoS <ref type="bibr" target="#b4">(Yang et al., 2018)</ref>, on the Penn Treebank <ref type="bibr" target="#b0">(Marcus et al., 1993)</ref> and WikiText-2 <ref type="bibr" target="#b2">(Merity et al., 2017)</ref> language modeling datasets. For each model, the hyperparameters (including b and s) are not modified from their original values. In addition, we present results for finetuned <ref type="bibr" target="#b1">(Merity et al., 2018</ref>) models, with and without the Partial Shuffle. Our shuffling method improves the performance of all models, and achieves new state of the art results on both datasets. Our method does not require any additional parameters or hyperparameters, and runs in less than 1 100 th of a second per epoch on the Penn Treebank dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Model perplexity on WikiText-2, without and with the Partial Shuffle method. Finetune * denotes repeating the finetuning operation three times.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For example, the target output sub-sequences here are [B C D] [E F G] [H I J] [K L * ], where * is the end-of-sequence token.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Acknowledgements</head><p>This note benefited from feedback from Judit Acs, Shimi Salant and Noah A. Smith, which is acknowledged with gratitude.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Direct output connection for a high-rank language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sho Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4599" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
