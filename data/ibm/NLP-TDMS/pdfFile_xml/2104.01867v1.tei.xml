<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lipstick ain&apos;t enough: Beyond Color Matching for In-the-Wild Makeup Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thao</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11790</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11790</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
							<email>v.hoainm@vinai.io</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11790</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Hanoi, Vietnam</roleName><forename type="first">Vinai</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11790</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinuniversity</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11790</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanoi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11790</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vietnam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11790</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lipstick ain&apos;t enough: Beyond Color Matching for In-the-Wild Makeup Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source image</head><p>Source image with transferred makeup from reference styles <ref type="figure">Figure 1</ref>: In-the-wild facial makeup consists of both color transfer and pattern addition. We propose a holistic method that can transfer the color and pattern from a reference makeup style to another image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Makeup transfer is the task of applying on a source face the makeup style from a reference image. Real-life makeups are diverse and wild, which cover not only color-changing but also patterns, such as stickers, blushes, and jewelries. However, existing works overlooked the latter components and confined makeup transfer to color manipulation, focusing only on light makeup styles. In this work, we propose a holistic makeup transfer framework that can handle all the mentioned makeup components. It consists of an improved color transfer branch and a novel pattern transfer branch to learn all makeup properties, including color, shape, texture, and location. To train and evaluate such a system, we also introduce new makeup datasets for real and synthetic extreme makeup. Experimental results show that our framework achieves the state of the art performance on both light and extreme makeup styles. Code is available at https://github.com/VinAIResearch/CPM .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>treme makeups, but its results are far from satisfactory.</p><p>In this work, we consider makeup as a combination of color transformation and pattern addition. We aim to transform the color distribution like previous methods while also preserving the shape and appearance of the makeup pattern. To achieve this objective, we introduce a framework with two branches: Color Transfer Branch and Pattern Transfer Branch, which could be run independently in parallel. In the Color Transfer Branch, we employ a CycleGAN-like network structure driven by Histogram Matching as suggested by BeautyGAN <ref type="bibr" target="#b15">[16]</ref>. In the Pattern Transfer Branch, we learn to extract the makeup pattern mask in a supervised manner. Noticeably, unlike previous methods, both our branches work on warped faces in UV space, thus discarding the discrepancy between these faces in terms of shape, head pose, and expression. The results of the two branches are fused to generate the desired output.</p><p>We also introduce new makeup-transfer datasets, consisting of both synthetic and real images, and covering a wide range of makeup styles. They include extreme makeup styles, which do not exist in previous makeup datasets.</p><p>Using the novel network architecture and the newly collected datasets for training, we obtain an all-inclusive makeup transfer method that outperforms all previous methods in terms of coverage, as shown in <ref type="figure" target="#fig_0">Fig. 2b</ref>. We also run comprehensive experiments, both qualitative and quantitative, and proposed makeup-transfer benchmarks. Our method outperforms other methods on both light and extreme makeup transfer by a wide margin.</p><p>In short, our contributions are: <ref type="bibr" target="#b0">(1)</ref> We pose makeup as a combination of color transformation and pattern addition, and develop a comprehensive makeup transfer method that works for both light and extreme styles. <ref type="bibr" target="#b1">(2)</ref> We design a novel architecture with two branches for color and pattern transfer, and we propose to use warped faces in the UV space when training two network branches to discard the discrepancy between input faces in terms of shape, head pose, and expression. <ref type="bibr" target="#b2">(3)</ref> We introduce new makeuptransfer datasets containing extreme styles that have not been considered in the previous datasets. <ref type="bibr" target="#b3">(4)</ref> We obtain state-of-the-art quantitative and qualitative performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Facial Makeup Transfer. Facial makeup has been studied <ref type="bibr" target="#b17">[18]</ref> in computer vision. Given an arbitrary facial image with the desired makeup style, makeup transfer aims to analyze and replicate that makeup to a source image.</p><p>Traditional methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> focused on image prepossessing techniques, such as landmark extraction and adjustment <ref type="bibr" target="#b27">[28]</ref> or reflectance manipulation <ref type="bibr" target="#b14">[15]</ref>. Recently, due to high-performance hardware and the ability to generate aesthetic images, GANs are widely-used for image-toimage translation tasks, including facial makeup synthesis.</p><p>CycleGAN-based models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref> were introduced to transfer face-to-face makeup styles in an unsupervised manner. For more realistic outcomes, BeautyGAN <ref type="bibr" target="#b15">[16]</ref> used Histogram Matching at each facial region to guide the instancelevel makeup synthesis. BeautyGLOW <ref type="bibr" target="#b5">[6]</ref> proposed to decompose makeup and non-makeup components in the latent space, using the GLOW framework <ref type="bibr" target="#b12">[13]</ref>. LADN <ref type="bibr" target="#b9">[10]</ref> incorporated multiple and overlapping local discriminators for extreme makeup transfers. PSGAN <ref type="bibr" target="#b11">[12]</ref> employed an Attentive Makeup Morphing module to handle transfer across different head poses and facial expressions. Lately, CA-GAN <ref type="bibr" target="#b13">[14]</ref> proposed color discriminators to improve finegrain makeup color transfer at the lips and eye regions.</p><p>Most aforementioned methods only consider light makeup based on color transformation in cosmetic regions such as lips and eye-shadows. In-the-wild makeup styles, however, can also cover pattern-based components such as stickers, face drawings, and decoration. To the best of our knowledge, only LADN <ref type="bibr" target="#b9">[10]</ref> focused on those extreme makeup styles, however, it has several limitations. First, due to the unsupervised setup, it cannot handle complicated makeup patterns with fine details. Second LADN suffers when the head pose of the source and the reference faces are different, producing noticeable artifacts. Finally, it generates low-quality outputs with evident image degradation traits such as JPEG compression noise and blurry edges.</p><p>In this paper, we propose a holistic makeup framework that handles both makeup color and pattern transfer. Our method overcomes the limitations of LADN; it can deal with complicated makeup patterns, be robust to head pose, and produce high-quality outputs.</p><p>Although several datasets of makeup faces have been assembled <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref>, they mainly cover either light or color-focused makeup styles. Since adding patterns is an important part of makeup, pattern-included makeup transfer datasets should be built. We, therefore, introduce such novel datasets for both real and synthetic makeups.</p><p>3D Face Modelling from a single image. To transfer makeup components between faces, we need to understand their facial structures. The human face is a 3D object, and there are many 3D features affecting its appearance in images such as shape, pose, and expression. Thus, reconstructing a 3D face for each input image is crucial to our task. 3D face modeling from single images has been studied for more than two decades <ref type="bibr" target="#b25">[26]</ref>. Among various classical approaches, 3D-morphable models (3DMMs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref> were the most successful. A 3DMM is a parameterized statistical representation of the 3D faces' manifold learned from 3D face scans. Basel Face Model (BFM) <ref type="bibr" target="#b16">[17]</ref> is the most popular 3DMM, which approximates any 3D face as a weighted sum of a mean face and principal shape components. The 3D modeling task is then converted to weight optimization so that the composited 3D face is similar to the  Like most other computer vision tasks, 3D face modeling is experiencing fast growth in recent years thanks to deep learning. The first attempts to apply deep learning on this task relied on 3DMM parameter regression via supervised training <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref>. MoFA <ref type="bibr" target="#b20">[21]</ref> proposed to use an autoencoder for unsupervised training. Tran and Liu <ref type="bibr" target="#b24">[25]</ref> exploited the auto-encoder to learn a nonlinear 3DMM model for better 3D fitting. PRNet <ref type="bibr" target="#b8">[9]</ref> designed a 2D representation, called UV position map, to encode the aligned 3D face shape, thus converting 3D face modeling to an image-toimage translation problem. The UV representation is easy to use and manipulate, thanks to its 2D form while removing the effect of head poses and expressions. The later 3D face modeling works mainly focused on reconstructing 3D details for realistic modeling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>In this paper, we employ PRNet <ref type="bibr" target="#b8">[9]</ref> in our system since its accuracy is sufficiently good for our task. Furthermore, we take advantage of its UV representation for effective makeup swapping across faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Color-&amp;-Pattern Makeup Transfer Method</head><p>Let I n s denote the source image (s) with nonmakeup (n) and I m r the reference image (r) with the desired makeup (m). Our goal is to obtain I m s , an image of the source face with transferred makeup from the reference image. It requires learning a function F such that:</p><formula xml:id="formula_0">F(I n s , I m r ) = I m s<label>(1)</label></formula><p>In this section, we will describe our method to learn this function. Our method is designed based on the following two insights. First, the source and target images are not aligned due to different 3D head poses, face shapes, and facial expressions. To remove the misalignment, we should register these images to a uniform template before transferring makeup, and we specifically propose to use the UV map representation. Second, makeup transfer should be viewed as a combination of color transformation and pattern addition. Pattern addition is categorically different from color transformation, and it should be explicitly handled by a specific module of the proposed solution.</p><p>In overview, our method consists of the following three steps. First, the input images I n s , I m r are converted to UV texture maps T n s , T m r . Second, the texture maps are passed to two parallel branches for color-based and pattern-based makeup transferred. Third, the makeup-transferred texture T m s is formed by combining the outputs of those branches, and this UV texture map is converted to the image space to obtain the final output I m s . The pipeline of our method is depicted in <ref type="figure" target="#fig_0">Fig. 2a</ref>. In the rest of this section, we will describe the details of the main components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">UV map conversion</head><p>UV map representation is a common technique for 3D object texture mapping in computer graphics. The object's texture is flattened into a 2D image, and each 3D vertex of the object is associated with a 2D location on the image, called UV coordinates, for color sampling. PRNet <ref type="bibr" target="#b8">[9]</ref> extended this idea and introduced a UV position map representation to encode any 3D face shape. It is a 2D image with three channels encoding the XYZ coordinates of the 3D face with respect to the camera coordinates. This UV map is well registered; each pixel in the map corresponds to a fixed semantic point on the face regardless of the input head pose. Alongside the UV position map, we do texture mapping to get the paired texture map. The UV position map packs all information about face shape, head pose, and facial expression, while the mapped texture is invariant to those aspects.</p><p>Given an input facial image I, we can use the pre-trained model of PRNet, denoted as UV, to extract the corresponding UV position map S and the UV texture T . The input image can be recovered from these UV representations via a rendering function UV −1 .</p><p>S, T := UV(I) and I := UV −1 (S, T ).</p><p>(</p><p>To transfer makeup between source and reference images with different head poses, we use these UV map presentations. First, we apply the conversion function UV on each input image I n s and I m r to get the corresponding UV maps (S s , T n s ) and (S r , T m r ). Note that the UV position maps S s and S r depend only on 3D face shapes, thus being independent of the makeup styles. Then, we pass the texture maps T n s and T m r to the color and pattern transfer branches to get makeup swapped in UV space. The outputs of two branches are blended into final texture images T m s . Finally, we apply the rendering function to convert it back to standard image representation:</p><formula xml:id="formula_2">I m s = UV −1 (S s , T m s ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Color transfer branch</head><p>This branch adopts the architecture and training losses proposed in BeautyGAN <ref type="bibr" target="#b15">[16]</ref>. The main component is a color-based makeup swapping network C that swaps makeup color on cosmetic regions between the source and the reference image:</p><formula xml:id="formula_3">T m C s , T n C r := C(T n s , T m r )</formula><p>. To train C, it uses a loss function as a weighted sum of the following:</p><p>• Adversarial Loss L adv enforces the output maps T m C s and T n C r to be in makeup and non-makeup domain, respectively, using two discriminators, • Cycle Consistency Loss L cyc enforces the cycle consistency constraints proposed by CycleGAN <ref type="bibr" target="#b31">[32]</ref>, • Perceptual Loss L per aims to preserve the identity between the before and after makeup transfer images by using the VGG-16 model pre-trained on ImageNet, • Histogram Matching Loss L hist aims to match the color distributions of the reference image and the source image after makeup transfer.</p><p>The first three loss functions are common, so we omit the detailed discussion here. The final loss, i.e., L hist , is the key loss function proposed by BeautyGAN for transferring makeup color in cosmetic regions. It employs a Histogram Matching (HM ) function that alters the histogram of the source image to match the reference one in each of several predefined regions: eye shadows, lips, and facial skin. The total loss is a weighted sum of the regional losses:</p><formula xml:id="formula_4">L hist = λ eyes L eyes hist + λ lips L lips hist + λ skin L skin hist ,<label>(3)</label></formula><p>where λ eyes , λ lips , λ skin are tunable hyper-parameters. Each loss term L i hist (i can be eyes, lips, or skin) is the distance between the after-makeup image and the histogram-matched version:</p><formula xml:id="formula_5">L i hist = T m C s Γ i s − HM (T n s Γ i s , T m r Γ i r ) . (4)</formula><p>where is pixel-wise multiplication, Γ i s and Γ i r are the segmentation masks for region i in the source and the reference image, respectively.</p><p>In BeautyGAN, the cosmetic regions are not aligned; they highly differ in size, location, and perspective warping.</p><p>It severely impacts the histogram match results, reducing the effectiveness of this histogram loss. While being similar to BeautyGAN, our Color Transfer Branch uses the UV texture maps for makeup swapping instead of the original images. This seemingly small innovation actually leads to much improvement. The texture maps are registered pixelto-pixel, enabling the histogram matching function to work accurately. The region mask is image-invariant and equals to a universal mask: Γ i s = Γ i r = Γ i . We observe that our Color Transfer Branch produces better results compared to BeautyGAN. It captures not only color but also structure and location of the cosmetic makeups, which is crucial to some makeup components such as blushes. We will discuss more this result in Sec. 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pattern transfer branch</head><p>Besides the Color Transfer Branch, we propose a novel Pattern Transfer Branch aiming to detect and transfer the pattern-based makeup components such as stickers, facial drawings, and decorative accessories. When transferring these patterns, we need to keep them unchanged in terms of shape, texture, and location but warped to the target 3D surface. In the natural image form, this process is complicated, which includes segmenting the pattern, unwarping it, and re-warping onto the target. Thanks to the UV position map representation, we do not need the unwarping and rewarping steps. The problem reduces to simple image segmentation.</p><p>Given the input texture map T m r , we aim to extract a binary segmentation mask for its makeup patterns. We can do so by using any segmentation network. In our implementation, a typical UNet structure with a pre-trained Resnet-50 encoder is used. We employ dice loss for training: L DC = 2|Γ gt ∩Γ pr | |Γ gt |+|Γ pr | , where Γ gt and Γ pr are the ground truth and predicted segmentation masks for the pattern.</p><p>To train this network, we need a makeup dataset with annotated masks for the makeup patterns. However, such datasets do not exist, so we developed ourselves a synthetic dataset, called CPM-Synt-1, for image-mask pair training data. Details of this dataset will be discussed in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Combination</head><p>The output of the Pattern Transfer Branch is the pattern mask Γ m , while the output of the Color Transfer Branch is an entire UV texture map T m C s . The forms of these two outputs are different, reflecting the fundamental differences between two makeup categories. That is why we propose two separate branches for dedicated processing.</p><p>To get the UV texture map for the source image with the desired transferred makeup, we can combine the outputs of the two branches, by blending the reference makeup pattern, defined by the predicted mask Γ m , with the color- </p><formula xml:id="formula_6">T m s = T m r Γ m + T m C s (1 − Γ m ).<label>(5)</label></formula><p>Finally, we convert this texture map to the output image I m s , using the rendering function I m s = UV −1 (S s , T m s ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Color-&amp;-Pattern Makeup (CPM) Datasets</head><p>Given the lack of annotated data with extreme makeup styles for the development of in-the-wild makeup transfer methods, we collected this type of data ourselves. In this section, we describe our data collection and generation procedure that led to three Color and Pattern Makeup (CPM) datasets, called CPM-Real, CPM-Synt-1, and CPM-Synt-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CPM-Real -In-the-Wild Makeup Dataset</head><p>This is a dataset of real faces with real in-the-wild makeups. It is very diverse in terms of makeup styles, containing both color and pattern makeups. The degree of makeup can vary from light to heavy, from color-oriented to patterndriven. Many images contain extreme makeups, including facial gems, face paintings, hennas, and festival makeups.</p><p>To compile this dataset, we first retrieved a set of initial images using keyword searches (e.g., glitter makeup, festival makeup, creative makeup, gems makeup, face painting). We then used the MTCNN face detector <ref type="bibr" target="#b30">[31]</ref> to detect and crop faces in each image. We discarded faces smaller than 150×150. Finally, we manually removed low-quality and inappropriate ones. The final set has 3895 makeup images, which is 43% larger than the number of makeup images in MT <ref type="bibr" target="#b15">[16]</ref>, the previously largest available makeup dataset. This dataset is designed purely for testing purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CPM-Synt-1 -Added Pattern Dataset</head><p>This is a dataset of real faces with synthetically added makeup patterns. To build it, we needed a set of makeup  <ref type="figure">Figure 4</ref>: CPM-Synt-2 synthesizing process #images Light Heavy Pattern Unsupervised datasets MT <ref type="bibr" target="#b15">[16]</ref> 3834 LADN <ref type="bibr" target="#b9">[10]</ref> 698 M-Wild <ref type="bibr" target="#b11">[12]</ref> 772 CPM-Real (Ours) 3895 Supervised datasets CPM-Synt-1 (Ours) 5555 CPM-Synt-2 (Ours) 1625 <ref type="table">Table 1</ref>: Overview of makeup datasets patterns. Unfortunately, automatic segmentation of makeup patterns from real images was non-trivial, while manual annotation would be laborious due to the tiny little details in many patterns such as henna. To circumvent this problem, we collected some patterns from the Internet with keyword search (e.g., flowers, crystals, gems, henna, daisy, leaf, tattoo) to compile the so-called Stickers dataset with 577 highquality images. We only used PNG images that had alpha channels, which could later be used for image blending. Next, we applied the patterns on images from the MT dataset <ref type="bibr" target="#b15">[16]</ref>. As a pattern should conform to the face's surface, we did not directly blend the pattern to the face image. Instead, we applied the blending process in the UV space. The face image was first converted to a UV texture map, as described in Sec. 3.1. We then blended the pattern on the texture map using its alpha mask with random size, location, and opacity. To make the makeup realistic, we set the pattern's size around the cheek size and put its location inside the face but not at the center. Besides creating the blended texture, we also kept the blending mask as ground-truth for training pattern segmentation module (Sec. 3.3). Finally, we rendered the blended texture to get the after-makeup facial image, together with the sticker segmentation mask.</p><p>In total, CPM-Synt-1 has 5555 after-makeup images. Each image is associated with the ground truth segmentation mask for the pattern and the corresponding UV maps. This dataset split into disjoint training and testing subsets of size 4182 and 1373, respectively. The subjects and the makeup patterns in two subsets are disjointed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">CPM-Synt-2 -Transferred Pattern Dataset</head><p>Despite having ground-truth labels, CPM-Synt-1 does not follow the transfer setup, so it cannot be used to evaluate the pattern-based makeup transfer algorithms. Hence, we built another synthetic dataset called CPM-Synt-2. This dataset contains image triplets: (source image, reference image, ground-truth), specially designed for the patterntransferred evaluation task.</p><p>One requirement for this test is to have the source and the reference image of the same color-makeup style. Otherwise, we need to impose color-makeup transfer in the ground-truth image. Creating such ground-truth is nontrivial, and no practical solution has been proposed. We can start from non-makeup images, but even these images have a visible cosmetic color difference that requires swapping.</p><p>To overcome the mentioned problem, we rely on an assumption of makeup transfer stability: When using the same reference image, a good makeup transfer method will output images of the same makeup style. Based on this assumption, we propose a method to construct the CPM-Synt-2 dataset, as described in <ref type="figure">Fig. 4</ref>. First, two non-makeup images are randomly picked from the MT dataset. Then, we transfer both of them to the same makeup style n, defined by a Color Style image, using BeautyGAN. This process results in two images with the same color style, called Color-makeup Source I n s and Color-makeup Reference I n r , respectively. Next, we blend the sticker into both images, forming the ground-truth I gt s and reference image I m r . Finally, the triplets (I n s , I m r , I gt s ) are formed. CPM-Synt-2 consists of 1625 triplets for evaluation purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>We implemented our system with PyTorch. The UV conversation function UV and the inverse rendering module were based on the existing code and model of PRNet <ref type="bibr" target="#b8">[9]</ref>. We trained Pattern Transfer Branch and Color Transfer Branch separately, using respective training datasets.</p><p>Color Transfer Branch. For fair comparisons with other methods, we trained C on the MT dataset <ref type="bibr" target="#b15">[16]</ref>. We aligned and resized all images to 256×256 and then computed their texture maps and facial segmentation in the UV space. The color transfer branch was trained in an unsupervised manner; in each iteration, we randomly sampled one makeup and one non-makeup image to form a swapping pair.</p><p>The hyper-parameters were set as follows. The weights for the loss components were: λ adv =1, λ cyc =10, λ per =0.005, and λ hist =1. The weights for histogram matching regions were: λ skin =0.1, λ eyes =1, and λ lips =1. Batch size was set to one. We used Adam optimizer with learning rate 0.0002 to train the network until convergence.</p><p>Pattern Transfer Branch. This branch was trained with supervised learning, using the CPM-Synt-1 dataset. Each training image came with the pattern segmentation mask, both having size 256×256. We utilized UNet structure with Resnet-50 as the pre-trained encoder. Since the original segmentation mask was non-binary, we used sigmoid as activation function. The model was trained for 300 epochs with batch size 8, Adam optimizer, and learning rate 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Qualitative experiments</head><p>We compared the proposed method to the state-of-theart methods, including DMT <ref type="bibr" target="#b29">[30]</ref>, BeautyGAN <ref type="bibr" target="#b15">[16]</ref>, LADN <ref type="bibr" target="#b9">[10]</ref>, and PSGAN <ref type="bibr" target="#b11">[12]</ref>. We skipped some baselines, such as BeautyGlow <ref type="bibr" target="#b5">[6]</ref> and CA-GAN <ref type="bibr" target="#b13">[14]</ref> because they are both color-only and have no released model. The evaluations were conducted on both the existing and proposed datasets. We present here a few qualitative results but more examples can be found in the supplementary.</p><p>MT dataset. We conducted an experiment on the existing MT dataset <ref type="bibr" target="#b15">[16]</ref> to examine the ability to transfer colorbased makeup styles. As can be seen in the first row of <ref type="figure">Fig. 5</ref>, our model can capture well the lips' color, similar to the state-of-the-art BeautyGAN <ref type="bibr" target="#b15">[16]</ref>. Moreover, thanks to UV-based swapping solution, our method can successfully transfer the face blushes and is the only method that captures the glowing skin foundation.</p><p>CPM-Synt-1 dataset. We evaluated the transfer results with the presence of synthesized makeup patterns. For each reference makeup in the test set of CPM-Synt-1, we randomly picked a source image in the MT dataset and do makeup transfer. A representative result is shown in the second row of <ref type="figure">Fig. 5</ref>. Although the makeup pattern was unseen during training, our network could capture its pattern well and transport it to the output. All other methods, including LADN, failed to handle such a complicated style.</p><p>CPM-Real dataset. Finally, we tested with real in-the-wild makeup styles. This time, we used the reference makeup in the CPM-Real dataset, while the source image was still from the MT dataset. We present two examples in the last rows of <ref type="figure">Fig. 5</ref>. Although providing realistic results, color-based methods completely ignored facial drawings and decoration. LADN could partially replicate the reference styles, but its results are unnatural and unappealing. Our method could retain the makeup pattern details and return the results that are closest to desirable makeups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">User surveys</head><p>For the subjective evaluation of the results, we conducted a user survey for each dataset above. Each survey consisted of 20 questions. For each question, the participants were asked to rank the after-makeup images from the best to the worst. Subsequently, we assigned a score of 5 to the   highest-ranked method, and 1 to the lowest one. There were 40 participants, leading to 800 answers for each survey.</p><p>The average survey scores are reported in <ref type="table" target="#tab_3">Table 2</ref>. Our method outperforms the others by a wide margin on all tests. Its scores are close to the perfect score of 5, suggesting the superiority of our method in almost all questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ground-truth experiments</head><p>By building labelled datasets, we can conduct quantitative experiments that have not been done in the previous studies. We first compute our pattern segmentation network's accuracy, then evaluate the quality of the makeuptransfer results produced by ours and baseline methods.</p><p>Makeup pattern segmentation. We evaluated the performance of our pattern segmentation branch on the test set of CPM-Synt-1, and the result is shown in the first row of <ref type="table" target="#tab_4">Table 3</ref>. Our pattern segmentation branch achieved 0.788 mIoU. It is not perfect, but sufficiently good for the downstream task of makeup transfer.</p><p>Makeup transfer quality. To quantitatively compare our method and other baselines in the makeup-transfer setting, we conducted experiments on the CPM-Synt-2 dataset. We used the MS-SSIM metric to evaluate the quality of the after-makeup images in comparison with ground-truth ones. The average score for each method is reported in the second row of <ref type="table" target="#tab_4">Table 3</ref>. The MS-SSIM of our method is 0.977, surpassing the second method by a wide margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Studies</head><p>UV-based makeup transfer. As discussed in Sec. 3.3, the UV representation is critical to the Pattern Branch. <ref type="figure" target="#fig_3">Fig. 7</ref> shows in the first row a comparison between pattern in image space and in UV space. Our method aligns the source and target faces pixel-by-pixel, removing the differences in 3D poses, shapes, and expressions. Hence, we can transfer the pattern easily and precisely.</p><p>Last row of <ref type="figure" target="#fig_3">Fig. 7</ref> compares the makeup transfer results between BeautyGAN, trained on original faces, and our Color Transfer Branch, trained on the UV space. As can be seen, our method replicates the reference style much more accurately. It preserves both the purple eye shadow and the glowing skin foundation.</p><p>Identity preservation. We used ArcFace <ref type="bibr" target="#b7">[8]</ref> to calcu-   late the similarity score between the faces before and after makeup transfer. The average similarity scores on the MT and CPM-Synt-1 datasets are 0.851 and 0.781, respectively. Based on the recommended face verification threshold of 0.5, our makeup transfer method preserves the identities of the subjects. These similarity scores are lower than the maximum score of 1, but this is expected because real-life facial makeup may also change facial characteristics dramatically.</p><p>Branch Analysis. Both color and pattern branches are vital, as illustrated in <ref type="figure" target="#fig_4">Fig. 8</ref>. The Color Transfer Branch alone failed to bring the face drawings and stickers from the reference image to the target face. When using Pattern Branch only, the lip color of the output image stays the same as the original. We need to combine two branches to replicate all makeup components of the reference image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Interpolation and Partial Makeup Transfer</head><p>Interpolation. Makeup interpolation is an interesting application of makeup transfer. While interpolating between makeup patterns is not practical, interpolating between makeup colors is pretty common and easy. Given a single  input image I n s and two reference styles I m1 r1 and I m2 r2 , we can run two makeup transfer processes in parallel to get the color-transferred texture maps T m1 s and T m2 s . We can then mix these texture maps by a mixing parameter α ∈ [0, 1], and render to get the interpolated output. <ref type="figure" target="#fig_2">Fig. 6</ref> displays some interpolated results in case one or two reference styles are given. The results are smooth and natural, even in extreme regions such as heavy eye-shadow and cheek color.</p><p>Partial makeup transfer. Further exploiting the UV position map, we can use it together with facial segmentation to perform partial makeup transfer. Instead of transferring makeup on the entire face, we can do it on a face region defined by some input mask. This controllable mechanism was proposed in the previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> and can be easily implemented in our system. <ref type="figure" target="#fig_6">Fig. 9</ref> provides an example in which we transferred makeup partially for the lips, eye shadow, and skin region, then generated a makeup composition on the entire face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we extend the definition of the makeup transfer task and propose a novel holistic framework to deal with in-the-wild makeup styles. Makeup styles are now interpreted as a combination of color-matching and patternaddition, respectively, solved by our Color Transfer Branch and Pattern Transfer Branch. UV representation is incorporated to improve the results of both branches. The experiments show our framework can achieve state-of-the-art qualitative and quantitative results. Moreover, we propose novel datasets to leverage makeup-transfer studies and encourage future development.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed framework. 'Control.' indicates controllable for partial makeup transfer. one in the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Features of makeup datasets. First row are from MT<ref type="bibr" target="#b15">[16]</ref> and LADN dataset<ref type="bibr" target="#b9">[10]</ref>. The rest are from our datasets: CPM-Real, CPM-Synt-1 and CPM-Synt-2.transferred texture map T m C s from the color transfer branch:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Makeup style interpolation. The middle images have makeup style interpolated from two reference styles with a mixing parameter α.Reference UV Face Source</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Benefits of the UV space to Pattern Transfer Branch (first row) and Color Transfer Branch (second row). From left to right: Source image, results obtained by training on the original image space, results obtained by training on the UV space, and the reference image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Branch Analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Partial makeup transfer. First rows are different styles. Second rows are results from partial makeup transfer only (lip, eye-shadow, skin), and combination of all styles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>User survey results for the qualitative results from three datasets. The numbers shown are the average user ratings, with 5 being the perfect score and 1 the lowest. Our method achieves the highest scores on all three surveys.</figDesc><table><row><cell>Dataset Metric</cell><cell cols="5">DMT BGAN LADN PSGAN Ours</cell></row><row><cell>Synt-1 mIOU</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.788</cell></row><row><cell cols="6">Synt-2 MS-SSIM 0.918 0.918 0.656 0.723 0.977</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ground</figDesc><table><row><cell>-truth experiments: Pattern segmenta-</cell></row><row><cell>tion on CPM-Synt-1 (top row) and Makeup transfer on</cell></row><row><cell>CPM-Synt-2 (bottom row).</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGGRAPH Conference on Computer Graphics</title>
		<meeting>the ACM SIGGRAPH Conference on Computer Graphics</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Asymmetric style transfer for applying and removing makeup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pairedcyclegan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Photo-realistic facial details synthesis from single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anpei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic facial makeup detection with application in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spoofing faces using makeup: An investigative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Swearingen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Identity, Security and Behavior Analysis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beautyglow: Ondemand makeup transfer framework with reversible generative network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka-Ming</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wu</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Han</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Huang</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can facial cosmetics affect the matching accuracy of face recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Biometrics: Theory, Applications and Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Niannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ladn: Local adversarial disentangling network for facial makeup and de-makeup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang Tik</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuria</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<title level="m">Image analogies. Proceedings of the ACM SIGGRAPH Conference on Computer Graphics</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Psgan: Pose and expression robust spatial-aware gan for customizable makeup transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ca-gan: Weakly supervised color aware gan for controllable makeup transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Kips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Bloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simulating makeup through physics-based manipulation of intrinsic image layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beautygan: Instance-level facial makeup transfer with deep generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihe</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on Multimedia</title>
		<meeting>the ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Romhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<meeting>the IEEE International Conference on Advanced Video and Signal Based Surveillance</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Impact and detection of facial beautification in face recognition: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rathgeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antitza</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Busch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d face reconstruction by learning from synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient, robust and accurate fitting of a 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MoFA: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Example-based cosmetic transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Shun</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Qing</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Conference on Computer Graphics and Applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3D morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extreme 3d face reconstruction: Seeing through occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonlinear 3d face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Estimating coloured 3d face models from single images: An example based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cosmetic industry -Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/w/index.php?title=Cosmetic_industry" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An automatic framework for example-based virtual makeup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangzhou</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facescape: A largescale high quality 3d face dataset and detailed riggable 3d face prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Disentangled makeup transfer with generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Jin</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3D solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
