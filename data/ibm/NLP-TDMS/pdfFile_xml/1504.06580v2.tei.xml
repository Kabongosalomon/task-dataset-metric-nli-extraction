<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classifying Relations by Ranking with Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
							<email>cicerons@br.ibm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dos</forename><surname>Santos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
							<email>zhou@us.ibm.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<addrLine>138/146 Av</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Pasteur Rio de Janeiro</orgName>
								<address>
									<region>RJ</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">IBM Watson</orgName>
								<address>
									<addrLine>1101 Kitchawan Yorktown Heights</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">IBM Watson</orgName>
								<address>
									<addrLine>1101 Kitchawan Yorktown Heights</addrLine>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Classifying Relations by Ranking with Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2010 Task 8 dataset, which is designed for the task of classifying the relationship between two nominals marked in a sentence. Using CR-CNN, we outperform the state-of-the-art for this dataset and achieve a F1 of 84.1 without using any costly handcrafted features. Additionally, our experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation classification is an important Natural Language Processing (NLP) task which is normally used as an intermediate step in many complex NLP applications such as question-answering and automatic knowledge base construction. Since the last decade there has been increasing interest in applying machine learning approaches to this task <ref type="bibr" target="#b20">(Zhang, 2004;</ref><ref type="bibr" target="#b12">Qian et al., 2009;</ref><ref type="bibr" target="#b13">Rink and Harabagiu, 2010)</ref>. One reason is the availability of benchmark datasets such as the SemEval-2010 task 8 dataset <ref type="bibr" target="#b6">(Hendrickx et al., 2010)</ref>, which encodes the task of classifying the relationship between two nominals marked in a sentence. The following sentence contains an example of the Component-Whole relation between the nominals "introduction" and "book". Some recent work on relation classification has focused on the use of deep neural networks with the aim of reducing the number of handcrafted features <ref type="bibr" target="#b14">(Socher et al., 2012;</ref><ref type="bibr" target="#b19">Zeng et al., 2014;</ref><ref type="bibr" target="#b18">Yu et al., 2014)</ref>. However, in order to achieve state-ofthe-art results these approaches still use some features derived from lexical resources such as Word-Net or NLP tools such as dependency parsers and named entity recognizers <ref type="bibr">(NER)</ref>.</p><p>In this work, we propose a new convolutional neural network (CNN), which we name Classification by Ranking CNN (CR-CNN), to tackle the relation classification task. The proposed network learns a distributed vector representation for each relation class. Given an input text segment, the network uses a convolutional layer to produce a distributed vector representation of the text and compares it to the class representations in order to produce a score for each class. We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform an extensive number of experiments using the the SemEval-2010 Task 8 dataset. Using CR-CNN, and without the need for any costly handcrafted feature, we outperform the state-of-the-art for this dataset. Our experimental results are evidence that: (1) CR-CNN is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals.</p><p>The remainder of the paper is structured as follows. Section 2 details the proposed neural network. In Section 3, we present details about the setup of experimental evaluation, and then describe the results in Section 4. In Section 5, we discuss previous work in deep neural networks for relation classification and for other NLP tasks. Section 6 presents our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Neural Network</head><p>Given a sentence x and two target nouns, CR-CNN computes a score for each relation class c ∈ C. For each class c ∈ C, the network learns a distributed vector representation which is encoded as a column in the class embedding matrix W classes . As detailed in <ref type="figure" target="#fig_1">Figure 1</ref>, the only input for the network is the tokenized text string of the sentence. In the first step, CR-CNN transforms words into realvalued feature vectors. Next, a convolutional layer is used to construct a distributed vector representations of the sentence, r x . Finally, CR-CNN computes a score for each relation class c ∈ C by performing a dot product between r x and W classes .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word Embeddings</head><p>The first layer of the network transforms words into representations that capture syntactic and semantic information about the words. Given a sentence x consisting of N words x = {w 1 , w 2 , ..., w N }, every word w n is converted into a real-valued vector r wn . Therefore, the input to the next layer is a sequence of real-valued vectors emb x = {r w 1 , r w 2 , ..., r w N } Word representations are encoded by column vectors in an embedding matrix W wrd ∈ R d w ×|V | , where V is a fixed-sized vocabulary. Each column W wrd i ∈ R d w corresponds to the word embedding of the i-th word in the vocabulary. We transform a word w into its word embedding r w by using the matrix-vector product:</p><formula xml:id="formula_0">r w = W wrd v w</formula><p>where v w is a vector of size |V | which has value 1 at index w and zero in all other positions. The matrix W wrd is a parameter to be learned, and the size of the word embedding d w is a hyperparameter to be chosen by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Position Embeddings</head><p>In the task of relation classification, information that is needed to determine the class of a relation between two target nouns normally comes from words which are close to the target nouns. <ref type="bibr" target="#b19">Zeng et al. (2014)</ref> propose the use of word position embeddings (position features) which help the CNN by keeping track of how close words are to the target nouns. These features are similar to the position features proposed by <ref type="bibr" target="#b2">Collobert et al. (2011)</ref> for the Semantic Role Labeling task.</p><p>In this work we also experiment with the word position embeddings (WPE) proposed by <ref type="bibr" target="#b19">Zeng et al. (2014)</ref>. The WPE is derived from the relative distances of the current word to the target noun 1 and noun 2 . For instance, in the sentence shown in <ref type="figure" target="#fig_1">Figure 1</ref>, the relative distances of left to car and plant are -1 and 2, respectively. As in <ref type="bibr" target="#b2">(Collobert et al., 2011)</ref>, each relative distance is mapped to a vector of dimension d wpe , which is initialized with random numbers. d wpe is a hyperparameter of the network. Given the vectors wp 1 and wp 2 for the word w with respect to the targets noun 1 and noun 2 , the position embedding of w is given by the concatenation of these two vectors, wpe w = [wp 1 , wp 2 ].</p><p>In the experiments where word position embeddings are used, the word embedding and the word position embedding of each word are concatenated to form the input for the convolutional layer, emb</p><formula xml:id="formula_1">x = {[r w 1 , wpe w 1 ], [r w 2 , wpe w 2 ], ..., [r w N , wpe w N ]}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentence Representation</head><p>The next step in the NN consists in creating the distributed vector representation r x for the input sentence x. The main challenges in this step are the sentence size variability and the fact that important information can appear at any position in the sentence. In recent work, convolutional approaches have been used to tackle these issues when creating representations for text segments of different sizes <ref type="bibr" target="#b19">(Zeng et al., 2014;</ref><ref type="bibr" target="#b8">Hu et al., 2014;</ref><ref type="bibr">dos Santos and Gatti, 2014)</ref> and characterlevel representations of words of different sizes (dos Santos and Zadrozny, 2014). Here, we use a convolutional layer to compute distributed vector representations of the sentence. The convolutional layer first produces local features around each word in the sentence. Then, it combines these local features using a max operation to create a fixed-sized vector for the input sentence.</p><p>Given a sentence x, the convolutional layer applies a matrix-vector operation to each window of size k of successive windows in emb x = {r w 1 , r w 2 , ..., r w N }. Let us define the vector z n ∈ R d w k as the concatenation of a sequence of k word embeddings, centralized in the n-th word: z n = (r w n−(k−1)/2 , ..., r w n+(k−1)/2 ) In order to overcome the issue of referencing words with indices outside of the sentence boundaries, we augment the sentence with a special padding token replicated k − 1 2 times at the beginning and the end.</p><p>The convolutional layer computes the j-th element of the vector r x ∈ R d c as follows:</p><formula xml:id="formula_2">[r x ] j = max 1&lt;n&lt;N f W 1 z n + b 1 j</formula><p>where W 1 ∈ R d c ×d w k is the weight matrix of the convolutional layer and f is the hyperbolic tangent function. The same matrix is used to extract local features around each word window of the given sentence. The fixed-sized distributed vector representation for the sentence is obtained by using the max over all word windows. Matrix W 1 and vector b 1 are parameters to be learned. The number of convolutional units d c , and the size of the word context window k are hyperparameters to be chosen by the user. It is important to note that d c corresponds to the size of the sentence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Class embeddings and Scoring</head><p>Given the distributed vector representation of the input sentence x, the network with parameter set θ computes the score for a class label c ∈ C by using the dot product</p><formula xml:id="formula_3">s θ (x) c = r x [W classes ] c</formula><p>where W classes is an embedding matrix whose columns encode the distributed vector representations of the different class labels, and [W classes ] c is the column vector that contains the embedding of the class c. Note that the number of dimensions in each class embedding must be equal to the size of the sentence representation, which is defined by d c . The embedding matrix W classes is a parameter to be learned by the network. It is initialized by randomly sampling each value from an uniform distribution: U (−r, r), where r = 6 |C| + d c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training Procedure</head><p>Our network is trained by minimizing a pairwise ranking loss function over the training set D. The input for each training round is a sentence x and two different class labels y + ∈ C and c − ∈ C, where y + is a correct class label for x and c − is not. Let s θ (x) y + and s θ (x) c − be respectively the scores for class labels y + and c − generated by the network with parameter set θ. We propose a new logistic loss function over these scores in order to train CR-CNN:</p><formula xml:id="formula_4">L = log(1 + exp(γ(m + − s θ (x) y + )) + log(1 + exp(γ(m − + s θ (x) c − ))<label>(1)</label></formula><p>where m + and m − are margins and γ is a scaling factor that magnifies the difference between the score and the margin and helps to penalize more on the prediction errors. The first term in the right side of Equation 1 decreases as the score s θ (x) y + increases. The second term in the right side decreases as the score s θ (x) c − decreases. Training CR-CNN by minimizing the loss function in Equation 1 has the effect of training to give scores greater than m + for the correct class and (negative) scores smaller than −m − for incorrect classes. In our experiments we set γ to 2, m + to 2.5 and m − to 0.5. We use L2 regularization by adding the term β θ 2 to Equation 1. In our experiments we set β to 0.001. We use stochastic gradient descent (SGD) to minimize the loss function with respect to θ.</p><p>Like some other ranking approaches that only update two classes/examples at every training round <ref type="bibr" target="#b5">Gao et al., 2014)</ref>, we can efficiently train the network for tasks which have a very large number of classes. This is an advantage over softmax classifiers.</p><p>On the other hand, sampling informative negative classes/examples can have a significant impact in the effectiveness of the learned model. In the case of our loss function, more informative negative classes are the ones with a score larger than −m − . The number of classes in the relation classification dataset that we use in our experiments is small. Therefore, in our experiments, given a sentence x with class label y + , the incorrect class c − that we choose to perform a SGD step is the one with the highest score among all incorrect classes</p><formula xml:id="formula_5">c − = arg max c ∈ C; c =y + s θ (x) c .</formula><p>For tasks where the number of classes is large, we can fix a number of negative classes to be considered at each example and select the one with the largest score to perform a gradient step. This approach is similar to the one used by <ref type="bibr" target="#b17">Weston et al. (2014)</ref> to select negative examples.</p><p>We use the backpropagation algorithm to compute gradients of the network. In our experiments, we implement the CR-CNN architecture and the backpropagation algorithm using Theano <ref type="bibr" target="#b1">(Bergstra et al., 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Special Treatment of Artificial Classes</head><p>In this work, we consider a class as artificial if it is used to group items that do not belong to any of the actual classes. An example of artificial class is the class Other in the SemEval 2010 relation classification task. In this task, the artificial class Other is used to indicate that the relation between two nominals does not belong to any of the nine relation classes of interest. Therefore, the class Other is very noisy since it groups many different types of relations that may not have much in common.</p><p>An important characteristic of CR-CNN is that it makes it easy to reduce the effect of artificial classes by omitting their embeddings. If the embedding of a class label c is omitted, it means that the embedding matrix W classes does not contain a column vector for c. One of the main benefits from this strategy is that the learning process focuses on the "natural" classes only. Since the embedding of the artificial class is omitted, it will not influence the prediction step, i.e., CR-CNN does not produce a score for the artificial class.</p><p>In our experiments with the SemEval-2010 relation classification task, when training with a sentence x whose class label y = Other, the first term in the right side of Equation 1 is set to zero. During prediction time, a relation is classified as Other only if all actual classes have negative scores. Otherwise, it is classified with the class which has the largest score.</p><p>3 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation Metric</head><p>We use the SemEval-2010 Task 8 dataset to perform our experiments. This dataset contains 10,717 examples annotated with 9 different relation types and an artificial relation Other, which is used to indicate that the relation in the example does not belong to any of the nine main relation types. The nine relations are Cause-Effect, Component-Whole, Content-Container, Entity-Destination, Entity-Origin, Instrument-Agency, Member-Collection, Message-Topic and Product-Producer. Each example contains a sentence marked with two nominals e 1 and e 2 , and the task consists of predicting the relation between the two nominals taking into consideration the directionality. That means that the relation Cause-Effect(e1,e2) is different from the relation Cause-Effect(e2,e1), as shown in the examples below. More information about this dataset can be found in <ref type="bibr" target="#b6">(Hendrickx et al., 2010)</ref>. The SemEval-2010 Task 8 dataset is already partitioned into 8,000 training instances and 2,717 test instances. We score our systems by using the SemEval-2010 Task 8 official scorer, which computes the macro-averaged F1-scores for the nine actual relations (excluding Other) and takes the directionality into consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Embeddings Initialization</head><p>The word embeddings used in our experiments are initialized by means of unsupervised pre-training. We perform pre-training using the skip-gram NN architecture <ref type="bibr" target="#b11">(Mikolov et al., 2013)</ref> available in the word2vec tool. We use the December 2013 snapshot of the English Wikipedia corpus to train word embeddings with word2vec. We preprocess the Wikipedia text using the steps described in (dos Santos and Gatti, 2014): (1) removal of paragraphs that are not in English;</p><p>(2) substitution of non-western characters for a special character; (3) tokenization of the text using the tokenizer available with the Stanford POS Tagger <ref type="bibr" target="#b15">(Toutanova et al., 2003)</ref>; (4) removal of sentences that are less than 20 characters long (including white spaces) or have less than 5 tokens. (5) lowercase all words and substitute each numerical digit by a 0. The resulting clean corpus contains about 1.75 billion tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Neural Network Hyper-parameter</head><p>We use 4-fold cross-validation to tune the neural network hyperparameters. Learning rates in the range of 0.03 and 0.01 give relatively similar results. Best results are achieved using between 10 and 15 training epochs, depending on the CR-CNN configuration. In <ref type="table">Table 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word Position Embeddings and Input Text Span</head><p>In the experiments discussed in this section we assess the impact of using word position embeddings (WPE) and also propose a simpler alternative approach that is almost as effective as WPEs. The main idea behind the use of WPEs in relation classification task is to give some hint to the convolutional layer of how close a word is to the target nouns, based on the assumption that closer words have more impact than distant words.</p><p>Here we hypothesize that most of the information needed to classify the relation appear between the two target nouns. Based on this hypothesis, we perform an experiment where the input for the convolutional layer consists of the word embeddings of the word sequence {w e 1 − 1, ..., w e 2 + 1} where e 1 and e 2 correspond to the positions of the first and the second target nouns, respectively.</p><p>In <ref type="table">Table 2</ref> we compare the results of different CR-CNN configurations. The first column indicates whether the full sentence was used (Yes) or whether the text span between the target nouns was used (No). The second column informs if the WPEs were used or not. It is clear that the use of WPEs is essential when the full sentence is used, since F1 jumps from 74.3 to 84.1. This effect of WPEs is reported by <ref type="bibr" target="#b19">(Zeng et al., 2014)</ref>. On the other hand, when using only the text span between the target nouns, the impact of WPE is much smaller. With this strategy, we achieve a F1 of 82.8 using only word embeddings as input, which is a result as good as the previous state-of-the-art F1 of 83.0 reported in <ref type="bibr" target="#b18">(Yu et al., 2014)</ref> for the SemEval-2010 Task 8 dataset. This experimental result also suggests that, in this task, the CNN works better for short texts.</p><p>All experiments reported in the next sections use CR-CNN with full sentence and WPEs. In this experiment we assess the impact of omitting the embedding of the class Other. As we mentioned above, this class is very noisy since it groups many different infrequent relation types. Its embedding is difficult to define and therefore brings noise into the classification process of the natural classes. In <ref type="table">Table 3</ref> we present the results comparing the use and omission of embedding for the class Other. The two first lines of results present the official F1, which does not take into account the results for the class Other. We can see that by omitting the embedding of the class Other both precision and recall for the other classes improve, which results in an increase of 1.4 in the F1. These results suggest that the strategy we use in CR-CNN to avoid the noise of artificial classes is effective.  <ref type="table">Table 3</ref>: Impact of not using an embedding for the artificial class Other.</p><p>In the two last lines of <ref type="table">Table 3</ref> we present the results for the class Other. We can note that while the recall for the cases classified as Other remains 48.7, the precision significantly decreases from 60.1 to 52.0 when the embedding of the class Other is not used. That means that more cases from natural classes (all) are now been classified as Other. However, as both the precision and the recall of the natural classes increase, the cases that are now classified as Other must be cases that are also wrongly classified when the embedding of the class Other is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CR-CNN versus CNN+Softmax</head><p>In this section we report experimental results comparing CR-CNN with CNN+Softmax. In order to do a fair comparison, we've implemented a CNN+Softmax and trained it with the same data, word embeddings and WPEs used in CR-CNN. Concretely, our CNN+Softmax consists in getting the output of the convolutional layer, which is the vector r x in <ref type="figure" target="#fig_1">Figure 1</ref>, and giving it as input for a softmax classifier. We tune the parameters of CNN+Softmax by using a 4-fold cross-validation with the training set. Compared to the hyperparameter values for CR-CNN presented in <ref type="table">Table 1</ref>, the only difference for CNN+Softmax is the number of convolutional units d c , which is set to 400.</p><p>In <ref type="table" target="#tab_4">Table 4</ref> we compare the results of CR-CNN and CNN+Softmax. CR-CNN outperforms CNN+Softmax in both precision and recall, and improves the F1 by 1.6. The third line in <ref type="table" target="#tab_4">Table 4</ref> shows the result reported by <ref type="bibr" target="#b19">Zeng et al. (2014)</ref> when only word embeddings and WPEs are used as input to the network (similar to our CNN+Softmax). We believe that the word embeddings employed by them is the main reason their result is much worse than that of CNN+Softmax. We use word embeddings of size 400 while they use word embeddings of size 50, which were trained using much less unlabeled data than we did.</p><p>Neural Net.</p><p>Prec. Rec. F1 CR-CNN 83.7 84.7 84.1 CNN+SoftMax 82.1 83.1 82.5 CNN+SoftMax --78.9 <ref type="bibr" target="#b19">(Zeng et al., 2014)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the State-of-the-art</head><p>In <ref type="table" target="#tab_6">Table 5</ref> we compare CR-CNN results with results recently published for the SemEval-2010 Task 8 dataset. <ref type="bibr" target="#b13">Rink and Harabagiu (2010)</ref> present a support vector machine (SVM) classifier that is fed with a rich (traditional) feature set. It obtains an F1 of 82.2, which was the best result at SemEval-2010 Task 8. <ref type="bibr" target="#b14">Socher et al. (2012)</ref> present results for a recursive neural network (RNN) that employs a matrix-vector representation to every node in a parse tree in order to compose the distributed vector representation for the complete sentence. Their method is named the matrix-vector recursive neural network (MVRNN) and achieves a F1 of 82.4 when POS, NER and WordNet features are used. In <ref type="bibr" target="#b19">(Zeng et al., 2014)</ref>, the authors present results for a CNN+Softmax classifier which employs lexical and sentencelevel features. Their classifier achieves a F1 of 82.7 when adding a handcrafted feature based on the WordNet. <ref type="bibr" target="#b18">Yu et al. (2014)</ref> present the Factor-based Compositional Embedding Model (FCM), which achieves a F1 of 83.0 by deriving sentencelevel and substructure embeddings from word embeddings utilizing dependency trees and named entities.</p><p>As we can see in the last line of <ref type="table" target="#tab_6">Table 5</ref>, CR-CNN using the full sentence, word embeddings and WPEs outperforms all previous reported results and reaches a new state-of-the-art F1 of 84.1. This is a remarkable result since we do not use any complicated features that depend on external lexical resources such as WordNet and NLP tools such as named entity recognizers (NERs) and dependency parsers.</p><p>We can see in <ref type="table" target="#tab_6">Table 5</ref> that CR-CNN 1 also achieves the best result among the systems that use word embeddings as the only input features. The closest result (80.6), which is produced by the FCM system of <ref type="bibr" target="#b18">Yu et al. (2014)</ref>, is 2.2 F1 points behind CR-CNN result (82.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Most Representative Trigrams for each Relation</head><p>In <ref type="table">Table 6</ref>, for each relation type we present the five trigrams in the test set which contributed the most for scoring correctly classified examples.</p><p>Remember that in CR-CNN, given a sentence x the score for the class c is computed by s θ (x) c = r x [W classes ] c . In order to compute the most representative trigram of a sentence x, we trace back each position in r x to find the trigram responsible for it. For each trigram t, we compute its particular contribution for the score by summing the terms in score that use positions in r x that trace back to t. The most representative trigram in x is the one with the largest contribution to the improvement of the score. In order to create the results presented in <ref type="table">Table 6</ref>, we rank the trigrams which were selected as the most representative of any sentence in decreasing order of contribution value. If a trigram appears as the largest contributor for more than one sentence, its contribuition value becomes the sum of its contribution for each sentence. We can see in <ref type="table">Table 6</ref> that for most classes, the trigrams that contributed the most to increase the score are indeed very informative regarding the relation type. As expected, different trigrams play an important role depending on the direction of the relation. For instance, the most informative tri-gram for Entity-Origin(e1,e2) is "away from the", while reverse direction of the relation, Entity-Origin(e2,e1) or Origin-Entity, has "the source of" as the most informative trigram. These results are a step towards the extraction of meaningful knowledge from models produced by CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Over the years, various approaches have been proposed for relation classification <ref type="bibr" target="#b20">(Zhang, 2004;</ref><ref type="bibr" target="#b12">Qian et al., 2009;</ref><ref type="bibr" target="#b6">Hendrickx et al., 2010;</ref><ref type="bibr" target="#b13">Rink and Harabagiu, 2010)</ref>. Most of them treat it as a multiclass classification problem and apply a variety of machine learning techniques to the task in order to achieve a high accuracy.</p><p>Recently, deep learning <ref type="bibr" target="#b0">(Bengio, 2009</ref>) has become an attractive area for multiple applications, including computer vision, speech recognition and natural language processing. Among the different deep learning strategies, convolutional neural networks have been successfully applied to different NLP task such as part-of-speech tagging (dos Santos and Zadrozny, 2014), sentiment analysis <ref type="bibr" target="#b10">(Kim, 2014;</ref><ref type="bibr">dos Santos and Gatti, 2014)</ref>, question classification <ref type="bibr" target="#b9">(Kalchbrenner et al., 2014)</ref>, semantic role labeling <ref type="bibr" target="#b2">(Collobert et al., 2011)</ref>, hashtag prediction <ref type="bibr" target="#b17">(Weston et al., 2014)</ref>, sentence completion and response matching <ref type="bibr" target="#b8">(Hu et al., 2014)</ref>.</p><p>Some recent work on deep learning for relation classification include <ref type="bibr" target="#b14">Socher et al. (2012)</ref>, <ref type="bibr" target="#b19">Zeng et al. (2014)</ref> and <ref type="bibr" target="#b18">Yu et al. (2014)</ref>. In <ref type="bibr" target="#b14">(Socher et al., 2012)</ref>, the authors tackle relation classification using a recursive neural network (RNN) that assigns a matrix-vector representation to every node in a parse tree. The representation for the complete sentence is computed bottom-up by recursively combining the words according to the syntactic structure of the parse tree Their method is named the matrix-vector recursive neural network (MVRNN). <ref type="bibr" target="#b19">Zeng et al. (2014)</ref> propose an approach for relation classification where sentence-level features are learned through a CNN, which has word embedding and position features as its input. In parallel, lexical features are extracted according to given nouns. Then sentence-level and lexical features are concatenated into a single vector and fed into a softmax classifier for prediction. This approach achieves state-of-the-art performance on the SemEval-2010 Task 8 dataset. <ref type="bibr" target="#b18">Yu et al. (2014)</ref> propose a Factor-based Com-  Component-Whole e1 of the, of the e2, part of the, e2 's e1, with its e1, e2 has a, in the e2, e1 on the e2 comprises the, e2 with e1</p><p>Content-Container was in a, was hidden in, were in a, e2 full of, e2 with e1, e2 was full, was inside a, was contained in e2 contained a, e2 with cold Entity-Destination e1 into the, e1 into a, e1 to the, was put inside, imported into the Entity-Origin away from the, derived from a, had the source of, e2 grape e1, left the, derived from an, e1 from the e2 butter e1</p><p>Instrument-Agency are used by, e1 for e2, is used by, with a e1, by using e1, e2 finds a, trade for e2, with the e2 e2 with a, e2 , who</p><p>Member-Collection of the e2, in the e2, of this e2, e2 of e1, of wild e1, of elven e1, the political e2, e1 collected in e2 of different, of 0000 e1</p><p>Message-Topic e1 is the, e1 asserts the, e1 that the, described in the, discussed in the, on the e2, e1 inform about featured in numerous, discussed in cabinet, documented in two, Product-Producer e1 by the, by a e2, of the e2, e2 of the, e2 has constructed, e2 's e1, by the e2, from the e2 e2 came up, e2 who created <ref type="table">Table 6</ref>: List of most representative trigrams for each relation type.</p><p>positional Embedding Model (FCM) by deriving sentence-level and substructure embeddings from word embeddings, utilizing dependency trees and named entities. It achieves slightly higher accuracy on the same dataset than <ref type="bibr" target="#b19">(Zeng et al., 2014)</ref>, but only when syntactic information is used.</p><p>There are two main differences between the approach proposed in this paper and the ones proposed in <ref type="bibr" target="#b14">(Socher et al., 2012;</ref><ref type="bibr" target="#b19">Zeng et al., 2014;</ref><ref type="bibr" target="#b18">Yu et al., 2014)</ref>: (1) CR-CNN uses a pair-wise ranking method, while other approaches apply multiclass classification by using the softmax function on the top of the CNN/RNN; and (2) CR-CNN employs an effective method to deal with artificial classes by omitting their embeddings, while other approaches treat all classes equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work we tackle the relation classification task using a CNN that performs classification by ranking. The main contributions of this work are: (1) the definition of a new state-of-the-art for the SemEval-2010 Task 8 dataset without using any costly handcrafted features; (2) the proposal of a new CNN for classification that uses class embeddings and a new rank loss function; (3) an effective method to deal with artificial classes by omitting their embeddings in CR-CNN; (4) the demonstration that using only the text between target nominals is almost as effective as using WPEs; and (5) a method to extract from the CR-CNN model the most representative contexts of each relation type. Although we apply CR-CNN to relation classification, this method can be used for any classification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The [introduction]e 1 in the [book]e 2 is a summary of what is in the text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>CR-CNN: a Neural Network for classifying by ranking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The [war]e 1 resulted in other collateral imperial [conquests]e 2 as well. ⇒ Cause-Effect(e1,e2) The [burst]e 1 has been caused by water hammer [pressure]e2. ⇒ Cause-Effect(e2,e1)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of results of CR-CNN and CNN+Softmax.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison with results published in the literature.</figDesc><table><row><cell>Relation</cell><cell>(e1,e2)</cell><cell>(e2,e1)</cell></row><row><cell>Cause-Effect</cell><cell>e1 resulted in, e1 caused a, had caused the, poverty cause e2, caused a e2</cell><cell>e2 caused by, was caused by, are caused by, been caused by, e2 from e1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This is the result using only the text span between the target nouns.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Nina Wacholder for her valuable suggestions to improve the final version of the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bergstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference</title>
		<meeting>the Python for Scientific Computing Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Collobert et al.2011</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Cícero Nogueira dos Santos and Maíra Gatti</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Cícero Nogueira dos Santos and Bianca Zadrozny</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling interestingness with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hendrickx</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Semeval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems</title>
		<meeting>the Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A convolutional neural netork for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods for Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods for Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for semantic relation classification using stratified sampling strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1437" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Utd: Classifying semantic relations by combining lexical and semantic resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Semantic Evaluation</title>
		<meeting>International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="256" to="259" />
		</imprint>
	</monogr>
	<note>Rink and Harabagiu2010</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Socher et al.2012</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">#tagspace: Semantic embeddings from hashtags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1822" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Factor-based compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Learning Semantics</title>
		<meeting>the 2nd Workshop on Learning Semantics<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly-supervised relation classification for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Information and Knowledge Management</title>
		<meeting>the ACM International Conference on Information and Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="581" to="588" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
