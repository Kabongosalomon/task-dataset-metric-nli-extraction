<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Measles Rash Identification Using Residual Deep Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Glock</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computing and Technology</orgName>
								<orgName type="institution">Lipscomb University</orgName>
								<address>
									<settlement>Nashville</settlement>
									<region>TN, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Napier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computing and Technology</orgName>
								<orgName type="institution">Lipscomb University</orgName>
								<address>
									<settlement>Nashville</settlement>
									<region>TN, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Louie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computing and Technology</orgName>
								<orgName type="institution">Lipscomb University</orgName>
								<address>
									<settlement>Nashville</settlement>
									<region>TN, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Gary</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computing and Technology</orgName>
								<orgName type="institution">Lipscomb University</orgName>
								<address>
									<settlement>Nashville</settlement>
									<region>TN, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gigante</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Pediatrics</orgName>
								<orgName type="institution">Vanderbilt University School of Medicine</orgName>
								<address>
									<region>TN, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schaffner</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Health Policy</orgName>
								<orgName type="institution">Vanderbilt University School of Medicine</orgName>
								<address>
									<region>TN, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingguo</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computing and Technology</orgName>
								<orgName type="institution">Lipscomb University</orgName>
								<address>
									<settlement>Nashville</settlement>
									<region>TN, US</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Measles Rash Identification Using Residual Deep Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 + These authors contributed equally to this work</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Measles</term>
					<term>Measles Rash</term>
					<term>Image Recognition</term>
					<term>Deep Learning</term>
					<term>Transfer Learning</term>
					<term>Convolutional Neural Network</term>
					<term>CNN</term>
					<term>Residual Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Measles is extremely contagious and is one of the leading causes of vaccine-preventable illness and death in developing countries, claiming more than 100,000 lives each year. Measles was declared eliminated in the US in 2000 due to decades of successful vaccination for the measles. As a result, an increasing number of US healthcare professionals and the public have never seen the disease. Unfortunately, the Measles resurged in the US in 2019 with 1,282 confirmed cases. To assist in diagnosing measles, we collected more than 1300 images of a variety of skin conditions, with which we employed residual deep convolutional neural network to distinguish measles rash from other skin conditions, in an aim to create a phone application in the future. On our image dataset, our model reaches a classification accuracy of 95.2%, sensitivity of 81.7%, and specificity of 97.1%, indicating the model is effective in facilitating an accurate detection of measles to help contain measles outbreaks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The measles virus is among the oldest recorded viruses that infect humans. According to the Centers for Disease Control and Prevention (CDC), the first scientific description occurred in the 9th century by Persian physician Rhazes, who identified it as a separate virus from smallpox and chickenpox <ref type="bibr" target="#b0">[1]</ref>. In 1912, measles became a nationally notifiable disease in the United States, requiring healthcare providers and laboratories to report all diagnosed cases. In the first decade of reporting, an average of 6,000 measles-related deaths were reported each year. Before the measles vaccine was first introduced in 1963, it is estimated that 3 to 4 million people in the US were infected each year, with an estimated 400 to 500 among reported cases resulting in death. Globally, before widespread vaccination, the virus caused 2 million to 3 million deaths per year <ref type="bibr" target="#b0">[1]</ref>.</p><p>Measles is extreme contagious. It is estimated that up to 90% of people who are close to an infected individual will contract the virus if they aren't immune. Furthermore, infected individuals are capable of spreading the disease well before the skin rash appears (as many as four days prior), thus increasing the risk of transmission even further <ref type="bibr" target="#b1">[2]</ref>.</p><p>Thanks to the implementation of the vaccine, measles was declared eliminated from the US in 2000, where individual cases of measles remained exceptionally rare for the next 19 years <ref type="bibr" target="#b0">[1]</ref>. However, the virus resurged in the US in 2019 with a total 1,282 individual cases in 31 different states <ref type="bibr" target="#b2">[3]</ref>. After examining the resurgence of measles in the US, a recent study by <ref type="bibr">Paules et al.</ref> linked the cause to travel-related transmissions with subsequent spread through undervaccinated populations <ref type="bibr" target="#b3">[4]</ref>. At the same time measles cases increased globally with more than 500,000 confirmed cases of measles and an estimated of 140,000 cases resulted in death <ref type="bibr" target="#b2">[3]</ref>.</p><p>With the virus continuing to spread worldwide, proper and efficient diagnosis of measles will be essential in mitigating the rate of infection. However, measles cases have been exceptionally rare in the US since its official elimination in 2000, and as a result diagnosing it has become more difficult, particularly for younger healthcare professionals who have never seen the disease before.</p><p>The most defining symptom of measles is the skin rash that it causes, as the other symptoms closely mimic other illnesses. The distinctive pattern of the rash, as well as the method in which the rash progresses across the body are critical signs that healthcare providers make use of to visually diagnose the disease. Without immediate medical attention, complications can occur such as inflammation of the brain, loss of hearing, and pneumonia.</p><p>In this paper, we leverage deep convolutional neural network (CNN) to identify measles rash, in an aim to develop a smartphone-based application in the future to assist physicians and patients alike in the diagnosis of the disease. Currently, there are no other existing algorithms designed specifically for the visual detection of the measles rash. A model capable of measles rash characterization can be applied in many fields. Besides phone application, which is particular needed in developing countries where health workers are scarce, it can also be deployed as an application to be used in telemedicine to facilitate the recognition of measles, or airport security to prevent transmission of the disease.</p><p>The rest of the paper is organized as follows. In the next section, we review the related work of utilized CNN models. In Section 3, a brief description of our data and CNN model is presented. In Section 4, we provide our experimental results. Lastly, in Section 5 we offer conclusions and directions for future development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Recent advances in artificial intelligence have made deep convolutional neural network (CNN) the go-to model on virtually every image related problem. Deep CNN, as a class of deep learning algorithms, is composed of stacks of processing layers, which give it the ability to learn complex features hierarchically from imaging data. With outstanding performance in image processing, CNN has been applied increasingly widely to detect, classify, and diagnose skin diseases [5]- <ref type="bibr" target="#b6">[7]</ref>. For example, Nasr-Esfahani et al. presented a CNN model for classifying images of melanoma lesions <ref type="bibr" target="#b4">[5]</ref>. They segmented skin lesion and sent segmented images to CNN for detection. Their model distinguishes between melanoma and benign cases with an accuracy of 81%. Pham et al. combined data augmentation with deep CNN for skin lesion classification <ref type="bibr" target="#b5">[6]</ref>. Using a CNN model Inception-V4, they achieved a classification rate equal to 89%. Additionally, Yu et al. used deep residual CNN to recognize melanoma in dermoscopy images and achieved an accuracy of 85.5% <ref type="bibr" target="#b6">[7]</ref>.</p><p>CNN requires a large sample size for model training. But it is often a challenge to acquire sufficient labeled medical images due to the expertise required and high labor intensity for image curation. Transfer learning, a technique that imitates the learning of human beings, was proposed to address this issue <ref type="bibr" target="#b10">[12]</ref>- <ref type="bibr" target="#b12">[14]</ref>. The idea of transfer learning is to leverages data from a well-trained similar domain to address the lack of data in a target domain. Transfer learning has been proven highly effective in numerous applications and has been used widely in various fields to achieve high classification performance <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[16]</ref>- <ref type="bibr" target="#b20">[22]</ref>. For example, the two aforementioned studies <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, which utilized Inception-V4 and residual CNN respectively, both used transfer learning. In comparison with the approach of building a deep CNN from scratch, transfer learning requires fewer training samples and significantly reduced time accordingly to build a CNN.</p><p>Stimulated by the crucial need for early diagnosis and treatment of skin diseases and a shortage of dermatologists, many CNN-based systems were developed in the past year using transfer learning to classify skin photographs <ref type="bibr" target="#b14">[16]</ref>- <ref type="bibr" target="#b20">[22]</ref>. For example, Mobiny et al. used densely connected convolutional networks (DenseNets) for skin lesion diagnosis <ref type="bibr" target="#b14">[16]</ref>. They showed that a hybrid physicianmachine workflow can reach a classification accuracy of 90% while only referring 35% of the cases to physicians. To address the 26 most common skin conditions seen in primary care, Liu et al. developed a deep learning system (DLS) with CNN model Inception-V4 as classifiers <ref type="bibr" target="#b15">[17]</ref>. On their validation dataset, they showed the accuracy of their DLS system is non-inferior to board-certified dermatologists and higher than primary care physicians and nurse practitioners.</p><p>In addition, a study by <ref type="bibr">Burlina et</ref>   <ref type="bibr" target="#b20">[22]</ref>. In the 2019 Skin Lesion Classification Challenge hosted by the International Skin Imaging Collaboration (ISIC), their method won the top place in the two tasks coming with the Challenge. Moreover, a recent work presented a residual CNN algorithm ResNet-34 to distinguish 11 skin conditions and rashes <ref type="bibr" target="#b17">[19]</ref>. Interestingly, Measles rash was among their 11 skin conditions. But the evidence (and accuracy) of measles identification was not reported by the developers. Apart from this work, we are not aware of other CNN model for measles rash identification.</p><p>With the lack of tools for visual recognition of measles and recent resurgence of measles in the US, the models that can accurately detect measles rash are therefore urgently needed. Improved capability of measles identification would help healthcare professionals effectively address the challenge of potential measles reemergence in the nation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MATERIALS AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data collection</head><p>As there are no public resources available that contain an extensive library of measles images specifically, we collected the data for our study using the Bing Web Search API (part of the Microsoft Azure package) to parse images from the web. The data set we collected contains rash images of 11 different disease states: Bowens disease, chickenpox, chigger bites, dermatofibroma, eczema, enterovirus, keratosis, measles, psoriasis, ringworm and scabies. Additionally, images of normal skin are also included in the data set. <ref type="table" target="#tab_1">Table I</ref> shows the complete list of samples we collected. In total, there are 158 images of the measles rash, and 1158 non-measles images present in the dataset. <ref type="figure">Fig. 1</ref> below shows two example images in our data set. The left photo is enterovirus rash and the one on the right is measles rash. The similarity of the two types of rashes in appearance represents a challenge in distinguishing the measles rash from other skin conditions. A crucial part of this work is to curate the images collected. Two clinicians at Vanderbilt University, Dr. Joseph Gigante and Dr. William Schaffner, who are also co-authors of this paper, assisted us with data collection and curation. Dr. Schaffner, an infectious disease specialist, consulted with us in the early stage of the study and provided several medical links to use for parsing measles images. Dr. Gigante, a pediatrician, reviewed the measles images prior to model training and advised which were likely the measles. Ultimately, only the images he approved of were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Model</head><p>We used transfer learning to develop our CNN model to detect measles rash through skin images. As mentioned in Section 2, many transfer learning-based CNN architectures, e.g. Inception-V4, DenseNets and AlexNet, have been utilized to process medical images. In this study, we sought to employ deep residual networks (ResNet) to create our CNN model, because of the performance the family of ResNet models (ResNet-34, ResNet-50, ResNet-101 and ResNet-152) had achieved on other similar work <ref type="bibr" target="#b16">[18]</ref>, <ref type="bibr" target="#b17">[19]</ref>. ResNet was the winner of the 2015 ImageNet Large Scale Visual Recognition Challenge (ILSVRC 2015) in image classification, detection, and localization. In the COCO 2015 competitions, ResNet also won the first place on the tasks of ImageNet detection, localization, etc. The family of ResNet models were published in 2015 by Microsoft Asia <ref type="bibr" target="#b9">[11]</ref>, and since have seen many successful applications <ref type="bibr" target="#b16">[18]</ref>, <ref type="bibr" target="#b17">[19]</ref>. <ref type="figure">Fig. 2</ref> illustrates the basic processes that take place in a ResNet. There are 5 stages in a ResNet, each with a convolution block. Stage 1 consists of convolution and max pooling layers. A convolutional layer uses a filter called a kernel to pass over an image to create a feature map. The initial kernel size is 7x7 with 64 output channels and a stride of 2. Max pooling reduces image size by keeping the max value of each matrix square that the kernel passes over. Stages 2-5 are residual blocks. Residual blocks are special highway networks without gates in their skip connections to allow information flow from the initial layers to the final layers. Following stage 5, there is an average pooling and a fully connected layer. Average pooling reduces image size by using the average value of each matrix square. The fully connected layer takes the end result of the convolution and pooling and outputs the final probabilities for image classification.</p><p>The version of the ResNet models used in our study is the latest implementation (2020) in a Python package fastai <ref type="bibr">[10]</ref>. In fastai, ResNet is layered on top of Pytorch library, a Python environment in fastai. For a full documentation of the fastai library, interested readers are referred to <ref type="bibr">[10]</ref>. The ResNet models in fastai were already pretrained on ImageNet, a large dataset that contains over 1.4 million images <ref type="bibr" target="#b21">[23]</ref>. Because the two image classes in our dataset, i.e., measles vs. non-measles, are highly imbalanced, to achieve the best classification performance possible, we also tried oversampling and image augmentation techniques using the keras library <ref type="bibr" target="#b22">[24]</ref>. To perform image augmentation, we created duplicates of measles images with minor adjustments such as random rotations and horizontal flips. Our comparative analysis indicated, however, that image augmentation did not result in improvement and that ResNet-50 provided the highest accuracy on our original image dataset. Here, ResNet-50 stands for 50-layer residual network. The complete architecture of ResNet-50 is provided in Appendix A. Our results also showed that the accuracy of deeper ResNet models, e.g. ResNet-152, which is three times deeper than ResNet-50, is similar as that of ResNet-50 (see Appendix B), although they required way more time to train. Thus, hereafter we will focus our discussion on ResNet-50. The results in section below were obtained using ResNet-50 without oversampling and image augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model training and testing</head><p>With ResNet-50 already pretrained on ImageNet, only the last part of the model, i.e. the fully connected layer as shown in <ref type="figure">Fig. 2</ref>, needs to be adapted in order to classify measles rash. Therefore, in the initial phase of model training, we kept all the convolutional layers, i.e. the backbone of ResNet-50, with their pretrained weights and trained only the last few layers of the model. A stratified 5fold cross validation was conducted to train and evaluate the model. For each round of training, the images were divided randomly into training and validation sets with an 80/20% split, respectively. We set batch size to be 64 and number of epochs to be 8.</p><p>Each image in our dataset was classified into one of the two classes: measles (positive) and non-measles (negative). On each iteration, we calculated three commonly used metrics to evaluate our method: sensitivity, specificity and accuracy, which are defined as follows:</p><formula xml:id="formula_0">= / ( + ) , = /( + ) , = ( + ) / ( + + + ),</formula><p>where TP, TN, FP and FN denote the number of true positive, true negative, false positive and false negative classifications, respectively. After the 5 iterations, the average performances of the models were computed. Table II below provides the three computed metrics. It shows the average accuracy, sensitivity, and specificity of the model are 94.8%, 74.1%, and 97.6%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model refinement</head><p>After creating the initial model, we then fine-tuned the whole model by unfreezing backbone layers of the model for retraining. As learning rate affects model performance significantly, to find consensus learning rates for the five cross-validation iterations, we visualized the relationship between learning rate and loss function on each iteration. <ref type="figure">Fig. 3</ref> provides an example plot we created on an iteration. It shows in <ref type="figure">Fig. 3</ref> that the recorded loss tends to decrease with the increase of learning rate, before it diverges quickly after a point close to 1e-3. Based on the observation (as well as four other plots), we specified the range [1e-6, 1e-4] as our differential learning rates for model refinement, with which three epochs were then performed to obtain our final model.  We also computed the average area under the curve (AUC) score based on the receiver operator characteristic (ROC) curve. <ref type="figure">Fig. 4</ref> below shows the improvement of the AUC score in the process of model refinement. After model refinement, the average AUROC score of the final model is 0.958.</p><p>These results indicate the ResNet-50 model is effective in identifying the measles rash.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORK</head><p>With the spread of the measles virus continuing to increase worldwide, and fewer healthcare providers in the US that can accurately identify it due to its rarity in recent decades, a properly trained model capable of identifying measles rash is essential in combatting the outbreak.</p><p>In this paper, we used residual deep convolutional neural network, ResNet-50, to distinguish the distinctive measles rash from a variety of other skin conditions, in an aim to create a phone application in the future to help contain measles outbreaks. The proposed method showed promising results with an average accuracy of 95.2%, sensitivity of 81.7%, and specificity of 97.1%, indicating ResNet-50 is capable of accurate detection of measles rash.</p><p>Given the small size of the dataset used, the performance of our model can be improved with the addition of more images. It may also be necessary to expend our dataset to include a larger spectrum of rash illnesses in children, e.g. rubella, drug-induced rash, roseola, erythema infectiosum, toxic shock syndrome, Kawasaki disease as well as the newly-recognized multisystem inflammatory syndrome in children, along with others.</p><p>In this pilot study, we focused on the appearance of the rash and did not take into account the distribution of the rash on the body and its development (the measles rash characteristically begins on the head/face and then spreads down the body). Moreover, we did not include information about other concurrent symptoms, e.g. whether the patient has a fever or the classic "3Cs" of measles: cough, coryza and conjunctivitis. In future work, we plan to address these issues by integrating such information with the ResNet-50 model so as to improve diagnosis efficacy.</p><p>Our dataset possesses a variety of age, gender, and body parts across samples, but does not have a wide diversity of skin color. Currently, the dataset is predominately composed of images of Caucasian skin, with fewer than 20 images representing minority skin tones. In our next step, we anticipate to obtain more ethnically diverse images to use for model retraining. By incorporating more images from diverse ethnical groups, this work can be more readily deployed to aid healthcare providers in diagnosis.</p><p>As stated earlier, the ultimate goal of this study is to deploy our model as a phone application. It is estimated that there are already 3.5 billion smartphone users in the world today. With a growing generation of younger doctors that utilize smartphone in the field, a phone-based application could serve as a powerful tool in the diagnosis of the measles disease. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>al. used ResNet-50, a residual deep convolutional neural network, to detect acute Lyme disease from erythema migrans images [18]. Their model reached an accuracy of 86.5%, ROCAUC of 95.1% and Kappa of 71.4% for detecting erythema migrans. Hosny et al. proposed an automatic skin lesions classification system using pretrained deep neural network Alex-net [20]. On the three datasets, MED-NODE, Derm and ISIC, their method achieved accuracy of 96.86%, 97.70%, and 95.91%, respectively. Instead of relying on single CNN architecture, Gessert et al. addressed skin lesion classification with an ensemble of models including EfficientNets, SENet, and ResNeXt WSL</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc>DESCRIPTION OF OUR IMAGE DATA SET</figDesc><table><row><cell>Image class</cell><cell>Number of images</cell></row><row><cell>Bowen's Disease</cell><cell>124</cell></row><row><cell>Chickenpox</cell><cell>170</cell></row><row><cell>Chigger Bites</cell><cell>87</cell></row><row><cell>Dermatofibroma</cell><cell>80</cell></row><row><cell>Eczema</cell><cell>95</cell></row><row><cell>Enterovirus</cell><cell>117</cell></row><row><cell>Keratosis</cell><cell>112</cell></row><row><cell>Measles</cell><cell>158</cell></row><row><cell>Normal Skin</cell><cell>41</cell></row><row><cell>Psoriasis</cell><cell>122</cell></row><row><cell>Ringworm</cell><cell>131</cell></row><row><cell>Scabies</cell><cell>79</cell></row><row><cell>Total</cell><cell>1316</cell></row><row><cell cols="2">Fig. 1. Enterovirus rash (left) vs. Measles rash (right)</cell></row><row><cell></cell><cell>Fig. 2. Illustration of residual neural networks (ResNet)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table III</head><label>III</label><figDesc>provides the performance of our final model. It shows model refinement improved the sensitivity significantly, from original 74.1% to 81.7%.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II RESULTS</head><label>II</label><figDesc>OF 5-FOLD CROSS VALIDATION</figDesc><table><row><cell>Iteration</cell><cell>Sensitivity (%)</cell><cell>Specificity (%)</cell><cell>Accuracy (%)</cell></row><row><cell>1</cell><cell>83.87</cell><cell>96.54</cell><cell>95.04</cell></row><row><cell>2</cell><cell>78.13</cell><cell>96.98</cell><cell>94.70</cell></row><row><cell>3</cell><cell>67.74</cell><cell>98.70</cell><cell>95.04</cell></row><row><cell>4</cell><cell>68.75</cell><cell>97.84</cell><cell>94.32</cell></row><row><cell>5</cell><cell>71.88</cell><cell>97.84</cell><cell>94.70</cell></row><row><cell>Average</cell><cell>74.07</cell><cell>97.58</cell><cell>94.76</cell></row><row><cell></cell><cell cols="3">Fig. 3. Values of the loss function vs learning rate</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III RESULTS</head><label>III</label><figDesc>OF OUR FINAL MODEL A comparison of four ResNet models, ResNet-34, ResNet-50, ResNet-101 and ResNet-152, on our image data set (5-fold cross validation applied)</figDesc><table><row><cell cols="2">Iteration 1 2 3 4 5 Average 0.2 0.4 0.6 0.8 B. 0 1</cell><cell>Sensitivity (%) 87.10 84.38 80.65 71.88 84.38 81.67</cell><cell>Specificity (%) 96.10 96.55 98.27 96.12 98.28 97.06</cell><cell>Accuracy (%) 95.04 95.08 96.18 93.18 96.59 95.21</cell></row><row><cell></cell><cell></cell><cell>Sensitivity</cell><cell>Specificity</cell><cell>Accuracy</cell></row><row><cell></cell><cell cols="4">RseNet-34 RseNet-50 RseNet-101 RseNet-152</cell></row><row><cell></cell><cell>0.965</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.960</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.958</cell><cell>0.958</cell></row><row><cell>Auroc</cell><cell>0.950 0.955</cell><cell>0.951</cell><cell></cell></row><row><cell></cell><cell>0.945</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.940</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Epoch</cell></row><row><cell cols="5">Fig. 4. Improvement of the area under the curve (AUC) score over</cell></row><row><cell></cell><cell></cell><cell cols="2">the process of model refinement</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank the College of Computing &amp; Technology at Lipscomb University for the support of this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDICES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Complete Resnet50 Architecture</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measles (Rubeola) Pre-vaccine Era</title>
		<ptr target="https://www.cdc.gov/measles/about/history.html" />
	</analytic>
	<monogr>
		<title level="j">Centers for Disease Control and Prevention</title>
		<imprint>
			<date type="published" when="2018-02-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measles (Rubeola) Transmission of Measles</title>
	</analytic>
	<monogr>
		<title level="j">Centers for Disease Control and Prevention</title>
		<imprint>
			<date type="published" when="2018-02-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measles (Rubeola) Measles Cases and Outbreaks</title>
		<ptr target="https://www.cdc.gov/measles/cases-outbreaks.html" />
	</analytic>
	<monogr>
		<title level="j">Centers for Disease Control and Prevention</title>
		<imprint>
			<date type="published" when="2020-06-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Paules</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Marston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Fauci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Measles in 2019 -Going Backward</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">380</biblScope>
			<biblScope unit="page" from="2185" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Melanoma detection by analysis of clinical images using convolutional neural network,&quot; in Proc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nasr-Esfahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M R</forename><surname>Soroushmehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Najarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Eng. Med. Biol. Soc</title>
		<imprint>
			<biblScope unit="page" from="1373" to="1376" />
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep CNN and Data Augmentation for Skin Lesion Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Visani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Information and Database Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">10752</biblScope>
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automated Melanoma Recognition in Dermoscopy Images via Very Deep Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Med Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="994" to="1004" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network based medical image classification for disease diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jadhav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">113</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identifying medical diagnoses and treatable diseases by image-based deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Valentim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1122" to="1153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE conf. computer vision and pattern recognition</title>
		<meeting>of IEEE conf. computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spring Research Presentation: A Theoretical Foundation for Inductive Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Warnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brigham Young University, College of Physical and Mathematical Sciences</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-organizing maps for storage and transfer of knowledge in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Karimpanal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bouffanais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behavior</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Survey on Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Risk-Aware Machine Learning Classifier for Skin Lesion Diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mobiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1241</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">a deep learning system for differential diagnosis of skin diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bui</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-020-0842-3</idno>
		<ptr target="https://doi.org/10.1038/s41591-020-0842-3" />
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated detection of erythema migrans and other confounding skin lesions via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Billings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Rebman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Aucott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="151" to="156" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">DermaDetect: A computer vision and deep learning approach for an accurate diagnosis of skin conditions and rashes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="DOI">10.13140/RG.2.2.11636.91522</idno>
		<ptr target="https://www.researchgate.net/publication/335083461" />
		<imprint>
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Classification of skin lesions using transfer learning and augmentation with Alex-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hosny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kassem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Foaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">217293</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automated Skin Lesion Classification Using Ensemble of Deep Neural Networks in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md Ashraful Alam</forename><surname>Milton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10802</idno>
	</analytic>
	<monogr>
		<title level="j">Skin Lesion Analysis Towards Melanoma Detection Challenge</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Skin lesion classification using ensembles of multi-resolution EfficientNets with meta data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gessert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schlaefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MethodsX</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">100864</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">ImageNet: crowdsourcing, benchmarking &amp; other cool things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>CMU VASC Seminar</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keras</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Skin Lesion Segmentation with Improved Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first"></forename><surname>ztrk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>zkaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
