<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SegBlocks: Block-Based Dynamic Resolution Networks for Real-Time Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Verelst</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
						</author>
						<title level="a" type="main">SegBlocks: Block-Based Dynamic Resolution Networks for Real-Time Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Conditional execution</term>
					<term>convolutional neural networks</term>
					<term>model compression</term>
					<term>semantic segmentation !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>SegBlocks reduces the computational cost of existing neural networks, by dynamically adjusting the processing resolution of image regions based on their complexity. Our method splits an image into blocks and downsamples blocks of low complexity, reducing the number of operations and memory consumption. A lightweight policy network, selecting the complex regions, is trained using reinforcement learning. In addition, we introduce several modules implemented in CUDA to process images in blocks. Most important, our novel BlockPad module prevents the feature discontinuities at block borders of which existing methods suffer, while keeping memory consumption under control. Our experiments on Cityscapes and Mapillary Vistas semantic segmentation show that dynamically processing images offers a better accuracy versus complexity trade-off compared to static baselines of similar complexity. For instance, our method reduces the number of floating-point operations of SwiftNet-RN18 by 60% and increases the inference speed by 50%, with only 0.3% decrease in mIoU accuracy on Cityscapes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. SegBlocks adjusts the processing resolution of image regions based on their complexity. A lightweight policy network decides which blocks should be processed in high resolution mode. The BlockSample module splits the image and downsamples blocks according to the policy. The resulting BlockRepresentation can be processed by typical deep convolutional networks after replacing zero-padding with our custom BlockPad modules. We propose a method to dynamically process lowcomplexity regions at a lower resolution, as illustrated in <ref type="figure">Figure 1</ref>. Our method splits an image into blocks, and then downsamples non-important blocks. Reducing the processing resolution not only reduces the computational cost, but also decreases the memory consumption. The policy network, determining whether a region should be processed in high or low resolution, is trained using reinforcement learning.</p><p>Efficiently processing images in blocks without losing accuracy is not trivial. One could treat each block as a separate image and then combine the individual block outputs. However, features cannot propagate between individual blocks <ref type="figure" target="#fig_0">(Figure 2b</ref>), leading to a loss in global context and significant decrease in accuracy. Existing works partially address this by including global features extracted by a separate network branch, requiring custom architectures <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>Our method addresses this problem by replacing zeropadding with our BlockPad operation: this custom CUDA module copies features from neighboring blocks into the padding border and therefore preserves feature propagation between blocks, as if the network was never split into blocks. A comparison between zero-padding and the BlockPad module is given in <ref type="figure" target="#fig_1">Figure 3</ref>, showing that our module avoids artifacts at block borders.</p><p>The contributions of this work are as follows:</p><p>• We introduce the concept of dynamic block-based convolutional neural networks, where blocks are downsampled based on their complexity, to reduce their computational cost. In addition, we provide CUDA modules for PyTorch to efficiently implement block-based methods 1 .</p><p>• We train the policy network with reinforcement learning, in order to select complex regions for highresolution processing. <ref type="bibr">•</ref> We demonstrate our method using a state of the art semantic segmentation network, and show that our method reduces the number of floating-point operations (FLOPS) and increases the inference speed (FPS) with only a slight decrease in mIoU accuracy. Our method achieves better accuracy than static baseline networks of similar complexity. <ref type="bibr" target="#b0">1</ref>. The code is available at https://github.com/thomasverelst/segblocks-segmentation-pytorch 2 RELATED WORK</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic segmentation</head><p>Traditional works in semantic segmentation focus on improving segmentation accuracy, without taking into account the network complexity <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. In contrast to other tasks such as classification or object detection, segmentation requires pixel-wise labels having the same resolution as the input. Preserving spatial information in the network requires high-resolution latent representations with a high computational and memory cost. At the same time, a large receptive field is needed to incorporate global context. In order to keep the network size under control, semantic segmentation networks use encoder-decoder architectures with skip connections <ref type="bibr" target="#b28">[29]</ref>, dilated convolutions <ref type="bibr" target="#b29">[30]</ref> and spatial pyramid pooling <ref type="bibr" target="#b4">[5]</ref>.</p><p>Applications such as driverless cars have raised interest in real-time inference on embedded devices. Several highresolution datasets for these applications have emerged. The Cityscapes dataset <ref type="bibr" target="#b0">[1]</ref> provides images of 2048×1024 pixels, with highly detailed annotations for 30 semantic classes. A more extensive dataset is Mapillary Vistas <ref type="bibr" target="#b30">[31]</ref> with 25000 high-resolution images and 152 object categories. As real-time inference is crucial for these applications and datasets, smaller and more efficient segmentation networks have been developed for this task specifically.</p><p>ICNet <ref type="bibr" target="#b9">[10]</ref> proposes a custom encoder architecture processing an image pyramid, with multi-resolution feature maps fused before the decoder. ERFNet <ref type="bibr" target="#b10">[11]</ref> factorizes convolution kernels into 1×3 and 3×1 kernels to reduce the computational cost. Bilateral Segmentation Network (BiSeNet) <ref type="bibr" target="#b31">[32]</ref> presents a network with two branches: a Spatial Path to encode high-resolution spatial information and a Context Path to achieve a high receptive field. DABNet <ref type="bibr" target="#b32">[33]</ref> proposes a Depthwise Asymmetric Bottleneck module to capture local and contextual information. Other methods use attention modules <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> to incorporate global context, for instance by weighting spatial features <ref type="bibr" target="#b37">[38]</ref>. SwiftNet <ref type="bibr" target="#b38">[39]</ref> demonstrates that state of the art results can be achieved with a more simple architecture: a ResNet-18 encoder <ref type="bibr" target="#b39">[40]</ref>, pretrained on ImageNet <ref type="bibr" target="#b40">[41]</ref>, is combined with a spatial pyramid pooling (SPP) module <ref type="bibr" target="#b41">[42]</ref> and basic decoder. Model compression techniques can further reduce the computational cost, by applying pruning <ref type="bibr" target="#b11">[12]</ref>, knowledge distillation <ref type="bibr" target="#b12">[13]</ref>, quantization <ref type="bibr" target="#b13">[14]</ref> or factorization <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dynamic segmentation</head><p>These efficient architectures and compression techniques achieve impressive performance on segmentation tasks, but do not exploit spatial properties of the image as they process each pixel with the same operation. Some recent methods address this by processing the image dynamically in blocks or patches. It is worth noting that dynamic methods are often complementary to static compression methods, and both can be combined to further reduce the computational cost.</p><p>The method proposed by Huang et al. <ref type="bibr" target="#b19">[20]</ref> combines a fast segmentation model with a large model. First, the image is processed by a fast network, such as ICNet <ref type="bibr" target="#b9">[10]</ref>. Then, a selection criterion, based on the softmax certainty  of the output, uses the rough predictions to decide which image regions should be re-evaluated by the large model (e.g. PSPNet <ref type="bibr" target="#b27">[28]</ref>). Such a two-stage approach has two drawbacks. First, the large network processes the image in blocks and the discontinuities at block borders stop feature propagation. Therefore, the method uses large patches of at least 256×256 pixels to include some global context. Secondly, the feature maps and predictions of the small network are not re-used by the large network, making the method less efficient and only applicable when the accuracy and cost difference between the small and large network is substantial.</p><p>Wu et al. <ref type="bibr" target="#b21">[22]</ref> propose a custom architecture where a high-resolution branch improves rough predictions of a lowresolution branch. Again, this method struggles with the feature propagation problem at patch borders and therefore uses blocks of 256×256 pixels. In addition, they incorporate global features from the low-resolution network into the high-resolution branch to provide more global context. The Patch Proposal Network <ref type="bibr" target="#b20">[21]</ref> method of Wu et al. has a similar architecture, with a low-resolution global branch and high-resolution refinement branch. The local refinement branch only operates on selected patches, based on a trained selection criterion where regions with below-average accuracy are refined. Features of both branches are fused before generating final predictions.</p><p>In contrast to these methods, our dynamic method does not require modifications to existing network architectures. We address the discontinuities at block borders without requiring additional global features in the patch-based processing. Furthermore, by being applicable on existing architectures, our method benefits from further advancements in network architectures.</p><p>Another approach to reduce spatial redundancy is to warp the input image to enlarge important regions <ref type="bibr" target="#b45">[45]</ref>. The work of Marin et al. <ref type="bibr" target="#b46">[46]</ref> uses non-uniform image sampling to focus on complex regions. However, as the internal representation of the network is still a regular-spaced pixel grid, the flexibility of non-uniform downsampling is limited, and the network typically focuses on a single region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Conditional execution</head><p>Adapting the network architecture based on the input image is also known as conditional execution <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b49">[49]</ref> or dynamic neural networks. For instance, some methods reduce the processing cost of simple images by skipping complete residual blocks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> or by dynamically pruning feature channels <ref type="bibr" target="#b17">[18]</ref>. However, these methods are intended for classification tasks, with a large variety in features between images of different classes. In segmentation, many images contain the same objects and features in different spatial arrangements.</p><p>Some dynamic methods rely on spatial properties to reduce the computational cost: they skip computations on low-complexity regions. Spatially Adaptive Computation Time <ref type="bibr" target="#b23">[24]</ref> and cascade-based methods <ref type="bibr" target="#b22">[23]</ref> halt the computation of features when features are 'good enough'. Dyn-Conv <ref type="bibr" target="#b18">[19]</ref> demonstrates inference speed improvements on human pose estimation tasks, where large regions of the image can be completely ignored by the network. The methods of Xie et al. <ref type="bibr" target="#b50">[50]</ref> and Figurnov et al. <ref type="bibr" target="#b51">[51]</ref> dynamically interpolate features in low-complexity regions. Requiring sparse convolutions, these methods either demonstrate no inference speed improvements <ref type="bibr" target="#b23">[24]</ref>, only on depthwise convolutions <ref type="bibr" target="#b18">[19]</ref> or only on CPU <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b50">[50]</ref>, or are intended for specialized hardware <ref type="bibr" target="#b52">[52]</ref>. Block-based approaches are considered to be more feasible to implement efficiently on most platforms <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. In addition, methods completely skipping non-complex regions are less suitable for pixel-wise labeling tasks such as segmentation. Our method processes every image region, but reduces the cost of noncomplex regions.</p><p>An important aspect of conditional execution is the policy which determines the layers, channels or regions to execute. Often, this policy cannot be trained by standard backpropagation due to its discrete output. Recently, the Gumbel-Softmax <ref type="bibr" target="#b53">[53]</ref>, <ref type="bibr" target="#b54">[54]</ref> trick gained popularity in dynamic methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In our case, it would require predictions for each region in both high and low-resolution, increasing the computational cost at training time. Instead, we show that the policy can be trained efficiently using reinforcement learning <ref type="bibr" target="#b55">[55]</ref>.  The policy net's gradients are estimated using REINFORCE <ref type="bibr" target="#b56">[56]</ref> with a separate reward per block, based on the task loss in a block's region and the number of blocks processed at high resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>SegBlocks splits an image into blocks and adjusts the processing resolution of each block based on its complexity. A lightweight policy network processes a low-resolution version of the image and outputs resolution decisions for each image region. For implementation simplicity, we restrict the resolution to two options, with either high-or lowresolution processing. Note that high-and low-resolution regions are processed by the same convolutional filters, which consequently perceive objects at different scales. This does not affect the performance negatively though, as segmentation images typically have large variations in object size, and convolutional neural networks are trained to be robust against scale variations. Since resolution decisions are discrete, the policy network is trained with reinforcement learning, using a reward function to determine the expected gradient.</p><p>First, we discuss how the reinforcement learning is incorporated using a reward taking into account both computational complexity and task accuracy. The policy is trained jointly with the main segmentation network. In the second part, we elaborate on custom modules making block-based adaptive processing of images possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Downsampling policy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Policy network</head><p>The policy network is a convolutional neural network that outputs a discrete decision per block. The network should be small compared to the main segmentation network. We use a simple architecture with 4 convolutional layers of 64 channels, batch normalization and ReLU non-linearities followed by a softmax layer over the channel dimension. A four times downsampled version of the image is given as input. The policy network f pn , parametrized by θ, outputs probabilities s b per block, indicating the likelihood that the block should be processed in high resolution based on the input image I:</p><formula xml:id="formula_0">s = f pn (I; θ),<label>(1)</label></formula><formula xml:id="formula_1">with s = [s 1 , . . . , s b , . . . , s B ] ∈ [0, 1] B .<label>(2)</label></formula><p>The soft probabilities are sampled to actions a, where a b = 1 results in high-resolution processing of block b:</p><formula xml:id="formula_2">s b = P (a b = 1).<label>(3)</label></formula><p>Standard backpropagation cannot be applied due to the sampling of discrete actions. Finding optimal decisions a for context I, with a constrained budget, is also referred to in literature as contextual bandits <ref type="bibr" target="#b55">[55]</ref>. This can be seen as a reinforcement learning scenario with a single time step: the policy network predicts all actions a b at once and directly obtains the reward. The probability of actions a, for image I and parameters θ is given by</p><formula xml:id="formula_3">π θ (a | I) = B b=1 p θ (a b | I).<label>(4)</label></formula><p>The objective is to find policy parameters θ, so that the predicted actions a for image I maximize the policy reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Policy reward</head><p>The goal of the policy network is to select the most important blocks for high-resolution processing. To avoid early convergence to a sub-optimal state where all blocks are processed at high resolution, the reward takes into account both the number of blocks processed at high resolution and the task accuracy. <ref type="figure" target="#fig_2">Figure 4</ref> illustrates how both rewards are integrated in the training process. We assume that a resolution decision a b only affects the segmentation output of block b and does not influence adjacent blocks. The total reward for an image can then be expressed as the sum of individual block rewards. A block reward consists of one term optimizing accuracy and second term keeping the number of operations under control, weighted by hyperparameter γ. The reward per image is then written as</p><formula xml:id="formula_4">R(a) = B b R b (a) = B b R b,task (a) + γR b,complexity (a) .<label>(5)</label></formula><p>The percentage of blocks processed in high-resolution is given by</p><formula xml:id="formula_5">σ(a) = 1 B B b=1 a b .<label>(6)</label></formula><p>Then, the following reward minimizes the difference between σ and the desired percentage τ :</p><formula xml:id="formula_6">R b,complexity (a) = −(σ(a) − τ ) if a b = 1, σ(a) − τ if a b = 0.<label>(7)</label></formula><p>This reward is positive for blocks processed at high resolution when the actual percentage σ is smaller than the desired percentage τ , resulting in an incentive for the policy network to process more blocks at high resolution. Using a target τ instead of simply minimizing σ results in more stable training and lower sensitivity to hyperparameter γ. The task reward encourages the policy network to select regions with a high segmentation loss for high resolution processing. In general, those regions correspond to the most complex ones in the image. The task criterion, e.g. pixelwise cross entropy, is denoted by L. The task loss per image is then given by</p><formula xml:id="formula_7">L task = L( y, y)<label>(8)</label></formula><p>where y and y denote the predictions and ground truth labels respectively. Our reward is based on the task loss per block. Values y b and y b are obtained by only considering values in the block region. The task reward of block b can then be defined as</p><formula xml:id="formula_8">R b,task (a) = L( y b , y b ) − L task if a b = 1, −(L( y b , y b ) − L task ) if a b = 0.<label>(9)</label></formula><p>Subtracting L task is not strictly necessary but reduces the variance of the rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Expected gradient</head><p>The policy network should predict actions that maximize the expected reward:</p><formula xml:id="formula_9">max J (θ) = max E a∼π θ [R(a)].<label>(10)</label></formula><p>The policy network's parameters θ can then be updated using gradient ascent with learning rate α:</p><formula xml:id="formula_10">θ ← θ + α∇ θ [J (θ)]<label>(11)</label></formula><p>The gradients of J can be derived similarly to the policy gradient method REINFORCE <ref type="bibr" target="#b56">[56]</ref>:</p><formula xml:id="formula_11">∇ θ J (θ) = ∇ θ E a [R(a)] = ∇ θ a π θ (a | I)R(a) = a ∇ θ π θ (a | I)R(a) = a π θ (a | I)∇ θ [log π θ (a | I)R(a)] = a π θ (a | I)∇ θ [log B b=1 p θ (a b | I)]R(a) = a π θ (a | I)R(a) B b=1 ∇ θ log p θ (a b | I) = E a R(a) B b=1 ∇ θ log p θ (a b | I) = E a B b=1 R b (a)∇ θ log p θ (a b | I) .<label>(12)</label></formula><p>In practice, the expectation of Equation 12 is approximated with Monte-Carlo sampling using samples in the minibatch. The result can be interpreted as applying REIN-FORCE on each block individually with reward R b (a), giving unbiased but high-variance gradient estimates. In practice, we found the policy network stable to train for a large range of hyperparameters. The segmentation network and policy network are jointly trained. The hybrid loss to be minimized is then given by</p><formula xml:id="formula_12">L hybrid = L task + β N N n=1 B b=1 R b (a)log p θ (a b | I)<label>(13)</label></formula><p>with N the batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Block Modules</head><p>We introduce three new modules for block-based processing with convolutional neural networks: BlockSample, Block-Pad, and BlockCombine. <ref type="figure">Figure 5</ref> gives an overview of a simple block-processing pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">BlockSample</head><p>The BlockSample module splits the image into blocks and downsamples low-complexity blocks consecutively. Average pooling is used for efficient downsampling. We use a downsampling factor of 2, reducing the computational cost of low-complexity regions by a factor 4. The CUDA implementation fuses the splitting and downsampling steps into a single operation. Images are split into blocks of a predefined block size, for instance 128×128 pixels. Downsampling operations, such as max pooling, further reduce the spatial dimensions of blocks throughout the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">BlockPad</head><p>The BlockPad module replaces zero-padding and eliminates discontinuities at block borders by copying features from neighboring blocks into the padding. Evaluating the network with only high-resolution blocks is equivalent to normal processing without blocks. When two high-resolution or two low-resolution blocks are adjacent, pixels can be copied directly while preserving their spatial relationship. When copying features from low-resolution blocks into the padding of high-resolution blocks, features are nearest neighbor upsampled in order to preserve the spatial relationship, as illustrated in <ref type="figure">Figure 5</ref>. When copying features from high-resolution blocks into the low-resolution block's padding, we considered two possibilities: strided subsampling and average sampling. <ref type="figure">Figure 6a</ref> illustrates that subsampling copies features in a strided pattern., whilst average sampling <ref type="figure">(Fig 6b)</ref> combines multiple pixels. Our experiments show that average sampling achieves better performance with a small increase in overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">BlockCombine</head><p>The BlockCombine module upsamples low-resolution blocks to the same resolution as the high-resolution ones, and then combines all blocks into a single tensor. Our CUDA implementation merges the nearest neighbour upsampling and block combining steps to reduce the overhead of this operation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We integrate our dynamic processing method in Swift-Net <ref type="bibr" target="#b38">[39]</ref>, a state of the art network for real-time semantic segmentation of road scenes. We test semantic segmentation on the Cityscapes <ref type="bibr" target="#b0">[1]</ref> and Mapillary Vistas <ref type="bibr" target="#b30">[31]</ref> datasets. In addition, we provide an ablation study, analyzing the overhead of block-based processing. Our method is implemented in PyTorch 1.5 and CUDA 10.2, without TensorRT optimizations. We report two model complexity metrics: the number of computations and the frames per second. The number of computations is reported in billions of multiply-accumulates (GMACs) per image, being half the number of floating-point operations (FLOPS). The FLOPS metric is often used to compare model complexity in a platform-agnostic manner, since the efficiency of the implementation has no impact. For our dynamic method, where the number of computations varies per image, we report the average GMAC count.</p><p>Frames per second (FPS) is a more practical metric to measure inference speed, but depends largely on the implementation of building blocks. Therefore, both FPS and GMACs metrics should be taken into account for fair comparison. We measure the inference speed on both a high-end Nvidia GTX 1080 Ti 11GB GPU and low-end Nvidia GTX 1050 Ti 4 GB GPU, paired with an Intel i7 processor. The model is warmed up on the first half of the image set, and the speed is measured on the second half. To compare the inference speed to those reported by others, we normalize the FPS numbers (norm FPS) of different GPUs based on their relative performance, using the same scaling factors as Orsic et al. <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cityscapes semantic segmentation</head><p>The Cityscapes <ref type="bibr" target="#b0">[1]</ref> dataset for semantic segmentation consists of 2975 training, 500 validation and 1525 test images of 2048×1024 pixels. We use the standard 19 classes for semantic segmentation and do not use of the extra coarsely labeled images.</p><p>We train SwiftNet <ref type="bibr" target="#b38">[39]</ref> with three different backbones: ResNet-18 (RN18), ResNet-50 (RN50) <ref type="bibr" target="#b39">[40]</ref>, and ShuffleNet-V2 <ref type="bibr" target="#b8">[9]</ref>. Our dynamic processing method is integrated in these baseline networks and we trained models for different τ ∈ [0, 1], indicating the desired percentage of highresolution blocks. The models are trained on 768×768 crops, augmented by image scaling between factor 0.5 and 2, [−10, 10] degrees of rotation, color jitter and random horizontal flip. The optimizer is Adam, the learning rate is cosine annealed from 4e−4 to 1e−6 over 300 epochs, weight decay is set to 1e−4 and the batch size is 8. The backbone is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 2</head><p>IoU per class on the Cityscapes validation set: our method improves the IoU score for most classes. Instance classes such as person, rider, car, bus and train benefit more from high-resolution processing. The percentage of pixels processed in high-resolution, given per class, shows that our method mostly processes road and sky regions at low resolution. pre-trained on ImageNet <ref type="bibr" target="#b40">[41]</ref>. Our method uses a block size of 128×128 pixels, resulting in 16×8 blocks per image that can adapt the processing resolution to the content. <ref type="figure" target="#fig_4">Figure 7</ref> shows the mIoU accuracy of models at various computational costs (GMACs). Static baseline networks of different computational complexity are obtained by adjusting the training and evaluation resolution to 2048×1024, 1536×768 and 1344×672. The complexity of our SegBlocks methods is determined by the number of high-resolution blocks, changed by training for different values of hyperparameter τ ∈ {0.2, 0.4, 0.6}. Our SegBlocks methods consistently outperform the static baseline networks, showing that dynamic resolution processing is more beneficial than sampling all regions at lower resolution. For instance, SegBlocks-RN18 with τ = 0.4 reduces the computational complexity of the baseline network by 40%, without decreasing the mIoU. In contrast, a static baseline network of similar complexity, trained on images of 1536×768 pixels, has 1.7% lower mIoU. <ref type="table" target="#tab_3">Table 1</ref> compares the performance of our method with other dynamic methods and non-dynamic efficient segmentation architectures. Our method outperforms other dynamic methods, such as Patch Proposal Network <ref type="bibr" target="#b20">[21]</ref> or the work by Huang et al. <ref type="bibr" target="#b19">[20]</ref>, by achieving better accu- racy at faster inference speeds (FPS) and fewer operations (GMACs). We are also competitive with state of the art architectures for fast semantic segmentation, such as BiSeNet <ref type="bibr" target="#b31">[32]</ref> or ERFNet <ref type="bibr" target="#b10">[11]</ref>. It is worthwhile to note that our proposed dynamic resolution method is complementary to further improvements in network architectures. The table also demonstrates the inference speed improvement achieved using our efficient implementation. For instance, our method improves the inference speed of SwiftNet-RN50 from 16 FPS to 30 FPS on a GTX 1080 Ti, achieving real-time performance. This increase of 84 percent is obtained by reducing the theoretical complexity (GMACs) with 63%, while the mean IoU is decreased by 1.8%. Finally, the memory usage, being the maximum memory consumption measured during inference over the validation set, indicates that our method also reduces memory consumption by storing low-complexity regions at lower resolution. The IoU result per class is provided in <ref type="table">Table 2</ref>, combined with the percentage of pixels processed in high-resolution for each class. Our method mainly processes classes such as road, sky and vegetation in low resolution, whereas rider, motorcycle and bicycle are typically sampled in high resolution. Our method adapts the operations to each individual image, and <ref type="figure" target="#fig_5">Figure 8</ref> shows the distribution of operations per image. Qualitative results are shown in <ref type="figure">Figure 9</ref>, visualizing the resolution decisions of the policy network and the respective segmentation output, for τ set to 0.4 and 0.2. The policy network, trained with reinforcement learning, properly selects complex regions for high-resolution processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mapillary Vistas semantic segmentation</head><p>Mapillary Vistas <ref type="bibr" target="#b30">[31]</ref> is a large dataset containing highresolution road scene images with labels for semantic and instance segmentation. We use the standard 15000/2000 train/val split, resize all images to 2048×1024 pixels and use the 66 classes for semantic segmentation. We report validation results, since the test server is not available. The hyperparameters are the same as the Cityscapes experiments, but the model is trained for 150 epochs without rotation and color jitter augmentations due to the larger dataset. Similar to the experiments on Cityscapes, we integrate our method in the SwiftNet-RN50 baseline network. <ref type="table" target="#tab_5">Table 3</ref> demonstrates that SegBlocks also outperforms static baseline networks on this dataset, resulting in better accuracy when comparing models with either similar computational complexity (GMACs) or inference speed (FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Module execution time analysis</head><p>We profile the time characteristics of our block modules to analyze their overhead.  <ref type="table" target="#tab_8">Table 5</ref> compares the impact of the method's block sizes on accuracy and performance. <ref type="figure">Figure 10</ref> demonstrates the    <ref type="table" target="#tab_9">Table 6</ref> compares average-sampling with strided subsampling and shows that average sampling achieves better accuracy (mIoU) with only a slight increase in overhead. In addition, we show that using zero-padding has a significant impact on accuracy, highlighting the importance of BlockPad when processing images in blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Block size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Single residual block analysis</head><p>The overhead introduced by our block modules strongly depends on the block size: larger blocks have fewer pixels in the padding, and require fewer copy operations. Moreover, standard operations, implemented in PyTorch and cuDNN, are often optimized for larger spatial dimensions. We analyze a single residual block to measure the impact of both block size and the percentage of high-resolution blocks. <ref type="figure" target="#fig_7">Figure 11a</ref> studies the impact of the block size, when evaluating half of the blocks at high and half of the blocks at low resolution. The number of floating-point operations is reduced by 56%, resulting in a theoretical 2.3 times speed increase. In practice, we measure 1.92 times faster inference of the residual block when the block size is larger than 32. Block sizes smaller than 4 pixels result in slower inference. Note that, even though block sizes are typically large at the network input, e.g. 128×128, downsampling in the network reduces the block size by the same factor. The SwiftNet network downsamples by a factor 32 throughout the network, resulting in block sizes of 4×4 for the deepest network layers. <ref type="figure" target="#fig_7">Figure 11a</ref> shows that, when evaluating using block size 8, the percentage of high-res blocks should be lower than 75% to achieve practical speedup. For lower percentages, the practical speedup is around 70 percent of the theoretical one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We proposed a method to reduce the computational cost of existing convolutional neural networks, by evaluating images in blocks and adjusting the processing resolution of each block dynamically. We introduced custom block operations, enabling efficient inference and training of these dynamic neural networks. Our BlockPad module is essential to enable feature propagation between individual blocks. A lightweight policy network, trained with reinforcement learning, selects complex regions for high-resolution processing. Our method is demonstrated on semantic segmentation of street scenes and we show that the computational complexity of the SwiftNet architecture can be reduced with only a small decrease in accuracy. Dynamically adjusting the processing resolution per block achieves better accuracy than simply downscaling the input image to a lower resolution.</p><p>The idea of block-based processing and dynamic resolution adaptation can be integrated in various network architectures and computer vision tasks. In particular, our method is suited for dense pixel-wise classification tasks such as depth estimation, instance segmentation or human pose estimation. Furthermore, our block modules such as BlockPad pave the way towards other variants, for instance by processing blocks with varying quantization levels. Dynamic processing can help to keep the number of operations and energy consumption under control. Thomas Verelst is a PhD student at the PSI group of the Electrical Engineering department of KU Leuven. He received a MS degree in Electrical Engineering at the same university in 2018. His research focuses on computer vision using efficient neural networks for classification, pose estimation and segmentation tasks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>arXiv:2011.12025v1 [cs.CV] 24 Nov 2020 kernel (a) Convolution on full image kernel (b) Convolution on blocks Illustration of the zero-padding problem when processing blocks: convolutions pad individual blocks with zeros (grey), stopping the propagation of features between blocks and therefore resulting in a loss of global context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Comparison of feature maps when processing an image in blocks with either standard zero-padding or our custom BlockPad module. Features are visualized by summing activation magnitudes over the channel dimension and combining the outputs of individual blocks into a single image. Standard zero-padding introduces noticeable artifacts at block borders. In addition, the output shows inconsistencies, e.g. in the car, due to the lack of global context for individual blocks. In contrast, our BlockPad module enables feature propagation is if the network was never evaluated in blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Training the policy network with reinforcement learning: The lightweight policy net predicts soft decisions s, which are sampled to actions a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Illustration of the BlockSample, BlockPad and BlockCombine modules. BlockSample splits the image and downsamples low-complexity blocks. BlockPad replaces zero-padding and enables feature propagation between blocks. The module copies features from neighboring blocks into the padding. When padding low-resolution blocks adjacent to high-resolution blocks, multiple pixel values of the high-resolution blocks are averaged to better preserve the spatial coherence of features.(a) Strided subsampling (b) Average sampling BlockPad aims to respect the spatial relationship between highand low-resolution blocks by sampling using the illustrated patterns. Low-resolution blocks of resolution 2×2 pixels are colored dark grey, while the 4×4 high-resolution blocks are colored light grey. The blue and red grid shows the sampling pattern when padding high-resolution and low-resolution blocks respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Cityscapes validation results and comparison with static baselines of similar complexity. For a given computational complexity (GMACs), our adaptive resolution method SegBlocks consistently outperforms static networks trained at lower resolution, as well as smaller static backbones. Each backbone is trained and evaluated with τ ∈ 0.2, 0.4, 0.6 for dynamic models and resolutions 2048×1024, 1536×768, 1344×672 for static baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Percentage of high-resolution blocks per image, as a histogram over the 500 validation images of Cityscapes, for SegBlocks-RN18 (τ =0.4). Simple images use fewer high-resolution blocks, down to 20%, and complex images process up to 55% of the blocks in high resolution (colored yellow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 Fig. 9 .</head><label>29</label><figDesc>Examples of resolution decisions made by the policy network and corresponding segmentation outputs, for SegBlocks-RN18 with τ = 0.4 and τ = 0.2. High-resolution blocks are colored in yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Comparison of theoretical versus practical speedup for a single residual block of ResNet-18, studying the impact of the block size and the percentage of high-resolution blocks on the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Tinne</head><label></label><figDesc>Tuytelaars is a full professor at the Electrotechnical department of KU Leuven. Her research focuses on image understanding, with a focus on representation learning, multimodal learning (images and text) and continual learning. In 2009, she received an ERC starting independent researcher grant, and in 2016, she received the Koenderink award. She has been one of the Program Chairs of the European Conference on Computer Vision 2014 and of IEEE/CVF Conference on Computer Vision and Pattern Recognition 2021, and one of the General Chairs of IEEE/CVF Conference on Computer Vision and Pattern Recognition 2016. She has been associate editor in chief of the IEEE Transactions on Pattern Analysis and Machine Intelligence and serves as area editor for the International Journal on Computer Vision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>High-resolution image: 2048 x 1024</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Expected gradient</cell></row><row><cell>4x</cell><cell>Conv</cell><cell>Conv</cell><cell>Conv</cell><cell></cell><cell></cell><cell>Sample</cell></row><row><cell>Downsample</cell><cell>BN</cell><cell>BN</cell><cell>BN</cell><cell>Conv</cell><cell>softmax</cell><cell>actions</cell></row><row><cell></cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Block</cell><cell>Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sample</cell><cell>Combine</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1</head><label>1</label><figDesc>Results on Cityscapes semantic segmentation. Our SegBlocks models are based on the respective SwiftNet baselines, and integrate block-based dynamic resolution processing in these networks.</figDesc><table><row><cell></cell><cell></cell><cell>Method</cell><cell>mIoU val</cell><cell>test</cell><cell>GMACs</cell><cell>FPS</cell><cell cols="2">norm FPS memory (max)</cell></row><row><cell></cell><cell></cell><cell>SwiftNet-RN50 (baseline, our impl.)</cell><cell>78.0</cell><cell>-</cell><cell>206.3</cell><cell>16.3 @ 1080 Ti, 3.8 @ 1050 Ti</cell><cell>16.3</cell><cell>2724 MB</cell></row><row><cell></cell><cell></cell><cell>SegBlocks-RN50 (τ =0.4)</cell><cell>77.5</cell><cell>-</cell><cell>127.2</cell><cell>23.3 @ 1080 Ti, 5.7 @ 1050 Ti</cell><cell>23.3</cell><cell>2957 MB</cell></row><row><cell cols="2">Our method</cell><cell>SegBlocks-RN50 (τ =0.2) SwiftNet-RN18 (baseline, our impl.) SegBlocks-RN18 (τ =0.4) SegBlocks-RN18 (τ =0.2) SwiftNet-SNv2 (baseline, our impl.)</cell><cell>76.2 76.2 76.3 75.9 75.2</cell><cell>-74.4 73.8 --</cell><cell>88.5 104.1 60.5 43.5 49.1</cell><cell>30.0 @ 1080 Ti, 7.3 @ 1050 Ti 38.4 @ 1080 Ti, 9.9 @ 1050 Ti 48.6 @ 1080 Ti, 13.5 @ 1050 Ti 57.8 @ 1080 Ti, 16.8 @ 1050 Ti 34.7 @ 1080 Ti, 10.5 @ 1050 Ti</cell><cell>30.0 38.4 48.6 57.8 34.7</cell><cell>2044 MB 2256 MB 1862 MB 1928 MB 1338 MB</cell></row><row><cell></cell><cell></cell><cell>SegBlocks-SNv2 (τ =0.4)</cell><cell>74.8</cell><cell>73.8</cell><cell>27.6</cell><cell>45.3 @ 1080 Ti, 13.1 @ 1050 Ti</cell><cell>45.3</cell><cell>1210 MB</cell></row><row><cell></cell><cell></cell><cell>SegBlocks-SNv2 (τ =0.2)</cell><cell>74.5</cell><cell>-</cell><cell>20.6</cell><cell>56.0 @ 1080 Ti, 16.1 @ 1050 Ti</cell><cell>56.0</cell><cell>1192 MB</cell></row><row><cell>Dynamic</cell><cell>networks</cell><cell>Patch Proposal Network (AAAI2020) [21] Huang et al. (MVA2019) [20] Wu et al. [22] Learning Downsampling (ICCV2019) [46]</cell><cell>75.2 76.4 72.9 65.0</cell><cell>----</cell><cell>---34</cell><cell>24 @ 1080 Ti 1.8 @ 1080 Ti 15 @ 980 Ti -</cell><cell>24 1.8 33 -</cell><cell>1137 MB ---</cell></row><row><cell>Efficient</cell><cell>static networks</cell><cell>ShelfNet18-lw (CVPR2019) [57] SwiftNet-RN18 (CVPR2019) [39] DABNet (BMVC2019) [33] BiSeNet-RN18 (ECCV2018) [32] ICNet (ECCV2018) [10] GUNet (BMVC2018) [58] ERFNet (TITS2017) [11]</cell><cell>-75.4 70.1 74.8 --72.7</cell><cell>74.8 --74.7 69.5 70.4 69.7</cell><cell>95 104.0 -67 30 -27.7</cell><cell>36.9 @ 1080 Ti 39.9 @ 1080 Ti 27.7 @ 1080 Ti 65.5 @ Titan Xp 30.3 @ Titan X 33.3 @ Titan Xp 11.2 @ Titan X</cell><cell>36.9 39.9 27.7 58.5 49.7 29.7 18.4</cell><cell>-------</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 Mapillary</head><label>3</label><figDesc>Vistas validation results for semantic segmentation. The SwiftNet-RN50<ref type="bibr" target="#b38">[39]</ref> baseline is trained and evaluated at various resolutions. Our dynamic SegBlocks models outperform static baselines of similar complexity. The FPS is measured on an Nvidia GTX 1080 Ti.</figDesc><table><row><cell>Method</cell><cell cols="3">mIoU GMACs FPS</cell></row><row><cell>SwiftNet-RN50 (2048×1024) (our impl.)</cell><cell>39.8</cell><cell>207.1</cell><cell>15.2</cell></row><row><cell>SwiftNet-RN50 (1536×768) (our impl.)</cell><cell>38.3</cell><cell>116.5</cell><cell>28.7</cell></row><row><cell>SwiftNet-RN50 (1344×672) (our impl.)</cell><cell>38.0</cell><cell>89.1</cell><cell>37.0</cell></row><row><cell>SegBlocks-RN50 (τ =0.4)</cell><cell>39.7</cell><cell>113.5</cell><cell>21.5</cell></row><row><cell>SegBlocks-RN50 (τ =0.2)</cell><cell>39.4</cell><cell>84.4</cell><cell>28.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>compares the execution time of the Policy Net, BlockSample, BlockPad and Block-Combine modules with the total runtime. The overhead of the Policy Net, BlockSample and BlockCombine modules is negligible, and the overhead of the BlockPad operation is reasonable with around 10 percent of the total execution time. The BlockPad module has less impact on the ResNet-50 backbone, as its 1×1 convolutions in the bottleneck function do not require padding. Note that the cost of the BlockPad operation scales with the total number of processed pixels, and thus with the number of high-resolution blocks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4</head><label>4</label><figDesc>Time profiling of block modules, as total time taken during the inference of 250 validation images on an Nvidia GTX 1080 Ti GPU.</figDesc><table><row><cell>Method</cell><cell cols="4">mIoU GMACs policy GMACs Runtime</cell><cell cols="2">Policy Net BlockSample</cell><cell cols="2">BlockPad BlockCombine</cell></row><row><cell>SegBlocks-RN50 (τ =0.4)</cell><cell>77.5%</cell><cell>127.2</cell><cell>0.09</cell><cell cols="2">11.6 s 0.04 s (&lt;1%)</cell><cell>0.12 s (1%)</cell><cell>0.75 s (6%)</cell><cell>0.04 s (&lt;1%)</cell></row><row><cell>SegBlocks-RN50 (τ =0.2)</cell><cell>76.2%</cell><cell>88.6</cell><cell>0.09</cell><cell cols="2">9.3 s 0.04 s (&lt;1%)</cell><cell>0.13 s (1%)</cell><cell>0.61 s (7%)</cell><cell>0.03 s (&lt;1%)</cell></row><row><cell>SegBlocks-RN18 (τ =0.4)</cell><cell>76.3%</cell><cell>60.5</cell><cell>0.09</cell><cell cols="2">5.8 s 0.04 s (&lt;1%)</cell><cell cols="2">0.11 s (2%) 0.68 s (12%)</cell><cell>0.02 s (&lt;1%)</cell></row><row><cell>SegBlocks-RN18 (τ =0.2)</cell><cell>75.9%</cell><cell>43.5</cell><cell>0.09</cell><cell cols="2">4.9 s 0.04 s (&lt;1%)</cell><cell cols="2">0.12 s (2%) 0.56 s (11%)</cell><cell>0.02 s (&lt;1%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5</head><label>5</label><figDesc>Block size comparison for SegBlocks-RN18 with τ =0.4 on the Cityscapes validation set Fig. 10. Granularity of blocks of 64, 128 and 256 pixels on Cityscapes' images of 2048×1024 pixels.</figDesc><table><row><cell cols="4">Block size mIoU GMACs FPS (1050 Ti)</cell></row><row><cell>64×64</cell><cell>76.3%</cell><cell>59.3</cell><cell>11.3</cell></row><row><cell>128×128</cell><cell>76.3%</cell><cell>60.5</cell><cell>13.5</cell></row><row><cell>256×256</cell><cell>75.2%</cell><cell>56.4</cell><cell>14.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6</head><label>6</label><figDesc>Comparison of BlockPad sampling patterns and zero-padding for SegBlocks-RN18 (τ =0.4) As described in Section 3.2.2, the BlockPad module pads individual blocks by sampling their neighbors. When low-resolution blocks are padded with features of highresolution blocks, these features should be subsampled.</figDesc><table><row><cell>Method</cell><cell cols="2">mIoU Runtime</cell><cell>BlockPad time</cell></row><row><cell>Avg-sampling</cell><cell>76.3%</cell><cell>5.8 s</cell><cell>0.68 s (12%)</cell></row><row><cell>Strided sampling</cell><cell>75.6%</cell><cell>5.7 s</cell><cell>0.63 s (11%)</cell></row><row><cell>Zero-padding</cell><cell>70.2%</cell><cell cols="2">5.2 s N.A. (integrated in conv.)</cell></row><row><cell cols="4">granularity of each block size on Cityscapes. Larger block</cell></row><row><cell cols="4">sizes offer better inference speeds, by padding fewer pixels</cell></row><row><cell cols="4">and having less overhead, whereas smaller block sizes offer</cell></row><row><cell cols="3">finer control of adaptive processing.</cell><cell></cell></row><row><cell cols="2">4.3.3 Sampling pattern</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Int. Symp. on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koperski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting><address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Geoscience and Remote Sensing Symp. (IGARSS)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3226" to="3229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An Analysis of Deep Neural Network Models for Practical Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Canziani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07678</idno>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient processing of deep neural networks: A tutorial and survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="2295" to="2329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition. Salt Lake City</title>
		<meeting><address><addrLine>UT</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ICNet for Real-Time Semantic Segmentation on High-Resolution Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer Int. Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11207</biblScope>
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Álvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal Brain Damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>2, D. S. Touretzky</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quantized Convolutional Neural Networks for Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="4820" to="4828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SkipNet: Learning Dynamic Routing in Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer Int. Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11217</biblScope>
			<biblScope unit="page" from="420" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional networks with adaptive inference graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BlockDrop: Dynamic Inference Paths in Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="8817" to="8826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Batch-Shaping for Learning Conditional Channel Gated Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06627</idno>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic Convolutions: Exploiting Spatial Sparsity for Faster Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Verelst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="2317" to="2326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Uncertainty based model selection for fast semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Int. Conf. on Machine Vision Applications (MVA)</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Patch Proposal Network for Fast Semantic Segmentation of High-Resolution Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artificial Intelligence</title>
		<meeting>AAAI Conf. on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00213</idno>
		<title level="m">Real-time Semantic Image Segmentation via Spatial Sparsity</title>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatially Adaptive Computation Time for Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1790" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic Block Sparse Reparameterization of Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Vooturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kothapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision Workshop (ICCVW)</title>
		<meeting><address><addrLine>Seoul, Korea (South</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="3046" to="3053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SBNet: Sparse Blocks Network for Fast Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition. Salt Lake City</title>
		<meeting><address><addrLine>UT</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="8711" to="8720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Computation on sparse neural networks: an inspiration for future hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11946</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="5000" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">BiSeNet: Bilateral Segmentation Network for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer Int. Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11217</biblScope>
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Dabnet: Depth-wise asymmetric bottleneck for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11357</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision</title>
		<meeting>IEEE Int. Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">384</biblScope>
			<biblScope unit="page" from="182" to="191" />
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Asymmetric Non-Local Neural Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV). Seoul</title>
		<meeting><address><addrLine>Korea (South</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03815</idno>
		<title level="m">Real-time Semantic Segmentation with Fast Attention</title>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07971</idno>
		<title level="m">Non-local Neural Networks</title>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">In Defense of Pre-Trained ImageNet Architectures for Real-Time Semantic Segmentation of Road-Driving Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="12" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ima-geNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Conf. on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="6655" to="6659" />
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3866</idno>
		<title level="m">Speeding up Convolutional Neural Networks with Low Rank Expansions</title>
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to zoom: a saliency-based sampling layer for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient Segmentation: Learning Downsampling Near Semantic Boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Seoul, Korea (South</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="2131" to="2141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06297</idno>
		<title level="m">Conditional Computation in Neural Networks for faster models</title>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1305.0445</idno>
		<title level="m">Deep Learning of Representations: Looking Forward</title>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08866</idno>
		<title level="m">Spatially Adaptive Inference with Stochastic Feature Sampling and Interpolation</title>
		<imprint>
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Perforatedcnns: Acceleration through elimination of redundant convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ibraimova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="947" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Channel gating neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1886" to="1896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
	</analytic>
	<monogr>
		<title level="j">Categorical Reparameterization with Gumbel-Softmax</title>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Reinforcement learning: an introduction, ser. Adaptive computation and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Shelfnet for fast semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision Workshops (ICCVW)</title>
		<meeting>IEEE Int. Conf. on Computer Vision Workshops (ICCVW)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Guided upsampling network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazzini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07466</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
