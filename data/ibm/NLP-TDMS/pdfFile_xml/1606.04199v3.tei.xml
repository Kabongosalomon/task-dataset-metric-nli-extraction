<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>zhoujie01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research -Institute of Deep Learning Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
							<email>caoying03@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research -Institute of Deep Learning Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Wang</surname></persName>
							<email>wangxuguang@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research -Institute of Deep Learning Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
							<email>lipeng17@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research -Institute of Deep Learning Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
							<email>wei.xu@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research -Institute of Deep Learning Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fastforward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT'14 Englishto-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT'14 English-to-German task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) has attracted a lot of interest in solving the machine translation (MT) problem in recent years <ref type="bibr" target="#b13">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b24">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. Unlike conventional statistical machine translation (SMT) systems <ref type="bibr" target="#b16">(Koehn et al., 2003;</ref><ref type="bibr" target="#b5">Durrani et al., 2014)</ref> which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon. Moreover, NMT models can also be easily adapted to other tasks such as dialog systems , question answering systems <ref type="bibr" target="#b29">(Yu et al., 2015)</ref> and image caption generation <ref type="bibr" target="#b20">(Mao et al., 2015)</ref>.</p><p>In general, there are two types of NMT topologies: the encoder-decoder network <ref type="bibr" target="#b24">(Sutskever et al., 2014)</ref> and the attention network <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>. The encoder-decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word by word. The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words. Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems <ref type="bibr" target="#b19">(Luong et al., 2015;</ref><ref type="bibr" target="#b12">Jean et al., 2015)</ref>.</p><p>However, a single neural model of either of the above types has not been competitive with the best conventional system <ref type="bibr" target="#b5">(Durrani et al., 2014)</ref> when evaluated on the WMT'14 English-to-French task. The best BLEU score from a single model with six layers is only 31.5 <ref type="bibr" target="#b19">(Luong et al., 2015)</ref> while the conventional method of <ref type="bibr" target="#b5">(Durrani et al., 2014)</ref> achieves 37.0.</p><p>We focus on improving the single model perfor-mance by increasing the model depth. Deep topology has been proven to outperform the shallow architecture in computer vision. In the past two years the top positions of the ImageNet contest have always been occupied by systems with tens or even hundreds of layers <ref type="bibr" target="#b25">(Szegedy et al., 2015;</ref><ref type="bibr" target="#b8">He et al., 2016)</ref>. But in NMT, the biggest depth used successfully is only six <ref type="bibr" target="#b19">(Luong et al., 2015)</ref>. We attribute this problem to the properties of the Long Short-Term Memory (LSTM) <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997)</ref> which is widely used in NMT. In the LSTM, there are more non-linear activations than in convolution layers. These activations significantly decrease the magnitude of the gradient in the deep topology, especially when the gradient propagates in recurrent form. There are also many efforts to increase the depth of the LSTM such as the work by <ref type="bibr" target="#b14">Kalchbrenner et al. (2016)</ref>, where the shortcuts do not avoid the nonlinear and recurrent computation.</p><p>In this work, we introduce a new type of linear connections for multi-layer recurrent networks. These connections, which are called fast-forward connections, play an essential role in building a deep topology with depth of 16. In addition, we introduce an interleaved bi-directional architecture to stack LSTM layers in the encoder. This topology can be used for both the encoder-decoder network and the attention network. On the WMT'14 Englishto-French task, this is the deepest NMT topology that has ever been investigated. With our deep attention model, the BLEU score can be improved to 37.7 outperforming the shallow model which has six layers <ref type="bibr" target="#b19">(Luong et al., 2015)</ref> by 6.2 BLEU points. This is also the first time on this task that a single NMT model achieves state-of-the-art performance and outperforms the best conventional SMT system <ref type="bibr" target="#b5">(Durrani et al., 2014)</ref> with an improvement of 0.7. Even without using the attention mechanism, we can still achieve 36.3 with a single model. After model ensembling and unknown word processing, the BLEU score can be further improved to 40.4. When evaluated on the subset of the test corpus without unknown words, our model achieves 41.4. As a reference, previous work showed that oracle rescoring of the 1000-best sequences generated by the SMT model can achieve the BLEU score of about 45 <ref type="bibr" target="#b24">(Sutskever et al., 2014)</ref>. Our models are also validated on the more difficult WMT'14 English-to-German task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>Neural machine translation aims at generating the target word sequence y = {y 1 , . . . , y n } given the source word sequence x = {x 1 , . . . , x m } with neural models. In this task, the likelihood p(y | x, θ) of the target sequence will be maximized <ref type="bibr">(Forcada andÑeco, 1997)</ref> with parameter θ to learn:</p><formula xml:id="formula_0">p(y | x; θ) = m+1 j=1 p(y j | y 0:j−1 , x; θ)<label>(1)</label></formula><p>where y 0:j−1 is the sub sequence from y 0 to y j−1 . y 0 and y m+1 denote the start mark and end mark of target sequence respectively. The process can be explicitly split into an encoding part, a decoding part and the interface between these two parts. In the encoding part, the source sequence is processed and transformed into a group of vectors e = {e 1 , · · · , e m } for each time step. Further operations will be used at the interface part to extract the final representation c of the source sequence from e. At the decoding step, the target sequence is generated from the representation c.</p><p>Recently, there have been two types of NMT models which are different in the interface part. In the encoder-decoder model <ref type="bibr" target="#b24">(Sutskever et al., 2014)</ref>, a single vector extracted from e is used as the representation. In the attention model <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>, c is dynamically obtained according to the relationship between the target sequence and the source sequence.</p><p>The recurrent neural network (RNN), or its specific form the LSTM, is generally used as the basic unit of the encoding and decoding part. However, the topology of most of the existing models is shallow. In the attention network, the encoding part and the decoding part have only one LSTM layer respectively. In the encoder-decoder network, researchers have used at most six LSTM layers <ref type="bibr" target="#b19">(Luong et al., 2015)</ref>. Because machine translation is a difficult problem, we believe more complex encoding and decoding architecture is needed for modeling the relationship between the source sequence and the target sequence. In this work, we focus on enhancing the complexity of the encoding/decoding architecture by increasing the model depth.</p><p>Deep neural models have been studied in a wide range of problems. In computer vision, models with more than ten convolution layers outperform shallow ones on a series of image tasks in recent years <ref type="bibr" target="#b23">(Srivastava et al., 2015;</ref><ref type="bibr" target="#b8">He et al., 2016;</ref><ref type="bibr" target="#b25">Szegedy et al., 2015)</ref>. Different kinds of shortcut connections are proposed to decrease the length of the gradient propagation path. Training networks based on LSTM layers, which are widely used in language problems, is a much more challenging task. Because of the existence of many more nonlinear activations and the recurrent computation, gradient values are not stable and are generally smaller. Following the same spirit for convolutional networks, a lot of effort has also been spent on training deep LSTM networks. <ref type="bibr" target="#b28">Yao et al. (2015)</ref> introduced depth-gated shortcuts, connecting LSTM cells at adjacent layers, to provide a fast way to propagate the gradients. They validated the modification of these shortcuts on an MT task and a language modeling task. However, the best score was obtained using models with three layers. Similarly, <ref type="bibr" target="#b14">Kalchbrenner et al. (2016)</ref> proposed a two dimensional structure for the LSTM. Their structure decreases the number of nonlinear activations and path length. However, the gradient propagation still relies on the recurrent computation. The investigations were also made on question-answering to encode the questions, where at most two LSTM layers were stacked <ref type="bibr" target="#b9">(Hermann et al., 2015)</ref>.</p><p>Based on the above considerations, we propose new connections to facilitate gradient propagation in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Topology</head><p>We build the deep LSTM network with the new proposed linear connections. The shortest paths through the proposed connections do not include any nonlinear transformations and do not rely on any recurrent computation. We call these connections fastforward connections. Within the deep topology, we also introduce an interleaved bi-directional architecture to stack the LSTM layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network</head><p>Our entire deep neural network is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. This topology can be divided into three parts: the encoder part (P-E) on the left, the decoder part (P-D) on the right and the interface between these two parts (P-I) which extracts the representation of the source sequence. We have two instantiations of this topology: Deep-ED and Deep-Att, which correspond to the extension of the encoder-decoder network and the attention network respectively. Our main innovation is the novel scheme for connecting adjacent recurrent layers. We will start with the basic RNN model for the sake of clarity. Recurrent layer: When an input sequence {x 1 , . . . , x m } is given to a recurrent layer, the output h t at each time step t can be computed as (see <ref type="figure" target="#fig_0">Fig. 1</ref> </p><formula xml:id="formula_1">(a)) h t = σ(W f x t + W r h t−1 ) = RNN (W f x t , h t−1 ) = RNN (f t , h t−1 ),<label>(2)</label></formula><p>where the bias parameter is not included for simplicity. We use a red circle and a blue empty square to denote an input and a hidden state. A blue square with a "-" denotes the previous hidden state. A dotted line means that the hidden state is used recurrently. This computation can be equivalently split into two consecutive steps:</p><p>• Feed-Forward computation: f t = W f x t . Left part in <ref type="figure" target="#fig_0">Fig. 1 (b</ref>). "f" block.</p><p>• Recurrent computation: RNN (f t , h t−1 ). Right part and the sum operation (+) followed by activation in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>. "r" block.</p><p>For a deep topology with stacked recurrent layers, the input of each block "f" at recurrent layer k (denoted by f k ) is usually the output of block "r" at its previous recurrent layer k − 1 (denoted by h k−1 ). In our work, we add fast-forward connections (F-F connections) which connect two feed-forward computation blocks "f" of adjacent recurrent layers. It means that each block "f" at recurrent layer k takes both the outputs of block "f" and block "r" at its previous layer as input ( <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>). F-F connections are denoted by dashed red lines in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref> and <ref type="figure" target="#fig_1">Fig. 2</ref>. The path of F-F connections contains neither nonlinear activations nor recurrent computation. It provides a fast path for information to propagate, so we call this path fast-forward connections. Additionally, in order to learn more temporal dependencies, the sequences can be processed in different directions at each pair of adjacent recurrent layers. This is quantitatively expressed in Eq. 3:</p><formula xml:id="formula_2">t t t - - - - - - - - (a) (b) (c) F-F W f W r h t h t-1 x h t W f W r x h t-1 x h t-1 W f 2 W f 1 h t 1 h t 2 1 h t-1 2 f t f t 1 f t 2 block f block r</formula><formula xml:id="formula_3">f k t = W k f · [f k−1 t , h k−1 t ], k &gt; 1 f k t = W k f x t k = 1 h k t = RNN k (f k t , h k t+(−1) k )<label>(3)</label></formula><p>The opposite directions are marked by the direction term (−1) k . At the first recurrent layer, the block "f" takes x t as the input.</p><p>[ , ] denotes the concatenation of vectors. This is shown in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>. The two changes are summarized here:</p><p>• We add a connection between f k t and f k−1 t . Without f k−1 t , our model will be reduced to the traditional stacked model.</p><p>• We alternate the RNN direction at different layers k with the direction term (−1) k . If we fix the direction term to −1, all layers work in the forward direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM layer:</head><p>In our experiments, instead of an RNN, a specific type of recurrent layer called LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b7">Graves et al., 2009</ref>) is used. The LSTM is structurally more complex than the basic RNN in Eq. 2. We define the computation of the LSTM as a function which maps the input f and its state-output pair (h, s) at the previous time step to the current stateoutput pair. The exact computations for (h t , s t ) = LSTM(f t , h t−1 , s t−1 ) are the following:</p><formula xml:id="formula_4">[z, z ρ , z φ , z π ] = f t + W r h t−1 s t = σ i (z) • σ g (z ρ + s t−1 • θ ρ ) + σ g (z φ + s t−1 • θ φ ) • s t−1 h t = σ o (s t ) • σ g (z π + s t • θ π ) (4) where [z, z ρ , z φ , z π ]</formula><p>is the concatenation of four vectors of equal size, • means element-wise multiplication, σ i is the input activation function, σ o is the output activation function, σ g is the activation function for gates, and W r , θ ρ , θ φ , and θ π are the parameters of the LSTM. It is slightly different from the standard notation in that we do not have a matrix to multiply with the input f in our notation. With this notation, we can write down the computations for our deep bi-directional LSTM model with F-F connections:</p><formula xml:id="formula_5">f k t = W k f · [f k−1 t , h k−1 t ], k &gt; 1 f k t = W k f x t , k = 1 (h k t , s k t ) = LSTM k f k t , h k t+(−1) k , s k t+(−1) k (5)</formula><p>where x t is the input to the deep bi-directional LSTM model. For the encoder, x t is the embedding of the t th word in the source sentence. For the decoder x t is the concatenation of the embedding of the t th word in the target sentence and the encoder representation for step t. In our final model two additional operations are used with Eq. 5, which is shown in Eq. 6. Half(f ) denotes the first half of the elements of f , and Dr(h) is the dropout operation  which randomly sets an element of h to zero with a certain probability. The use of Half(·) is to reduce the parameter size and does not affect the performance. We observed noticeable performance degradation when using only the first third of the elements of "f".</p><formula xml:id="formula_6">f k t = W k f · [Half(f k−1 t ), Dr(h k−1 t )], k &gt; 1 (6)</formula><p>With the F-F connections, we build a fast channel to propagate the gradients in the deep topology. F-F  connections can accelerate the model convergence and while improving the performance. A similar idea was also used in <ref type="bibr" target="#b8">(He et al., 2016;</ref><ref type="bibr" target="#b31">Zhou and Xu, 2015)</ref>. Encoder: The LSTM layers are stacked following Eq. 5. We call this type of encoder interleaved bidirectional encoder. In addition, there are two similar columns (a 1 and a 2 ) in the encoder part. Each column consists of n e stacked LSTM layers. There is no connection between the two columns. The first layers of the two columns process the word representations of the source sequence in different directions. At the last LSTM layers, there are two groups of vectors representing the source sequence. The group size is the same as the length of the input sequence.</p><p>Interface: Prior encoder-decoder models and attention models are different in their method of extracting the representations of the source sequences. In our work, as a consequence of the introduced F-F connections, we have 4 output vectors (h ). The max-operation and last time step state extraction provide compli-mentary information but do not affect the performance much. e t is used as the final representation c t .</p><p>For Deep-Att, we do not need the above two operations. We only concatenate the 4 output vectors at each time step to obtain e t , and a soft attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> is used to calculate the final representation c t from e t . e t is summarized as:</p><formula xml:id="formula_7">Deep-ED: e t [h ne,a 1 m , Max(h ne,a 2 t ), Max(f ne,a 1 t ), Max(f ne,a 2 t )] Deep-Att: e t [h ne,a 1 t , h ne,a 2 t , f ne,a 1 t , f ne,a 2 t ]<label>(7)</label></formula><p>Note that the vector dimensionality of f is four times larger than that of h (see Eq. 4). c t is summarized as:</p><formula xml:id="formula_8">Deep-ED: c t = e t , (const) Deep-Att: c t = m t =1 α t,t W p e t<label>(8)</label></formula><p>α t,t is the normalized attention weight computed by:</p><formula xml:id="formula_9">α t,t = exp(a(W p e t , h 1,dec t−1 ))</formula><p>the concatenated vector e t to a vector with 1/4 dimension size, denoted by the (fully connected) block "fc" in <ref type="figure" target="#fig_1">Fig. 2</ref>. Decoder: The decoder follows Eq. 5 and Eq. 6 with fixed direction term −1. At the first layer, we use the following x t :</p><formula xml:id="formula_10">x t = [c t , y t−1 ]<label>(10)</label></formula><p>y t−1 is the target word embedding at the previous time step and y 0 is zero. There is a single column of n d stacked LSTM layers. We also use the F-F connections like those in the encoder and all layers are in the forward direction. Note that at the last LSTM layer, we only use h t to make the prediction with a softmax layer. Although the network is deep, the training technique is straightforward. We will describe this in the next part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training technique</head><p>We take the parallel data as the only input without using any monolingual data for either word representation pre-training or language modeling. Because of the deep bi-directional structure, we do not need to reverse the sequence order as <ref type="bibr" target="#b24">Sutskever et al. (2014)</ref>.</p><p>The deep topology brings difficulties for the model training, especially when first order methods such as stochastic gradient descent (SGD) <ref type="bibr" target="#b17">(LeCun et al., 1998)</ref> are used. The parameters should be properly initialized and the converging process can be slow. We tried several optimization techniques such as AdaDelta (Zeiler, 2012), RMSProp (Tieleman and  and <ref type="bibr">Adam (Kingma and Ba, 2015)</ref>. We found that all of them were able to speed up the process a lot compared to simple SGD while no significant performance difference was observed among them. In this work, we chose Adam for model training and do not present a detailed comparison with other optimization methods.</p><p>Dropout  is also used to avoid over-fitting. It is utilized on the LSTM nodes h k t (See Eq. 5) with a ratio of p d for both the encoder and decoder.</p><p>During the whole model training process, we keep all hyper parameters fixed without any intermediate interruption. The hyper parameters are selected according to the performance on the development set.</p><p>For such a deep and large network, it is not easy to determine the tuning strategy and this will be considered in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generation</head><p>We use the common left-to-right beam-search method for sequence generation. At each time step t, the word y t can be predicted by:</p><formula xml:id="formula_11">y t = arg max y P(y|ŷ 0:t−1 , x; θ)<label>(11)</label></formula><p>whereŷ t is the predicted target word.ŷ 0:t−1 is the generated sequence from time step 0 to t − 1. We keep n b best candidates according to Eq. 11 at each time step, until the end of sentence mark is generated. The hypotheses are ranked by the total likelihood of the generated sequence, although normalized likelihood is used in some works <ref type="bibr" target="#b12">(Jean et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our method mainly on the widely used WMT'14 English-to-French translation task. In order to validate our model on more difficult language pairs, we also provide results on the WMT'14 English-to-German translation task. Our models are implemented in the PADDLE (PArallel Distributed Deep LEarning) platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data sets</head><p>For both tasks, we use the full WMT'14 parallel corpus as our training data. The detailed data sets are listed below:</p><p>• English-to-French: Europarl v7, Common Crawl, UN, News Commentary, Gigaword</p><p>• English-to-German: Europarl v7, Common Crawl, News Commentary</p><p>In total, the English-to-French corpus includes 36 million sentence pairs, and the English-to-German corpus includes 4.5 million sentence pairs. The news-test-2012 and news-test-2013 are concatenated as our development set, and the news-test-2014 is the test set. Our data partition is consistent with previous works on NMT <ref type="bibr" target="#b19">(Luong et al., 2015;</ref><ref type="bibr" target="#b12">Jean et al., 2015)</ref> to ensure fair comparison.</p><p>For the source language, we select the most frequent 200K words as the input vocabulary. For the target language we select the most frequent 80K French words and the most frequent 160K German words as the output vocabulary. The full vocabulary of the German corpus is larger <ref type="bibr" target="#b12">(Jean et al., 2015)</ref>, so we select more German words to build the target vocabulary. Out-of-vocabulary words are replaced with the unknown symbol unk . For complete comparison to previous work on the Englishto-French task, we also show the results with a smaller vocabulary of 30K input words and 30K output words on the sub train set with selected 12M parallel sequences <ref type="bibr" target="#b22">(Schwenk, 2014;</ref><ref type="bibr" target="#b24">Sutskever et al., 2014;</ref><ref type="bibr" target="#b4">Cho et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model settings</head><p>We have two models as described above, named Deep-ED and Deep-Att. Both models have exactly the same configuration and layer size except the interface part P-I.</p><p>We use 256 dimensional word embeddings for both the source and target languages. All LSTM layers, including the 2×n e layers in the encoder and the n d layers in the decoder, have 512 memory cells. The output layer size is the same as the size of the target vocabulary. The dimension of c t is 5120 and 1280 for Deep-ED and Deep-Att respectively. For each LSTM layer, the activation functions for gates, inputs and outputs are sigmoid, tanh, and tanh respectively.</p><p>Our network is narrow on word embeddings and LSTM layers. Note that in previous work <ref type="bibr" target="#b24">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>, 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used. We also tried larger scale models but did not obtain further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimization</head><p>Note that each LSTM layer includes two parts as described in Eq. 3, feed-forward computation and recurrent computation. Since there are non-linear activations in the recurrent computation, a larger learning rate l r = 5 × 10 −4 is used, while for the feed-forward computation a smaller learning rate l f = 4 × 10 −5 is used. Word embeddings and the softmax layer also use this learning rate l f . We refer all the parameters not used for recurrent computation as non-recurrent part of the model.</p><p>Because of the large model size, we use strong L 2 regularization to constrain the parameter matrix v in the following way:</p><formula xml:id="formula_12">v ← v − l · (g + r · v)<label>(12)</label></formula><p>Here r is the regularization strength, l is the corresponding learning rate, g stands for the gradients of v. The two embedding layers are not regularized.</p><p>All the other layers have the same r = 2.</p><p>The parameters of the recurrent computation part are initialized to zero. All non-recurrent parts are randomly initialized with zero mean and standard deviation of 0.07. A detailed guide for setting hyperparameters can be found in <ref type="bibr" target="#b2">(Bengio, 2012)</ref>.</p><p>The dropout ratio p d is 0.1. In each batch, there are 500 ∼ 800 sequences in our work. The exact number depends on the sequence lengths and model size. We also find that larger batch size results in better convergence although the improvement is not large. However, the largest batch size is constrained by the GPU memory. We use 4 ∼ 8 GPU machines (each has 4 K40 GPU cards) running for 10 days to train the full model with parallelization at the data batch level. It takes nearly 1.5 days for each pass.</p><p>One thing we want to emphasize here is that our deep model is not sensitive to these settings. Small variation does not affect the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We evaluate the same way as previous NMT works <ref type="bibr" target="#b24">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b19">Luong et al., 2015;</ref><ref type="bibr" target="#b12">Jean et al., 2015)</ref>. All reported BLEU scores are computed with the multi-bleu.perl 1 script which is also used in the above works. The results are for tokenized and case sensitive evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Single models</head><p>English-to-French: First we list our single model results on the English-to-French task in Tab. 1. In the first block we show the results with the full corpus. The previous best single NMT encoderdecoder model (Enc-Dec) with six layers achieves BLEU=31.5 <ref type="bibr" target="#b19">(Luong et al., 2015)</ref>. From Deep-ED, we obtain the BLEU score of 36.3, which outperforms Enc-Dec model by 4.8 BLEU points. This result is even better than the ensemble result of eight Enc-Dec models, which is 35.6 <ref type="bibr" target="#b19">(Luong et al., 2015)</ref>. This shows that, in addition to the convolutional layers for computer vision, deep topologies can also work for LSTM layers. For Deep-Att, the performance is further improved to 37.7. We also list the previous state-of-the-art performance from a conventional SMT system <ref type="bibr" target="#b5">(Durrani et al., 2014)</ref> with the BLEU of 37.0. This is the first time that a single NMT model trained in an end-to-end form beats the best conventional system on this task.</p><p>We also show the results on the smaller data set with 12M sentence pairs and 30K vocabulary in the second block. The two attention models, RNNsearch <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> and RNNsearch-LV <ref type="bibr" target="#b12">(Jean et al., 2015)</ref>, achieve BLEU scores of 28.5 and 32.7 respectively. Note that RNNsearch-LV uses a large output vocabulary of 500K words based on the standard attention model RNNsearch. We obtain BLEU=35.9 which outperforms its corresponding shallow model RNNsearch by 7.4 BLEU points.</p><p>The SMT result from <ref type="bibr" target="#b22">(Schwenk, 2014)</ref> is also listed and falls behind our model by 2.6 BLEU points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Data Voc BLEU Enc-Dec <ref type="bibr">(Luong,2015)</ref> 36M 80K 31.5 SMT <ref type="bibr">(Durrani,2014)</ref> 36M  <ref type="table">Table 1</ref>: English-to-French task: BLEU scores of single neural models. We also list the conventional SMT system for comparison.</p><p>Moreover, during the generation process, we obtained the best BLEU score with beam size = 3 (when the beam size is 2, there is only a 0.1 difference in BLEU score). This is different from other works listed in Tab. 1, where the beam size is 12 <ref type="bibr" target="#b12">(Jean et al., 2015;</ref><ref type="bibr" target="#b24">Sutskever et al., 2014)</ref>. We attribute this difference to the improved model performance, where the ground truth generally exists in the top hypothesis. Consequently, with the much smaller beam size, the generation efficiency is significantly improved.</p><p>Next we list the effect of the novel F-F connections in our Deep-Att model of shallow topology in Tab. 2. When n e = 1 and n d = 1, the BLEU scores are 31.2 without F-F and 32.3 with F-F. Note that the model without F-F is exactly the standard attention model <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>. Since there is only a single layer, the use of F-F connections means that at the interface part we include f t into the representation (see Eq. 7). We find F-F connections bring an improvement of 1.1 in BLEU. After we increase our model depth to n e = 2 and n d = 2, the improvement is enlarged to 1.4. When the model is trained with larger depth without F-F connections, we find that the parameter exploding problem <ref type="bibr" target="#b1">(Bengio et al., 1994)</ref> happens so frequently that we could not finish training. This suggests that F-F connections provide a fast way for gradient propagation.  <ref type="table">Table 2</ref>: The effect of F-F. We list the BLEU scores of Deep-Att with and without F-F. Because of the parameter exploding problem, we can not list the model performance of larger depth without F-F. For n e = 1 and n d = 1, F-F connections only contribute to the representation at interface part (see Eq. 7).</p><p>Removing F-F connections also reduces the corresponding model size. In order to figure out the effect of F-F comparing models with the same parameter size, we increase the LSTM layer width of Deep-Att without F-F. In Tab. 3 we show that, after using a two times larger LSTM layer width of 1024, we can only obtain a BLEU score of 33.8, which is still worse than the corresponding Deep-Att with F-F.</p><p>We also notice that the interleaved bi-directional encoder starts to work when the encoder depth is larger than 1. The effect of the interleaved bidirectional encoder is shown in Tab. 4. For our largest model with n e = 9 and n d = 7, we compared the BLEU scores of the interleaved bi-directional encoder and the uni-directional encoder (where all LSTM layers work in forward direction). We find   Next we look into the effect of model depth. In Tab. 5, starting from n e = 1 and n d = 1 and gradually increasing the model depth, we significantly increase BLEU scores. With n e = 9 and n d = 7, the best score for Deep-Att is 37.7. We tried to increase the LSTM width based on this, but obtained little improvement. As we stated in Sec.2, the complexity of the encoder and decoder, which is related to the model depth, is more important than the model size. We also tried a larger depth, but the results started to get worse. With our topology and training technique, n e = 9 and n d = 7 is the best depth we can achieve.   English-to-German: We also validate our deep topology on the English-to-German task. The English-to-German task is considered a relatively more difficult task, because of the lower similarity between these two languages. Since the German vocabulary is much larger than the French vocabulary, we select 160K most frequent words as the target vocabulary. All the other hyper parameters are exactly the same as those in the English-to-French task.</p><p>We list our single model Deep-Att performance in Tab. 7. Our single model result with BLEU=20.6 is similar to the conventional SMT result of 20.7 <ref type="bibr" target="#b3">(Buck et al., 2014)</ref>. We also outperform the shallow attention models as shown in the first two lines in Tab. 7. All the results are consistent with those in the English-to-French task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Data Voc BLEU RNNsearch <ref type="bibr">(Jean,2015)</ref> 4.5M 50K 16.5 RNNsearch-LV <ref type="bibr">(Jean,2015)</ref> 4.5M 500K 16.9 SMT <ref type="bibr">(Buck,2014)</ref> 4.5M Full 20.7 Deep-Att (Ours) 4.5M 160K 20.6 <ref type="table">Table 7</ref>: English-to-German task: BLEU scores of single neural models. We also list the conventional SMT system for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Post processing</head><p>Two post processing techniques are used to improve the performance further on the English-to-French task.</p><p>First, three Deep-Att models are built for ensemble results. They are initialized with different random parameters; in addition, the training corpus for these models is shuffled with different random seeds. We sum over the predicted probabilities of the target words and normalize the final distribution to generate the next word. It is shown in Tab. 8 that the model ensemble can improve the performance further to 38.9. In <ref type="bibr" target="#b19">Luong et al. (2015)</ref> and <ref type="bibr" target="#b12">Jean et al. (2015)</ref> there are eight models for the best scores, but we only use three models and we do not obtain further gain from more models.  <ref type="table">Table 8</ref>: BLEU scores of different models. The first two blocks are our results of two single models and models with post processing. In the last block we list two baselines of the best conventional SMT system and NMT system.</p><p>Second, we recover the unknown words in the generated sequences with the Positional Unknown (PosUnk) model introduced in <ref type="bibr" target="#b19">(Luong et al., 2015)</ref>. The full parallel corpus is used to obtain the word mappings <ref type="bibr" target="#b18">(Liang et al., 2006)</ref>. We find this method provides an additional 1.5 BLEU points, which is consistent with the conclusion in <ref type="bibr" target="#b19">Luong et al. (2015)</ref>. We obtain the new BLEU score of 39.2 with a single Deep-Att model. For the ensemble models of Deep-Att, the BLEU score rises to 40.4. In the last two lines, we list the conventional SMT model <ref type="bibr" target="#b5">(Durrani et al., 2014)</ref> and the previous best neural models based system Enc-Dec <ref type="bibr" target="#b19">(Luong et al., 2015)</ref> for comparison. We find our best score outperforms the previous best score by nearly 3 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Length</head><p>On the English-to-French task, we analyze the effect of the source sentence length on our models as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>  <ref type="bibr" target="#b24">(Sutskever et al., 2014)</ref> and an SMT model <ref type="bibr" target="#b5">(Durrani et al., 2014)</ref>. We find our Deep-Att model works better than the previous two models (Enc-Dec and SMT) on nearly all sentence lengths. It is also shown that for very long sequences with length over 70 words, the performance of our Deep-Att does not degrade, when compared to another NMT model Enc-Dec. Our Deep-ED also has much better performance than the shallow Enc-Dec model on nearly all lengths, although for long sequences it degrades and starts to fall behind Deep-Att.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Unknown words</head><p>Next we look into the detail of the effect of unknown words on the English-to-French task. We select the subset without unknown words on target sentences from the original test set. There are 1705 such sentences (56.8%). We compute the BLEU scores on this subset and the results are shown in Tab. 9. We also list the results from SMT model <ref type="bibr" target="#b5">(Durrani et al., 2014)</ref>    the score 37.7 on the full test set. On this subset, the SMT model achieves 37.5, which is similar to its score 37.0 on the full test set. This suggests that the difficulty on this subset is not much different from that on the full set. We therefore attribute the larger gap for Deep-att to the existence of unknown words. We also compute the BLEU score on the subset of the ensemble model and obtain 41.4. As a reference related to human performance, in <ref type="bibr" target="#b24">Sutskever et al. (2014)</ref>, it has been tested that the BLEU score of oracle re-scoring the LIUM 1000-best results <ref type="bibr" target="#b22">(Schwenk, 2014)</ref> is 45.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Over-fitting</head><p>Deep models have more parameters, and thus have a stronger ability to fit the large data set. However, our experimental results suggest that deep models are less prone to the problem of over-fitting.</p><p>In <ref type="figure" target="#fig_4">Fig. 4</ref>, we show three results from models with a different depth on the English-to-French task. These three models are evaluated by token error rate, which is defined as the ratio of incorrectly predicted words in the whole target sequence with correct historical input. The curve with square marks corresponds to Deep-Att with n e = 9 and n d = 7. The curve with circle marks corresponds to n e = 5 and n d = 3. The curve with triangle marks corresponds to n e = 1 and n d = 1. We find that the deep model has better performance on the test set when the token error rate is the same as that of the shallow models on the training set. This shows that, with decreased token error rate, the deep model is more advantageous in avoiding the over-fitting phenomenon. We only plot the early training stage curves because, during the late training stage, the curves are not smooth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>With the introduction of fast-forward connections to the deep LSTM network, we build a fast path with neither non-linear transformations nor recurrent computation to propagate the gradients from the top to the deep bottom. On this path, gradients decay much slower compared to the standard deep network. This enables us to build the deep topology of NMT models.</p><p>We trained NMT models with depth of 16 including 25 LSTM layers and evaluated them mainly on the WMT'14 English-to-French translation task. This is the deepest topology that has been investigated in the NMT area on this task. We showed that our Deep-Att exhibits 6.2 BLEU points improvement over the previous best single model, achieving a 37.7 BLEU score. This single end-toend NMT model outperforms the best conventional SMT system <ref type="bibr" target="#b5">(Durrani et al., 2014)</ref> and achieves a state-of-the-art performance. After utilizing unknown word processing and model ensemble of three models, we obtained a BLEU score of 40.4, an improvement of 2.9 BLEU points over the previous best result. When evaluated on the subset of the test corpus without unknown words, our model achieves 41.4. Our model is also validated on the more difficult English-to-German task.</p><p>Our model is also efficient in sequence generation. The best results from both a single model and model ensemble are obtained with a beam size of 3, much smaller than previous NMT systems where beam size is about 12 <ref type="bibr" target="#b12">(Jean et al., 2015;</ref><ref type="bibr" target="#b24">Sutskever et al., 2014)</ref>. From our analysis, we find that deep models are more advantageous for learning for long sequences and that the deep topology is resistant to the over-fitting problem.</p><p>We tried deeper models and did not obtain further improvements with our current topology and training techniques. However, the depth of 16 is not very deep compared to the models in computer vision <ref type="bibr" target="#b8">(He et al., 2016)</ref>. We believe we can benefit from deeper models, with new designs of topologies and training techniques, which remain as our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>RNN models. The recurrent use of a hidden state is denoted by dotted lines. A "-" mark denotes the hidden value of the previous time step. (a): Basic RNN. (b): Basic RNN with intermediate computational state and the sum operation (+) followed by activation. It consists of block "f" and block "r", and is equivalent to (a). (c):Two stacked RNN layers with F-F connections denoted by dashed red lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The network. It includes three parts from left to right: encoder part (P-E), interface (P-I) and decoder part (P-D). We only show the topology of Deep-Att as an example. "f" and "r" blocks correspond to the feed-forward part and the subsequent LSTM computation. The F-F connections are denoted by dashed red lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ne t and f ne t of both columns). The representations are modified for both Deep-ED and Deep-Att. For Deep-ED, e t is static and consists of four parts. 1: The last time step output h ne m of the first column. 2: Max-operation Max(·) of h ne t at all time steps of the second column, denoted by Max(h ne,a 2 t ). Max(·) denotes obtaining the maximal value for each dimension over t. 3: Max(f ne,a 1 t ). 4: Max(f ne,a 2 t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>BLEU scores vs. source sequence length. Five lines are our Deep-Att single model, Deep-Att ensemble model, our Deep-ED model, previous Enc-Dec model with four layers and SMT model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Token error rate on train set vs. test set. Square: Deep-Att (n e = 9, n d = 7). Circle: Deep-Att (n e = 5, n d = 3). Triagle: Deep-Att (n e = 1, n d = 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>BLEU scores with different LSTM layer width in Deep-Att. After using two times larger LSTM layer width of 1024, we can only obtain BLEU score of 33.8.</figDesc><table><row><cell cols="5">It is still behind the corresponding Deep-Att with F-F.</cell></row><row><cell cols="5">there is a gap of about 1.5 points between these two</cell></row><row><cell cols="5">encoders for both Deep-Att and Deep-ED.</cell></row><row><cell>Models</cell><cell cols="4">Encoder ne n d BLEU</cell></row><row><cell>Deep-Att</cell><cell>Bi</cell><cell>9</cell><cell>7</cell><cell>37.7</cell></row><row><cell>Deep-Att</cell><cell>Uni</cell><cell>9</cell><cell>7</cell><cell>36.2</cell></row><row><cell>Deep-ED</cell><cell>Bi</cell><cell>9</cell><cell>7</cell><cell>36.3</cell></row><row><cell>Deep-ED</cell><cell>Uni</cell><cell>9</cell><cell>7</cell><cell>34.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The effect of the interleaved bi-directional encoder. We list the BLEU scores of our largest Deep-Att and Deep-ED models. The encoder term Bi denotes that the interleaved bi-directional encoder is used. Uni denotes a model where all LSTM layers work in forward direction.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>BLEU score of Deep-Att with different model depth. With n e = 1 and n d = 1, F-F connections only contribute to the representation at interface part where f t is included (see Eq. 7).The last line in Tab. 5 shows the BLEU score of 36.6 of our deepest model, where only one encoding column (Col = 1) is used. We find a 1.1 BLEU points degradation with a single encoding column. Note that the uni-directional models in Tab. 4 with uni-direction still have two encoding columns.</figDesc><table><row><cell>In</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of encoders with different number of columns and LSTM layer width.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>as a comparison. We find that the BLEU score of Deep-Att on this subset rises to 40.3, which has a gap of 2.6 with</figDesc><table><row><cell>Model</cell><cell cols="3">Test set Ratio(%) BLEU</cell></row><row><cell>Deep-Att</cell><cell>Full</cell><cell>100.0</cell><cell>37.7</cell></row><row><cell>Ensemble</cell><cell>Full</cell><cell>100.0</cell><cell>38.9</cell></row><row><cell cols="2">SMT(Durrani) Full</cell><cell>100.0</cell><cell>37.0</cell></row><row><cell>Deep-Att</cell><cell>Subset</cell><cell>56.8</cell><cell>40.3</cell></row><row><cell>Ensemble</cell><cell>Subset</cell><cell>56.8</cell><cell>41.4</cell></row><row><cell cols="2">SMT(Durrani) Subset</cell><cell>56.8</cell><cell>37.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>BLEU scores of the subset of the test set without considering unknown words.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t exp(a(W p e t , h 1,dec t−1 ))(9)h 1,dec t−1 is the first hidden layer output in the decoding part. a(·) is an alignment model described in<ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>. For Deep-Att, in order to reduce the memory cost, we linearly project (with W p )</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Practical Recommendations for Gradient-Based Training of Deep Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="437" to="478" />
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">N-gram counts and language models from the common crawl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bas</forename><surname>Van Ooyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Resources and Evaluation Conference</title>
		<meeting>the Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Edinburgh&apos;s phrase-based machine translation systems for WMT-14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recursive hetero-associative memories for translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><forename type="middle">L</forename><surname>Forcada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><forename type="middle">P</forename><surname>Ñeco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biological and Artificial Computation: From Neuroscience to Technology</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empirical Methods in Natural Language Processing</title>
		<meeting>the Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Grid long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association of Computational Linguistics on Human Language Technology</title>
		<meeting>the North American Chapter of the Association of Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Deep captioning with multimodal recurrent neural networks (m-RNN)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<ptr target="http://www-lium.univ-lemans.fr/∼schwenk/cslmjointpaper" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>online; accessed 03-september-2014</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, Deep Learning Workshop</title>
		<meeting>the 32nd International Conference on Machine Learning, Deep Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, Deep Learning Workshop</title>
		<meeting>the 32nd International Conference on Machine Learning, Deep Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03790</idno>
		<title level="m">Depth-gated LSTM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Wei</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.07526</idno>
		<title level="m">Empirical study on deep learning models for QA</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">ADADELTA: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
