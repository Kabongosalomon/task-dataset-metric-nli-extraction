<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deformable ConvNets v2: More Deformable, Better Results</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
							<email>jifdai@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deformable ConvNets v2: More Deformable, Better Results</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The superior performance of Deformable Convolutional Networks arises from its ability to adapt to the geometric variations of objects. Through an examination of its adaptive behavior, we observe that while the spatial support for its neural features conforms more closely than regular ConvNets to object structure, this support may nevertheless extend well beyond the region of interest, causing features to be influenced by irrelevant image content. To address this problem, we present a reformulation of Deformable ConvNets that improves its ability to focus on pertinent image regions, through increased modeling power and stronger training. The modeling power is enhanced through a more comprehensive integration of deformable convolution within the network, and by introducing a modulation mechanism that expands the scope of deformation modeling. To effectively harness this enriched modeling capability, we guide network training via a proposed feature mimicking scheme that helps the network to learn features that reflect the object focus and classification power of R-CNN features. With the proposed contributions, this new version of Deformable ConvNets yields significant performance gains over the original model and produces leading results on the COCO benchmark for object detection and instance segmentation. * This work is done when Xizhou Zhu is an intern at Microsoft Research Asia.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Geometric variations due to scale, pose, viewpoint and part deformation present a major challenge in object recognition and detection. The current state-of-the-art method for addressing this issue is Deformable Convolutional Networks (DCNv1) <ref type="bibr" target="#b7">[8]</ref>, which introduces two modules that aid CNNs in modeling such variations. One of these modules is deformable convolution, in which the grid sampling locations of standard convolution are each offset by displacements learned with respect to the preceding feature maps. The other is deformable RoIpooling, where offsets are learned for the bin positions in RoIpooling <ref type="bibr" target="#b15">[16]</ref>. The incorporation of these modules into a neural network gives it the ability to adapt its feature representation to the configuration of an object, specifically by deforming its sampling and pooling patterns to fit the object's structure. With this approach, large improvements in object detection accuracy are obtained.</p><p>Towards understanding Deformable ConvNets, the authors visualized the induced changes in receptive field, via the arrangement of offset sampling positions in PASCAL VOC images <ref type="bibr" target="#b10">[11]</ref>. It is found that samples for an activation unit tend to cluster around the object on which it lies. However, the coverage over an object is inexact, exhibiting a spread of samples beyond the area of interest. In a deeper analysis of spatial support using images from the more challenging COCO dataset <ref type="bibr" target="#b28">[29]</ref>, we observe that such behavior becomes more pronounced. These findings suggest that greater potential exists for learning deformable convolutions.</p><p>In this paper, we present a new version of Deformable ConvNets, called Deformable ConvNets v2 (DCNv2), with enhanced modeling power for learning deformable convolutions. This increase in modeling capability comes in two complementary forms. The first is the expanded use of deformable convolution layers within the network. Equipping more convolutional layers with offset learning capacity allows DCNv2 to control sampling over a broader range of feature levels. The second is a modulation mechanism in the deformable convolution modules, where each sample not only undergoes a learned offset, but is also modulated by a learned feature amplitude. The network module is thus given the ability to vary both the spatial distribution and the relative influence of its samples.</p><p>To fully exploit the increased modeling capacity of DCNv2, effective training is needed. Inspired by work on knowledge distillation in neural networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>, we make use of a teacher network for this purpose, where the teacher provides guidance during training. We specifically utilize R-CNN <ref type="bibr" target="#b16">[17]</ref> as the teacher. Since it is a network trained for classification on cropped image content, R-CNN learns features unaffected by irrelevant information outside the region of interest. To emulate this property, DCNv2 incorporates a feature mimicking loss into its training, which favors learning of features consistent to those of R-CNN. In this way, DCNv2 is given a strong training signal for its enhanced deformable sampling.</p><p>With the proposed changes, the deformable modules remain lightweight and can easily be incorporated into existing network architectures. Specifically, we incorporate DCNv2 into the Faster R-CNN <ref type="bibr" target="#b32">[33]</ref> and Mask R-CNN <ref type="bibr" target="#b19">[20]</ref> systems, with a variety of backbone networks. Extensive experiments on the COCO benchmark demonstrate the significant improvement of DCNv2 over DCNv1 for object detection and instance segmentation. The code for DCNv2 will be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Analysis of Deformable ConvNet Behavior</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Spatial Support Visualization</head><p>To better understand the behavior of Deformable Con-vNets, we visualize the spatial support of network nodes by their effective receptive fields <ref type="bibr" target="#b30">[31]</ref>, effective sampling locations, and error-bounded saliency regions. These three modalities provide different and complementary perspectives on the underlying image regions that contribute to a node's response.</p><p>Effective receptive fields Not all pixels within the receptive field of a network node contribute equally to its response. The differences in these contributions are represented by an effective receptive field, whose values are calculated as the gradient of the node response with respect to intensity perturbations of each image pixel <ref type="bibr" target="#b30">[31]</ref>. We utilize the effective receptive field to examine the relative influence of individual pixels on a network node, but note that this measure does not reflect the structured influence of full image regions.</p><p>Effective sampling / bin locations In <ref type="bibr" target="#b7">[8]</ref>, the sampling locations of (stacked) convolutional layers and the sampling bins in RoIpooling layers are visualized for understanding the behavior of Deformable ConvNets. However, the relative contributions of these sampling locations to the network node are not revealed. We instead visualize effective sampling locations that incorporate this information, computed as the gradient of the network node with respect to the sampling / bin locations, so as to understand their contribution strength.</p><p>Error-bounded saliency regions The response of a network node will not change if we remove image regions that do not influence it, as demonstrated in recent research on image saliency <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7]</ref>. Based on this property, we can determine a node's support region as the smallest image region giving the same response as the full image, within a small error bound. We refer to this as the errorbounded saliency region, which can be found by progressively masking parts of the image and computing the resulting node response, as described in more detail in the Appendix. The error-bounded saliency region facilitates comparison of support regions from different networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Spatial Support of Deformable ConvNets</head><p>We analyze the visual support regions of Deformable ConvNets in object detection. The regular ConvNet we employ as a baseline consists of a Faster R-CNN + ResNet-50 <ref type="bibr" target="#b20">[21]</ref> object detector with aligned RoIpooling 1 <ref type="bibr" target="#b19">[20]</ref>. All the convolutional layers in ResNet-50 are applied on the whole input image. The effective stride in the conv5 stage is reduced from 32 to 16 pixels to increase feature map resolution. The RPN <ref type="bibr" target="#b32">[33]</ref> head is added on top of the conv4 features of ResNet-101. On top of the conv5 features we add the Fast R-CNN head <ref type="bibr" target="#b15">[16]</ref>, which is composed of aligned RoIpooling and two fully-connected (fc) layers, followed by the classification and bounding box regression branches. We follow the procedure in <ref type="bibr" target="#b7">[8]</ref> to turn the object detector into its deformable counterpart. The three layers of 3 × 3 convolutions in the conv5 stage are replaced by deformable convolution layers. Also, the aligned RoIpooling layer is replaced by deformable RoIPooling. Both networks are trained and visualized on the COCO benchmark. It is worth mentioning that when the offset learning rate is set to zero, the Deformable Faster R-CNN detector degenerates to regular Faster R-CNN with aligned RoIpooling.</p><p>Using the three visualization modalities, we examine the spatial support of nodes in the last layer of the conv5 stage in <ref type="figure" target="#fig_0">Figure 1</ref> (a)∼(b). The sampling locations analyzed in <ref type="bibr" target="#b7">[8]</ref> are also shown. From these visualizations, we make the following observations:</p><p>1. Regular ConvNets can model geometric variations to some extent, as evidenced by the changes in spatial support with respect to image content. Thanks to the strong representation power of deep ConvNets, the network weights are learned to accommodate some degree of geometric transformation.</p><p>2. By introducing deformable convolution, the network's ability to model geometric transformation is considerably enhanced, even on the challenging COCO benchmark. The spatial support adapts much more to image content, with nodes on the foreground having support that covers the whole object, while nodes on the background have ex- <ref type="bibr" target="#b0">1</ref> Aligned RoIpooling is called RoIAlign in <ref type="bibr" target="#b19">[20]</ref>. We use the term "aligned RoIpooling" in this paper to more clearly describe it in the context of other related terms. panded support that encompasses greater context. However, the range of spatial support may be inexact, with the effective receptive field and error-bounded saliency region of a foreground node including background areas irrelevant for detection.</p><p>3. The three presented types of spatial support visualizations are more informative than the sampling locations used in <ref type="bibr" target="#b7">[8]</ref>. This can be seen, for example, with regular ConvNets, which have fixed sampling locations along a grid, but actually adapt its effective spatial support via network weights. The same is true for Deformable ConvNets, whose predictions are jointly affected by learned offsets and network weights. Examining sampling locations alone, as done in <ref type="bibr" target="#b7">[8]</ref>, can result in misleading conclusions about Deformable ConvNets. <ref type="figure" target="#fig_1">Figure 2</ref> (a)∼(b) display the spatial support of the 2fc node in the per-RoI detection head, which is directly followed by the classification and the bounding box regression branches. The visualization of effective bin locations suggests that bins on the object foreground generally receive larger gradients from the classification branch, and thus exert greater influence on prediction. This observation holds for both aligned RoIpooling and Deformable RoIpooling. In Deformable RoIpooling, a much larger proportion of bins cover the object foreground than in aligned RoIpooling, thanks to the introduction of learnable bin offsets. Thus, more information from relevant bins is available for the downstream Fast R-CNN head. Meanwhile, the error-bounded saliency regions in both aligned RoIpooling and Deformable RoIpooling are not fully focused on the object foreground, which suggests that image content outside of the RoI affects the prediction result. According to a recent study <ref type="bibr" target="#b5">[6]</ref>, such feature interference could be harmful for detection.</p><p>While it is evident that Deformable ConvNets have markedly improved ability to adapt to geometric variation in comparison to regular ConvNets, it can also be seen that their spatial support may extend beyond the region of interest. We thus seek to upgrade Deformable ConvNets so that they can better focus on pertinent image content and deliver greater detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">More Deformable ConvNets</head><p>To improve the network's ability to adapt to geometric variations, we present changes to boost its modeling power and to help it take advantage of this increased capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Stacking More Deformable Conv Layers</head><p>Encouraged by the observation that Deformable Con-vNets can effectively model geometric transformation on challenging benchmarks, we boldly replace more regular conv layers by their deformable counterparts. We expect that by stacking more deformable conv layers, the geometric transformation modeling capability of the entire network can be further strengthened. In this paper, deformable convolutions are applied in all the 3 × 3 conv layers in stages conv3, conv4, and conv5 in ResNet-50. Thus, there are 12 layers of deformable convolution in the network. In contrast, just three layers of deformable convolution are used in <ref type="bibr" target="#b7">[8]</ref>, all in the conv5 stage. It is observed in <ref type="bibr" target="#b7">[8]</ref> that performance saturates when stacking more than three layers for the relatively simple and small-scale PASCAL VOC benchmark. Also, misleading offset visualizations on COCO may have hindered further exploration on more challenging benchmarks. In experiments, we observe that utilizing deformable layers in the conv3-conv5 stages achieves the best tradeoff between accuracy and efficiency for object detection on COCO. See Section 5.2 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Modulated Deformable Modules</head><p>To further strengthen the capability of Deformable Con-vNets in manipulating spatial support regions, a modulation mechanism is introduced. With it, the Deformable Con-vNets modules can not only adjust offsets in perceiving input features, but also modulate the input feature amplitudes from different spatial locations / bins. In the extreme case, a module can decide not to perceive signals from a particular location / bin by setting its feature amplitude to zero. Consequently, image content from the corresponding spatial location will have considerably reduced or no impact on the module output. Thus, the modulation mechanism provides the network module another dimension of freedom to adjust its spatial support regions.</p><p>Given a convolutional kernel of K sampling locations, let w k and p k denote the weight and pre-specified offset for the k-th location, respectively. For example, K = 9 and p k ∈ {(−1, −1), (−1, 0), . . . , (1, 1)} defines a 3 × 3 convolutional kernel of dilation 1. Let x(p) and y(p) denote the features at location p from the input feature maps x and output feature maps y, respectively. The modulated deformable convolution can then be expressed as</p><formula xml:id="formula_0">y(p) = K k=1 w k · x(p + p k + ∆p k ) · ∆m k ,<label>(1)</label></formula><p>where ∆p k and ∆m k are the learnable offset and modulation scalar for the k-th location, respectively. The modulation scalar ∆m k lies in the range [0, 1], while ∆p k is a real number with unconstrained range. As p + p k + ∆p k is fractional, bilinear interpolation is applied as in <ref type="bibr" target="#b7">[8]</ref> in computing x(p + p k + ∆p k ). Both ∆p k and ∆m k are obtained via a separate convolution layer applied over the same input feature maps x. This convolutional layer is of the same spatial resolution and dilation as the current convolutional layer. The output is of 3K channels, where the first 2K channels correspond to the learned offsets {∆p k } K k=1 , and the remaining K channels are further fed to a sigmoid layer to obtain the modulation scalars {∆m k } K k=1 . The kernel weights in this separate convolution layer are initialized to zero. Thus, the initial values of ∆p k and ∆m k are 0 and 0.5, respectively. The learning rates of the added conv layers for offset and modulation learning are set to 0.1 times those of the existing layers.</p><p>The design of modulated deformable RoIpooling is similar. Given an input RoI, RoIpooling divides it into K spatial bins (e.g. 7 × 7). Within each bin, sampling grids of even spatial intervals are applied (e.g. 2 × 2). The sampled values on the grids are averaged to compute the bin output. Let ∆p k and ∆m k be the learnable offset and modulation scalar for the k-th bin. The output binning feature y(k) is computed as</p><formula xml:id="formula_1">y(k) = n k j=1 x(p kj + ∆p k ) · ∆m k /n k ,<label>(2)</label></formula><p>where p kj is the sampling location for the j-th grid cell in the k-th bin, and n k denotes the number of sampled grid cells. Bilinear interpolation is applied to obtain features x(p kj + ∆p k ). The values of ∆p k and ∆m k are produced by a sibling branch on the input feature maps. In this branch, RoIpooling generates features on the RoI, followed by two fc layers of 1024-D (initialized with Gaussian distribution of standard derivation of 0.01). On top of that, an additional fc layer produces output of 3K channels (weights initialized to be zero). The first 2K channels are the normalized learnable offsets, where element-wise multiplications with the RoI's width and height are computed to obtain {∆p k } K k=1 . The remaining K channels are normalized by a sigmoid layer to produce {∆m k } K k=1 . The learning rates of the added fc layers for offset learning are the same as those of the existing layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">R-CNN Feature Mimicking</head><p>As observed in <ref type="figure" target="#fig_1">Figure 2</ref>, the error-bounded saliency region of a per-RoI classification node can stretch beyond the RoI for both regular ConvNets and Deformable ConvNets. Image content outside of the RoI may thus affect the extracted features and consequently degrade the final results of object detection.</p><p>In <ref type="bibr" target="#b5">[6]</ref>, the authors find redundant context to be a plausible source of detection error for Faster R-CNN. Together with other motivations (e.g., to share fewer features between the classification and bounding box regression branches), the authors propose to combine the classification scores of Faster R-CNN and R-CNN to obtain the final detection score. Since R-CNN classification scores are focused on cropped image content from the input RoI, incorporating them would help to alleviate the redundant context problem and improve detection accuracy. However, the combined system is slow because both the Faster-RCNN and R-CNN branches need to be applied in both training and inference.</p><p>Meanwhile, Deformable ConvNets are powerful in adjusting spatial support regions. For Deformable ConvNets v2 in particular, the modulated deformable RoIpooling module could simply set the modulation scalars of bins in a way that excludes redundant context. However, our experiments in Section 5.3 show that even with modulated deformable modules, such representations cannot be learned well through the standard Faster R-CNN training procedure. We suspect that this is because the conventional Faster R-CNN training loss cannot effectively drive the learning of such representations. Additional guidance is needed to steer the training.</p><p>Motivated by recent work on feature mimicking <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref>, we incorporate a feature mimic loss on the per-RoI features of Deformable Faster R-CNN to force them to be similar to R-CNN features extracted from cropped images. This auxiliary training objective is intended to drive Deformable Faster R-CNN to learn more "focused" feature representations like R-CNN. We note that, based on the visualized spatial support regions in <ref type="figure" target="#fig_1">Figure 2</ref>, a focused feature representation may well not be optimal for negative RoIs on the image background. For background areas, more context information may need to be considered so as not to produce false positive detections. Thus, the feature mimic loss is enforced only on positive RoIs that sufficiently overlap with ground-truth objects.</p><p>The network architecture for training Deformable Faster R-CNN is presented in <ref type="figure" target="#fig_2">Figure 3</ref>. In addition to the Faster R-CNN network, an additional R-CNN branch is added for feature mimicking. Given an RoI b for feature mimicking, the image patch corresponding to it is cropped and resized to 224 × 224 pixels. In the R-CNN branch, the backbone network operates on the resized image patch and produces feature maps of 14 × 14 spatial resolution. A (modulated) deformable RoIpooling layer is applied on top of the feature maps, where the input RoI covers the whole resized image patch (top-left corner at (0, 0), and height and width are 224 pixels). After that, 2 fc layers of 1024-D are applied, producing an R-CNN feature representation for the input image patch, denoted by f RCNN (b). A (C +1)-way Softmax classifier follows for classification, where C denotes the number of foreground categories, plus one for background. The feature mimic loss is enforced between the R-CNN feature representation f RCNN (b) and the counterpart in Faster R-CNN, f FRCNN (b), which is also 1024-D and is produced by the 2 fc layers in the Fast R-CNN head. The feature mimic loss is defined on the cosine similarity between f RCNN (b) and f FRCNN (b), computed as</p><formula xml:id="formula_2">L mimic = b∈Ω [1 − cos(f RCNN (b), f FRCNN (b))],<label>(3)</label></formula><p>where Ω denotes the set of RoIs sampled for feature mimic training. In the SGD training, given an input image, 32 positive region proposals generated by RPN are randomly sampled into Ω. A cross-entropy classification loss is enforced on the R-CNN classification head, also computed on the RoIs in Ω. Network training is driven by the feature mimic loss and the R-CNN classification loss, together with the original loss terms in Faster R-CNN. The loss weights of the two newly introduced loss terms are 0.1 times those of the original Faster R-CNN loss terms. The network parameters between the corresponding modules in the R-CNN and the Faster R-CNN branches are shared, including the backbone network, (modulated) deformable RoIpooling, and the 2 fc heads (the classification heads in the two branches are unshared). In inference, only the Faster R-CNN network is applied on the test images, without the auxiliary R-CNN branch. Thus, no additional computation is introduced by R-CNN feature mimicking in inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Deformation Modeling is a long-standing problem in computer vision, and there has been tremendous effort in designing translation-invariant features. Prior to the deep learning era, notable works include scale-invariant feature transform (SIFT) <ref type="bibr" target="#b29">[30]</ref>, oriented FAST and rotated BRIEF (ORB) <ref type="bibr" target="#b33">[34]</ref>, and deformable part-based models (DPM) <ref type="bibr" target="#b11">[12]</ref>. Such works are limited by the inferior representation power of handcrafted features and the constrained family of geometric transformations they address (e.g., affine transformations). Spatial transformer networks (STN) <ref type="bibr" target="#b24">[25]</ref> is the first work on learning translation-invariant features for deep CNNs. It learns to apply global affine Relation Networks and Attention Modules are first proposed in natural language processing <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref> and physical system modeling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32</ref>]. An attention / relation module effects an individual element (e.g., a word in a sentence) by aggregating features from a set of elements (e.g., all the words in the sentence), where the aggregation weights are usually defined on feature similarities among the elements. They are powerful in capturing longrange dependencies and contextual information in these tasks. Recently, the concurrent works of <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b36">[37]</ref> successfully extend relation networks and attention modules to the image domain, for modeling long-range object-object and pixel-pixel relations, respectively. In <ref type="bibr" target="#b18">[19]</ref>, a learnable region feature extractor is proposed, unifying the previous region feature extraction modules from the pixel-object relation perspective. A common issue with such approaches is that the aggregation weights and the aggregation operation need to be computed on the elements in a pairwise fashion, incurring heavy computation that is quadratic to the number of elements (e.g., all the pixels in an image). Our developed approach can be perceived as a special attention mechanism where only a sparse set of elements have non-zero aggregation weights (e.g., 3 × 3 pixels from among all the image pixels). The attended elements are specified by the learnable offsets, and the aggregation weights are controlled by the modulation mechanism. The computational overhead is just linear to the number of elements, which is negligible compared to that of the entire network (See <ref type="table">Table 1</ref>).</p><p>Spatial Support Manipulation. For atrous convolution, the spatial support of convolutional layers has been enlarged by padding zeros in the convolutional kernels <ref type="bibr" target="#b4">[5]</ref>. The padding parameters are handpicked and predetermined. In active convolution <ref type="bibr" target="#b25">[26]</ref>, which is contemporary with Deformable ConvNets, convolutional kernel offsets are learned via back-propagation. But the offsets are static model parameters fixed after training and shared over different spatial locations. In a multi-path network for object detection <ref type="bibr" target="#b39">[40]</ref>, multiple RoIpooling layers are employed for each input RoI to better exploit multi-scale and context information. The multiple RoIpooling layers are centered at the input RoI, and are of different spatial scales. A common issue with these approaches is that the spatial support is controlled by static parameters and does not adapt to image content.</p><p>Effective Receptive Field and Salient Region. Towards better interpreting how a deep network functions, significant progress has been made in understanding which image regions contribute most to network prediction. Recent works on effective receptive fields <ref type="bibr" target="#b30">[31]</ref> and salient regions <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7]</ref> reveal that only a small proportion of pixels in the theoretical receptive field contribute significantly to the final network prediction. The effective support region is controlled by the joint effect of network weights and sampling locations. Here we exploit the developed techniques to better understand the network behavior of Deformable ConvNets. The resulting observations guide and motivate us to improve over the original model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Mimicking and Distillation are recently introduced techniques for model acceleration and compression.</head><p>Given a large teacher model, a compact student model is trained by mimicking the teacher model output or feature responses on training images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref>. The hope is that the compact model can be better trained by distilling knowledge from the large model.</p><p>Here we employ a feature mimic loss to help the network learn features that reflect the object focus and classification power of R-CNN features. Improved accuracy is obtained and the visualized spatial supports corroborate this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment Settings</head><p>Our models are trained on the 118k images of the COCO 2017 train set. In ablation, evaluation is done on the 5k images of the COCO 2017 validation set. We also evaluate performance on the 20k images of the COCO 2017 test-dev set. The standard mean average-precision scores at different box and mask IoUs are used for measuring object detection and instance segmentation accuracy, respectively.</p><p>Faster R-CNN and Mask R-CNN are chosen as the baseline systems. ImageNet <ref type="bibr" target="#b8">[9]</ref> pre-trained ResNet-50 is utilized as the backbone. The implementation of Faster R-CNN is the same as in Section 3.3. For Mask R-CNN, we follow the implementation in <ref type="bibr" target="#b19">[20]</ref>. To turn the networks into their deformable counterparts, the last set of 3 × 3 regular conv layers (close to the output in the bottom-up computation) are replaced by (modulated) deformable conv layers. Aligned RoIpooling is replaced by (modulated) deformable RoIpooling. Specially for Mask R-CNN, the two aligned RoIpooling layers with 7 × 7 and 14 × 14 bins are replaced by two (modulated) deformable RoIpooling layers with the same bin numbers. In R-CNN feature mimicking, the feature mimic loss is enforced on the RoI head for classification only (excluding that for mask estimation). For both systems, the choice of hyper-parameters follows the latest Detectron <ref type="bibr" target="#b17">[18]</ref> code base except for the image resolution, which is briefly presented here. In both training and inference, images are resized so that the shorter side is 1,000 pixels 2 . Anchors of 5 scales and 3 aspect ratios are utilized. 2k and 1k region proposals are generated at a nonmaximum suppression threshold of 0.7 at training and inference respectively. In SGD training, 256 anchor boxes (of positive-negative ratio 1:1) and 512 region proposals (of positive-negative ratio 1:3) are sampled for backpropagating their gradients. In our experiments, the networks are trained on 8 GPUs with 2 images per GPU for 16 epochs. The learning rate is initialized to 0.02 and is divided by 10 at the 10-th and the 14-th epochs. The weight decay and the momentum parameters are set to 10 −4 and 0.9, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Enriched Deformation Modeling</head><p>The effects of enriched deformation modeling are examined from ablations shown in <ref type="table">Table 1</ref> AP bbox score of 38.0% for Faster R-CNN, and AP bbox and AP mask scores of 40.4% and 35.3% respectively for Mask R-CNN. The deformable modules considerably improve accuracy as observed in <ref type="bibr" target="#b7">[8]</ref>.</p><p>By replacing more 3 × 3 regular conv layers by their deformable counterparts, the accuracy of both Faster R-CNN and Mask R-CNN steadily improve, with gains between 2.0% and 3.0% for AP bbox and AP mask scores when the conv layers in conv3-conv5 are replaced. No additional improvement is observed on the COCO benchmark by further replacing the regular conv layers in the conv2 stage. By upgrading the deformable modules to modulated deformable modules, we obtain further gains between 0.3% and 0.7% in AP bbox and AP mask scores. In total, enriching the deformation modeling capability yields a 41.7% AP bbox score on Faster R-CNN, which is 3.7% higher than that of the DCNv1 baseline. On Mask R-CNN, 43.1% AP bbox and 37.3% AP mask scores are obtained with the enriched deformation modeling, which are respectively 2.7% and 2.0% higher than those of the DCNv1 baseline. Note that the added parameters and FLOPs for enriching the deformation modeling are minor compared to those of the overall networks. <ref type="figure" target="#fig_0">Figure 1 (b)∼(c)</ref>, the spatial support of the enriched deformable modeling exhibits better adaptation to image content compared to that of DCNv1. <ref type="table" target="#tab_0">Table 2</ref> presents the results at input image resolution of 800 pixels, which follows the default setting in the Detectron code base. The same conclusion holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">R-CNN Feature Mimicking</head><p>Ablations of the design choices in R-CNN feature mimicking are shown in <ref type="table">Table 3</ref>. With the enriched deformation modeling, R-CNN feature mimicking further improves the AP bbox and AP mask scores by about 1% to 1.4% in both the Faster R-CNN and Mask R-CNN systems. Mimicking features of positive boxes on the object foreground is found to be particularly effective, and the results when mimicking all the boxes or just negative boxes are much lower. As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (c)∼(d), feature mimicking can help the network features better focus on the object foreground, which is beneficial for positive boxes. For the negative boxes, the network tends to exploit more context information (see <ref type="figure">Figure</ref> 2), where feature mimicking would not be helpful.</p><p>We also apply R-CNN feature mimicking to regular Con-vNets without any deformable layers. Almost no accuracy gains are observed. The visualized spatial support regions are shown in <ref type="figure" target="#fig_1">Figure 2</ref> (e), which are not focused on the object foreground even with the auxiliary mimic loss. This is likely because it is beyond the representation capability of regular ConvNets to focus features on the object foreground, and thus this cannot be learned. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Application on Stronger Backbones</head><p>Results on stronger backbones, by replacing ResNet-50 with ResNet-101 and ResNext-101 <ref type="bibr" target="#b38">[39]</ref>, are presented in <ref type="table" target="#tab_1">Table 4</ref>. For the entries of DCNv1, the regular 3 × 3 conv layers in the conv5 stage are replaced by the deformable counterpart, and aligned RoIpooling is replaced by deformable RoIpooling. For the DCNv2 entries, all the 3 × 3 conv layers in the conv3-conv5 stages are of modulated deformable convolution, and modulated deformable RoIpooling is used instead, with supervision from the R-CNN feature mimic loss. DCNv2 is found to outperform regular ConvNet and DCNv1 considerably on all the network backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Despite the superior performance of Deformable Con-vNets in modeling geometric variations, its spatial support extends well beyond the region of interest, causing features to be influenced by irrelevant image content. In this paper, we present a reformulation of Deformable ConvNets which improves its ability to focus on pertinent image regions, through increased modeling power and stronger training. Significant performance gains are obtained on the COCO benchmark for object detection and instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Error-bounded Image Saliency</head><p>In existing research on image saliency <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7]</ref>, a widely utilized formulation is as follows. Given an input image I and a trained network The optimized mask M is called the saliency map. The problem is it is hard to obtain the salient region at a specified reconstruction error. Thus it is hard to compare the salient regions from two networks at the same reconstruction error.</p><p>We seek to strictly constrain the reconstruction loss in the image saliency formulation, so as to facilitate comparison among the salient regions derived from different networks. Thus, the optimization problem is slightly modified to be min||M || 1</p><formula xml:id="formula_3">s.t. L rec (N (I), N (I M )) &lt; ,<label>(4)</label></formula><p>where L rec (N (I), N (I M )) denotes an arbitrary form of reconstruction loss, which is strictly bounded by . We term the collection of image pixels where {p|M (p) = 1} in the optimized mask as visual support region. The formulation in Eq. (4) is hard to be optimized, due to the hard reconstruction error constraint introduced. Here we develop a heuristic two-step procedure to reduce the search space in deriving the visual support region. At the first step, the visual support region is constrained to be rectangular of arbitrary shape. The rectangular is centered on the node to be interpreted. The rectangular is initialized of area size 0, and is enlarged gradually (at even area increment). The enlargement stops upon the reconstruction error constraint is satisfied. At the second step, pixel-level visual support region is derived within the rectangular area. The image is segmented into super-pixels by the algorithm in <ref type="bibr" target="#b0">[1]</ref>, so as to restrict the solution space. At initial, all the super-pixels within the rectangular are counted in the visual support region (taking mask value 1). Then the super-pixels are gradually removed in a greedy manner. At each iteration, the super-pixel causing the smallest rise in reconstruction error is removed. The iteration stops till the constraint would be violated by removing anymore super-pixels.</p><p>We apply the two-step procedure to visualize network nodes in Faster R-CNN object detector <ref type="bibr" target="#b32">[33]</ref>. We visualize  both feature map nodes shared on the whole image, and the 2fc node in the per-RoI detection head, which is directly followed by the classification and the bounding box regression branches. For image-wise feature map nodes (at a certain location), square rectangular is applied in the two-step procedure. For RoI-wise feature nodes, the rectangular is of the same aspect ratio as the input RoI. For both image-wise and RoI-wise nodes, the reconstruction loss is one minus the cosine similarity between the feature vectors derived from masked and original images. The error upper bound is set as 0.1. of DCNv2 are almost unchanged. This phenomenon is more obvious for objects of large and medium sizes. As shown in <ref type="figure">Figure 5</ref>, the spatial support of regular ConvNets can just cover a small portion of the large objects at such high resolution, and the accuracy suffers. Meanwhile, the spatial support of DCNv2 can effectively adapt to objects at various resolutions. <ref type="table" target="#tab_3">Table 5</ref> presents the results of multi-scale testing using ResNet-101. We first apply the DCNv2 model trained on the best single-scale setting (shorter side of 1000 pixels) on multi-scale testing images. Following the latest Detectron <ref type="bibr" target="#b17">[18]</ref> code base, test images range from shorter side of 400 to 1400 pixels with step size of 200 pixels. Multi-scale testing of DCNv2 improves the AP bbox score by 1.2% compared with the best single-scale setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2. DCNv2 with Various Image Resolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3. ImageNet Pre-Trained DCNv2</head><p>It is well known that many vision tasks benefit from Im-ageNet pre-training. This section investigates pre-training the learnable offsets and modulation scalars of DCNv2 on ImageNet <ref type="bibr" target="#b8">[9]</ref>, and finetuning on several tasks.</p><p>ImageNet Pretraining DCNv2 together with its DCNv1 <ref type="bibr" target="#b7">[8]</ref> and regular ConvNet counterparts are pre-trained on the ImageNet-1K training set. In experiments, we follow <ref type="bibr" target="#b38">[39]</ref> for the training and inference settings. In DCNv1, the 3 × 3 conv layers in the conv5 stage are replaced by deformable conv layers. In DCNv2, all the 3 × 3 conv layers in the conv3∼conv5 stages are replaced by modulated deformable conv layers. <ref type="table">Table 6</ref> presents the top-1 and top-5 classification accuracies on the validation set. DCNv2 achieves noticeable improvements over both the regular and DCNv1 baselines, with minor additional computation overhead. The enriched deformation modeling capability of DCNv2 is beneficial for the ImageNet classification task itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning for Specific Tasks</head><p>We investigate the effect of ImageNet pretrained DCNv2 models on several tasks, including object detection on Pascal VOC, ImageNet VID and COCO, and semantic segmentation on Pascal VOC 3 . In experiments, Faster R-CNN and DeepLab-v1 <ref type="bibr" target="#b4">[5]</ref> are adopted as the baseline systems for object detection and semantic segmentation, respectively. For object detection on COCO, we follow the same settings as in Section 5.1. For experiments on Pascal VOC, we mainly follow <ref type="bibr" target="#b7">[8]</ref> for the training and inference settings. Note that the baseline accuracy is higher than that reported in <ref type="bibr" target="#b7">[8]</ref> mainly because of a better ImageNet pretrained model and the introduction of RoIAlign in object detection. For object detection on Ima-geNet VID, we mainly follow the protocol in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b41">42]</ref> for the training and inference settings. The details are presented at the end of this section. <ref type="table" target="#tab_4">Table 7</ref> compares the performance of DCNv2 on various tasks using different pre-trained models. By pre-training the learnable offsets and modulation scalars on ImageNet, rather than initializing them as zeros prior to fine-tuning, noticeably accuracy improvements are observed on PAS-CAL VOC object detection and semantic segmentation. Meanwhile, the effect of pre-training on COCO detection is minor. This is probably because the larger and more challenging benchmark of COCO is sufficient for learning the offsets and the modulation scalars from scratch. ImageNet VID settings. The models are trained on the union of the ImageNet VID training set and the ImageNet DET training set (only the same 30 category labels are used), and are evaluated on the ImageNet VID validation set. In both training and inference, the input images are resized to a shorter side of 600 pixels. In RPN, the anchors are of 3 aspect ratios {1:2, 1:1, 2:1} and 4 scales {64 2 , 128 2 , 256 2 , 512 2 }. 300 region proposals are generated for each frame at an NMS threshold of 0.7 IoU. SGD training is performed, with one image at each mini-batch. 120k iterations are performed on 4 GPUs, with each GPU holding one mini-batch. The learning rates are 10 −3 and 10 −4 in the rst 80k and last 40k iterations,respectively. In each mini-batch,images are sampled from ImageNet DET and ImageNet VID at a 1:1 ratio. The weight decay and the momentum parameters are set to 0.0001 and 0.9, respectively. In inference, detection boxes are generated at an NMS threshold of 0.3 IoU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Spatial support of nodes in the last layer of the conv5 stage in a regular ConvNet, DCNv1 and DCNv2. The regular ConvNet baseline is Faster R-CNN + ResNet-50. In each subfigure, the effective sampling locations, effective receptive field, and error-bounded saliency regions are shown from the top to the bottom rows. Effective sampling locations are omitted in (c) as they are similar to those in (b), providing limited additional information. The visualized nodes (green points) are on a small object (left), a large object (middle), and the background (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>aligned RoIpooling, with deformable conv@conv5 stage high low (b) deformable RoIpooling, with deformable conv@conv5 stage (DCNv1) high low (c) modulated deformable RoIpooling, with modulated deformable conv@conv3∼5 stages low (d) with R-CNN feature mimicking on setting (c) (DCNv2) low (e) with R-CNN feature mimicking in regular ConvNet Spatial support of the 2fc node in the per-RoI detection head, directly followed by the classification and the bounding box regression branches. Visualization is conducted on a regular ConvNet, DCNv1 and DCNv2. The regular ConvNet baseline is Faster R-CNN + ResNet-50. In each subfigure, the effective bin locations, effective receptive fields, and error-bounded saliency regions are shown from the top to the bottom rows, except for (c)∼(e) where the effective bin locations are omitted as they provide little additional understanding over those in (a)∼(b). The input RoIs (green boxes) are on a small object (left), a large object (middle), and the background (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Network training with R-CNN feature mimicking. transformations to warp feature maps, but such transformations inadequately model the more complex geometric variations encountered in many vision tasks. Instead of performing global parametric transformations and feature warping, Deformable ConvNets sample feature maps in a local and dense manner, via learnable offsets in the proposed deformable convolution and deformable RoIpooling modules. Deformable ConvNets is the first work to effectively model geometric transformations in complex vision tasks (e.g., object detection and semantic segmentation) on challenging benchmarks. Our work extends Deformable ConvNets by enhancing its modeling power and facilitating network training. This new version of Deformable ConvNets yields significant performance gains over the original model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>N , let N (I) denote the network response on the original image. A binary mask M , which is of the same spatial dimension as I, can be applied on the image as I M . For the image pixel p where M (p) = 1, its content is kept in the masked image. Meanwhile, if M (p) = 0, the content is set as 0 in the masked image. The saliency map is obtained by optimizing loss function L(M ) = ||N (I) − N (I M )|| 2 + λ||M || 1 as a function of M , where λ is the hyper-parameter balancing the output reconstruction error ||N (I) − N (I M )|| 2 and the salient area loss ||M || 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>AP bbox scores of DCNv2 and regular ConvNets (Faster R-CNN + ResNet-50 / ResNet-101) on input images of varies resolution on the COCO 2017 test-dev set. (a) regular conv (b) modulated deformable conv@conv3∼5 stages (DCNv2) Spatial support of nodes in the last layer of the conv5 stage in DCNv2 and regular ConvNets. Input images are of shorter side 400 pixels (left), 800 pixels (middle), and 1400 pixels (right), respectively. The effective receptive field and error-bounded saliency regions are shown in the top and bottom rows, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>. The baseline with regular CNN modules obtains an AP bbox score of 34.7% for Faster R-CNN, and AP bbox and AP mask scores of 36.6% and 32.2% respectively for Mask R-CNN. To obtain a DCNv1 baseline, we follow the original Deformable ConvNets paper by replacing the last three layers of 3 × 3 convolution in the conv5 stage and the aligned RoIpooling layer by their deformable counterparts. This DCNv1 baseline achieves an Ablation study on enriched deformation modeling. The input images are of shorter side 800 pixels. Results are reported on the COCO 2017 validation set.</figDesc><table><row><cell>method</cell><cell>setting (shorter side 1000)</cell><cell cols="2">AP bbox AP bbox S</cell><cell cols="2">Faster R-CNN AP bbox M AP bbox L</cell><cell cols="4">Mask R-CNN param FLOP AP bbox AP mask param FLOP</cell></row><row><cell></cell><cell>regular (RoIpooling)</cell><cell>32.1</cell><cell>14.9</cell><cell>37.5</cell><cell cols="2">44.4 51.3M 326.7G</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>baseline</cell><cell cols="2">regular (aligned RoIpooling) 34.7</cell><cell>19.3</cell><cell>39.5</cell><cell cols="3">45.3 51.3M 326.7G 36.6</cell><cell cols="2">32.2 39.5M 447.5G</cell></row><row><cell></cell><cell cols="2">dconv@c5 + dpool (DCNv1) 38.0</cell><cell>20.7</cell><cell>41.8</cell><cell cols="3">52.2 52.7M 328.2G 40.4</cell><cell cols="2">35.3 40.9M 449.0G</cell></row><row><cell></cell><cell>dconv@c5</cell><cell>37.4</cell><cell>20.0</cell><cell>40.9</cell><cell cols="3">51.0 51.5M 327.1G 40.2</cell><cell cols="2">35.1 39.8M 447.8G</cell></row><row><cell>enriched deformation</cell><cell>dconv@c4∼c5 dconv@c3∼c5 dconv@c3∼c5 + dpool</cell><cell>40.0 40.4 41.0</cell><cell>21.4 21.6 22.0</cell><cell>43.8 44.2 45.1</cell><cell cols="3">55.3 51.7M 328.6G 41.8 56.2 51.8M 330.6G 42.2 56.6 53.0M 331.8G 42.4</cell><cell cols="2">36.8 40.0M 449.4G 37.0 40.1M 451.4G 37.0 41.3M 452.5G</cell></row><row><cell></cell><cell>mdconv@c3∼c5 + mdpool</cell><cell>41.7</cell><cell>22.2</cell><cell>45.8</cell><cell cols="3">58.7 65.5M 346.2G 43.1</cell><cell cols="2">37.3 53.8M 461.1G</cell></row><row><cell cols="10">Table 1. Ablation study on enriched deformation modeling. The input images are of shorter side 1,000 pixels (default in paper). In</cell></row><row><cell cols="10">the setting column, "(m)dconv" and "(m)dpool" stand for (modulated) deformable convolution and (modulated) deformable RoIpooling,</cell></row><row><cell cols="10">respectively. Also, "dconv@c3∼c5" stands for applying deformable conv layers at stages conv3∼conv5, for example. Results are reported</cell></row><row><cell cols="2">on the COCO 2017 validation set.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>method</cell><cell>setting (shorter side 800)</cell><cell cols="2">AP bbox AP bbox S</cell><cell cols="2">Faster R-CNN AP bbox M AP bbox L</cell><cell cols="4">Mask R-CNN param FLOP AP bbox AP mask param FLOP</cell></row><row><cell></cell><cell>regular (RoIpooling)</cell><cell>32.8</cell><cell>13.6</cell><cell>37.2</cell><cell cols="2">48.7 51.3M 196.8G</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>baseline</cell><cell cols="2">regular (aligned RoIpooling) 35.6</cell><cell>18.2</cell><cell>40.3</cell><cell cols="3">48.7 51.3M 196.8G 37.8</cell><cell cols="2">33.4 39.5M 303.5G</cell></row><row><cell></cell><cell cols="2">dconv@c5 + dpool (DCNv1) 38.2</cell><cell>19.1</cell><cell>42.2</cell><cell cols="3">54.0 52.7M 198.9G 40.3</cell><cell cols="2">35.0 40.9M 304.9G</cell></row><row><cell></cell><cell>dconv@c5</cell><cell>37.6</cell><cell>19.3</cell><cell>41.4</cell><cell cols="3">52.6 51.5M 197.7G 39.9</cell><cell cols="2">34.9 39.8M 303.7G</cell></row><row><cell>enriched deformation</cell><cell>dconv@c4∼c5 dconv@c3∼c5 dconv@c3∼c5 + dpool</cell><cell>39.2 39.5 40.0</cell><cell>19.9 21.0 21.1</cell><cell>43.4 43.5 44.6</cell><cell cols="3">55.5 51.7M 198.7G 41.2 55.6 51.8M 200.0G 41.5 56.3 53.0M 201.2G 41.8</cell><cell cols="2">36.1 40.0M 304.7G 36.4 40.1M 306.0G 36.4 41.3M 307.2G</cell></row><row><cell></cell><cell>mdconv@c3∼c5 + mdpool</cell><cell>40.8</cell><cell>21.3</cell><cell>45.0</cell><cell cols="3">58.5 65.5M 214.7G 42.7</cell><cell cols="2">37.0 53.8M 320.3G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Results of DCNv2, DCNv1 and regular ConvNets on various backbones on the COCO 2017 test-dev set.</figDesc><table><row><cell>setting</cell><cell cols="2">regions to mimic</cell><cell cols="3">Faster R-CNN AP bbox AP bbox AP mask Mask R-CNN</cell></row><row><cell></cell><cell>None</cell><cell></cell><cell>41.7</cell><cell>43.1</cell><cell>37.3</cell></row><row><cell>mdconv3∼5 +</cell><cell cols="3">FG &amp; BG 42.1</cell><cell>43.4</cell><cell>37.6</cell></row><row><cell>mdpool</cell><cell>BG Only</cell><cell></cell><cell>41.7</cell><cell>43.3</cell><cell>37.5</cell></row><row><cell></cell><cell>FG Only</cell><cell></cell><cell>43.1</cell><cell>44.3</cell><cell>38.3</cell></row><row><cell>regular</cell><cell>None FG Only</cell><cell></cell><cell>34.7 35.0</cell><cell>36.6 36.8</cell><cell>32.2 32.3</cell></row><row><cell cols="6">Table 3. Ablation study on R-CNN feature mimicking. Results are</cell></row><row><cell cols="4">reported on the COCO 2017 validation set.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Faster</cell><cell cols="2">Mask</cell></row><row><cell>backbone</cell><cell>method</cell><cell cols="2">R-CNN</cell><cell cols="2">R-CNN</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">AP bbox AP bbox AP mask</cell></row><row><cell></cell><cell>regular</cell><cell></cell><cell>35.1</cell><cell>37.0</cell><cell>32.4</cell></row><row><cell>ResNet-50</cell><cell>DCNv1</cell><cell></cell><cell>38.4</cell><cell>40.7</cell><cell>35.5</cell></row><row><cell></cell><cell>DCNv2</cell><cell></cell><cell>43.3</cell><cell>44.5</cell><cell>38.4</cell></row><row><cell></cell><cell>regular</cell><cell></cell><cell>39.2</cell><cell>40.9</cell><cell>35.3</cell></row><row><cell>ResNet-101</cell><cell>DCNv1</cell><cell></cell><cell>41.4</cell><cell>42.9</cell><cell>37.1</cell></row><row><cell></cell><cell>DCNv2</cell><cell></cell><cell>44.8</cell><cell>45.8</cell><cell>39.7</cell></row><row><cell></cell><cell>regular</cell><cell></cell><cell>40.1</cell><cell>41.7</cell><cell>36.2</cell></row><row><cell>ResNext-101</cell><cell>DCNv1</cell><cell></cell><cell>41.7</cell><cell>43.4</cell><cell>37.7</cell></row><row><cell></cell><cell>DCNv2</cell><cell></cell><cell>45.3</cell><cell>46.7</cell><cell>40.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Figure 4presents the results of applying regular Con-vNets and DCNv2 on images of various resolutions. The baseline model is Faster R-CNN with ResNet-50<ref type="bibr" target="#b20">[21]</ref> and ResNet-101. Models are trained and applied on images of shorter side {400, 600, 800, 1000, 1200, 1400} pixels, respectively. DCNv2 is found to outperform regular ConvNet on all input resolutions. For DCNv2, the highest AP bbox scores are obtained at input images of shorter side 1,000 pixels. With the shorter side larger than 1,000 pixels, AP bbox scores of regular ConvNet decrease noticeably, while those Ablation study on input image resolution. Results are reported on the COCO 2017 test-dev set.</figDesc><table><row><cell></cell><cell cols="2">method setting</cell><cell></cell><cell></cell><cell cols="3">Faster R-CNN + ResNet-101 AP bbox AP bbox 50 AP bbox 75 AP bbox S AP bbox M</cell><cell>AP bbox L</cell></row><row><cell></cell><cell></cell><cell cols="3">single-scale, shorter side 800</cell><cell>39.1</cell><cell>60.6</cell><cell>42.2</cell><cell>20.5</cell><cell>42.9</cell><cell>51.3</cell></row><row><cell></cell><cell>regular</cell><cell cols="4">single-scale, shorter side 1000 (best) 39.2</cell><cell>60.6</cell><cell>42.4</cell><cell>21.6</cell><cell>42.2</cell><cell>51.3</cell></row><row><cell></cell><cell></cell><cell cols="2">multi-scale test</cell><cell></cell><cell>41.2</cell><cell>62.4</cell><cell>45.2</cell><cell>24.6</cell><cell>44.3</cell><cell>52.7</cell></row><row><cell></cell><cell></cell><cell cols="3">single-scale, shorter side 800</cell><cell>44.0</cell><cell>65.9</cell><cell>48.1</cell><cell>23.2</cell><cell>47.7</cell><cell>59.6</cell></row><row><cell></cell><cell>DCNv2</cell><cell cols="4">single-scale, shorter side 1000 (best) 44.8</cell><cell>66.3</cell><cell>48.8</cell><cell>24.4</cell><cell>48.1</cell><cell>59.6</cell></row><row><cell></cell><cell></cell><cell cols="2">multi-scale test</cell><cell></cell><cell>46.0</cell><cell>67.9</cell><cell>50.8</cell><cell>27.8</cell><cell>49.1</cell><cell>59.5</cell></row><row><cell>backbone</cell><cell>method</cell><cell>top-1 acc (%)</cell><cell>top-5 acc (%)</cell><cell>param FLOP</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">regular 76.5</cell><cell cols="2">93.1 26.6M 4.1G</cell><cell></cell><cell></cell></row><row><cell>ResNet-50</cell><cell cols="2">DCNv1 76.6</cell><cell cols="2">93.2 26.8M 4.1G</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DCNv2 78.2</cell><cell cols="2">94.0 27.4M 4.3G</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">regular 78.4</cell><cell cols="2">94.2 45.5M 7.8G</cell><cell></cell><cell></cell></row><row><cell>ResNet-101</cell><cell cols="2">DCNv1 78.4</cell><cell cols="2">94.2 45.8M 7.8G</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DCNv2 79.2</cell><cell cols="2">94.6 47.4M 8.2G</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">regular 78.8</cell><cell cols="2">94.4 45.1M 8.0G</cell><cell></cell><cell></cell></row><row><cell>ResNeXt-101</cell><cell cols="2">DCNv1 78.9</cell><cell cols="2">94.4 45.6M 8.0G</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DCNv2 79.8</cell><cell cols="2">94.8 49.0M 8.7G</cell><cell></cell><cell></cell></row><row><cell cols="5">Table 6. ImageNet classification accuracies of DCNv2, DCNv1</cell><cell></cell><cell></cell></row><row><cell cols="2">and regular ConvNets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Finetuning the ImageNet-pretrained DCNv2 model for various tasks and benchmarks. ResNet-101 is utilized as the backbone.</figDesc><table><row><cell>method</cell><cell>offset&amp;modulation pretraining</cell><cell cols="2">VOC det AP bbox 50 AP bbox 70</cell><cell cols="3">VOC seg ImageNet VID det COCO det mIoU AP bbox AP bbox</cell></row><row><cell>regular</cell><cell>none</cell><cell>81.9</cell><cell>68.2</cell><cell>72.0</cell><cell>74.9</cell><cell>39.2</cell></row><row><cell>DCNv2</cell><cell>none</cell><cell>83.7</cell><cell>72.4</cell><cell>76.1</cell><cell>79.2</cell><cell>44.8</cell></row><row><cell>DCNv2</cell><cell>ImageNet</cell><cell>84.9</cell><cell>73.5</cell><cell>78.3</cell><cell>80.7</cell><cell>44.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The previous default setting in Detectron is 800 pixels. Ablation on input image resolution is present in Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note the mimicking module is not involved in semantic segmentation experiments.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-ofthe-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<title level="m">teraction networks for learning about objects, relations and physics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Massive exploration of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Revisiting rcnn: On awakening the classification power of faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real time image saliency for black box classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dabkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06383</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Programmable agents. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes (VOC) Challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A convolutional encoder model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<title level="m">Convolutional sequence to sequence learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning region features for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">STAT</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vain: Attentional multi-agent predictive modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Active convolution: Learning the shape of convolution for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-class multi-object tracking using changing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdenee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mimicking very efficient network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04128</idno>
		<title level="m">Understanding the effective receptive field in deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Discovering objects and their relations from entangled scene representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Orb: an efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visual interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A multipath network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Visualizing deep neural network decisions: Prediction difference analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
