<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GECToR -Grammatical Error Correction: Tag, Not Rewrite</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-29">29 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostiantyn</forename><surname>Omelianchuk</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Atrasevych</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Chernodub</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Skurzhanskyi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grammarly</surname></persName>
						</author>
						<title level="a" type="main">GECToR -Grammatical Error Correction: Tag, Not Rewrite</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-29">29 May 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a simple and efficient GEC sequence tagger using a Transformer encoder. Our system is pre-trained on synthetic data and then fine-tuned in two stages: first on errorful corpora, and second on a combination of errorful and error-free parallel corpora. We design custom token-level transformations to map input tokens to target corrections. Our best single-model/ensemble GEC tagger achieves an F 0.5 of 65.3/66.5 on CoNLL-2014 (test) and F 0.5 of 72.4/73.6 on BEA-2019 (test). Its inference speed is up to 10 times as fast as a Transformer-based seq2seq GEC system. The code and trained models are publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural Machine Translation (NMT)-based approaches <ref type="bibr" target="#b19">(Sennrich et al., 2016a)</ref> have become the preferred method for the task of Grammatical Error Correction (GEC) 2 . In this formulation, errorful sentences correspond to the source language, and error-free sentences correspond to the target language. Recently, Transformer-based <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> sequence-to-sequence (seq2seq) models have achieved state-of-the-art performance on standard GEC benchmarks <ref type="bibr" target="#b1">(Bryant et al., 2019)</ref>. Now the focus of research has shifted more towards generating synthetic data for pretraining the Transformer-NMT-based GEC systems <ref type="bibr" target="#b6">(Grundkiewicz et al., 2019;</ref><ref type="bibr" target="#b9">Kiyono et al., 2019)</ref>. NMT-based GEC systems suffer from several issues which make them inconvenient for real world deployment: (i) slow inference speed, (ii) demand for large amounts of training data * Authors contributed equally to this work, names are given in an alphabetical order. 1 https://github.com/grammarly/gector 2 http://nlpprogress.com/english/ grammatical_error_correction.html (Accessed 1 April 2020). and (iii) interpretability and explainability; they require additional functionality to explain corrections, e.g., grammatical error type classification <ref type="bibr" target="#b2">(Bryant et al., 2017)</ref>.</p><p>In this paper, we deal with the aforementioned issues by simplifying the task from sequence generation to sequence tagging. Our GEC sequence tagging system consists of three training stages: pretraining on synthetic data, fine-tuning on an errorful parallel corpus, and finally, fine-tuning on a combination of errorful and error-free parallel corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work.</head><p>LaserTagger <ref type="bibr" target="#b13">(Malmi et al., 2019)</ref> combines a BERT encoder with an autoregressive Transformer decoder to predict three main edit operations: keeping a token, deleting a token, and adding a phrase before a token. In contrast, in our system, the decoder is a softmax layer. PIE <ref type="bibr" target="#b0">(Awasthi et al., 2019)</ref> is an iterative sequence tagging GEC system that predicts tokenlevel edit operations. While their approach is the most similar to ours, our work differs from theirs as described in our contributions below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>We develop custom g-transformations: token-level edits to perform (g)rammatical error corrections. Predicting g-transformations instead of regular tokens improves the generalization of our GEC sequence tagging system.</p><p>2. We decompose the fine-tuning stage into two stages: fine-tuning on errorful-only sentences and further fine-tuning on a small, high-quality dataset containing both errorful and error-free sentences.</p><p>3. We achieve superior performance by incorporating a pre-trained Transformer encoder in our GEC sequence tagging system. In our experiments, encoders from XLNet and RoBERTa outperform three other cutting-edge Transformer encoders (ALBERT, BERT, and GPT-2).   <ref type="table" target="#tab_1">Table 1</ref> describes the finer details of datasets used for different training stages. Synthetic data. For pretraining stage I, we use 9M parallel sentences with synthetically generated grammatical errors <ref type="bibr">(Awasthi et al., 2019) 3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Datasets</head><p>Training data. We use the following datasets for fine-tuning stages II and III: National University of Singapore Corpus of Learner English (NU-CLE) 4 <ref type="bibr" target="#b4">(Dahlmeier et al., 2013)</ref>, Lang-8 Corpus of Learner English (Lang-8) 5 <ref type="bibr" target="#b21">(Tajiri et al., 2012)</ref>, FCE dataset 6 <ref type="bibr" target="#b24">(Yannakoudakis et al., 2011)</ref>, the publicly available part of the Cambridge Learner Corpus <ref type="bibr" target="#b16">(Nicholls, 2003)</ref> and Write &amp; Improve + LOCNESS Corpus <ref type="bibr">(Bryant et al., 2019) 7</ref> .</p><p>Evaluation data. We report results on CoNLL-2014 test set <ref type="bibr" target="#b15">(Ng et al., 2014</ref>) evaluated by official M 2 scorer <ref type="bibr" target="#b3">(Dahlmeier and Ng, 2012)</ref>, and on BEA-2019 dev and test sets evaluated by ER-RANT <ref type="bibr" target="#b2">(Bryant et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Token-level transformations</head><p>We developed custom token-level transformations T (x i ) to recover the target text by applying them to the source tokens (x 1 . . . x N ). Transformations increase the coverage of grammatical error corrections for limited output vocabulary size for the most common grammatical errors, such as Spelling, Noun Number, Subject-Verb Agreement and Verb Form <ref type="bibr">(Yuan, 2017, p. 28)</ref>.</p><p>The edit space which corresponds to our default tag vocabulary size = 5000 consists of 4971 basic transformations (token-independent KEEP, DELETE and 1167 token-dependent APPEND, 3802 REPLACE) and 29 token-independent gtransformations.</p><p>Basic transformations perform the most common token-level edit operations, such as: keep the current token unchanged (tag $KEEP), delete current token (tag $DELETE), append new token t 1 next to the current token x i (tag $APPEND t 1 ) or replace the current token x i with another token t 2 (tag $REPLACE t 2 ). g-transformations perform task-specific operations such as: change the case of the current token (CASE tags), merge the current token and the next token into a single one (MERGE tags) and split the current token into two new tokens (SPLIT tags). Moreover, tags from NOUN NUMBER and VERB FORM transformations encode grammatical properties for tokens. For instance, these transformations include conversion of singular nouns to plurals and vice versa or even change the form of regular/irregular verbs to express a different number or tense.</p><p>To obtain the transformation suffix for the VERB FORM tag, we use the verb conjugation dictionary 8 . For convenience, it was converted into the following format: token 0 token 1 : tag 0 tag 1 (e.g., go goes : V B V BZ). This means that there is a transition from word 0 and word 1 to the respective tags. The transition is unidirectional, so if there exists a reverse transition, it is presented separately.</p><p>The experimental comparison of covering capabilities for our token-level transformations is in Table 2. All transformation types with examples are listed in Appendix, <ref type="table" target="#tab_16">Table 9</ref>.</p><p>Preprocessing. To approach the task as a sequence tagging problem we need to convert each target sentence from training/evaluation sets into a sequence of tags where each tag is mapped to a single source token. Below is a brief description of our 3-step preprocessing algorithm for color-coded sentence pair from <ref type="table" target="#tab_6">Table 3:</ref> Step 1). Map each token from source sentence to subsequence of tokens from target sentence.  For this purpose, we first detect the minimal spans of tokens which define differences between source tokens (x 1 . . . x N ) and target tokens (y 1 . . . y M ). Thus, such a span is a pair of selected source tokens and corresponding target tokens. We can't use these span-based alignments, because we need to get tags on the token level. So then, for each source token x i , 1 ≤ i ≤ N we search for best-fitting subsequence Υ i = (y j 1 . . . y j 2 ), 1 ≤ j 1 ≤ j 2 ≤ M of target tokens by minimizing the modified Levenshtein distance (which takes into account that successful gtransformation is equal to zero distance).</p><p>Step 2). For each mapping in the list, find tokenlevel transformations which convert source token to the target subsequence: Step 3). Leave only one transformation for each source token: A ⇔ $KEEP, ten</p><formula xml:id="formula_0">⇔ $MERGE HYPHEN, years ⇔ $NOUN NUMBER SINGULAR, old ⇔ $KEEP, go ⇔ $VERB FORM VB VBZ, school ⇔ $APPEND {.}.</formula><p>The iterative sequence tagging approach adds a constraint because we can use only a single tag for each token. In case of multiple transformations we take the first transformation that is not a $KEEP tag. For more details, please, see the preprocessing script in our repository 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tagging model architecture</head><p>Our GEC sequence tagging model is an encoder made up of pretrained BERT-like transformer 9 https://github.com/grammarly/gector  stacked with two linear layers with softmax layers on the top. We always use cased pretrained transformers in their Base configurations. Tokenization depends on the particular transformer's design: BPE <ref type="bibr" target="#b20">(Sennrich et al., 2016b)</ref> is used in RoBERTa, WordPiece <ref type="bibr" target="#b18">(Schuster and Nakajima, 2012)</ref> in BERT and SentencePiece <ref type="bibr" target="#b10">(Kudo and Richardson, 2018)</ref> in XLNet. To process the information at the token-level, we take the first subword per token from the encoders representation, which is then forwarded to subsequent linear layers, which are responsible for error detection and error tagging, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Iterative sequence tagging approach</head><p>To correct the text, for each input token x i , 1 ≤ i ≤ N from the source sequence (x 1 . . . x N ), we predict the tag-encoded token-level transformation T (x i ) described in Section 3. These predicted tagencoded transformations are then applied to the sentence to get the modified sentence.</p><p>Since some corrections in a sentence may depend on others, applying GEC sequence tagger only once may not be enough to fully correct the sentence. Therefore, we use the iterative correction approach from <ref type="bibr" target="#b0">(Awasthi et al., 2019)</ref>: we use the GEC sequence tagger to tag the now modified sequence, and apply the corresponding transformations on the new tags, which changes the sentence further (see an example in <ref type="table" target="#tab_6">Table 3</ref>). Usually, the number of corrections decreases with each successive iteration, and most of the corrections are done during the first two iterations <ref type="table" target="#tab_8">(Table 4)</ref>. Limiting the number of iterations speeds up the overall pipeline while trading off qualitative performance.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Training stages. We have 3 training stages (details of data usage are in <ref type="table" target="#tab_1">Table 1):</ref> I Pre-training on synthetic errorful sentences as in <ref type="bibr" target="#b0">(Awasthi et al., 2019)</ref>.</p><p>II Fine-tuning on errorful-only sentences.</p><p>III Fine-tuning on subset of errorful and errorfree sentences as in <ref type="bibr" target="#b9">(Kiyono et al., 2019)</ref>.</p><p>We found that having two fine-tuning stages with and without error-free sentences is crucial for performance <ref type="table" target="#tab_9">(Table 5</ref>). All our models were trained by Adam optimizer (Kingma and Ba, 2015) with default hyperparameters. Early stopping was used; stopping criteria was 3 epochs of 10K updates each without improvement. We set batch size=256 for pre-training stage I (20 epochs) and batch size=128 for finetuning stages II and III (2-3 epochs each). We also observed that freezing the encoder's weights for the first 2 epochs on training stages I-II and using a batch size greater than 64 improves the convergence and leads to better GEC performance.</p><p>Encoders from pretrained transformers. We fine-tuned BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, RoBERTa , <ref type="bibr">GPT-2 (Radford et al., 2019)</ref>, XLNet <ref type="bibr" target="#b23">(Yang et al., 2019)</ref>, and ALBERT <ref type="bibr" target="#b11">(Lan et al., 2019)</ref> with the same hyperparameters setup. We also added LSTM with randomly initialized embeddings (dim = 300) as a baseline. As follows from <ref type="table" target="#tab_11">Table 6</ref>, encoders from fine-tuned Transformers significantly outperform LSTMs. BERT, RoBERTa and XLNet encoders perform better than GPT-2 and ALBERT, so we used them only in our next experiments. All models were trained out-of-the-box 10 which seems to not work well for GPT-2. We hypothesize that encoders from Transformers which were pretrained as a part of the entire encoder-decoder pipeline are less useful for GECToR.  Tweaking the inference. We forced the model to perform more precise corrections by introducing two inference hyperparameters (see Appendix, <ref type="table" target="#tab_1">Table 11</ref>), hyperparameter values were found by random search on BEA-dev.</p><p>First, we added a permanent positive confidence bias to the probability of $KEEP tag which is responsible for not changing the source token. Second, we added a sentence-level minimum error probability threshold for the output of the error detection layer. This increased precision by trading off recall and achieved better F 0.5 scores <ref type="table" target="#tab_9">(Table 5</ref>).</p><p>Finally, our best single-model, GECToR (XL-Net) achieves F 0.5 = 65.3 on CoNLL-2014 (test) and F 0.5 = 72.4 on <ref type="bibr">BEA-2019 (test)</ref>. Best ensemble model, GECToR (BERT + RoBERTa + XL-Net) where we simply average output probabilities from 3 single models achieves F 0.5 = 66.5 on CoNLL-2014 (test) and F 0.5 = 73.6 on BEA-2019 (test), correspondingly <ref type="table" target="#tab_13">(Table 7)</ref>.</p><p>Speed comparison. We measured the models average inference time on NVIDIA Tesla V100 on batch size 128. For sequence tagging we don't need to predict corrections one-by-one as in autoregressive transformer decoders, so inference is naturally parallelizable and therefore runs many times faster. Our sequence tagger's inference speed is up to 10 times as fast as the state-ofthe-art Transformer from <ref type="bibr" target="#b26">Zhao et al. (2019)</ref>, beam size=12 <ref type="table" target="#tab_14">(Table 8)</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We show that a faster, simpler, and more efficient GEC system can be developed using a sequence tagging approach, an encoder from a pretrained Transformer, custom transformations and 3-stage training.</p><p>Our best single-model/ensemble GEC tagger achieves an F 0.5 of 65.3/66.5 on <ref type="bibr">CoNLL-2014 (test)</ref> and F 0.5 of 72.4/73.6 on <ref type="bibr">BEA-2019 (test)</ref>. We achieve state-of-the-art results for the GEC task with an inference speed up to 10 times as fast as Transformer-based seq2seq systems.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>[A → A]: $KEEP, [ten → ten, -]: $KEEP, $MERGE HYPHEN, [years → year, -]: $NOUN NUMBER SINGULAR, $MERGE HYPHEN], [old → old]: $KEEP, [go → goes, to]: $VERB FORM VB VBZ, $AP-PEND to, [school → school, .]: $KEEP, $AP-PEND {.}].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Training datasets. Training stage I is pretraining on synthetic data. Training stages II and III are for fine-tuning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>[A → A], [ten → ten, -], [years → year, -], [old → old], [go → goes, to], [school → school, .].</figDesc><table><row><cell>Tag</cell><cell cols="2">Transformations</cell></row><row><cell>vocab. size</cell><cell cols="2">Basic transf. All transf.</cell></row><row><cell>100</cell><cell>60.4%</cell><cell>79.7%</cell></row><row><cell>1000</cell><cell>76.4%</cell><cell>92.9%</cell></row><row><cell>5000</cell><cell>89.5%</cell><cell>98.1%</cell></row><row><cell>10000</cell><cell>93.5%</cell><cell>100.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Share of covered grammatical errors in CoNLL-2014 for basic transformations only (KEEP, DELETE, APPEND, REPLACE) and for all transformations w.r.t. tag vocabulary's size. In our work, we set the default tag vocabulary size = 5000 as a heuristical compromise between coverage and model size.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Example of iterative correction process where GEC tagging system is sequentially applied at each iteration. Cumulative number of corrections is given for each iteration. Corrections are in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Cumulative number of corrections and corresponding scores on CoNLL-2014 (test) w.r.t. number of iterations for our best single model.</figDesc><table><row><cell>Training</cell><cell cols="3">CoNLL-2014 (test)</cell><cell cols="3">BEA-2019 (dev)</cell></row><row><cell>stage #</cell><cell>P</cell><cell>R</cell><cell>F0.5</cell><cell>P</cell><cell>R</cell><cell>F0.5</cell></row><row><cell>Stage I.</cell><cell cols="6">55.4 35.9 49.9 37.0 23.6 33.2</cell></row><row><cell>Stage II.</cell><cell cols="6">64.4 46.3 59.7 46.4 37.9 44.4</cell></row><row><cell>Stage III.</cell><cell cols="6">66.7 49.9 62.5 52.6 43.0 50.3</cell></row><row><cell cols="7">Inf. tweaks 77.5 40.2 65.3 66.0 33.8 55.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Performance of GECToR (XLNet) after each training stage and inference tweaks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Varying encoders from pretrained Transformers in our sequence labeling system. Training was done on data from training stage II only.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Comparison of single models and ensembles. The M 2 score forCoNLL-2014 (test)  and ERRANT for the BEA-2019 (test) are reported. In ensembles we simply average output probabilities from single models.</figDesc><table><row><cell>GEC system</cell><cell>Time (sec)</cell></row><row><cell>Transformer-NMT, beam size = 12</cell><cell>4.35</cell></row><row><cell>Transformer-NMT, beam size = 4</cell><cell>1.25</cell></row><row><cell>Transformer-NMT, beam size = 1</cell><cell>0.71</cell></row><row><cell>GECToR (XLNet), 5 iterations</cell><cell>0.40</cell></row><row><cell>GECToR (XLNet), 1 iteration</cell><cell>0.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Inference time for NVIDIA Tesla V100 on</cell></row><row><cell>CoNLL-2014 (test), single model, batch size=128.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>VERB FORM VBD VB . . . each of these items will {fell ⇒ fall} in price . . . g-23 VERB FORM VBD VBZ $VERB FORM VBD VBZ . . . the lake {froze ⇒ freezes} every year . . . g-24 VERB FORM VBD VBN $VERB FORM VBD VBN . . . he has been went {went ⇒ gone} since last week . . . g-25 VERB FORM VBD VBG $VERB FORM VBD VBG . . . talked her into {gave ⇒ giving} me the whole day . . . g-26 VERB FORM VBG VB $VERB FORM VBG VB . . . free time, I just {enjoying ⇒ enjoy} being outdoors . . . g-27 VERB FORM VBG VBZ $VERB FORM VBG VBZ . . . there still {existing ⇒ exists} many inevitable factors . . . g-28 VERB FORM VBG VBN $VERB FORM VBG VBN . . . people are afraid of being {tracking ⇒ tracked} . . . g-29 VERB FORM VBG VBD $VERB FORM VBG VBD . . . there was no {mistook ⇒ mistaking} his sincerity . . .</figDesc><table><row><cell cols="2">A Appendix</cell><cell></cell><cell></cell><cell></cell></row><row><cell>id</cell><cell>Core transformation</cell><cell>Transformation suffix</cell><cell>Tag</cell><cell>Example</cell></row><row><cell>basic-1</cell><cell>KEEP</cell><cell>∅</cell><cell>$KEEP</cell><cell>. . . many people want to travel during the summer . . .</cell></row><row><cell>basic-2</cell><cell>DELETE</cell><cell>∅</cell><cell>$DELETE</cell><cell>. . . not sure if you are {you ⇒ ∅} gifting . . .</cell></row><row><cell>basic-3</cell><cell>REPLACE</cell><cell>a</cell><cell>$REPLACE a</cell><cell>. . . the bride wears {the ⇒ a} white dress . . .</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell>basic-3804</cell><cell>REPLACE</cell><cell>cause</cell><cell>$REPLACE cause</cell><cell>. . . hope it does not {make ⇒ cause} any trouble . . .</cell></row><row><cell>basic-3805</cell><cell>APPEND</cell><cell>for</cell><cell>$APPEND for</cell><cell>. . . he is {waiting ⇒ waiting for} your reply . . .</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell>basic-4971</cell><cell>APPEND</cell><cell>know</cell><cell>$APPEND know</cell><cell>. . . I {don't ⇒ don't know} which to choose. . .</cell></row><row><cell>g-1</cell><cell>CASE</cell><cell>CAPITAL</cell><cell>$CASE CAPITAL</cell><cell>. . . surveillance is on the {internet ⇒ Internet} . . .</cell></row><row><cell>g-2</cell><cell>CASE</cell><cell>CAPITAL 1</cell><cell>$CASE CAPITAL 1</cell><cell>. . . I want to buy an {iphone ⇒ iPhone} . . .</cell></row><row><cell>g-3</cell><cell>CASE</cell><cell>LOWER</cell><cell>$CASE LOWER</cell><cell>. . . advancement in {Medical ⇒ medical} technology . . .</cell></row><row><cell>g-4</cell><cell>CASE</cell><cell>UPPER</cell><cell>$CASE UPPER</cell><cell>. . . the {it ⇒ IT} department is concerned that. . .</cell></row><row><cell>g-5</cell><cell>MERGE</cell><cell>SPACE</cell><cell>$MERGE SPACE</cell><cell>. . . insert a special kind of gene {in to ⇒ into} the cell . . .</cell></row><row><cell>g-6</cell><cell>MERGE</cell><cell>HYPHEN</cell><cell>$MERGE HYPHEN</cell><cell>. . . and needs {in depth ⇒ in-depth} search . . .</cell></row><row><cell>g-7</cell><cell>SPLIT</cell><cell>HYPHEN</cell><cell>$SPLIT HYPHEN</cell><cell>. . . support us for a {long-run ⇒ long run} . . .</cell></row><row><cell>g-8</cell><cell>NOUN NUMBER</cell><cell>SINGULAR</cell><cell>$NOUN NUMBER SINGULAR</cell><cell>. . . a place to live for their {citizen ⇒ citizens}</cell></row><row><cell>g-9</cell><cell>NOUN NUMBER</cell><cell>PLURAL</cell><cell>$NOUN NUMBER PLURAL</cell><cell>. . . carrier of this {diseases ⇒ disease} . . .</cell></row><row><cell>g-10</cell><cell>VERB FORM</cell><cell>VB VBZ</cell><cell>$VERB FORM VB VBZ</cell><cell>. . . going through this {make ⇒ makes} me feel . . .</cell></row><row><cell>g-11</cell><cell>VERB FORM</cell><cell>VB VBN</cell><cell>$VERB FORM VB VBN</cell><cell>. . . to discuss what {happen ⇒ happened} in fall . . .</cell></row><row><cell>g-12</cell><cell>VERB FORM</cell><cell>VB VBD</cell><cell>$VERB FORM VB VBD</cell><cell>. . . she sighed and {draw ⇒ drew} her . . .</cell></row><row><cell>g-13</cell><cell>VERB FORM</cell><cell>VB VBG</cell><cell>$VERB FORM VB VBG</cell><cell>. . . shown success in {prevent ⇒ preventing} such . . .</cell></row><row><cell>g-14</cell><cell>VERB FORM</cell><cell>VB VBZ</cell><cell>$VERB FORM VB VBZ</cell><cell>. . . a small percentage of people {goes ⇒ go} by bike . . .</cell></row><row><cell>g-15</cell><cell>VERB FORM</cell><cell>VBZ VBN</cell><cell>$VERB FORM VBZ VBN</cell><cell>. . . development has {pushes ⇒ pushed} countries to . . .</cell></row><row><cell>g-16</cell><cell>VERB FORM</cell><cell>VBZ VBD</cell><cell>$VERB FORM VBZ VBD</cell><cell>. . . he {drinks ⇒ drank} a lot of beer last night . . .</cell></row><row><cell>g-17</cell><cell>VERB FORM</cell><cell>VBZ VBG</cell><cell>$VERB FORM VBZ VBG</cell><cell>. . . couldn't stop {thinks ⇒ thinking} about it . . .</cell></row><row><cell>g-18</cell><cell>VERB FORM</cell><cell>VBN VB</cell><cell>$VERB FORM VBN VB</cell><cell>. . . going to {depended ⇒ depend} on who is hiring . . .</cell></row><row><cell>g-19</cell><cell>VERB FORM</cell><cell>VBN VBZ</cell><cell>$VERB FORM VBN VBZ</cell><cell>. . . yet he goes and {eaten ⇒ eats} more melons . . .</cell></row><row><cell>g-20</cell><cell>VERB FORM</cell><cell>VBN VBD</cell><cell>$VERB FORM VBN VBD</cell><cell>. . . he {driven ⇒ drove} to the bus stop and . . .</cell></row><row><cell>g-21</cell><cell>VERB FORM</cell><cell>VBN VBG</cell><cell>$VERB FORM VBN VBG</cell><cell>. . . don't want you fainting and {broken ⇒ breaking} . . .</cell></row><row><cell>g-22</cell><cell>VERB FORM</cell><cell>VBD VB</cell><cell>$</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>List of token-level transformations (section 3). We denote a tag which defines a token-level transformation as concatenation of two parts: a core transformation and a transformation suffix. 47.1 63.0 54.2 41.0 50.9Inf. tweaks 73.9 41.5 64.0 62.3 35.1 54.0</figDesc><table><row><cell>Training</cell><cell cols="3">CoNLL-2014 (test)</cell><cell cols="3">BEA-2019 (dev)</cell></row><row><cell>stage #</cell><cell>P</cell><cell>R</cell><cell>F0.5</cell><cell>P</cell><cell>R</cell><cell>F0.5</cell></row><row><cell>Stage I.</cell><cell cols="6">57.8 33.0 50.2 40.8 22.1 34.9</cell></row><row><cell>Stage II.</cell><cell cols="6">68.1 42.6 60.8 51.6 33.8 46.7</cell></row><row><cell>Stage III.</cell><cell>68.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>Performance of GECToR (RoBERTa) after each training stage and inference tweaks. Results are given in addition to results for our best single model, GECToR (XLNet) which are given inTable 5.</figDesc><table><row><cell>System name</cell><cell cols="2">Confidence bias Minimum error probability</cell></row><row><cell>GECToR (BERT)</cell><cell>0.10</cell><cell>0.41</cell></row><row><cell>GECToR (RoBERTa)</cell><cell>0.20</cell><cell>0.50</cell></row><row><cell>GECToR (XLNet)</cell><cell>0.35</cell><cell>0.66</cell></row><row><cell>GECToR (RoBERTa + XLNet)</cell><cell>0.24</cell><cell>0.45</cell></row><row><cell>GECToR (BERT + RoBERTa + XLNet)</cell><cell>0.16</cell><cell>0.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Inference tweaking values which were found by random search on BEA-dev.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/gutfeeling/ word_forms/blob/master/word_forms/ en-verbs.txt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://huggingface.co/transformers/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>This research was supported by Grammarly. We thank our colleagues Vipul Raheja, Oleksiy Syvokon, Andrey Gryshchuk and our ex-colleague Maria Nadejde who provided insight and expertise that greatly helped to make this paper better. We would also like to show our gratitude to Abhijeet Awasthi and Roman Grundkiewicz for their support in providing data and answering related questions. We also thank 3 anonymous reviewers for their contribution.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parallel iterative edit models for local sequence transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasna</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabyasachi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4260" to="4270" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The BEA-2019 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Øistein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="52" to="75" />
		</imprint>
	</monogr>
	<note>Andersen, and Ted Briscoe. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic annotation and evaluation of error types for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="793" to="805" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="572" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner english: The nus corpus of learner english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew Mei</forename><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth workshop on innovative use of NLP for building educational applications</title>
		<meeting>the eighth workshop on innovative use of NLP for building educational applications</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural grammatical error correction systems with unsupervised pre-training on synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to combine grammatical error corrections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edo</forename><surname>Cohen-Karlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Toledo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Menczel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam (2014), a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR), arXiv preprint arXiv</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR), arXiv preprint arXiv</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1412</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical study of incorporating pseudo data into grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1236" to="1242" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Encode, tag, realize: High-precision text editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Mirylenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5054" to="5065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">China</forename><surname>Kong</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The conll-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The cambridge learner corpus: Error coding and analysis for lexicography and elt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Nicholls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Corpus Linguistics</title>
		<meeting>the Corpus Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="572" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Edinburgh neural machine translation systems for WMT 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tense and aspect error correction for esl learners using global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshikazu</forename><surname>Tajiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="198" to="202" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading esol texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Grammatical error correction in non-native english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Computer Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving grammatical error correction via pre-training a copy-augmented architecture with unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
