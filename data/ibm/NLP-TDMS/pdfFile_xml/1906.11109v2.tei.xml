<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. ESAT</orgName>
								<orgName type="department" key="dep2">Center for Processing Speech and Images KU</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>De Brabandere</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. ESAT</orgName>
								<orgName type="department" key="dep2">Center for Processing Speech and Images KU</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Proesmans</forename><surname>Luc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. ESAT</orgName>
								<orgName type="department" key="dep2">Center for Processing Speech and Images KU</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. ESAT</orgName>
								<orgName type="department" key="dep2">Center for Processing Speech and Images KU</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current state-of-the-art instance segmentation methods are not suited for real-time applications like autonomous driving, which require fast execution times at high accuracy. Although the currently dominant proposal-based methods have high accuracy, they are slow and generate masks at a fixed and low resolution. Proposal-free methods, by contrast, can generate masks at high resolution and are often faster, but fail to reach the same accuracy as the proposal-based methods. In this work we propose a new clustering loss function for proposal-free instance segmentation. The loss function pulls the spatial embeddings of pixels belonging to the same instance together and jointly learns an instance-specific clustering bandwidth, maximizing the intersection-over-union of the resulting instance mask. When combined with a fast architecture, the network can perform instance segmentation in real-time while maintaining a high accuracy. We evaluate our method on the challenging Cityscapes benchmark and achieve top results (5% improvement over Mask R-CNN) at more than 10 fps on 2MP images. Code will be available at: https:// github.com/davyneven/SpatialEmbeddings</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic instance segmentation is the task of locating all objects in an image, assigning each object to a specific class and generating a pixel-perfect mask for each one, perfectly delineating their shape. This contrasts with the standard bounding-box detection methods, where each object is represented by a crude rectangular box. Since having a binary mask for each object is desired (and necessary) in many applications, ranging from autonomous driving and robotics applications to photo-editing/analyzing applications, instance segmentation remains an important research topic.</p><p>Currently, the dominant method for instance segmenta- <ref type="figure">Figure 1</ref>. Our loss function encourages pixels to point into an optimal, object-specific region around the object's center, maximizing the intersection-over-union of each object's mask. For big objects, this region will be bigger, relaxing the loss for edge-pixels, which are further away from the center. Bottom left displays the learned offset vectors, encoded in color. Bottom right displays the displaced pixels, displaced with the learned offset vectors. Instances are recovered by clustering around each center with the learned, optimal clustering region.</p><p>tion is based on a detect-and-segment approach, where objects are detected using a bounding-box detection method and then a binary mask is generated for each one. Despite many attempts in the past, the Mask R-CNN framework was the first one to achieve outstanding results on many benchmarks, and is still the most used method for instance segmentation to date. While this method provides good results in terms of accuracy, it generates low resolution masks which are not always desirable (e.g. for photo-editing applications) and operates at a low frame rate, making it impractical for real-time applications such as autonomous driving.</p><p>Another popular branch of instance segmentation methods are proposal-free methods, which are mostly based on embedding loss functions or pixel affinity learning. Since these methods typically rely on dense-prediction networks, their generated instance masks can have a high resolution. Additionally, proposal-free methods often report faster runtimes than proposal-based ones. Although these methods are promising, they fail to perform as well as the above mentioned detect-and-segment approaches like Mask R-CNN.</p><p>In this paper, we formulate a new loss function for proposal-free instance segmentation, combining the benefits of both worlds: accurate, high resolution masks combined with real-time performance. Our method is based on the principle that pixels can be associated with an object by pointing to that object's center. Unlike previous works that apply a standard regression loss on all pixels, forcing them to point directly at the object's center, we introduce a new loss function which optimizes the intersection-over-union of each object's mask. Our loss function will therefore indirectly force object pixels to point into an optimal region around the object's center. For big objects, the network will learn to make this region bigger, relaxing the loss on pixels which are further away from the object's center. At inference time, instances are recovered by clustering around each object's center with the learned, object-specific region. See <ref type="figure">figure 1</ref>.</p><p>We test our method on the challenging Cityscapes dataset and show that we achieve top results, surpassing Mask R-CNN with an Average Precision score of 27.6 versus 26.2, at a frame rate of more than 10 fps. We also observe that our method does very well on cars and pedestrians, reaching similar accuracy scores as a Mask R-CNN model which was trained on a combination of Cityscapes and COCO. On the Cityscapes dataset, our method is the first one which runs in real time while maintaining a high accuracy.</p><p>In summary, we (1) propose a new loss function which directly optimizes the intersection-over-union of each instance by pulling pixels into an optimal, object-specific clustering region and (2) achieve top results in real-time on the Cityscapes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The current best performing instance segmentation methods are proposal-based, and rely on the Faster R-CNN <ref type="bibr" target="#b23">[24]</ref> object detection framework, which is the current leader in most object detection benchmarks. Previous instance segmentation approaches relied on their detection output to get object proposals, which they then refine into instance masks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> and its derivative PANet <ref type="bibr" target="#b16">[17]</ref> refine and simplify this pipeline by augmenting the Faster R-CNN network with a branch for predicting an object mask. Although they are the bestscoring methods on popular benchmarks, such as COCO, their instance masks are generated at a low resolution (32x32 pixels) and in practice are not often used in real-time applications.</p><p>Another branch of instance segmentation methods rely on dense-prediction, segmentation networks to generate instance masks at input resolution. Most of these methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref> are based on an embedding loss function, which forces the feature vectors of pixels belonging to the same object to be similar to each other and sufficiently dissimilar from feature vectors of pixels belonging to other objects. Recently, works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15]</ref> have shown that the spatial-invariant nature of Fully Convolutional Networks is not ideal for embedding methods and propose to either incorporate coordinate maps <ref type="bibr" target="#b14">[15]</ref> or use so-called semiconvolutions <ref type="bibr" target="#b19">[20]</ref> to alleviate this problem. Nevertheless, at the current time these methods still fail to achieve the same performance as the proposal-based ones.</p><p>In light of this, a more promising and simple method is proposed by Kendall et al. <ref type="bibr" target="#b10">[11]</ref>, inspired by <ref type="bibr" target="#b13">[14]</ref>, in which they propose to assign pixels to objects by pointing to its object's center. This way, they avoid the aforementioned problem of spatial-invariance by learning position-relative offset vectors. Our method is based on the same concept, but integrates the post-processing clustering step directly into the loss function and optimizes the intersection-over-union of each object's mask directly. Related to our method is the very recent work of Novotny et al. <ref type="bibr" target="#b19">[20]</ref>. Although similar in concepts, they use a different loss function and still apply a detection-first principle.</p><p>Also inspired by <ref type="bibr" target="#b10">[11]</ref> is Box2Pix, a work proposed by Uhrig et al. <ref type="bibr" target="#b25">[26]</ref>, where they first predict bounding boxes based on a single-shot detection method, and then associate pixels by pointing to object centers, which can afterwards be efficiently clustered. Its focus lays on real-time instance segmentation and shows promising results on the Cityscapes dataset. Our method also shows real-time performance on the Cityscapes dataset, but at a much higher accuracy.</p><p>Our loss relaxation by learning an optimal clustering margin shows some similarites with <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10]</ref>, where they integrate the aleatoric uncertainty into the loss function. In contrast to these works, we directly use the learned margin at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We treat instance segmentation as a pixel assignment problem, where we want to associate pixels with the correct objects. To this end we learn an offset vector for each pixel, pointing to its object's center. Unlike the standard regression approach, which we explain further in 3.1, we also learn an optimal clustering region for each object and by doing so we relax the loss for pixels far away from the center. This is explained in 3.2. To locate the object's centers, we learn a seed map for each semantic class, as described in 3.5. The pipeline is graphically depicted in figure 2.  <ref type="figure">Figure 2</ref>. Instance segmentation pipeline. The bottom branch of the network predicts: a) a sigma value for each pixel, which directly translates into a clustering margin for each object. Bigger objects are more blueish, meaning a bigger margin, and smaller objects are more yellowish, meaning a smaller margin. b) Offset vectors for each pixel, pointing at the center of attraction, and displayed using a color-encoding where the color indicates the angle of the vector. The top branch predicts a seed map for each semantic class. A high value indicates that the offset vector of that pixel points directly at the object center. Notice therefore that the borders have a low value, since they have more difficulty of knowing to which center to point. The pixel embeddings (= offset vectors + coordinate vectors) and margins calculated from the predicted sigma are also displayed. The cluster centers are derived from the seed maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Regression to the instance centroid</head><p>The goal of instance segmentation is to cluster a set of pixels</p><formula xml:id="formula_0">X = {x 0 , x 1 , x 2 , ..., x N }, with x a 2- dimensional coordinate vector, into a set of instances S = {S 0 , S 1 , ..., S K }.</formula><p>An often used method is to assign pixels to their corresponding instance centroid C k = 1 N x∈S k x . This is achieved by learning an offset vector o i for each pixel x i , so that the resulting (spatial) embedding e i = x i + o i points to its corresponding instance centroid. Typically, the offset vectors are learned using a regression loss function with direct supervision:</p><formula xml:id="formula_1">L regr = n i=1 o i −ô i<label>(1)</label></formula><p>whereô i = C k − x i for x i ∈ S k . However, the above method poses two issues at inference time. First, the locations of the instance centroids have to be determined and second, the pixels have to be assigned to a specific instance centroid. To solve these problems, previous methods rely on density-based clustering algorithms to first locate a set of centroids C = {C 0 , C 1 , ..., C K } and next assign pixels to a specific instance based on a minimum distance-to-centroid metric :</p><formula xml:id="formula_2">e i ∈ S k : k = arg min C e i − C<label>(2)</label></formula><p>Since this post-processing step (center localization and clustering) is not integrated within the loss function, the network cannot be optimized end-to-end for instance segmentation, leading to inferior results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learnable margin</head><p>The assignment of pixels to instance centroids can be incorporated into the loss function by replacing the standard regression loss with a hinge loss variant, forcing pixels to lay within a specified margin δ (the hinge margin) around the instance centroid:</p><formula xml:id="formula_3">L hinge = K k=1 ei∈S k max( e i − C k − δ, 0)<label>(3)</label></formula><p>This way, at test time, pixels are assigned to a centroid by clustering around the centroid with this fixed margin:</p><formula xml:id="formula_4">e i ∈ S k ⇐⇒ e i − C k &lt; δ<label>(4)</label></formula><p>However, a downside to this method is that the margin δ has to be selected based on the smallest object, ensuring that if two small objects are next to each other, they can still be clustered into two different instances. If a dataset contains both small and big objects, this constraint negatively influences the accuracy of big objects, since pixels far away from the centroid will not be able to point into this small region around the centroid. Although using a hinge loss incorporates the clustering into the loss function, given the said downside it is not usable in practice.</p><p>To solve this issue we propose to learn an instance specific margin. For small instances a small margin should be used, while for bigger objects, a bigger margin would be preferred. This way, we relax the loss for pixels further away from the instance centroid, as they are no longer forced to point exactly at the instance centroid.</p><p>In order to do so, we propose to use a gaussian function φ k for each instance S k , which converts the distance between a (spatial) pixel embedding e i = x i + o i and the instance centroid C k into a probability of belonging to that instance:</p><formula xml:id="formula_5">φ k (e i ) = exp − e i − C k 2 2σ 2 k<label>(5)</label></formula><p>A high probability means that the pixel embedding e i is close to the instance centroid and is likely to belong to that instance, while a low probability means that the pixel is more likely to belong to the background (or another instance). More specifically, if φ k (e i ) &gt; 0.5, than that pixel, at location x i , will be assigned to instance k.</p><p>Thus, by modifying the sigma parameter of the mapping function, the margin can be controlled:</p><formula xml:id="formula_6">margin = −2σ 2 k ln 0.5<label>(6)</label></formula><p>A large sigma will result in a bigger margin, while a small sigma will result in a smaller margin. This additionally requires the network to output a σ i at each pixel location. We define σ k as the average of all σ i belonging to instance k:</p><formula xml:id="formula_7">σ k = 1 |S k | σi∈S k σ i<label>(7)</label></formula><p>Since for each instance k the gaussian outputs a foreground/background probability map, this can be optimized by using a binary classification loss with the binary foreground/background map of each instance as ground-truth. As opposed to using the standard cross-entropy loss function, we opt for using the Lovasz-hinge loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref> instead. Since this loss function is a (piecewise linear) convex surrogate to the Jaccard loss, it directly optimizes the intersection-over-union of each instance. Therefore we do not need to account for the class imbalance between foreground and background.</p><p>Note that there is no direct supervision on the sigma and offset vector outputs of the network (as was the case in the standard regression loss). Instead, they are jointly optimized to maximize the intersection-over-union of each instance mask, receiving gradients by backpropagation through the Lovasz-hinge loss function and through the gaussian function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Intuition</head><p>Let us first consider the case where the sigma (margin) of the Gaussian function is kept fixed. In contrast with the standard regression loss explained above, we don't have an explicit loss term pulling instance pixels to the instance centroid. Instead, by minimizing the binary loss, instance pixels are now indirectly forced to lay within the region around the instance centroid and background pixels are forced to point outside this region.</p><p>When the sigma is not fixed but a learnable parameter, the network can now also modify sigma to minimize the loss more efficiently. Aside from pulling instance pixels within the (normally small) region around the instance centroid and pushing background pixels outside this region, it can now also modify sigma such that the size of the region is more appropriate for that specific instance. Intuitively this would mean that for a big object it would adapt sigma to make the region around the centroid bigger, so that more instance pixels can point inside this region, and for small objects to choose a smaller region, so that it is easier for background pixels to point outside the region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss extensions</head><p>Elliptical margin In the above formulation of the gaussian function we have used a scalar value for sigma. This will result in a circular margin. However, we can modify the mapping function to use a 2-dimensional sigma:</p><formula xml:id="formula_8">φ k (e i ) = exp − (e ix − C kx ) 2 2σ 2 kx − (e iy − C ky ) 2 2σ 2 ky<label>(8)</label></formula><p>By doing so, the network has the possibility of also learning an elliptical margin, which may be better suited for elongated objects such as pedestrians or trains. Note that in this case the network has to output two sigma maps, one for σ x and one for σ y .</p><p>Learnable Center of Attraction Another modification can be made on the center of the gaussian function. Currently, we place the gaussian in the centroid C k of each instance. By doing so, pixel embeddings are pulled towards the instance centroid. However, we can also let the network learn a more optimal Center of Attraction. This can be done by defining the center as the mean over the embeddings of instance k. This way, the network can influence the location of the center of attraction by changing the location of the embeddings:</p><formula xml:id="formula_9">φ k (e i ) = exp − e i − 1 |S k | ej ∈S k e j 2 2σ 2 k<label>(9)</label></formula><p>We will test these modifications in the ablation experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Seed map</head><p>At inference time we need to cluster around the center of each object. Since the above loss function forces pixel embeddings to lay close to the object's center, we can sample a good pixel embedding and use that location as instance center. Therefore, for each pixel embedding we learn how far it is removed from the instance center. Pixel embeddings who lay very close to their instance center will get a high score in the seed map, pixel embeddings which are far away from the instance center will get a low score in the seed map. This way, at inference time, we can select a pixel embedding with a high seed score, indicating that that embedding will be very close to an object's center.</p><p>In fact, the seediness score of a pixel embedding should equal the output of the gaussian function, since it converts the distance between an embedding and the instance center into a closeness score. The closer the embedding is laying to the center, the closer the output will be to 1.</p><p>Therefore, we train the seed map with a regression loss function. Background pixels are regressed to zero and foreground pixels are regressed to the output of the gaussian. We train a seed map for each semantic class, with the following loss function:</p><formula xml:id="formula_10">L seed = 1 N N i 1 {si∈S k } s i − φ k (e i ) 2 + 1 {si∈bg} s i − 0 2<label>(</label></formula><p>10) with s i the network's seed output of pixel i. Note that this time we consider φ k (e i ) to be a scalar: gradients are only calculated for s i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Post-processing</head><p>At inference time, we follow a sequential clustering approach for each class-specific seed map. The pixels in the seed map with the highest value indicate which embeddings lay closest to an object's center. The procedure is to sample the embedding with the highest seed value and use that location as instance centerĈ k . At the same location, we also take the sigma value,σ k . By using this center and accompanying sigma, we cluster the pixel embeddings into instance S k :</p><formula xml:id="formula_11">e i ∈ S k ⇐⇒ exp − e i −Ĉ k 2 2σ k 2 &gt; 0.5<label>(11)</label></formula><p>We next mask out all clustered pixels in the seed map and continue sampling until all seeds are masked. We repeat this process for all classes.</p><p>To ensure that during samplingσ k ≈ σ k = 1 |S k | σi∈S k σ i , we add a smoothness term for each instance to the total loss: <ref type="bibr" target="#b11">12)</ref> </p><formula xml:id="formula_12">L smooth = 1 |S k | σi∈S k σ i − σ k 2<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we evaluate the performance of our instance segmentation method on the Cityscapes dataset. To find the best settings of our loss function, we first analyze the different aspects in an ablation study. Afterwards we report results of our best model on the test set of Cityscapes and compare with other top performing methods. Since our method is optimized for fast instance segmentation, we also report a time comparison with other instance segmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>Network architecture We use the ERFNetarchitecture <ref type="bibr" target="#b24">[25]</ref> as base-network.</p><p>ERFNet is a dense-prediction encoder-decoder network optimized for real-time semantic segmentation. We convert the model into a 2-branch network, by sharing the encoder part and having 2 separate decoders. The first branch predicts the sigma and offset values, with 3 or 4 output channels depending on sigma (σ vs σ xy ). The other branch outputs N seed maps, one for each semantic class. The offset values are limited between [-1,1] with a tanh activation function, sigma is made strictly positive by using an exponential activation function, effectively letting the network predict log( 1 2σ 2 ).</p><p>Coordinate map Since the Cityscapes images are of size 2048x1024, we construct a pixel coordinate map so that the x-coordinates are within the range of [0,2] and the ycoordinates within the range of [0,1]. This way, the difference in coordinate between two neighboring pixel is 1/1024, both in x and y direction. Because the offset vectors can have a value between [-1,1], each pixel can point at most 1024 pixels away from its current location.</p><p>Training procedure We first pre-train our models on 500x500 crops, taken out of the original 2048x1024 train images and centered around an object, for 200 epochs with a batch-size of 12. This way, we don't spend to much computation time on background patches without any instances. Afterwards we finetune the network for another 50 epochs on 1024x1024 crops with a batch-size of 2 to increase the performance on the bigger objects who couldn't fit completely within the 500x500 crop. During this stage, we keep the batch normalization statistics fixed. We use the Adam optimizer and polynomial learning rate decay (1 − epoch max epoch ) 0.9 . During pre-training we use an initial learning rate of 5e-4, which we lower to 5e-5 for finetuning. Training takes roughly 24 hours on two NVIDIA 1080 Ti GPU's. Next to random cropping, we also apply random horizontal mirroring as data-augmentation.  <ref type="figure">Figure 3</ref>. Speed accuracy trade-off between instance segmentation methods on the Cityscapes benchmark. Our method is the first real-time method with high accuracies. Image adapted from <ref type="bibr" target="#b25">[26]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cityscapes dataset</head><p>The Cityscapes dataset is high quality dataset for urban scene understanding. It consists out of 5,000 finely annotated images (fine) of 2048 by 1024 pixels, with both semantic and instance-wise annotations, and 20,000 coarsely annotated images (coarse) with only semantic annotations. The wide range in object size and the varying scene layout makes this a challenging dataset for instance segmentation methods.</p><p>The instance segmentation task consists in detecting objects of 8 different semantic classes and generating a binary mask for each of them. The performance is evaluated by the average precision (AP) criterion on the region level and averaged over the different classes. Aside from AP, AP 50% for an overlap of 50 %, AP 100m and AP 50m for objects restricted to respectively 100m and 50m are also reported.</p><p>In the following experiments we will only use the fine train set to train our models, which consists out of the following classes with their respective number of objects: Note that some classes (truck, bus, train) are highly underrepresented, which will negatively effect the model's test performance on those specific classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Experiments</head><p>In this section we evaluate the influence of the different parameters of our loss function on the validation set of Cityscapes: we investigate the importance of a learnable sigma, the difference in using the instance centroid or a learnable center as the center of attraction, and the difference in using a scalar or a 2 dimensional sigma. Since we want to measure the effect on the instance part, we remove the object detection and classification part from the equation by using the ground truth annotations to localize the objects and assigning the correct semantic class, which is indicated in the tables as AP gt .</p><p>Fixed vs. learnable sigma In this experiment we evaluate the importance of a learnable, instance-unique sigma over a fixed one. As explained in section 3.2, when using a fixed sigma, the value has to be selected based on the size of the smallest object we still want to be able to separate, and is therefore set to correspond with a margin of 20 pixels. The results can be seen in table 2. The significant performance difference (28 AP vs. 38.7 AP) shows the importance of having a unique, learnable sigma for each instance. Notice also that for classes with relatively more small instances, the difference is less pronounced, as expected.</p><p>Fixed vs. learnable Center of Attraction As described in the method section, the center of attraction (CoA) of an instance can be defined as either the centroid, or more general, as a learnable center calculated by taking the mean over all spatial embeddings belonging to the instance. Intuitively, by giving the network the opportunity to decide on the location of the CoA itself, it can learn a more optimal location than the standard centroid. In table 2 we evaluate the two different approaches on the Cityscapes validation set using a ground-truth sampling approach,both in the case of a scalar or a 2-dimensional sigma. As predicted, in both  <ref type="table">Table 2</ref>. Ablation experiments evaluated on the Cityscapes validation set using a ground-truth sampling approach. We measure the performance of a fixed sigma, the difference in using a scalar vs. 2-dimensional sigma and the difference in using the centroid or learnable center as center of attraction. cases we achieve a higher AP-score when using a learnable center instead of the fixed centroid, with a noticeable improvement over all classes.</p><p>Circular vs. elliptical margin The margin for each instance is defined by the learnable sigma parameter in the gaussian function. This sigma can either be a scalar (σ), which results in a circular margin, or a two-dimensional vector (σ xy ), resulting in an elliptical margin. For rectangular objects (e.g. pedestrians) a circular margin is not optimal, since it can only expand until it reaches the shortest border. An elliptical margin however would have the possibility to stretch and adapt to the shape of an object, possibly resulting in a higher accuracy. In table 2 we compare both methods and verify that a 2-dimensional sigma (elliptical margin) indeed performs better than a scalar one (circular margin).</p><p>Since sigma is a learnable parameter, we have no direct control over its value. Intuitively, since sigma controls the clustering margin, we speculated that for big objects sigma will be bigger, resulting in a bigger margin, and smaller for small objects. To verify this, in <ref type="figure" target="#fig_1">figure 4</ref> we plotted sigma in function of the object's size. As predicted, there is indeed a positive correlation between an object's size and sigma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on Cityscapes</head><p>In table 1 we report results on the Cityscapes test set and compare with other high performing methods. Note however that it is important to pay attention at the training data on which a method is trained. Since the truck, bus and train classes are highly underrepresented in the fine set, methods who only train on this set will perform less on these classes than methods who augment their dataset with the coarse or COCO set.</p><p>Comparing our method against the other fine-only methods, we occupy the second place with an AP-score of 27.6, locating ourselves between between the popular Mask R-CNN (26.2) and PANet(31.8). Notice however that we do much better on the person (34.5 vs 30.5), rider (26.1 vs 23.7) and car class (52.4 vs 46.9) than Mask R-CNN. If we compare our method with GMIS, a method trained on both the fine and coarse set, we notice that although it has the same AP-score as our method, it only performs better on the truck, bus and train class (because of the extra coarse set) and performs worse on all other classes.</p><p>Although it is not fair to compare our method against methods trained on fine+COCO, we do notice that we achieve similar results on person (34.5 vs 34.8) and rider (26.1 vs 27.0), and even perform better on car (52.4 vs 49.1) and bicycle (18.9 vs 18.7) with respect to Mask R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Timing</head><p>In table 3 we compare the execution speed of different methods. This is also depicted in fig 3. Up to this moment, most methods have put there focus on accuracy rather than on execution speed. Mask-RCNN (26.2 AP -1fps) and derivatives have high accuracy, but slow execution speed. Other methods,like Discriminative loss (17.5 AP -5fps) or Box2Pix (13.1 AP -10.9fps) achieve higher frame rates by downsampling resolution or using single shot detection methods, but dramatically lack behind in accuracy compared to Mask R-CNN. Since our method is based on the ERFNet network and combined with a clustering loss function, we are the first ones to achieve high accuracy combined with real time performance (27.6 AP -11fps). More <ref type="figure">Figure 5</ref>. Results on the Cityscapes dataset. From left to right: input image, ground-truth and our predictions. Notice that our method is very good at detecting small objects and often predicts more correct objects than annotated in the ground-truth.  <ref type="table">Table 3</ref>. Approximate timing results of instance segmentation methods on a resolution of 2048x1024 with test set accuracy <ref type="bibr" target="#b25">[26]</ref>. Methods which are either to slow or have a very low accuracy are left out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>specifically, the forward pass at a resolution of 2MP takes 65ms and the clustering step requires 26ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work we have proposed a new clustering loss function for instance segmentation. By using a gaussian function to convert pixel embeddings into a foreground/background probability, we can optimize the intersection-over-union of each object's mask directly and learn an optimal, object-specific clustering margin. We show that when applied to a real-time, dense-prediction network, we achieve top results on the Cityscapes benchmark at more than 10 fps, making our method the first proposalfree, real-time instance segmentation method with high accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Learned margin against the object's size. Each dot represents an object in the dataset. As predicted, we notice a positive correlation between the margin and the object's size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on Cityscapes test set. With a score of 27.6 AP we reach second place on the benchmark, compared with the fine-only methods.</figDesc><table><row><cell>method</cell><cell>training data</cell><cell>AP</cell><cell cols="3">AP50 person rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train mcycle bicycle</cell></row><row><cell>DIN [2]</cell><cell cols="2">fine + coarse 23.4</cell><cell>45.2</cell><cell>20.9</cell><cell cols="4">18.4 31.7 22.8 31.1 31.0</cell><cell>19.6</cell><cell>11.7</cell></row><row><cell>SGN [16]</cell><cell cols="2">fine + coarse 25.0</cell><cell>44.9</cell><cell>21.8</cell><cell cols="4">20.1 39.4 24.8 33.2 30.8</cell><cell>17.7</cell><cell>12.4</cell></row><row><cell cols="2">PolygonRNN++ [1] fine</cell><cell>25.5</cell><cell>45.5</cell><cell>29.4</cell><cell cols="4">21.8 48.3 21.1 32.3 23.7</cell><cell>13.6</cell><cell>13.6</cell></row><row><cell>Mask R-CNN [9]</cell><cell>fine</cell><cell>26.2</cell><cell>49.9</cell><cell>30.5</cell><cell cols="4">23.7 46.9 22.8 32.2 18.6</cell><cell>19.1</cell><cell>16.0</cell></row><row><cell>GMIS [18]</cell><cell cols="2">fine + coarse 27.6</cell><cell>44.6</cell><cell>29.3</cell><cell cols="4">24.1 42.7 25.4 37.2 32.9</cell><cell>17.6</cell><cell>11.9</cell></row><row><cell>PANet [17]</cell><cell>fine</cell><cell>31.8</cell><cell>57.1</cell><cell>36.8</cell><cell cols="4">30.4 54.8 27.0 36.3 25.5</cell><cell>22.6</cell><cell>20.8</cell></row><row><cell>Mask R-CNN [9]</cell><cell>fine + COCO</cell><cell>31.9</cell><cell>58.1</cell><cell>34.8</cell><cell cols="4">27.0 49.1 30.1 40.9 30.9</cell><cell>24.1</cell><cell>18.7</cell></row><row><cell>PANet [17]</cell><cell>fine + COCO</cell><cell>36.4</cell><cell>63.1</cell><cell>41.5</cell><cell cols="4">33.6 58.2 31.8 45.3 28.7</cell><cell>28.2</cell><cell>24.1</cell></row><row><cell>ours</cell><cell>fine</cell><cell>27.6</cell><cell>50.9</cell><cell>34.5</cell><cell cols="4">26.1 52.4 21.7 31.2 16.4</cell><cell>20.1</cell><cell>18.9</cell></row><row><cell></cell><cell>Box2Pix</cell><cell>(ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>real-time</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FPS</cell><cell>Discriminate Loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Mask R-CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>PANet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SGN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Average Precision (AP)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This work was supported by Toyota, and was carried out at the TRACE Lab at KU Leuven (Toyota Research on Automated Cars in Europe -Leuven).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient interactive annotation of segmentation datasets with polygon-rnn++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="859" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pixelwise instance segmentation with a dynamically instantiated network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2858" to="2866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The lovász-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation via deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10277</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boundary-aware instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30Th Ieee Conference On Computer Vision And Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>number CONF. Ieee</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07115</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent pixel embedding for instance grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9018" to="9028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07709</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02636</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03247</idno>
		<title level="m">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sgn: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Affinity derivation and graph merge for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="686" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-convolutional operators for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jaroensri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10712</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning 3d object categories by looking around them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5218" to="5227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Box2pix: Single-shot instance segmentation by assigning pixels to object boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rehder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fröhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Instance-level segmentation of vehicles by deep contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Den Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="477" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning submodular losses with the lovász hinge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
