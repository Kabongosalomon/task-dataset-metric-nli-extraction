<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Unsupervised Sentence Embedding Method by Mutual Information Maximization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
							<email>yanzhang@mymail.sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Design 2 DAMO Academy</orgName>
								<orgName type="institution" key="instit1">Singapore University of Technology</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
							<email>ruidan.he@alibaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
							<email>zuozhuliu@intl.zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">ZJU-UIUC Institue</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan</forename><forename type="middle">Hui</forename><surname>Lim</surname></persName>
							<email>kwanhuilim@sutd.edu.sgl.bing@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Design 2 DAMO Academy</orgName>
								<orgName type="institution" key="instit1">Singapore University of Technology</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
						</author>
						<title level="a" type="main">An Unsupervised Sentence Embedding Method by Mutual Information Maximization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by learning semantically meaningful representations of single sentences, such that similarity comparison can be easily accessed. However, SBERT is trained on corpus with high-quality labeled sentence pairs, which limits its application to tasks where labeled data is extremely scarce. In this paper, we propose a lightweight extension on top of BERT and a novel self-supervised learning objective based on mutual information maximization strategies to derive meaningful sentence embeddings in an unsupervised manner. Unlike SBERT, our method is not restricted by the availability of labeled data, such that it can be applied on different domainspecific corpus. Experimental results show that the proposed method significantly outperforms other unsupervised sentence embedding baselines on common semantic textual similarity (STS) tasks and downstream supervised tasks. It also outperforms SBERT in a setting where in-domain labeled data is not available, and achieves performance competitive with supervised methods on various tasks. Our code is available at https://github.com/ yanzhangnlp/IS-BERT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>search because they need to evaluate combinatorially many sentence pairs during inference, which will result in a massive computational overhead. For example, finding the most similar pair in a collection of 10k sentences requires about 50 million ( 10k 2 ) inference computations with BERT, which requires about 65 hours on a V100 GPU <ref type="bibr" target="#b37">(Reimers and Gurevych, 2019)</ref>.</p><p>Much previous work attempted to address this problem by learning semantically meaningful representations for each sentence, such that similarity measures like cosine distance can be easily evaluated for sentence-pair regression tasks. The straightforward way to derive a fixed-size sentence embedding from BERT-based models is to average the token representations at the last layer or using the output of the [CLS] token. <ref type="bibr" target="#b37">Reimers and Gurevych (2019)</ref> showed that both approaches yield rather unsatisfactory sentence embeddings. They proposed a model, Sentence-BERT (SBERT), to further fine-tune BERT on natural language inference (NLI) tasks with labeled sentence pairs and achieved state-of-the-art performance on many semantic textual similarity tasks. However, such improvements are induced by high-quality supervision, and we find that their performance is degraded where labeled data of the target task is extremely scarce or the distribution of test set differs significantly from the NLI dataset used for training.</p><p>Learning sentence representations in an unsupervised manner is a critical step to work with unlabeled or partially labeled dataset to address the aforementioned challenge <ref type="bibr" target="#b14">Gan et al., 2017;</ref><ref type="bibr" target="#b16">Hill et al., 2016;</ref><ref type="bibr" target="#b33">Pagliardini et al., 2017;</ref>. A common approach for unsupervised sentence representation learning is to leverage on self-supervision with large unlabeled corpus. For example, early methods explored various auto-encoders for sentence embedding <ref type="bibr" target="#b39">(Socher et al., 2011;</ref><ref type="bibr" target="#b16">Hill et al., 2016)</ref>. Recent work such as skip-thought  and FastSent <ref type="bibr" target="#b16">(Hill et al., 2016)</ref> assumed that a sentence is likely to have similar semantics to its context, and designed self-supervised objectives that encourage models to learn sentence representations by predicting contextual information. However, the performance of these models is far behind that of supervised learning ones on many tasks, which unveils an urgent need of better unsupervised sentence embedding methods.</p><p>In this work, we propose a novel unsupervised sentence embedding model with light-weight feature extractor on top of BERT for sentence encoding, and train it with a novel self-supervised learning objective. Our model is not restricted by the availability of labeled data and can be applied to any domain of interest. Instead of simply averaging BERT token embeddings, we use convolutional neural network (CNN) layers with mean-over-time pooling that transform BERT token embeddings to a global sentence embedding <ref type="bibr" target="#b22">(Kim, 2014)</ref>. Moreover, we propose a novel self-supervised learning objective that maximises the mutual information (MI) between the global sentence embedding and all its local contexts embeddings, inspired by recent advances on unsupervised representation learning for images and graphs <ref type="bibr" target="#b42">Velickovic et al., 2019)</ref>. Our model is named Info-Sentence BERT (IS-BERT). In IS-BERT, the representation of a specific sentence is encouraged to encode all aspects of its local context information, using local contexts derived from other input sentences as negative examples for contrastive learning. This learning procedure encourages the encoder to capture the unique information that is shared across all local segments of the specific input sentence while different from other inputs, leading to more expressive and semantically meaningful sentence embeddings.</p><p>We evaluate our method on two groups of tasks -Semantic Textual Similarity (STS) and SentEval <ref type="bibr" target="#b10">(Conneau and Kiela, 2018)</ref>. Empirical results show that IS-BERT significantly outperforms other unsupervised baselines on STS and SentEval tasks. In addition, we show that IS-BERT substantially outperforms SBERT in a setting where task-specific labeled data is not available. This demonstrates that IS-BERT has the flexibility to be applied to new domains without label restriction. Finally, IS-BERT can achieve performance competitive with or even better than supervised learning methods in certain scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentence Representation Learning</head><p>Prior approaches for sentence embedding include two main categories: (1) unsupervised sentence embedding with unlabeled sentences, and (2) supervised learning with labeled sentences, while a few methods might leverage on both of them.</p><p>Unsupervised Sentence Embedding. There are two main directions to work with unlabeled corpus, according to whether the input sentences are ordered or not. In the scenario with unordered sentences, the input is usually a single sentence and models are designated to learn sentence representations base on the internal structures within each sentence, such as recursive auto-encoders <ref type="bibr" target="#b39">(Socher et al., 2011)</ref>, denoising auto-encoders <ref type="bibr" target="#b16">(Hill et al., 2016)</ref>, and the paragraph vector model <ref type="bibr" target="#b25">(Le and Mikolov, 2014)</ref>. Our model follows this setting as well but benefits from the model capacity of BERT and knowledge in large pretraining corpus.</p><p>Methods working with ordered sentences utilize the distributional hypothesis which assumes that a sentence is likely to have similar semantics to its context. Under this assumption, they formulate generative or discriminative tasks that require the models to correctly predict the contextual information , such as skip-thought  and FastSent <ref type="bibr" target="#b16">(Hill et al., 2016)</ref>, or to distinguish target sentences from contrastive ones <ref type="bibr" target="#b20">(Jernite et al., 2017;</ref><ref type="bibr" target="#b28">Logeswaran and Lee, 2018)</ref> for sentence embedding <ref type="bibr" target="#b20">(Jernite et al., 2017;</ref><ref type="bibr" target="#b28">Logeswaran and Lee, 2018)</ref>. These methods require ordered sentences or corpus with inter-sentential coherence for training, which limits their applications to domains with only short texts.</p><p>Supervised Sentence Embedding. There have also been attempts to use labeled data for sentence embedding. <ref type="bibr" target="#b11">Conneau et al. (2017)</ref> proposed the InferSent model that uses labeled data of the Stanford Natural Language Inference dataset (SNLI) <ref type="bibr" target="#b7">(Bowman et al., 2015)</ref> and the Multi-Genre NLI dataset <ref type="bibr" target="#b44">(Williams et al., 2018)</ref> to train a BiLSTM siamese network for sentence embedding. Universal Sentence Encoder  utilized supervised training with SNLI to augment the unsupervised training of a transformer network. SBERT <ref type="bibr" target="#b37">(Reimers and Gurevych, 2019)</ref> also trained a siamese network on NLI to encode sentences, but it further benefits from the pretraining procedure of BERT. Though effective, those models could be problematic to port to new domains where highquality labeled data is not available, or the text distribution is significantly different from the NLI dataset such that knowledge learned from NLI cannot be successfully transferred. Addressing this limitation requires unsupervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Representation Learning with MI</head><p>Unsupervised representation learning with mutual information has a long history, such as the informax principle and ICA algorithms <ref type="bibr" target="#b6">(Bell and Sejnowski, 1995;</ref><ref type="bibr" target="#b19">Hyvärinen and Oja, 2000)</ref>. Theoretically, many generative models for representation learning based on reconstruction such as auto-encoders or GANs <ref type="bibr" target="#b31">(Nowozin et al., 2016)</ref> are closely related to the idea of maximizing the MI between the model inputs and outputs. Despite the pivotal role in machine learning, MI is historically hard to compute, especially in high-dimensional and continuous settings such as neural networks. Recently, multiple estimators were proposed as lower bounds for mutual information estimation <ref type="bibr" target="#b5">(Belghazi et al., 2018;</ref><ref type="bibr" target="#b32">van den Oord et al., 2018)</ref>, which were demonstrated to be effective for unsupervised representation learning in various scenarios <ref type="bibr" target="#b21">Ji et al., 2019;</ref><ref type="bibr" target="#b41">Sun et al., 2020;</ref><ref type="bibr" target="#b24">Kong et al., 2020)</ref>. Our model is mainly inspired by the DIM model  for vision tasks, associated with a novel self-supervised learning objective to maximize the MI between the global sentence embedding and the representations of all its local contexts. Different from , we mainly work with sequential sentence data with the pretrainted BERT model and further investigate the generalization ability of the learned representation across different domains. <ref type="bibr" target="#b24">Kong et al. (2020)</ref> also used MI with BERT, but their objective is for language modeling while our focus is on sentence representation learning. The corresponding downstream tasks are completely different as well. Pr</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we outline a general model, the Info-Sentence BERT (IS-BERT), for unsupervised sentence representation learning. We first give the problem formulation, then we present the details of our method and the corresponding neural network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Given a set of input sentences X = {x 1 , x 2 , ..., x n }, our goal is to learn a representation y i ∈ R d in Y for each sentence x i in an unsupervised manner. For simplicity, we denote this process with a parameterized function E Θ : X −→ Y, and denote the empirical distribution of the input set X as P.</p><p>We aim to acquire sentence representations by maximizing the mutual information between the sentence-level global representation and the tokenlevel local representations. This idea was inspired by recent advances on unsupervised representation learning for images and graphs <ref type="bibr" target="#b41">Sun et al., 2020)</ref>. The motivation behind such learning strategy is to encourage sentence representations to encode multiple aspects shared by the local information of tokens such as n-gram contextual dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architecture</head><p>Our model architecture is illustrated in <ref type="figure">Figure 1</ref>. We first use BERT to encode an input sentence x to a length-l sequence of token embeddings h 1 , h 2 , ..., h l . Then we apply 1-D convolutional neural network (CNN) layers with different window (kernel) sizes on top of these token embeddings to capture the n-gram local contextual dependencies of the input sentence. Formally, an n-gram embedding c i generated by a CNN with window size k is computed as</p><formula xml:id="formula_0">c i = f (w · h i:i+k−1 + b),<label>(1)</label></formula><p>where h i:i+k−1 is the concatenation of the token embeddings within a window. w and b are learnable parameters of the CNN layer shared across all windows over the sequence, and f is the ReLU activation. We use padding to keep the sequence length of outputs the same as inputs.</p><p>To better capture contextual information with various ranges, we apply several CNNs with different window sizes (e.g. 1, 3, 5) to the input sentences. The final local representation of a token is the concatenation of its representations obtained with different window sizes, as shown in <ref type="figure">Figure 1</ref>. We denote the length-l local token representations sequence for a sentence x as F θ (x) := {F Figure 1: Model Architecture. Two sentences are encoded by BERT and multiple CNNs with different window sizes to get concatenated local n-gram token embeddings. A discriminator T takes all pairs of {sentence representation, token representation} as input and decides whether they are from the same sentence. In this example, we treat sentence "A" as the positive sample and "B" as negative, then n-gram embeddings of "A" will be summarized to a global sentence embedding via pooling. The discriminator produces scores for all token representations from both "A" and "B" to maximize the MI estimator in Eq.2.</p><formula xml:id="formula_1">(i) θ (x) ∈ R d } l i=1 , where F θ is</formula><p>representation of x denoted as E θ (x) ∈ R d is computed by applying a mean-over-time pooling layer on the token representations F θ (x). Both sentence and token representations are parameterized by θ as pooling does not introduce additional parameters. The induction of the these representations is different from the previous sentence-BERT model <ref type="bibr" target="#b37">(Reimers and Gurevych, 2019)</ref>. While Reimers and Gurevych (2019) simply used mean-or maxpooling strategies over the token representations from BERT outputs which can be regard as 1-gram embeddings, we use a set of parallel CNN layers with various window sizes to capture n-gram contextual dependencies. Both the sentence representation and token representations will be fed into a discriminator network to produce scores for MI estimation as presented in 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MI Maximization Learning</head><p>The learning objective is to maximize the mutual information (MI) between the global sentence representation E θ (x) and each of its local token representation F (i) θ (x). As MI estimation is generally intractable for continuous and highdimensional random variables, we usually maximizing over lower bound estimators of MI, such as the Noise-Contrastive estimator (Gutmann and <ref type="bibr" target="#b15">Hyvärinen, 2012)</ref> and Jensen-Shannon estimator <ref type="bibr" target="#b31">(Nowozin et al., 2016;</ref>. In this paper, we use the Jensen-Shannon estimator. Mathematically, the Jensen-Shannon estimator</p><formula xml:id="formula_2">I JSD ω (F (i) θ (x); E θ (x)) is defined as I JSD ω (F (i) θ (x); E θ (x)) := E P [−sp(−T ω (F (i) θ (x), E θ (x)))] − E P×P [sp(T ω (F (i) θ (x ), E θ (x)))],<label>(2)</label></formula><p>where T ω : F × E − → R is a discriminator parameterized by a neural network with learnable parameters ω. It takes all the pairs of a global sentence embedding and local token embeddings as input and generates corresponding scores to estimate I JSD ω , see <ref type="figure">Figure 1.</ref> x is the negative sample drawn from distributionP = P, and sp(z) = log(1 + e z ) is the softplus activation function. The end-goal learning objective over the whole dataset X is defined as:</p><formula xml:id="formula_3">ω * , θ * = argmax ω,θ 1 |X | x∈X lx i=1 I JSD ω (F (i) θ (x); E θ (x)) ,<label>(3)</label></formula><p>where |X | is the size of the dataset, l x is the length of sentence x, and ω * , θ * denote the optimum.</p><p>In Eq. 2, F</p><p>θ (x ) corresponds to a local representation of the negative sample x drawn from P = P. In practice, given a batch of sentences, we can treat each sentence and its local context representations as positive examples, and treat all the local context representations from other sentences in this batch as negative examples. Through maximizing I JSD , E θ (x) is encouraged to have high MI with its local context representations. This will push the encoder to capture the unique information that is shared across all local segments of the input sentence while different from other sentences, which leads to expressive sentence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>Following previous works <ref type="bibr" target="#b37">(Reimers and Gurevych, 2019;</ref><ref type="bibr" target="#b16">Hill et al., 2016)</ref>, we conduct evaluation on two kinds of tasks:</p><p>• Unsupervised Semantic Textual Similarity (STS): These tasks measure a model's performance on sentence similarity prediction.</p><p>The results are good indicators of effectiveness on unsupervised tasks such as clustering and semantic search. • Supervised downstream tasks: These tasks measure the effectiveness of sentence embeddings on downstream supervised tasks.</p><p>We consider two groups of baselines. The first group corresponds to models trained with unlabeled sentences. This includes the unigram-TFIDF mdoel, the Paragraph Vector model <ref type="bibr" target="#b25">(Le and Mikolov, 2014)</ref>, the Sequential Denoising Auto-Encoder (SDAE) <ref type="bibr" target="#b16">(Hill et al., 2016)</ref>, the Skipthought  model and the FastSent <ref type="bibr" target="#b16">(Hill et al., 2016)</ref> model, all trained on the Toronto book corpus  consisted of 70M sentences. We also consider representing sentence with the average of Glove embeddings, the average of the last layer representations of BERT, and the [CLS] embedding of BERT, respectively. The second group consists of models trained on labeled NLI data including InferSent <ref type="bibr" target="#b11">(Conneau et al., 2017)</ref>, Universal Sentence Encoder (USE) , and sentence BERT (SBERT-NLI) <ref type="bibr" target="#b37">(Reimers and Gurevych, 2019)</ref>. uncased-BERT-base is used for all BERT-related models including IS-BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unsupervised Evaluations</head><p>For STS tasks, we conduct evaluations on two types of datasets. 4.1.1 shows the results on seven STS benchmarks, which include texts from various domains and are commonly used for evaluating general-purpose sentence representations. 4.1.2 further shows the results on the challenging Argument Facet Similarity (AFS) dataset <ref type="bibr" target="#b30">(Misra et al., 2016)</ref>, which is more suitable for evaluating model's performance in domain-specific scenarios. For all methods compared in this subsection, cosine-similarity of the obtained sentence embeddings is used to compute their similarity, avoiding the time-consuming regression evaluation as with original BERT-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Unsupervised STS</head><p>Experimental Details: We evaluate our model on the STS tasks 2012-2016 <ref type="bibr" target="#b3">(Agirre et al., 2012</ref><ref type="bibr" target="#b4">(Agirre et al., , 2013</ref><ref type="bibr" target="#b1">(Agirre et al., , 2014</ref><ref type="bibr" target="#b0">(Agirre et al., , 2015</ref><ref type="bibr" target="#b2">(Agirre et al., , 2016</ref>, the STS benchmark (STSb for short) <ref type="bibr" target="#b8">(Cer et al., 2017)</ref>, and the SICK-Relatedness dataset <ref type="bibr" target="#b29">(Marelli et al., 2014)</ref>. The corresponding datasets consist of sentence pairs with labels from 0 to 5 indicating the semantic relatedness. As pointed out in <ref type="bibr" target="#b36">Reimers et al. (2016)</ref> that Pearson correlation is badly suited for STS, Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels is instead used as the evaluation metric.</p><p>Following SBERT which was trained on the combination of the SNLI (Bowman et al., 2015) and the Multi-Genre NLI (MultiNLI) <ref type="bibr" target="#b44">(Williams et al., 2018)</ref> datasets with gold labels, we train IS-BERT on the same collection of sentences, but without using the label information. We denote our model in this setting as IS-BERT-NLI. SNLI contains 570,000 sentence pairs annotated with the labels contradiction, entailment, and neutral. MultiNLI contains 430,000 sentence pairs which are from a wider range of genres of spoken and written texts. Note that IS-BERT-NLI was only trained on the 1 million pairs with the labels excluded.</p><p>We strictly follow the evaluation process of <ref type="bibr" target="#b37">Reimers and Gurevych (2019)</ref> to make our results comparable to theirs. The development set of STSb is used for hyperparameter tuning. On all datasets, we apply three CNNs with window sizes 1, 3, and 5. Each CNN has 256 filters, making the final concatenated token representations of size 256*3. The learning rate is set to 1e-6 and the batch size is 32.</p><p>Results: <ref type="table">Table 1</ref> presents the results. Models are grouped into two sets by the nature of the data on which they were trained. We make the following observations. First, BERT out-of-the-box gives surely poor results on STS tasks. Both the <ref type="bibr">[CLS]</ref> and averaging BERT embeddings perform worse than averaging GloVe embeddings. Second, all supervised methods outperform other unsupervised baselines, which suggests that the knowledge obtained from supervised learning on NLI can be well transfered to these STS tasks. This is also indicated in previous works <ref type="bibr" target="#b16">(Hill et al., 2016;</ref> Model STS12 STS13 STS14 STS15 STS16 STSb SICK-R Avg.  <ref type="table">Table 1</ref>: Spearman rank correlation ρ between the cosine similarity of sentence representations and the gold labels for various Semantic Textual Similarity (STS) tasks. ρ * 100 is reported in this paper. All BERT-based models use uncased-BERT-base as the transformer encoder. Results of baselines marked with † are extracted from <ref type="bibr" target="#b16">(Hill et al., 2016)</ref> (with a different number of decimal places). Results of baselines marked with ‡ are extracted from <ref type="bibr" target="#b37">(Reimers and Gurevych, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using unlabeled data (unsupervised methods)</head><p>that the dataset on which sentence embeddings are trained significantly impacts their performance on STS benchmarks and they found NLI datasets are particularly useful. On average, IS-BERT-NLI significantly outperforms other unsupervised baselines. It even outperforms InferSent trained on labeled SNLI and MultiNLI datasets in 5 out of 7 tasks. This demonstrates the effectiveness of the proposed training strategy. USE and SBERT are the top two performers. As expected, IS-BERT-NLI is in general inferior to these two supervised baselines because they are trained with the particularly useful labeled NLI data as well as large unlabeled data, but IS-BERT-NLI also achieves performance comparable to them in certain scenarios, e.g., STS13 and STS15, even it was only trained on the NLI unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Argument Facet Similarity</head><p>We have shown in Section 4.1.1 that the proposed model substantially outperforms other unsupervised methods. However, the STS benchmarks in Section 4.1.1 are not domain or task specific, and it has been shown that they favor the supervised methods trained on NLI more <ref type="bibr" target="#b16">(Hill et al., 2016;</ref>. In this subsection, we further conduct evaluation on an Argument Facet Similarity (AFS) dataset <ref type="bibr" target="#b30">(Misra et al., 2016)</ref> which is more task-specific. Models are compared in a setting without task or domain-specific labeled data. In this setting, SBERT needs to be trained on NLI and transferred to the AFS dataset for evaluation. Since IS-BERT does not require labeled data, it can be directly trained on the task-specific raw texts. We denote our model in this setting as IS-BERT-AFS.</p><p>Experimental Details: The AFS corpus annotated 6,000 sentential argument pairs from social media dialogs on three controversial topics: gun control, gay marriage, and death penalty. Each argument pair was annotated on a scale from 0 (different) to 5 (equivalent). To be considered similar, argument must not only make similar claims, but also provide a similar reasoning. In addition, the lexical gap between the sentences in AFS is much larger, making it a more challenging task compared to STS tasks. The proposed IS-BERT-AFS is trained on sentences from all three domains. It uses CNNs with window size set to 3, 5, and 7, as the average sentence length is longer in AFS. Other hyperparameters are the same as in Section 4.1.1. <ref type="table" target="#tab_3">Table 2</ref> presents the results. We also provide the Pearson correlation r to make the re-sults comparable to <ref type="bibr" target="#b37">(Reimers and Gurevych, 2019)</ref>. The models in the top group are trained without task-specific labeled data. IS-BERT-AFS clearly outperforms other models in this setting. One major finding is that SBERT-NLI and InferSent, the models trained on the labeled NLI data, perform the worst on this task. We believe this is due to the fact that the NLI corpus significantly differs from the AFS dataset. An improper training set could lead to extremely bad performance in the unsupervised transfer learning setting, which supports our claim that supervised sentence embedding methods are problematic to port to new domains when the distribution of target data (i.e. AFS) differs significantly from the pretraining one (i.e. NLI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>We also show results of BERT and SBERT in another two settings when trained with taskspecific labeled data as in <ref type="bibr" target="#b37">(Reimers and Gurevych, 2019)</ref>. When trained on all topics (10-fold crossvalidation), both BERT and SBERT easily achieve scores above 70, while we observe large performance drop when they are trained in a cross-topic setting (i.e. train on two topics of AFS and evaluate on the third topic). The even larger performance drop of SBERT when trained on NLI (ρ from 74.13 to 15.84) again demonstrates that the domain-relatedness between the training set and the target test set has a huge impact on supervised sentence embedding learning, as a result, the supervised methods are problematic to be applied to downstream tasks of domains without labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Supervised Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">SentEval</head><p>Experimental Details: Here we evaluate the sentence representations in IS-BERT on a set of supervised tasks. Following <ref type="bibr" target="#b37">Reimers and Gurevych (2019)</ref>, we use a set of classification tasks that covers various types of sentence classification, including sentiment analysis (CR <ref type="bibr" target="#b18">(Hu and Liu, 2004)</ref>, MR <ref type="bibr" target="#b35">(Pang and Lee, 2005)</ref> and SST <ref type="bibr" target="#b40">(Socher et al., 2013)</ref>), question-type classification (TREC <ref type="bibr" target="#b26">(Li and Roth, 2002)</ref>), subjectivity classification (SUBJ <ref type="bibr" target="#b34">(Pang and Lee, 2004)</ref>), opinion polarity classification (MPQA <ref type="bibr" target="#b43">(Wiebe et al., 2005)</ref>) and paraphrase identification (MRPC <ref type="bibr" target="#b13">(Dolan et al., 2004)</ref>).</p><p>Since these tasks are more domain-specific, we train IS-BERT on each of the task-specific dataset (without label) to produce sentence embeddings, which are then used for training downstream classifiers. We denote this setting as IS-BERT-task.   <ref type="bibr" target="#b37">(Reimers and Gurevych, 2019;</ref><ref type="bibr" target="#b37">Reimers et al., 2019)</ref> SentEval <ref type="bibr" target="#b10">(Conneau and Kiela, 2018)</ref> toolkit is used to automate the evaluation process. It takes sentence embeddings as fixed input features to a logistic regression classifier, which is trained in a 10-fold cross-validation setup and the prediction accuracy is computed for the test-fold.</p><p>Results: <ref type="table" target="#tab_5">Table 3</ref> presents the results. Overall, supervised methods outperform unsupervised baselines. This indicates that pretraining sentence encoder with high-quality labeled data such as NLI is helpful in a supervised transfer learning setting. Note that in this task, SentEval fits a logistic regression classifier to the sentence embeddings with labels of the downstream tasks. Thus, the models that achieve good results on this task do not necessarily work well on unsupervised tasks such as clustering. As shown in Section 4.1.2, training on NLI could lead to extremely bad performance on downstream unsupervised tasks when the domain data significantly differs from NLI. IS-BERT-task is able to outperform other unsupervised baselines on 6 out of 7 tasks, and it is on par with InferSent and USE which are strong supervised baselines trained on NLI task. This demonstrates the effectiveness of the proposed model in learning domain-specific sentence embeddings.    <ref type="bibr" target="#b37">(Reimers and Gurevych, 2019)</ref>. All systems are trained with 10 random seeds to counter variances <ref type="bibr" target="#b37">(Reimers and Gurevych, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Supervised STS</head><p>Experimental Details: Following Reimers and Gurevych (2019), we use the STSb <ref type="bibr" target="#b8">(Cer et al., 2017)</ref> dataset to evaluate models' performance on the supervised STS task. This dataset includes 8,628 sentence pairs from the three categories captions, news, and forums. It is divided into train (5,749), dev (1,500) and test <ref type="bibr">(1,</ref><ref type="bibr">379)</ref>.</p><p>We compare IS-BERT to the state-of-the-art BERT and SBERT methods on this task. BERT is trained with a regression head on the training set with both sentences passed to the network (BERT-STSb). SBERT is trained on the training set by encoding each sentence separately and using a regression objective function.</p><p>We experiment with two setups: 1) Without selfsupervised learning with the max-MI objective in Eq.3, IS-BERT is directly used for encoding each sentence and fine-tuned on the training set with a regression objective. We denote this setting as IS-BERT-STSb (ft). 2) IS-BERT is first trained on the training set without label using the self-supervised learning objective. Then, it is fine-tuned on the labeled data with a regression objective. We denote this setting as IS-BERT-STSb (ssl+ft). At the prediction time, cosine similarity is computed between each pair of sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>The results are depicted in <ref type="table" target="#tab_6">Table 4</ref>. BERT and SBERT performs similarly on this task. IS-BERT-STSb (ssl+ft) outperforms both baselines. Another interesting finding is that when directly fine-tuning IS-BERT on the labeled data, it performs much worse than SBERT. The only difference between them is that IS-BERT-STSb(ft) uses CNN layers with mean pooling to obtain sentence embeddings while SBERT simply uses a pooling layer to do so. This suggests that a more complex sentence encoder does not automatically lead to better sentence embeddings. However, when comparing IS-BERT-STSb(ft) with IS-BERT-STSb(ssl+ft), we observe that adding self-supervised learning before fine-tuning leads to more than 10% performance improvements. This indicates that the our self-supervise learning method can also be used as an effective domain-adaptation approach before fine-tuning the network.</p><p>In this paper, we proposed the IS-BERT model for unsupervised sentence representation learning with a novel MI maximization objective. IS-BERT outperforms all unsupervised sentence embedding baselines on various tasks and is competitive with supervised sentence embedding methods in certain scenarios. In addition, we show that sentence BERT (SBERT), the state-of-the-art supervised method, is problematic to apply to certain unsupervised tasks when the target domain significantly differs from the dataset it was trained on. IS-BERT achieves substantially better results in this scenario as it has the flexibility to be trained on the taskspecific corpus without label restriction. In the future, we want to explore semi-supervised methods for sentence embedding and its transferability across domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>the encoding function consisting of BERT and CNNs with trainable parameters θ, and i is the token index. The global sentence</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Token embeddings (A)</cell><cell>n-gram embeddings with multiple CNN windows</cell><cell></cell><cell></cell></row><row><cell>Sentence B: [CLS] A woman is cutting an onion [SEP]</cell><cell>Sentence A: [CLS] A girl is playing a guitar [SEP]</cell><cell>BERT</cell><cell>Token embeddings (B)</cell><cell>Local representation (A) Sentence representation (A) Concat Pooling Local representation (B)</cell><cell>Discriminator</cell><cell>Positive scores Negative scores</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Concat</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Average Pearson correlation r and average</cell></row><row><cell>Spearman's rank correlation ρ over three topics on the</cell></row><row><cell>Argument Facet Similarity (AFS) corpus. Results of</cell></row><row><cell>baselines are extracted from</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation accuracy using the SentEval toolkit. Scores are based on a 10-fold cross-validation. Results of baselines marked with † are extracted from<ref type="bibr" target="#b16">(Hill et al., 2016)</ref> (with a different number of decimal places). Results of baselines marked with ‡ are extracted from<ref type="bibr" target="#b37">(Reimers and Gurevych, 2019)</ref>.</figDesc><table><row><cell>Model</cell><cell>ρ</cell></row><row><cell>BERT-STSb</cell><cell>84.30 ± 0.76</cell></row><row><cell>SBERT-STSb</cell><cell>84.67 ± 0.19</cell></row><row><cell>Ours: IS-BERT-STSb (ft)</cell><cell>74.25 ± 0.94</cell></row><row><cell cols="2">Ours: IS-BERT-STSb (ssl + ft) 84.84 ± 0.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Spearman's rank correlation ρ on the STSb test set. Results of baselines are extracted from</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SemEval@ACL</title>
		<meeting>of SemEval@ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Se-mEval@ACL</title>
		<meeting>of Se-mEval@ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Se-mEval@ACL</title>
		<meeting>of Se-mEval@ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semeval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<editor>Se-mEval@ACL</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">*sem 2013 shared task: Semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Second Joint Conference on Lexical and Computational Semantics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<title level="m">Mine: mutual information neural estimation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An information-maximization approach to blind separation and blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1129" to="1159" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semeval-2017 task 1: Semantic textual similarity multilingual and cross-lingual focused evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Se-mEval@ACL</title>
		<meeting>of Se-mEval@ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Chris Tar, Brian Strope, and Ray Kurzweil</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhomni</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Universal sentence encoder for English</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SentEval: An evaluation toolkit for universal sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning generic sentence representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Discourse-based objectives for fast unsupervised sentence representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00557</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Sontag</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">João</forename><surname>Xu Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06653</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A mutual information maximization perspective of language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyprien</forename><surname>De Masson D&amp;apos;autume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Measuring the similarity of sentential arguments in dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amita</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised learning of sentence embeddings using compositional n-gram features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Task-oriented intrinsic evaluation of semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-IJCNLP</title>
		<meeting>of EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Johannes Daxenberger, Christian Stab, and Iryna Gurevych. 2019. Classification and clustering of arguments with contextualized word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilman</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning semantic textual similarity from conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">Matthew</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Yi Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RepL4NLP@ACL</title>
		<meeting>of RepL4NLP@ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
