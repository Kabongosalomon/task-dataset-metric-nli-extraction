<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parsing R-CNN for Instance-Level Human Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
							<email>wangzh@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">WiWide Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Parsing R-CNN for Instance-Level Human Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Instance-level human analysis is common in real-life scenarios and has multiple manifestations, such as human part segmentation, dense pose estimation, human-object interactions, etc. Models need to distinguish different human instances in the image panel and learn rich features to represent the details of each instance. In this paper, we present an end-to-end pipeline for solving the instance-level human analysis, named Parsing R-CNN. It processes a set of human instances simultaneously through comprehensive considering the characteristics of region-based approach and the appearance of a human, thus allowing representing the details of instances.</p><p>Parsing R-CNN is very flexible and efficient, which is applicable to many issues in human instance analysis. Our approach outperforms all state-of-the-art methods on CIHP (Crowd Instance-level Human Parsing), MHP v2.0 (Multi-Human Parsing) and DensePose-COCO datasets. Based on the proposed Parsing R-CNN, we reach the 1st place in the COCO 2018 Challenge DensePose Estimation task. Code and models are public available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human part segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43]</ref>, dense pose estimation <ref type="bibr" target="#b13">[14]</ref> and human-object interactions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref> are the most fundamental and critical tasks in analyzing human in the wild. These tasks require human details at the instance level, which involve several perceptual tasks including detection, segmentation, estimation, i.e. There is a commonality between them, which can be regarded as an instance-level human analysis task.</p><p>Due to the successful development of convolutional neural networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref>, great progress has been made in instance-level human analysis, especially in human part segmentation and dense pose estimation. Several related works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b42">43]</ref> follow the two stages pipeline, Mask R-CNN <ref type="bibr" target="#b14">[15]</ref>, which detects human in the image panel and predicts a class-aware mask in parallel with several convolutional layers. This method has achieved great success and wide application in instance segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41]</ref>. However, there are still several deficiencies in extending to the instance-level human analysis. One of the most important problems is that the design of the mask branch is used to predict a class-agnostic instance mask <ref type="bibr" target="#b14">[15]</ref>, but the instancelevel human analysis requires more detailed features, which can not be well solved by existing methods. Besides, human analysis needs to correlate geometric and semantic relations between human parts / dense points, which is also missing. Therefore, in order to solve these problems, we propose Parsing R-CNN, which provides a concise and effective scheme for the instance-level human analysis tasks. This scheme can be successfully applied to the human part segmentation and dense pose estimation <ref type="figure" target="#fig_0">(Figure 1</ref>).</p><p>Our research explores the problem of instance-level human analysis from four aspects. First, to enhance feature semantic information and maintain feature resolution, proposals separation sampling is adopted. Human instances often occupy a relatively large proportion in images <ref type="bibr" target="#b28">[29]</ref>. Therefore, RoIPool <ref type="bibr" target="#b7">[8]</ref> operations are often performed on the coarser-resolution feature maps <ref type="bibr" target="#b27">[28]</ref>. But this will lose a lot of details of the instance. In this work, we adopt the proposals separation sampling strategy, which using pyramid features at RPN <ref type="bibr" target="#b32">[33]</ref> phase, but the RoIPool only performed on the finest level. Second, to obtain more detailed information to distinguish different human parts or dense points in the instance, we enlarge the RoI resolution of the parsing branch. Human analysis tasks generally distinguish between dozens or even dozens of categories. It is necessary and effective to enlarge the resolution of the feature map.</p><p>Third, we propose a geometric and context encoding module to enlarge receptive field and capture the relationship between different parts of the human body. It is a lightweight component consisting of two parts. The first part is used to obtain multi-level receptive field and context information, and the second part is used to learn geometric correlation. With this module, class-aware masks with better quality are produced.</p><p>Finally, to explore the functions of each group operation in parsing, we decouple the branch into three parts: semantic space transformation, geometric and context encoding, semantic feature representation. Meanwhile, we propose an appropriate branch composition scheme with high accuracy and small computational overhead.</p><p>With the proposed Parsing R-CNN, we achieve state-ofthe-art performance on several datasets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43]</ref>. For human part segmentation, Parsing R-CNN outperforms all known top-down or bottom-up methods both on CIHP <ref type="bibr" target="#b11">[12]</ref> (Crowd Instance-level Human Parsing) and MHP v2.0 <ref type="bibr" target="#b42">[43]</ref> (Multi-Human Parsing) datasets. For dense pose estimation, Parsing R-CNN achieves 64.1% mAP on COCO DensePose <ref type="bibr" target="#b13">[14]</ref> test dataset, winning the 1st place in COCO 2018 Challenge DensePose task by a very large margin.</p><p>Parsing R-CNN is general and not limited to human part segmentation and dense pose estimation. We do not see any reason preventing it from finding broader applications in other human analysis tasks, such as human-object interactions, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Region-based Approach. The region-based approach <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref> is a very important series in object detection, which has high accuracy and good expansibility. Generally speaking, the region-based approach generates a series of candidate object regions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref>, then performs object classification and bounding-box regression in parallel within each candidate region. RoIPool and Region Proposal Network (RPN) are proposed by Fast R-CNN <ref type="bibr" target="#b7">[8]</ref> and Faster R-CNN <ref type="bibr" target="#b32">[33]</ref> respectively, which enable the region-based approach end-to-end learning and greatly improve speed and accuracy. Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> is an important milestone that successfully extending the region-based approach to instance segmentation and pose estimation, which has become an advanced pipeline in visual recognition. Mask R-CNN is flexible and robust to many follow-up improvements, and can be extended to more visual tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Human Part Segmentation. Human part segmentation is a core task of human analysis, which has been extensively studied in recent years. Recently, Zhao et al. <ref type="bibr" target="#b42">[43]</ref> put forward the MHP v2.0 (Multi-Human Parsing) dataset, which contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels. Gong et al. <ref type="bibr" target="#b11">[12]</ref> present another large-scale dataset called Crowd Instance-level Human Parsing (CIHP) dataset, which has 38,280 diverse human images. Each image in CIHP is labeled with pixel-wise annotations on 20 categories and instance-level identification. These datasets have greatly promoted the research of human part segmentation, and considerable progress has been made.</p><p>On the other hand, Zhao et al. <ref type="bibr" target="#b42">[43]</ref> propose the Nested Adversarial Network (NAN) for human part segmentation, which consists of three GAN-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. Gong et al. <ref type="bibr" target="#b11">[12]</ref> design a detection-free Part Grouping Network (PGN) for instancelevel human part segmentation. Although these works have achieved good performance, the segmentation result has great room for improvement and lack of an efficient end-toend pipeline to unify the solution of instance-level human analysis.</p><p>Dense Pose Estimation. Guler et al. <ref type="bibr" target="#b13">[14]</ref> propose an innovative dataset for instance-level human analysis, DensePose-COCO, a large-scale ground-truth dataset with image-tosurface correspondences manually annotated on 50k COCO images. Dense pose estimation can be understood as providing a refined version of human part segmentation and pose estimation, where one predicts continuous part labels of each human body. They also present the DensePose-RCNN, which combines the Dense Regression approach with the Mask-RCNN <ref type="bibr" target="#b14">[15]</ref> architecture. Cross-cascading architecture is applied to the system that further improves accuracy. DensePose-RCNN gives a concise pipeline for dense pose estimation with good accuracy. However, many problems in the task are not discussed, such as the scale of human instance, the feature resolution and so on.</p><p>We consider that we can not treat human part segmentation and dense pose estimation in isolation. They are both specific tasks of instance-level human analysis and have a lot of commonalities. Therefore, based on the successful region-based approach, we propose Parsing R-CNN, a unified solution for instance-level human analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Parsing R-CNN</head><p>Our goal is to leverage a unified pipeline for instancelevel human analysis, which can achieve good performance in both human part segmentation, dense pose estimation and has the high scalability to other similar tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34]</ref>. Like Mask R-CNN, the proposed Parsing R-CNN is conceptually simple, an additional parsing branch is used to generate the output of instance-level human analysis, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In this section, we will introduce the motivation and content of Parsing R-CNN in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proposals Separation Sampling</head><p>In FPN <ref type="bibr" target="#b27">[28]</ref> and Mask R-CNN <ref type="bibr" target="#b14">[15]</ref>, the assignment strategy is adopted to collect the RoIs (Regions of Interest) and assign them to the corresponding feature pyramid according to the scale of RoIs. Formally, large RoIs will be assigned to the coarser-resolution feature maps. This strategy is effective and efficient in object detection and instance segmentation. However, we find that this strategy is not the optimal solution in instance-level human analysis. Due to a small instance cannot be accurately annotated as part segmentation or dense pose, human instances often occupy a larger scale of the image. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, less than 20% of object instances in COCO dataset occupy more than 10% scale of the image, but this ratio is about 74% and 86% in CIHP and MHP v2.0 datasets respectively. According to the assignment strategy proposed by FPN, the most human instances will be assigned to the coarser-resolution feature maps. Instance-level human analysis often requires precise identification of some details of the human body, such as glasses and watches, or pixel areas of the left and right hand. But the coarser-resolution feature maps cannot provide more instance details, which is very harmful to human analysis.</p><p>To address this, we propose the proposals separation sampling (PSS) strategy that extracts features with details while preserves a multi-scale feature representation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. Our proposed change is simple: the bbox branch still adopts the scale assign strategy on the feature pyramid (P2-P5) according to FPN <ref type="bibr" target="#b27">[28]</ref>, but the RoIPool/RoIAlign operation of parsing branch is only performed on the finest scale feature map of P2, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In this way, we argue that object detection benefits from the pyramid representation while preserving human body details by extracting feature from the finest-resolution feature maps at parsing branch. With PSS, we observe that there has been a significant improvement in human part segmentation and dense pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Enlarging RoI Resolution</head><p>In some early region-based approaches, in order to make full use of the pre-train parameters, RoIPool operation converts an RoI into a small feature map with a fixed spatial extent of 7×7 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28]</ref> (or 14×14 followed by a convolutional layer with stride=2). This setup has been inherited in subsequent work and has proved its efficiency. Mask R-CNN uses 14×14 scale RoIs in mask branch to generate segmentation masks, and the DensePose-RCNN <ref type="bibr" target="#b13">[14]</ref> uses the same settings in the uv branch. But the most human instances occupy a large proportion of the feature maps, and too small RoI will lose a lot of detail. For example, a 160×64 size human body whose size on P2 is 40×16, and scaling to 14×14 will undoubtedly reduce the prediction accuracy. In the tasks of object detection and instance segmentation, it is not very necessary to accurately predict the details of the instance. But in the instance-level human analysis, this will cause severe accuracy degradation.</p><p>In this work, we present the most simple and intuitive method: enlarging RoI resolution (ERR). We employ 32×32 RoI in parsing branch, which increases the computational cost of the branch, but improves the accuracy significantly. To address the training time and memory overhead associated with ERR, we decoupled the batch size of instance-level human analysis tasks from the detection task to a fixed value (e.g. 32) and find that this greatly increases the training speed and does not lead to accuracy degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Geometric and Context Encoding</head><p>In previous works, the design of each branch is very succinct. A tiny FCN <ref type="bibr" target="#b31">[32]</ref> is applied on the pooled feature grid for predicting pixel-wise masks of instances. However, using a tiny FCN in the parsing branch of instance-level human analysis will have three obvious drawbacks. First, the scale of different human parts varies greatly, which requires the feature maps capturing multi-scale information. Secondly, each human part is geometrically related, which requires a non-local representation <ref type="bibr" target="#b0">[1]</ref> . Third, 32×32 RoI needs a large receptive field, and stacking four or eight 3×3 convolutional layers are not enough.</p><p>Atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> is an effective module in semantic segmentation, where parallel atrous convolutional layers with different rates capture multiscale information. Recently, Wang et al. presents the nonlocal operation and demonstrates outstanding performance on several benchmarks. Non-local <ref type="bibr" target="#b37">[38]</ref> operation is able to capture long-range dependencies which is of central importance in deep neural networks. For instance-level human analysis, we combine the advantages of ASPP and non-local, propose the Geometric and Context Encoding (GCE) module to replace FCN in parsing branch. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, the proposed GCE module can encode the geometric and context information of each instance, effectively distinguish different parts of the human body. In the GCE module, the ASPP part consists of one 1×1 convolution and three 3×3 convolutions with rates = <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18)</ref>. The image-level features are generated by global average pooling, which is followed by a 1×1 convolution, and then bilinearly upsample the feature to the original 32×32 spatial dimension. The non-local part adopts embedded Gaussian version, and a batch normalization <ref type="bibr" target="#b20">[21]</ref> layer is added to the last convolutional layer. All the convolutional layers in GCE module have 256 channels. See </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Parsing Branch Decoupling</head><p>In the design of neural network for visual task, we often divide the network into several parts according to the characteristics of the features learned by different convolutional layers. For example, high layers strongly respond to entire objects while other neurons are more likely to be activated by local texture and patterns. The region-based approach handles each RoI in parallel, so the branch of each task can be understood as an independent neural network. However, the existing works have not decoupled the branch into different parts and analyzed their roles.</p><p>In this work, we decouple the parsing branch for instancelevel human analysis into three parts. We consider that each part plays a different role for the task. The first part is for semantic space transformation, which is used to transform features into corresponding tasks. The second part is GCE module for geometric and context encoding. The last part converts semantic features to specific tasks, and can also be used to enhance the network capacity. We call them before GCE, GCE module and after GCE respectively. For instance-level human analysis, it is not simple to increase the computational complexity of each module to improve the accuracy. Therefore, it is necessary to decouple parsing branch and analyze the speed / accuracy trade-offs of each part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we compare the performance of Parsing R-CNN on three datasets, two human part segmentation datasets, and one dense pose estimation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We implement the Parsing R-CNN based on Detectron on a server with 8 NVIDIA Titan X GPUs. We adopt FPN and RoIAlign in all architectures, each of which is trained end-to-end. A mini-batch involves 2 images per GPU and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on Human Part Segmentation</head><p>Metrics and Baseline. We evaluate the performance of human part segmentation from two scenarios. For semantic segmentation, we follow <ref type="bibr" target="#b21">[22]</ref> to generate multi-person mask and adopt the standard mean intersection over union (mIoU) <ref type="bibr" target="#b31">[32]</ref> to evaluate the performance. For instance-level performance, we use the Average Precision based on part (AP p ) <ref type="bibr" target="#b42">[43]</ref> for multi-human parsing evaluation, which uses part-level pixel IoU of different semantic part categories within a person instance to determine if one instance is a true positive. We report the AP p 50 and AP p vol . The former has a IoU threshold equal to 0.5, and the latter is the mean of the AP p at IoU thresholds ranging from 0.1 to 0.9, in increments of 0.1. In addition, we also report Percentage of Correctly parsed semantic Parts (PCP) metric <ref type="bibr" target="#b42">[43]</ref>.</p><p>For a fair comparison, our baseline adopts ResNet-50-FPN <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40]</ref> as backbone. The parsing branch consists of a stack of eight 3×3 512-d convolutional layers, followed by a deconvolution <ref type="bibr" target="#b41">[42]</ref> layer and 2× bilinear upscaling. Following <ref type="bibr" target="#b14">[15]</ref>, the feature map resolution after RoIAlign is 14×14, so the output resolution is 56×56. During training, we apply a per-pixel softmax <ref type="bibr" target="#b31">[32]</ref> as the multinomial cross-entropy loss.  Component Ablation Studies on CIHP. We investigate various options of the proposed Parsing R-CNN in Section 3.</p><p>In addition, we also study two other methods to improve performance: increasing the number of iterations and COCO pretraining. Our ablation study on CIHP <ref type="bibr" target="#b11">[12]</ref> val from the baseline gradually to all components incorporated is shown in <ref type="table" target="#tab_2">Table 6</ref>.</p><p>1) Proposals Separation Sampling. Proposals separation sampling (PSS) strategy improves the mIoU about 1.0 than the baseline. We also only adopt the P2 feature map both for bbox branch and parsing branch, the mIoU is reduced by 0.5 and bbox mAP is much worse. As shown in <ref type="table">Table 1</ref>, instance-level metrics are promoted to a certain extent with PSS, which indicates that the proposed strategy is effective.</p><p>2) Enlarging RoI Resolution. In <ref type="table">Table 2</ref>, we employ 32×32 and 64×64 RoI scales respectively, and find that the performance can be significantly improved than the original 14×14 scale. The ERR (32×32) yields 2.8 improvement in terms of mIoU. For instance-level metrics, the improvements are even greater: 5.0, 1.7, 4.4 respectively. Moreover, the RoIs of paring branch is parallel, so the speed is reduced by only 12%. And we can increase the inference speed by reducing the number of RoIs. If we use 100 RoIs at inference phase, the speed can be greatly improved and the performance basically does not drop. Relative to 32×32, the 64×64 RoI scale can continue to improve the performance, but considering speed / accuracy trade-offs we consider that using ERR (32×32) is efficient.</p><p>3) Geometric and Context Encoding. GCE module is the core component of Parsing R-CNN, which can significantly improve the mIoU about 2.0 than stacking of eight 3×3 512-d convolutional layers, and it is even more lightweight. With or without Non-local operation, the ASPP part can still yield 1.2 improvement in terms of mIoU. But without ASPP part, only Non-local operation will cause performance degradation than the baseline. Results are shown In <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Parsing Branch</head><p>Decoupling. We decouple the parsing branch into three parts: before GCE, GCE module and after GCE. As shown in <ref type="table">Table 4</ref>, we find that the before GCE part is not necessary, and we infer that the GCE module is able to perform semantic space transformation. On the other hand, the after GCE part can both significantly improve the semantic segmentation and instance-level metrics (+0. <ref type="bibr" target="#b7">8</ref>  trade-offs, we adopt the GCE followed by four 3×3 512-d convolutional layers (PBD) as parsing branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>Increasing iterations and COCO pretraining. Increasing iterations is a common method for improving performance. As shown in <ref type="table">Table 5</ref>, we investigate the results of twice or three times as long as the standard schedule on CIHP val and find the improvements are obvious. We further pretrain the Parsing R-CNN models on the COCO keypoints annotations 2 , and initialize the parsing branch with the pose estimation weights. This strategy can further improve the performance about 1.1 to 2.4 in terms of mIoU. Combining these two methods, Parsing R-CNN yields 4.0 improvement in terms of mIoU. And for instance-level metrics, the improvements are 6.9, 2.9, 6.1 respectively. As shown in <ref type="table" target="#tab_2">Table 6</ref>, with these proposed components, the metrics of our Parsing R-CNN all exceed the baseline by a big margin. For semantic segmentation, Parsing R-CNN attains 57.6% mIoU which outperforms the baseline by a massive 10.3 points. For instance-level metrics, the improvement of Parsing R-CNN is more significant, which improves AP p 50 by 24.0 points, AP p vol by 9.2 points, and PCP 50 by 18.3 points.</p><p>Component Ablation Studies on MHP v2.0. We also gradually add Proposals separation sampling (PSS), Enlarging RoI Resolution (ERR), Geometric and Context Encoding (GCE) and Parsing Branch Decoupling (PBD) for ablation studies on MHP v2.0 <ref type="bibr" target="#b42">[43]</ref> val, the results are shown in <ref type="table">Table 7</ref>. There are 59 semantic categories in the MHP v2.0 dataset, and some of them are small-scale, so the baseline is worse than CIHP dataset. Parsing R-CNN is also significantly improving for MHP v2.0 dataset, which yields 10.3 improvement in terms of mIoU. For instance-level metrics, Parsing R-CNN improves AP p 50 by 16.5 points, AP p vol by 7.1 points, and PCP 50 by 18.2 points. <ref type="bibr" target="#b1">2</ref> Parsing R-CNN (without ERR) achieves 66.2% AP on COCO val, which yields 0.8 improvement than Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> with s1x LR.</p><p>Comparisons with State-of-the-Art Methods. Parsing R-CNN significantly improve the performance of human part segmentation. In order to further prove its effectiveness, we compare the proposed Parsing R-CNN to the state-of-the-art methods on CIHP and MHP v2.0 datasets, respectively.</p><p>For CIHP dataset, Parsing R-CNN using ResNet-50-FPN outperforms the PGN <ref type="bibr" target="#b11">[12]</ref> which using ResNet-101 by 1.7 points in terms of mIoU <ref type="table">(Table 8</ref>). It is worth noting that PGN adopts multi-scale inputs and left-right flipped images to improve performance, while the result of Parsing R-CNN is without test-time augmentation. We also report the performance of Parsing R-CNN using ResNeXt-101-32x8d-FPN backbone, which attains 59.8% mIoU. Moreover, using ResNeXt-101-32x8d-FPN we report the results with multiscale testing and horizontal flipping. This gives us a single model result of 61.1% mIoU. Because PGN only reports the Average Precision based on region (AP r ), we can not directly compare the instance-level metrics. But by the result of semantic segmentation, we can also infer that Parsing R-CNN is superior to PGN on human parts segmentation task.</p><p>For MHP v2.0 dataset, we also report the results of Parsing R-CNN using ResNet-50-FPN and ResNeXt-101-32x8d-FPN (with or without test-time augmentation) backbones. In <ref type="table">Table 8</ref>, compared with the previous state-of-the-art methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43]</ref>  <ref type="bibr" target="#b2">3</ref> , Parsing R-CNN further improves results, with a margin of 7.4 points AP p 50 , 1.0 points AP p vol and 15.7 points PCP 50 over the best previous entry. Unfortunately, all the methods do not give the metric of semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on Dense Pose Estimation</head><p>Metrics and Baseline. Following <ref type="bibr" target="#b13">[14]</ref>, we adopt the Average Precision (AP) at a number of geodesic point similarity (GPS) thresholds ranging from 0.5 to 0.95 as the evaluation metric. The structure of baseline model is exactly the same as the one of human part segmentation. We only replace the  per-pixel softmax loss with the dense pose estimation losses.</p><p>Component Ablation Studies on DensePose-COCO. Like human part segmentation, we adopt the proposed Parsing R-CNN for dense pose estimation. Corresponding results are shown in <ref type="table" target="#tab_5">Table 9</ref>. We adopt ResNet50-FPN and ResNeXt101-32x8d-FPN as backbone respectively. With ResNet50-FPN, Parsing R-CNN outperforms the baseline (DensePose-RCNN) by a good margin. Combining all the proposed components, our method achieves 55.0% AP, which yields 6.1 improvement than DensePose-RCNN. With COCO pretraining, Parsing R-CNN further improves 3.3 points AP. Parsing R-CNN also shows significant improvement of AP 75 (50.8% vs 66.9%), which indicates that our method is more accurate in points localization on the surface. As shown in <ref type="table" target="#tab_5">Table 9</ref>, our Parsing R-CNN still increase the performance of dense pose estimation, when the model is upgrade from ResNet50 to ResNeXt101-32x8d, showing good generalization of the Parsing R-CNN framework.</p><p>COCO 2018 Challenge. With Parsing R-CNN, we participated in the COCO 2018 DensePose Estimation Challenge, and reach the 1st place over all competitors. <ref type="table">Table 10</ref> summarizes the entries from the leaderboard of COCO 2018 Challenge. Our entry only utilizes a single model (ResNeXt101-32x8d), and attains 64.1% AP on DensePose-COCO test which surpasses the 2nd place by 6 points.</p><p>Qualitative results are illustrated in <ref type="figure" target="#fig_6">Figure 6</ref>. Images in each row are visual results of Parsing R-CNN using ResNet50-FPN on CIHP val, MHP v2.0 val and DensePose-COCO val, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present a novel region-based approach Parsing R-CNN for instance-level human analysis, which achieves state-of-the-art results on several challenging benchmarks. Our approach explores the problem of instance-level human analysis from four aspects, and verified the effectiveness on human part segmentation and dense pose estimation tasks. Based on the proposed Parsing R-CNN, we reach the 1st place in the COCO 2018 Challenge DensePose Estimation task. In the future, we will extend Parsing R-CNN to more applications of instance-level human analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1Figure 1 .</head><label>1</label><figDesc>https://github.com/soeaver/Parsing-R-CNN Example tasks of instance-level human analysis. (a) and (b) are samples for dense pose estimation. (c) and (d) are samples for human part segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Parsing R-CNN pipeline. We adopt FPN backbone and RoIAlign operation, parsing branch is used for instance-level human analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Scale of instances relative to the image (Relative Scale) vs fraction of instances in the dataset (CDF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The proposed Geometric and Context Encoding (GCE) module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visualization results with / without GCE module. The 1st row shows visualization results without GCE, and the 2nd shows ones with GCE. The GCE module can refine segmentation results of human instances (red circles).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Images in each row are visual results of Parsing R-CNN using ResNet50-FPN on CIHP val, MHP v2.0 val and DensePose-COCO val, respectively. AP AP50 AP75 APM APL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .Table 3</head><label>123</label><figDesc>Ablation study on proposals separation sampling (PSS) strategy. Ablation study on enlarging RoI resolution (ERR) operation, the numbers in brackets are the RoI scales.</figDesc><table><row><cell cols="4">mAP bbox mIoU AP p 50</cell><cell cols="2">AP p vol</cell><cell>PCP50</cell></row><row><cell>baseline</cell><cell>67.7</cell><cell>47.2</cell><cell>41.4</cell><cell cols="2">45.4</cell><cell>44.3</cell></row><row><cell>P2 only</cell><cell>66.4</cell><cell>47.7</cell><cell>42.6</cell><cell cols="2">45.8</cell><cell>45.1</cell></row><row><cell>PSS</cell><cell>67.5</cell><cell>48.2</cell><cell>42.9</cell><cell cols="2">46.0</cell><cell>45.5</cell></row><row><cell></cell><cell></cell><cell>fps</cell><cell cols="2">mIoU AP p 50</cell><cell cols="2">AP p vol</cell><cell>PCP50</cell></row><row><cell cols="2">baseline (14×14)</cell><cell>10.4</cell><cell>48.2</cell><cell>42.9</cell><cell cols="2">46.0</cell><cell>45.5</cell></row><row><cell cols="2">ERR (32×32)</cell><cell>9.1</cell><cell>50.7</cell><cell>47.9</cell><cell cols="2">47.6</cell><cell>49.7</cell></row><row><cell cols="3">ERR (32×32), 100 RoIs 11.5</cell><cell>50.5</cell><cell>47.5</cell><cell cols="2">47.3</cell><cell>49.0</cell></row><row><cell cols="2">ERR (64×64)</cell><cell>5.6</cell><cell>51.5</cell><cell>49.0</cell><cell cols="2">47.9</cell><cell>50.8</cell></row><row><cell></cell><cell></cell><cell cols="2">mIoU AP p 50</cell><cell>AP p vol</cell><cell></cell><cell>PCP50</cell></row><row><cell cols="2">baseline</cell><cell>50.7</cell><cell>47.9</cell><cell>47.6</cell><cell></cell><cell>49.7</cell></row><row><cell cols="2">ASPP only</cell><cell>51.9</cell><cell>51.1</cell><cell>48.3</cell><cell></cell><cell>51.4</cell></row><row><cell cols="2">Non-local only</cell><cell>50.5</cell><cell>47.0</cell><cell>47.6</cell><cell></cell><cell>48.9</cell></row><row><cell>GCE</cell><cell></cell><cell>52.7</cell><cell>53.2</cell><cell>49.7</cell><cell></cell><cell>52.6</cell></row></table><note>. Ablation study on Geometric and Context Encoding (GCE) module.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 6 .</head><label>6</label><figDesc>Human part segmentation results on CIHP val. We adopt ResNet50-FPN as backbone, and gradually add Proposals separation sampling (PSS), Enlarging RoI Resolution (ERR), Geometric and Context Encoding (GCE) and Parsing Branch Decoupling (PBD). 3x LR denotes that we increase the number of iterations to three times of standard. We also report the performance of pretraining the whole model on COCO keypoint annotations (COCO).</figDesc><table><row><cell>Baseline</cell><cell cols="2">PSS ERR GCE PBD 3x LR COCO mIoU AP p 50</cell><cell>AP p vol</cell><cell>PCP50</cell></row><row><cell>ResNet50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>47.2</cell><cell>41.4</cell><cell>45.4</cell><cell>44.3</cell></row><row><cell></cell><cell>48.2</cell><cell>42.9</cell><cell>46.0</cell><cell>45.5</cell></row><row><cell></cell><cell>50.7</cell><cell>47.9</cell><cell>47.6</cell><cell>49.7</cell></row><row><cell></cell><cell>52.7</cell><cell>53.2</cell><cell>49.7</cell><cell>52.6</cell></row><row><cell></cell><cell>53.5</cell><cell>58.5</cell><cell>51.7</cell><cell>56.5</cell></row><row><cell></cell><cell>56.3</cell><cell>63.7</cell><cell>53.9</cell><cell>60.1</cell></row><row><cell></cell><cell>57.5</cell><cell>65.4</cell><cell>54.6</cell><cell>62.6</cell></row><row><cell>∆</cell><cell cols="2">+10.3 +24.0</cell><cell>+9.2</cell><cell>+18.3</cell></row><row><cell>Baseline</cell><cell cols="2">PSS ERR GCE PBD 3x LR COCO mIoU AP p 50</cell><cell>AP p vol</cell><cell>PCP50</cell></row><row><cell>ResNet50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>28.7</cell><cell>10.1</cell><cell>33.4</cell><cell>21.8</cell></row><row><cell></cell><cell>29.8</cell><cell>10.6</cell><cell>33.8</cell><cell>22.2</cell></row><row><cell></cell><cell>32.3</cell><cell>14.0</cell><cell>34.1</cell><cell>27.4</cell></row><row><cell></cell><cell>33.7</cell><cell>17.4</cell><cell>36.3</cell><cell>30.5</cell></row><row><cell></cell><cell>34.3</cell><cell>20.0</cell><cell>37.6</cell><cell>32.7</cell></row><row><cell></cell><cell>36.2</cell><cell>24.5</cell><cell>39.5</cell><cell>37.2</cell></row><row><cell></cell><cell>37.0</cell><cell>26.6</cell><cell>40.3</cell><cell>40.0</cell></row><row><cell>∆</cell><cell>+8.3</cell><cell>+16.5</cell><cell>+7.1</cell><cell>+18.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Human part segmentation results on MHP v2.0 val, we adopt ResNet50-FPN as backbone. Results of state-of-the-art methods on CIHP and MHP v2.0 val.</figDesc><table><row><cell></cell><cell>method</cell><cell cols="2">mIoU AP p 50</cell><cell>AP p vol</cell><cell>PCP50</cell></row><row><cell></cell><cell>PGN (R101)  † [12]</cell><cell>55.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CIHP</cell><cell>Parsing R-CNN (R50) Parsing R-CNN (X101)</cell><cell>57.5 59.8</cell><cell>65.4 69.1</cell><cell>54.6 55.9</cell><cell>62.6 66.2</cell></row><row><cell></cell><cell>Parsing R-CNN (X101)  †</cell><cell>61.1</cell><cell>71.2</cell><cell>56.5</cell><cell>67.7</cell></row><row><cell></cell><cell>Mask R-CNN [15]</cell><cell>-</cell><cell>14.9</cell><cell>33.8</cell><cell>25.1</cell></row><row><cell></cell><cell>MH-Parser [25]</cell><cell>-</cell><cell>17.9</cell><cell>36.0</cell><cell>26.9</cell></row><row><cell>MHP</cell><cell>NAN [43]</cell><cell>-</cell><cell>25.1</cell><cell>41.7</cell><cell>32.2</cell></row><row><cell>v2.0</cell><cell>Parsing R-CNN (R50)</cell><cell>37.0</cell><cell>26.6</cell><cell>40.3</cell><cell>40.0</cell></row><row><cell></cell><cell>Parsing R-CNN (X101)</cell><cell>40.3</cell><cell>30.2</cell><cell>41.8</cell><cell>44.2</cell></row><row><cell></cell><cell>Parsing R-CNN (X101)  †</cell><cell>41.8</cell><cell>32.5</cell><cell>42.7</cell><cell>47.9</cell></row></table><note>† denotes using test-time augmentation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 .</head><label>9</label><figDesc>Dense pose estimation results on DensePose-COCO val. We adopt ResNet50-FPN and ResNeXt101-32x8d-FPN as backbone respectively. The baseline is DensePose-RCNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table 10. 2018 COCO Challenge results of Dense Pose Estimation task on test.</figDesc><table><row><cell>DensePose-RCNN</cell><cell>56</cell><cell>89</cell><cell>64</cell><cell>51</cell><cell>59</cell></row><row><cell>yuchen.ma</cell><cell>57</cell><cell>87</cell><cell>66</cell><cell>48</cell><cell>61</cell></row><row><cell>ML-LAB</cell><cell>57</cell><cell>89</cell><cell>64</cell><cell>51</cell><cell>59</cell></row><row><cell>Min-Byeonguk</cell><cell>58</cell><cell>89</cell><cell>66</cell><cell>50</cell><cell>61</cell></row><row><cell>Parsing R-CNN (ours)</cell><cell>64</cell><cell>92</cell><cell>75</cell><cell>57</cell><cell>67</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">All the previous state-of-the-art methods only report the results evaluated on MHP v2.0 test.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ican: Instance-centric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Instance-level human parsing via part grouping network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to segment every thing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-human parsing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07206</idno>
		<title level="m">Multi-human parsing in the wild</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fssd: Feature fusion single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00960</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Look into person: Joint human parsing and pose estimation network and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detect-and-track: Efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Georgia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lorenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Understanding humans in crowded scenes: Deep nested adversarial learning and a new benchmark for multi-human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
