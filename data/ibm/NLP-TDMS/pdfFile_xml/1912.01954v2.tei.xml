<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EmbedMask: Embedding Coupling for One-stage Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ying</surname></persName>
							<email>huiying@zju.edu.cnzhaojinhuang@hust.edu.cnliushuhust</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution" key="instit1">Zhejiang University ‡ Institute of AI</orgName>
								<orgName type="institution" key="instit2">Huazhong University of Science and Technology § Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution" key="instit1">Zhejiang University ‡ Institute of AI</orgName>
								<orgName type="institution" key="instit2">Huazhong University of Science and Technology § Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution" key="instit1">Zhejiang University ‡ Institute of AI</orgName>
								<orgName type="institution" key="instit2">Huazhong University of Science and Technology § Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjia</forename><surname>Shao</surname></persName>
							<email>tianjiashao@gmail.comkunzhou@acm.org</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution" key="instit1">Zhejiang University ‡ Institute of AI</orgName>
								<orgName type="institution" key="instit2">Huazhong University of Science and Technology § Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution" key="instit1">Zhejiang University ‡ Institute of AI</orgName>
								<orgName type="institution" key="instit2">Huazhong University of Science and Technology § Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EmbedMask: Embedding Coupling for One-stage Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current instance segmentation methods can be categorized into segmentation-based methods that segment first then do clustering, and proposal-based methods that detect first then predict masks for each instance proposal using repooling. In this work, we propose a one-stage method, named EmbedMask, that unifies both methods by taking advantages of them. Like proposal-based methods, Embed-Mask builds on top of detection models making it strong in detection capability. Meanwhile, EmbedMask applies extra embedding modules to generate embeddings for pixels and proposals, where pixel embeddings are guided by proposal embeddings if they belong to the same instance. Through this embedding coupling process, pixels are assigned to the mask of the proposal if their embeddings are similar. The pixel-level clustering enables EmbedMask to generate high-resolution masks without missing details from repooling, and the existence of proposal embedding simplifies and strengthens the clustering procedure to achieve high speed with higher performance than segmentation-based methods. Without any bells and whistles, EmbedMask achieves comparable performance as Mask R-CNN, which is the representative two-stage method, and can produce more detailed masks at a higher speed. Code is available at github.com/yinghdb/EmbedMask</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In light of the rapid development of deep learning and machine industry, a lot of tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref> in the field of computer vision have made tremendous progress. We can also observe that the application of deep networks in computer vision has extended from image-level to pixel-level. Specifically, instance segmentation can be viewed as an extension of object detection, which extends * The work was done when Hui Ying and Zhaojin Huang were interns in Tencent Youtu.  the detected objects from instance-level to pixel-level. There have been a variety of methods trying to solve this problem. Proposal-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref> treat instance segmentation as an extension of object detection. When detected instances are determined with their bounding boxes, the segmentation task can be processed inside the box of each instance. As a representative, Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> achieves outstanding results on many benchmarks to be the most popular method for instance segmentation. However, as a two-stage method, the "RoIPooling/RoIAlign" step results in the loss of features and the distortion to the aspect ratios, so that the masks it pro-duces may not preserve fine details. Besides, it still sustains weakness in being complex to adjust too many parameters. Recently, one-stage instance segmentation methods have become a popular topic, but those newly proposed methods cannot yet perform comparably to the two-stage ones. As one type of the one-stage method, segmentationbased methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref> prefer to process the image in pixel-level directly so that they do not suffer from repooling operations. They predict features for each pixel and then the clustering process is applied to group them up for each object instance. However, the bottlenecks of such methods are their clustering procedures, such as the difficulties in determining the number of clusters or the positions of the cluster centers, resulting in the incomparable performance with the proposal-based methods.</p><p>Our Contribution In this work, we propose a new instance segmentation method which aims at exploiting the advantage of both proposal-based and segmentation-based methods. It preserves strong detection capabilities as the proposal-based methods, and meanwhile keeps the details of images as the segmentation-based methods. In this way it is able to not only reach top scores in benchmark but also produce high-resolution masks and run at a high speed.</p><p>Our method, named EmbedMask, is a one-stage method that for the first time achieves comparable results as Mask R-CNN in the challenging dataset COCO with the same training settings. Fundamentally, EmbedMask follows the framework of one-stage detection methods that it predicts instance proposals, which are defined by their bounding boxes, categories, and scores. As the key of segmentationbased methods, embedding is also used in our method for clustering, which we separate the embedding into a couple definitions: (1) embedding for pixels, referred as pixel embedding, which is a representation for every pixel in the image, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>(c), and (2) embedding for instance proposals, referred as proposal embedding, which is a representation for the instance proposals besides the bounding box and classification, as shown in <ref type="figure" target="#fig_1">Figure 1</ref></p><formula xml:id="formula_0">(b).</formula><p>Embedding coupling is applied to the above embeddings that pixel embedding is supervised to couple with the proposal embedding if they correspond to the same instance.</p><p>In the inference procedure, each instance proposal surviving from the non-maximum suppression (NMS) is attached with a proposal embedding which is regarded as the cluster center that guides the clustering among pixel embeddings to generate the mask for the instance. With this process, we not only avoid determining the cluster centers as well as their number but remove the need for the computation of "RoIPooling/RoIAlign". In this way, we can keep the essential details while omitting the complex operations. Furthermore, we predict another parameter for the instance proposal to produce certain margin for the clustering pro-cedure, which is proposal-sensitive. The flexible margins make it more suitable to conduct instance segmentation for multi-scale objects, which is a mechanism that most of the one-stage methods do not possess.</p><p>EmbedMask simplifies the clustering procedure in the segmentation-based methods and avoid the repooling procedure in Mask R-CNN. While being simple but effective, our method produces a significant boost, compared to other contemporary work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>. Notably, EmbedMask achieves comparable results to Mask R-CNN, with the mask mAP of 37.7 vs. 38.1 in the challenging COCO dataset <ref type="bibr" target="#b21">[22]</ref> and speed of 13.7 fps vs. 8.7 fps (V100 GPU), both using the ResNet-101 <ref type="bibr" target="#b12">[13]</ref> as backbone network and under the same training settings. In summary, the main contributions of our work are mainly twofolds:</p><p>• We propose a framework that unites the proposalbased and segmentation-based methods, by introducing the concepts of proposal embedding and pixel embedding so that pixels are assigned to instance proposals according to their embedding similarity.</p><p>• As a one-stage instance segmentation method, our method can achieve comparable scores as Mask R-CNN in the COCO benchmark, and meanwhile it provides masks with a higher quality than Mask R-CNN, running at a higher speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Instance segmentation is a fundamental yet challenging task, which requires to predict a pixel-level mask with a category label for each instance of interest in an image. Various methods with different ideas have been proposed to solve this problem.</p><p>Two-stage Methods Two-stage methods can be thought of consisting of two consecutive stages: detection and segmentation, where the segmentation results depend on the detection results, as the segmentation is processed on each detected bounding box. Before the rise of the unified framework, Pinheiro et al. <ref type="bibr" target="#b26">[27]</ref> proposed DeepMask, which utilizes sliding windows to generate proposal regions, and then learns to classify and segment them. Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> unites the tasks of region proposing and segmentation built on top of Faster R-CNN <ref type="bibr" target="#b29">[30]</ref>, making it the representative of two-stage instance segmentation methods. Based on Mask R-CNN, PANet <ref type="bibr" target="#b22">[23]</ref> enhances the performance by merging multi-level information. MS R-CNN <ref type="bibr" target="#b13">[14]</ref> simply redefines the grading standard of instance mask. With the detection models built on top of FPN <ref type="bibr" target="#b19">[20]</ref> as the baseline, recent twostage instance segmentation methods achieve state-of-theart performance. However, there still some remain problems, such as the low speed and detail-missing masks of large objects due to the complicate network architectures and the repooling step.</p><p>One-stage Methods Compared to the two-stage methods, the one-stage instance segmentation methods remove the repooling step. To avoid re-extracting features for instance proposals, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref> generate position-sensitive mask maps that can be simply assembled to get the final masks. Ten-sorMask <ref type="bibr" target="#b3">[4]</ref> regard the instance segmentation task as an object detection task, and it replaces the 3D tensors for representing the bounding boxes to the 4D tensors to represent the masks over 2D spatial domain. YOLACT <ref type="bibr" target="#b1">[2]</ref> proposed the concept of prototype masks that can be linearly combined to generate instance masks. Though these methods have simpler procedures than the two-stage ones, they cannot produce masks that are accurate enough as the two-stage methods.</p><p>Segmentation-based Methods The segmentation-based methods are another kind of one-stage methods which divides the instance segmentation task to first segmenting and then clustering. Pixel-level predictions are obtained by the segmentation module and the clustering is applied to group them for each object. For the separation of pixels on different objects and clustering of pixels on the same objects, <ref type="bibr" target="#b6">[7]</ref> utilizes the discriminative loss while <ref type="bibr" target="#b25">[26]</ref> introduces a new loss function that learns margins extra for different objects. Such bottom-up methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref> can naturally fetch high-resolution masks, but their performance in perceiving instances is not high enough.</p><p>Our proposed EmbedMask, as a one-stage method, can achieve comparable results with Mask R-CNN, and outperforms the state-of-the-art one-stage methods. Specifically, EmbedMask is built on top of the one-stage object detection method like other proposal-based instance segmentation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>. As a key modification, we design a mask prediction module based on the proposal embedding and pixel embedding to perform efficient pixel clustering. Similar to the segmentation-based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref>, pixel clustering is performed on a predicted embedding map for the whole image so that EmbedMask can fetch highresolution masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EmbedMask</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Our instance segmentation framework is composed of two parallel modules, one for finding the positions of instance proposals, and the other for predicting the masks of instance proposals. In practice, we use the state-of-the-art object detection method FCOS <ref type="bibr" target="#b30">[31]</ref> as our baseline, which is the most recent one-stage object detection method. We note that our method can also be applied on other detection frameworks as <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>As the key of our method, we specially design extra modules to learn the pixel embeddings, proposal embeddings, and proposal margins to extract the instance masks. Specifically, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>, the three parameters are predicted from networks. First, the pixel embedding variants, referred as p, are computed in an additional single branch "Pixel Head" originating from the largest feature map of FPN, i.e., P3, with five 3 × 3 conv layers. Second, the proposal embedding variants, referred as q, are computed by a 3 × 3 conv layer added after the feature map from FPN with another 4 3 × 3 conv layers, which is shared with the prediction of center-ness and box regression. Third, the proposal margin variants, referred as σ, are computed by a 1 × 1 conv layer added after the box regression outputs. All the predicted feature maps in the "Proposal Head" are united to produce the proposal features. That is, the values at the same location x j of these feature maps are grouped as a tuple {class j , box j , center j , q j , σ j } that represents the parameters of the proposal j .</p><p>For each instance proposal and each pixel in the image, the distance between the couple of proposal embedding and the pixel embedding decides how likely this pixel belongs to the mask of the instance proposal, and the proposal margin gives a clear boundary for this likelihood to decide the final mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Embedding Definition</head><p>As we know, the main task in instance segmentation is to assign the pixels x i in the image to a set of instances S k . The previous segmentation-based methods directly do this assignment by clustering pixels with similar embeddings, while we propose two new definitions to the embeddings, which are pixel embedding and proposal embedding. The proposal embedding represents the object-level context features for the object instance, which is a good representation of entire instance, while the pixel embedding represents the pixel-level context features for each location on the image, which learns the relation between each pixel with corresponding instance. Proposal embeddings are used as cluster centers of instances to do pixel clustering among the pixel embeddings, so that the difficulties appeared in segmentation-based methods, such as finding the locations and counts of cluster centers, are avoided.</p><p>Specifically, during inference, proposal embeddings and pixel embeddings are utilized for mask generation. In detail, after the NMS applied to the tuples of {box j , class j , center j , q j , σ j }, a group of surviving instance proposals S k are fetched with these tuples as parameters. Here we name the corresponding q j for the surviving proposal S k as Q k . With the pixel embedding p i for each pixel x i in the image, a pixel x i is assigned to the instance proposal S k if the distance between the pixel embedding p i and the proposal embedding Q k are close enough. If we fix a margin δ to the distance, at inference time, the binary mask of S k can be computed by the pixel assignment</p><formula xml:id="formula_1">M ask k (x i ) = 1, p i − Q k ≤ δ 0, p i − Q k &gt; δ.<label>(1)</label></formula><p>During training, different from the inference time, S k is used to refer to each ground-truth instance, and the Q k , being the proposal embeddings for the ground-truth instance S k , now is the average of positive proposal embeddings. The positive proposal embeddings sampling strategy is described in section 3.5. Therefore, our objective is to bring the pixel embedding p i and proposal embedding Q k closer if the pixels x i belongs to the ground-truth mask of the instance S k , otherwise keep them away.</p><p>To perform such push and pull strategy for the foreground and background pixel embeddings, an intuitive method is to apply two fixed margins to two hinge losses, as</p><formula xml:id="formula_2">L hinge = 1 K K k=1 1 N k i∈B k 1 {i∈S k } [ p i − Q k − δ a ] 2 + + 1 K K k=1 1 N k i∈B k 1 {i / ∈S k } [δ b − p i − Q k ] 2 + .</formula><p>(2) In this function, K is the number of ground-truth instances. B k represents the set of pixel embeddings that need to be supervised for the instance S k , which is just the pixel embeddings locating inside the bounding box of S k , and N k is the number of pixel embeddings in B k . 1 {i∈S k } is an indicator function, being 1 if pixel x i is in the ground-truth mask of S k and 0 otherwise. [x] + = max(0, x), and the δ a and δ b are two margins designed for push and pull strategy. Specifically, the first term of the loss means to pull the distance between pixel embedding p i and proposal embedding Q k inside the margin δ a , and the second term means to push the distance outside the margin δ b .</p><p>However, we observe that such fixed margins may cause certain problems (see section 3.3), therefore we propose learnable margins to replace the fixed margins, which is more advantageous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learnable Margin</head><p>The loss function introduced above gives a solution to optimize the distance between pixel embeddings and proposal embeddings during training. However it uses the fixed margins for all instances, which may lead to some problems in training. First, the margins δ b and δ a , as well as δ for inference, all need to be set manually, so it is difficult to find the optimal values for the best performance. Second, fixed margins for all instances are not friendly to the training of multi-scale objects, as the pixel embeddings in large objects are always more scattered while those in small objects are always concentrated. In order to get rid of the problem, we propose the margins σ j for all instance proposals, which are flexible with multi-scale objects. Moreover, the flexible margins σ j can be learned directly from the training without the manual setting.</p><p>To reach this point, inspired by <ref type="bibr" target="#b25">[26]</ref>, we use a Gaussian function, as</p><formula xml:id="formula_3">φ(x i , S k ) = φ(p i , Q k , Σ k ) = exp − p i − Q k 2 2Σ 2 k ,<label>(3)</label></formula><p>to map the distance between the pixel embedding p i of the pixel x i and the proposal embedding Q k of the instance S k into a value ranged in [0, 1). The additional introduced variant Σ k comes from σ j just like how Q k comes from q j . The φ(x i , S k ) is the probability for the pixel x i belonging to the mask of the instance S k . When the pixel embed-ding p i is close to the proposal embedding Q k , φ(x i , S k ) is going to be 1, otherwise 0. As what is introduced in <ref type="bibr" target="#b25">[26]</ref>, the Σ k plays a role of margin for instance S k . So that in our method, the predicted σ j gives the learnable margin for each instance proposal.</p><p>For the instance S k , when the φ(x i , S k ) is applied to each pixel x i in the image, a foreground/background probability map for the instance is produced. Therefore it can be optimized by a binary classification loss, which is</p><formula xml:id="formula_4">L mask = 1 K K k=1 1 N k pi∈B k L (φ(x i , S k ), G(x i , S k )) ,<label>(4)</label></formula><p>where L(·) is a binary classification loss function, and in practice we use lovasz-hinge loss <ref type="bibr" target="#b32">[33]</ref> for better performance. G(x i , S k ) represents the ground truth label for pixel x i to judge whether it is in the mask of the proposal S k , which is a binary value. This loss function supervises the computed mask probability maps, which contains the parameters of pixel embedding p, proposal embedding q and proposal margin σ. So that the proposal margins can be learned automatically without manual settings. And the flexible margin for each instance makes it more advantageous than the hinge loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Smooth Loss</head><p>As mentioned above, the meanings of Q k and Σ k are with subtle difference in training and inference.</p><p>During training, S k represents the ground-truth object instance. For each instance S k , the computation of Q k and Σ k is by averaging a set of positive samples q j and σ j , and we name this set as M k (described in section 3.5). Specifically, the Q k and Σ k are computed as</p><formula xml:id="formula_5">Q k = 1 N k j∈M k q j ,<label>(5)</label></formula><formula xml:id="formula_6">Σ k = 1 N k j∈M k σ j ,<label>(6)</label></formula><p>where N k is the number of positive samples of S k for proposal embedding and margin. But in the inference procedure, S k represents each instance proposal surviving from NMS. And the corresponding q j and σ j for surviving S k are used as Q k and Σ k .</p><p>Because the Q k and Σ k are different when training and inference, we need to add a smooth loss for training to force them keeping close as</p><formula xml:id="formula_7">L smooth = 1 K K k=1 1 N k j∈M k q j − Q k 2 + 1 K K k=1 1 N k j∈M k σ j − Σ k 2 .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training</head><p>Objective EmbedMask is optimized end-to-end using a multi-task loss. Apart from the original classification loss L cls , center-ness loss L center and box regression loss L box in FCOS, we introduce additional losses L mask and L smooth for mask prediction. They are jointly optimized by</p><formula xml:id="formula_8">L = L cls + L center + L box + λ 1 L mask + λ 2 L smooth . (8)</formula><p>Training Samples for Box and Classification When computing the losses for box regression and classification, as well as center-ness, we define the positive samples as the parameters {box j , class j , center j } whose real locations mapped back to the original image locate on the center region of the ground-truth bounding box, and at the meantime the locations are in the mask of the ground-truth instances. The sampling strategy is a little more strict than the original one in FCOS, that we enforce the sample to be more accurate to the mask-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Samples for Proposal Embedding and Margin</head><p>Compared to the sampling for box and classification, the sampling of proposal embeddings q j and margins σ j , which are used to compute Q k and Σ k for training, need another condition, that the Intersection over Union (IoU) between the corresponding predicted box j in the sampled location and the ground-truth box for instance S k should be more than 0.5. This more strict selection strategy reduces positive samples, so that also reduces the training difficulty.</p><p>Training Samples for Pixel Embedding In the definition of mask loss, as shown in Equation 3, only the pixels belong to B k are supervised for the instance S k . In our experiment, the B k is the set of samples that lay inside the ground-truth bounding box of S k when training. But in practice, we find that if we slightly expand the box to increase the number of training samples, the results will be better. Thus, the manual expand to the box is used in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Inference</head><p>The inference procedure of EmbedMask is very clear. Given an input image, it will go through the object detection procedure as FCOS, and the instances that survive NMS are treated as instance proposals S k . Each surviving proposal S k is attached with its bounding box, the category with a related score, the proposal embedding q j , and the proposal margin σ j . The q j and σ j are just viewed as Q k and Σ k in inference. In the meantime, we can also obtain the pixel embedding p i for each pixel x i in the image. For each pixel x i in the bounding box of S k , we can use Equation 3 to calculate the probability of x i belong to S k , then translate the probability to binary value using a thresh 0.5. In this way, the final masks are produced. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Details</head><p>Training Data We follow the settings of FCOS in our experiments, which chooses the large-scale detection benchmark COCO, and uses the COCO trainval35k split (115K images) for training, minival split (5K images) for ablation study and test-dev (20K images) for reporting the main results. The input images are resized with the short side being 800 while the longer side being no longer than 1333.</p><p>Network Architecture ResNet-50 <ref type="bibr" target="#b12">[13]</ref> is used as our backbone network for ablation study; ResNet-101 is used for comparing results with state-of-the-art methods. Following the FPN architecture as FCOS, we also use five levels of feature maps defined as {P 3, P 4, P 5, P 6, P 7}. The architecture for outputting proposal margins is a little different, as it actually predicts 1 2σ 2 j instead of σ j directly, so the variant is activated by the exponential function to keep it positive.</p><p>Training and Inference Procedure We train all the models with SGD for 90k iterations using an initial learning rate of 0.01 and batch size of 16, with constant warm-up of 500 iterations. The backbone network is initialized with the pretrained ImageNet <ref type="bibr" target="#b7">[8]</ref> weights. In default we set λ 1 = 0.5, λ 2 = 0.1 and embedding dim d = 32. For the main results in <ref type="table" target="#tab_0">Table 1</ref>, the box expand with 1.2× is used in producing training samples B k for pixel embedding (while box expand with 1.0× for ablation study by default). For the alignment of φ(x i , S k ) and G(x i , S k ) in the mask loss 4 during training, we resize the feature map of pixel embedding and the ground-truth mask for each instance to be a quarter of the input image in length, using bilinear interpolation. During inference, we do the same for the feature map of pixel embedding and then re-scale it to the initial size to obtain the mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>Quantitative Results We compare the quantitative results of EmbedMask with other state-of-the-arts methods, including one-stage and two-stage methods, which are shown in <ref type="table" target="#tab_0">Table 1</ref>. The listed results are all trained with ResNet-50-FPN or ResNet-101-FPN as the backbone for fairness. It is worth noting that some of the one-stage methods applied several training tricks for better performance, such as more training epochs. Hence it is not fair to directly compare EmbedMask with those methods. Nevertheless, even without data augmentation or more training epochs, our method outperforms YOLACT with 4.4 AP. And with more training epochs and data augmentation, our method has better performance and faster speed than TensorMask, and achieves the best performance within one-stage methods. With the scale of 600, 'EmbedMask-600' can achieve faster speed but the accuracy decreases because the scale is not matched with the one in training procedure. From the table, we can find that 'EmbedMask-600' has similar speed as YOLACT but better performance.</p><p>We also focus on the comparison between Mask R-CNN and our method, which both use the same training settings. The gap of mask AP between our method and Mask R-CNN is about 0.4 in ResNet-101 with multi-scale training, which is quite close. We also observe that the gap between Em-bedMask and Mask R-CNN in ResNet-101 is 1.8 in AP 50 , but 0.4 in AP 75 . That means our method is more advantageous in providing more accurate masks. Additionally, in comparison with the speed of 8.6 fps in Mask R-CNN, our method can run in 16.7 fps with 33.6 mAP for an input image both use the backbone of ResNet-50 and with the short side being 800 on a V100 GPU. As for the speed in ResNet-101, our method runs in 13.7 fps while Mask R-CNN runs in 8.7 fps. Therefore, about running speed, our Qualitative Results Similar to other segmentation-based methods, EmbedMask generates masks directly from the output feature maps without the need of repooling. Specifically, in EmbedMask, the masks are generated from the feature map of pixel embedding which is predicted directly from the largest feature map of FPN with a stride 8, which can produce more detail masks. <ref type="figure" target="#fig_3">Figure 3</ref> visualizes the comparison of the mask quality between the Mask R-CNN and EmbedMask, and both of these results are from models trained with 12 epochs and without multi-scale. As we can see from the qualitative results, our method can provide more detailed masks than Mask R-CNN with sharper edges, and that is because our method do not use the repooling operation so that avoid missing details. More visualization results which come from the model trained with multi-scale and 36 epochs can be found in <ref type="figure" target="#fig_5">Figure 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Fixed vs. Learnable Margin In the section 3.2 and 3.3, we introduce two kinds of loss functions for training. For the distance between proposal embedding and pixel embedding, the hinge loss function (Equation 2) uses δ a and δ b to control the margins of foreground and background when training, and the margin δ is also fixed when training. While the mask loss function (Equation 4) uses a gaussian function (Equation 1) to map the distance to the probability, where the predicted Σ k is predicted to control the margin for each instance. But the Σ k can also be fixed as a constant for every instance. In the experiment, we set δ a = 0.5, δ b = 1.5 to test the performance of the hinge loss along with the fixed margins, and this leads to δ = 0.8 which shows best result in the inference phase. From the results in <ref type="table" target="#tab_2">Table 2a</ref>, we can find that the mask loss with the gaussian function outperforms the hinge loss a lot for avoiding the manual turning of parameters. And with the same mask loss, the flexible margins for each instance can perform better than the constant margin.</p><p>The Choice of Cluster Centers In EmbedMask, we use the embedding Q k as the cluster center of the proposal, which comes from the proposal embedding q j . In fact, for each lo-  cation x j on the feature map of proposal embedding, it can be mapped to the location x i on the feature map of pixel embedding with the embedding value p i , so what if we replace the proposal embedding q j at the location x j with the pixel embedding p i at the mapped location x i and use this pixel embedding as the cluster center? The answer can be found in <ref type="table" target="#tab_2">Table 2b</ref>. The replacement discards the need for predicting the proposal embedding, making the method more like a segmentation-based method. However, we can see that the additional predicted proposal embedding has a better performance. The credit may be given to the receptive field of proposal embedding, which is larger than that of pixel embedding and thus performs better.</p><p>Sampling Strategy As described in section 3.5, during training, the positive samples for the {box j , class j , center j } requires the sampled location to be inside the mask of the ground-truth instance. While the positive samples for {q j , σ j } needs an additional condition that the IoU between the predicted box box j and ground-truth box should be larger than 0.5. Here we discuss whether the sampling of {box j , class j , center j , q j , σ j } needs to be inside the ground-truth masks, and that of {q j , σ j } need with the IoU greater than 0.5. The results can be found in <ref type="table" target="#tab_2">Table 2c</ref>. We can find that the samples of the proposal embedding and margin constrained inside the ground-truth mask and with an IoU greater than 0.5 can obtain better results.</p><p>Training Samples for Pixel Embedding The training samples of pixel embedding for each ground-truth instance S k , which are named B k , are all the samples located inside the bounding box of the S k . The bounding box can be enlarged for more training samples. In the experiment described by <ref type="table" target="#tab_2">Table 2d</ref>, we explore the suitable number of samples for training. The larger number of training samples with a 1.2× bounding box in length can result in a better performance than the one with the original box. However, the performance drops when the bounding box gets larger. The experiment shows that we can select a suitable number of training samples to achieve the best results. <ref type="table" target="#tab_2">Table 2e</ref> shows the results of different embedding dimensions. We can find that our method is robust with the embedding dimension. Even with a dimension of 8, the final mAP is still similar to that of 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding Dimension</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have proposed a single shot instance segmentation method, named EmbedMask, which can absorb the advantages of proposal-based and segmentationbased methods and avoid their weakness. Specifically, EmbedMask makes use of the unique output feature map, named pixel embedding, to produce masks which can preserve fine details. Additionally, EmbedMask predicts the embedding for each instance proposal to cluster pixels in the feature map of pixel embedding according to their similarity. The predicted margins also adapt EmbedMask to the training for multi-scale objects. In summary, as a single shot method, EmbedMask achieves comparable scores with the two-stage methods and run faster. In the future, we will improve EmbedMask by applying a more suitable network architecture to make it run faster and perform better. We hope our EmbedMask can inspire further research in the one-stage instance segmentation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) is an input image. (b) is the output instance proposals, attached with the parameters of bounding boxes, class scores, and proposal embeddings encoded by different colors. (c) is the output of the pixel embedding map encoded by different colors. (d) is the final result conducted from (b) and (c). For each proposal, pixels in the proposal box which have similar embeddings to the proposal embedding will be assigned to the mask of the proposal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>EmbedMask shares most parts of network architecture with the FCOS<ref type="bibr" target="#b30">[31]</ref>. All the blue feature maps are newly added base on FCOS. In proposal head and pixel head, solid arrows indicate 3 × 3 conv layers and dotted arrow indicates 1 × 1 conv layer. The ×4 marks indicate feature maps pass 4 3 × 3 conv layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visualization results. The upsides are the masks of Mask R-CNN and the downsides are the masks of EmbedMask, both with the ResNet-101 backbone and under the same training settings. method runs much faster than Mask R-CNN. The implementation of EmbedMask and Mask R-CNN are all based on maskrcnn-benchmark [25], and the inference time of the both includes the forwarding time of the networks and the postprocessing time including mask postprocessing which restores the masks to the sizes of input images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Mask visualizations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>method backbonems rc epochs AP AP 50 AP 75 AP S AP M AP L AP bb Comparison with state-of-the-art methods for instance segmentation on COCO test-dev. The methods located above are two-stage ones, and below are one-stage. In the table, 'ms' and 'rc' means multi-scale and random crop for training. 'EmbedMask-600'uses the same trained model as 'EmbedMask', while doing inference with the smaller input images whose shorter sides are 600 and longer sides are no longer than 800.</figDesc><table><row><cell>fps</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>loss σ AP AP 50 AP 75 AP S AP M AP L L hinge -30.0 52.0 30.2 13.8 33.1 43.4 L mask const 33.1 53.8 34.7 14.7 36.4 47.9 L mask pred 33.3 54.0 35.0 15.8 36.6 47.7 (a) Fixed vs. learnable margin. Results of using fixes margin and learnable margin.q j AP AP 50 AP 75 AP S AP M AP L pixel 30.9 52.0 31.9 14.0 34.9 44.5 proposal 33.3 54.0 35.0 15.8 36.6 47.7 ∆ +2.4 +2.0 +3.1 +1.8 +1.7 +3.2 (b) The choice of cluster center. Results of using proposal embedding as cluster center and using pixel embedding as cluster center.</figDesc><table><row><cell>in mask IoU&gt;0.5 AP AP 50 AP 75</cell><cell>box</cell><cell>AP AP 50 AP 75</cell><cell>dim</cell><cell>AP AP 50 AP 75</cell></row><row><cell>32.6 53.9 34.0</cell><cell>1×</cell><cell>33.3 54.0 35.0</cell><cell>8</cell><cell>33.0 54.0 34.6</cell></row><row><cell>32.9 53.7 34.3</cell><cell>1.2×</cell><cell>33.5 54.1 35.3</cell><cell>16</cell><cell>33.2 54.0 34.8</cell></row><row><cell>33.3 54.0 35.0</cell><cell>1.5×</cell><cell>33.1 53.7 34.9</cell><cell>32</cell><cell>33.3 54.0 35.0</cell></row><row><cell>(c) Sampling strategy. Results of using dif-</cell><cell cols="2">(d) Training samples for pixel embed-</cell><cell cols="2">(e) Embedding dimension. Results of</cell></row><row><cell>ferent sampling strategy, verifying that whether</cell><cell cols="2">ding. Results of using different num-</cell><cell cols="2">using different embedding dimension.</cell></row><row><cell>the samples should be in the ground-truth mask</cell><cell cols="2">ber of training samples for pixel embed-</cell><cell></cell><cell></cell></row><row><cell>and with an IoU larger than 0.5.</cell><cell cols="2">dings, which is controlled by the box</cell><cell></cell><cell></cell></row><row><cell></cell><cell>size.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablations on EmbedMask evaluated on COCO minival. All models are training for 12 epochs with ResNet-50.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Masklab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation via deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10277</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Retinamask: Learning to predict masks improves stateof-the-art single-shot detection for free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhailo</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03353</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Instancecut: from edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent pixel embedding for instance grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Insert date here</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polarmask</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13226</idno>
		<title level="m">Single shot instance segmentation with polar representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning submodular losses with the lovász hinge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
