<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiru</forename><surname>Park</surname></persName>
							<email>park@acin.tuwien.ac.at</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Vision for Robotics Laboratory</orgName>
								<orgName type="institution">Automation and Control Institute, TU</orgName>
								<address>
									<settlement>Wien</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Patten</surname></persName>
							<email>patten@acin.tuwien.ac.at</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Vision for Robotics Laboratory</orgName>
								<orgName type="institution">Automation and Control Institute, TU</orgName>
								<address>
									<settlement>Wien</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Vincze</surname></persName>
							<email>vincze@acin.tuwien.ac.at</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Vision for Robotics Laboratory</orgName>
								<orgName type="institution">Automation and Control Institute, TU</orgName>
								<address>
									<settlement>Wien</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating the 6D pose of objects using only RGB images remains challenging because of problems such as occlusion and symmetries. It is also difficult to construct 3D models with precise texture without expert knowledge or specialized scanning devices. To address these problems, we propose a novel pose estimation method, Pix2Pose, that predicts the 3D coordinates of each object pixel without textured models. An auto-encoder architecture is designed to estimate the 3D coordinates and expected errors per pixel. These pixel-wise predictions are then used in multiple stages to form 2D-3D correspondences to directly compute poses with the PnP algorithm with RANSAC iterations. Our method is robust to occlusion by leveraging recent achievements in generative adversarial training to precisely recover occluded parts. Furthermore, a novel loss function, the transformer loss, is proposed to handle symmetric objects by guiding predictions to the closest symmetric pose. Evaluations on three different benchmark datasets containing symmetric and occluded objects show our method outperforms the state of the art using only RGB images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pose estimation of objects is an important task to understand the given scene and operate objects properly in robotic or augmented reality applications. The inclusion of depth images has induced significant improvements by providing precise 3D pixel coordinates <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref>. However, depth images are not always easily available, e.g., mobile phones and tablets, typical for augmented reality applications, offer no depth data. As such, substantial research is dedicated to estimating poses of known objects using RGB images only. <ref type="bibr">Figure</ref> 1. An example of converting a 3D model to a colored coordinate model. Normalized coordinates of each vertex are directly mapped to red, green and blue values in the color space. Pix2Pose predicts these colored images to build a 2D-3D correspondence per pixel directly without any feature matching operation.</p><p>A large body of work relies on the textured 3D model of an object, which is made by a 3D scanning device, e.g., Big-BIRD Object Scanning Rig <ref type="bibr" target="#b35">[36]</ref>, and provided by a dataset to render synthetic images for training <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref> or refinement <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>. Thus, the quality of texture in the 3D model should be sufficient to render visually correct images. Unfortunately, this is not applicable to domains that do not have textured 3D models such as industry that commonly use texture-less CAD models. Since the texture quality of a reconstructed 3D model varies with method, camera, and camera trajectory during the reconstruction process, it is difficult to guarantee sufficient quality for training. Therefore, it is beneficial to predict poses without textures on 3D models to achieve more robust estimation regardless of the texture quality.</p><p>Even though recent studies have shown great potential to estimate pose without textured 3D models using Convolutional Neural Networks (CNN) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>, a significant challenge is to estimate correct poses when objects are occluded or symmetric. Training CNNs is often distracted by symmetric poses that have similar appearance inducing very large errors in a naïve loss function. In previous work, a strategy to deal with symmetric objects is to limit the range of poses while rendering images for training <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref> or simply to apply a transformation from the pose outside of the limited range to a symmetric pose within the range <ref type="bibr" target="#b24">[25]</ref> for arXiv:1908.07433v1 [cs.CV] 20 Aug 2019 real images with pose annotations. This approach is sufficient for objects that have infinite and continuous symmetric poses on a single axis, such as cylinders, by simply ignoring the rotation about the axis. However, as pointed in <ref type="bibr" target="#b24">[25]</ref>, when an object has a finite number of symmetric poses, it is difficult to determine poses around the boundaries of view limits. For example, if a box has an angle of symmetry, π, with respect to an axis and a view limit between 0 and π, the pose at π + α(α ≈ 0, α &gt; 0) has to be transformed to a symmetric pose at α even if the detailed appearance is closer to a pose at π. Thus, a loss function has to be investigated to guide pose estimations to the closest symmetric pose instead of explicitly defined view ranges.</p><p>This paper proposes a novel method, Pix2Pose, that can supplement any 2D detection pipeline for additional pose estimation. Pix2Pose predicts pixel-wise 3D coordinates of an object using RGB images without textured 3D models for training. The 3D coordinates of occluded pixels are implicitly estimated by the network in order to be robust to occlusion. A specialized loss function, the transformer loss, is proposed to robustly train the network with symmetric objects. As a result of the prediction, each pixel forms a 2D-3D correspondence that is used to compute poses by the Perspective-n-Point algorithm (PnP) <ref type="bibr" target="#b17">[18]</ref>.</p><p>To summarize, the contributions of the paper are: (1) A novel framework for 6D pose estimation, Pix2Pose, that robustly regresses pixel-wise 3D coordinates of objects from RGB images using 3D models without textures during training. (2) A novel loss function, the transformer loss, for handling symmetric objects that have a finite number of ambiguous views. (3) Experimental results on three different datasets, LineMOD <ref type="bibr" target="#b8">[9]</ref>, LineMOD Occlusion <ref type="bibr" target="#b0">[1]</ref>, and T-Less <ref type="bibr" target="#b9">[10]</ref>, showing that Pix2Pose outperforms the state-ofthe-art methods even if objects are occluded or symmetric.</p><p>The remainder of this paper is organized as follows. A brief summary of related work is provided in Sec. 2. Details of Pix2Pose and the pose prediction process are explained in Sec. 3 and Sec. 4. Experimental results are reported in Sec. 5 to compare our approach with the state-of-the-art methods. The paper concludes in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>This section gives a brief summary of previous work related to pose estimation using RGB images. Three different approaches for pose estimation using CNNs are discussed and the recent advances of generative models are reviewed.</p><p>CNN based pose estimation The first, and simplest, method to estimate the pose of an object using a CNN is to predict a representation of a pose directly such as the locations of projected points of 3D bounding boxes <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref>, classified view points <ref type="bibr" target="#b14">[15]</ref>, unit quaternions and translations <ref type="bibr" target="#b32">[33]</ref>, or the Lie algebra representation, so(3), with the translation of z-axis <ref type="bibr" target="#b3">[4]</ref>. Except for methods that predict projected points of the 3D bounding box, which requires further computations for the PnP algorithm, the direct regression is computationally efficient since it does not require additional computation for the pose. The drawback of these methods, however, is the lack of correspondences that can be useful to generate multiple pose hypotheses for the robust estimation of occluded objects. Furthermore, symmetric objects are usually handled by limiting the range of viewpoints, which sometimes requires additional treatments, e.g., training a CNN for classifying view ranges <ref type="bibr" target="#b24">[25]</ref>. Xiang et al. <ref type="bibr" target="#b32">[33]</ref> propose a loss function that computes the average distance to the nearest points of transformed models in an estimated pose and an annotated pose. However, searching for the nearest 3D points is time consuming and makes the training process inefficient.</p><p>The second method is to match features to find the nearest pose template and use the pose information of the template as an initial guess <ref type="bibr" target="#b8">[9]</ref>. Recently, Sundermeyer et al. <ref type="bibr" target="#b28">[29]</ref> propose an auto-encoder network to train implicit representations of poses without supervision using RGB images only. Manual handling of symmetric objects is not necessary for this work since the implicit representation can be close to any symmetric view. However, it is difficult to specify 3D translations using rendered templates that only give a good estimation of rotations. The size of the 2D bounding box is used to compute the z-component of 3D translation, which is too sensitive to small errors of 2D bounding boxes that are given from a 2D detection method.</p><p>The last method is to predict 3D locations of pixels or local shapes in the object space <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>. Brachmann et al. <ref type="bibr" target="#b1">[2]</ref> regress 3D coordinates and predict a class for each pixel using the auto-context random forest. Oberwerger et al. <ref type="bibr" target="#b22">[23]</ref> predict multiple heat-maps to localize the 2D projections of 3D points of objects using local patches. These methods are robust to occlusion because they focus on local information only. However, additional computation is required to derive the best result among pose hypotheses, which makes these methods slow.</p><p>The method proposed in this paper belongs to the last category that predicts 3D locations of pixels in the object frame as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Instead of detecting an object using local patches from sliding windows, an independent 2D detection network is employed to provide areas of interest for target objects as performed in <ref type="bibr" target="#b28">[29]</ref>.</p><p>Generative models Generative models using autoencoders have been used to de-noise <ref type="bibr" target="#b31">[32]</ref> or recover the missing parts of images <ref type="bibr" target="#b33">[34]</ref>. Recently, using Generative Adversarial Network (GAN) <ref type="bibr" target="#b5">[6]</ref> improves the quality of generated images that are less blurry and more realistic, which are used for the image-to-image translation <ref type="bibr" target="#b13">[14]</ref>, image in-painting and de-noising <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref> tasks. Zakharov et al. <ref type="bibr" target="#b34">[35]</ref> propose a GAN based framework to convert a real depth image to a synthetic depth image without noise and background for classification and pose estimation.</p><p>Inspired by previous work, we train an auto-encoder architecture with GAN to convert color images to coordinate values accurately as in the image-to-image translation task while recovering values of occluded parts as in the image in-painting task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pix2Pose</head><p>This section provides a detailed description of the network architecture of Pix2Pose and loss functions for training. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, Pix2Pose predicts 3D coordinates of individual pixels using a cropped region containing an object. The robust estimation is established by recovering 3D coordinates of occluded parts and using all pixels of an object for pose prediction. A single network is trained and used for each object class. The texture of a 3D model is not necessary for training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>The architecture of the Pix2Pose network is described in <ref type="figure" target="#fig_0">Fig. 2</ref>. The input of the network is a cropped image I s using a bounding box of a detected object class. The outputs of the network are normalized 3D coordinates of each pixel I 3D in the object coordinate and estimated errors I e of each prediction, I 3D , I e = G(I s ), where G denotes the Pix2Pose network. The target output includes coordinate predictions of occluded parts, which makes the prediction more robust to partial occlusion. Since a coordinate consists of three values similar to RGB values in an image, the output I 3D can be regarded as a color image. Therefore, the ground truth output is easily derived by rendering the colored coordinate model in the ground truth pose. An example of 3D coordinate values in a color image is visualized in <ref type="figure">Fig. 1</ref>. The error prediction I e is regarded as a confidence score of each pixel, which is directly used to determine outlier and inlier pixels before the pose computation.</p><p>The cropped image patch is resized to 128×128px with three channels for RGB values. The sizes of filters and channels in the first four convolutional layers, the encoder, are the same as in <ref type="bibr" target="#b28">[29]</ref>. To maintain details of low-level feature maps, skip connections <ref type="bibr" target="#b27">[28]</ref> are added by copying the half channels of outputs from the first three layers to the corresponding symmetric layers in the decoder, which results in more precise estimation of pixels around geometrical boundaries. The filter size of every convolution and deconvolution layer is fixed to 5×5 with stride 1 or 2 denoted as s1 or s2 in <ref type="figure" target="#fig_0">Fig. 2</ref>. Two fully connected layers are applied for the bottle neck with 256 dimensions between the encoder and the decoder. The batch normalization <ref type="bibr" target="#b12">[13]</ref> and the LeakyReLU activation are applied to every output of the intermediate layers except the last layer. In the last layer, an output with three channels and the tanh activation produces a 3D coordinate image I 3D , and another output with one channel and the sigmoid activation estimates the expected errors I e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Training</head><p>The main objective of training is to predict an output that minimizes errors between a target coordinate image and a predicted image while estimating expected errors of each pixel. Transformer loss for 3D coordinate regression To reconstruct the desired target image, the average L1 distance of each pixel is used. Since pixels belonging to an object are more important than the background, the errors under the object mask are multiplied by a factor of β (≥ 1) to weight errors in the object mask. The basic reconstruction loss L r is defined as,</p><formula xml:id="formula_0">L r = 1 n β i∈M ||I i 3D − I i gt || 1 + i / ∈M ||I i 3D − I i gt || 1 ,<label>(1)</label></formula><p>where n is the number of pixels, I i gt is the i th pixel of the target image, and M denotes an object mask of the target image, which includes pixels belonging to the object when it is fully visible. Therefore, this mask also contains the occluded parts to predict the values of invisible parts for robust estimation of occluded objects. The loss above cannot handle symmetric objects since it penalizes pixels that have larger distances in the 3D space without any knowledge of the symmetry. Having the advantage of predicting pixel-wise coordinates, the 3D coordinate of each pixel is easily transformed to a symmetric pose by multiplying a 3D transformation matrix to the target image directly. Hence, the loss can be calculated for a pose that has the smallest error among symmetric pose candidates as formulated by,</p><formula xml:id="formula_1">L 3D = min p∈sym L r (I 3D , R p I gt ),<label>(2)</label></formula><p>where R p ∈ R 3x3 is a transformation from a pose to a symmetric pose in a pool of symmetric poses, sym, including an identity matrix for the given pose. The pool sym is assumed to be defined before the training of an object. This novel loss, the transformer loss, is applicable to any symmetric object that has a finite number of symmetric poses. This loss adds only a tiny effort for computation since a small number of matrix multiplications is required. The transformer loss in Eq. 2 is applied instead of the basic reconstruction loss in Eq. 1. The benefit of the transformer loss is analyzed in Sec. 5.7.</p><p>Loss for error prediction The error prediction I e estimates the difference between the predicted image I 3D and the target image I gt . This is identical to the reconstruction loss L r with β = 1 such that pixels under the object mask are not penalized. Thus, the error prediction loss L e is written as,</p><formula xml:id="formula_2">L e = 1 n i ||I i e − min L i r , 1 || 2 2 , β = 1.<label>(3)</label></formula><p>The error is bounded to the maximum value of the sigmoid function.</p><p>Traininig with GAN As discussed in Sec. 2, the network training with GAN generates more precise and realistic images in a target domain using images of another domain <ref type="bibr" target="#b13">[14]</ref>. The task for Pix2Pose is similar to this task since it converts a color image to a 3D coordinate image of an object. Therefore, the discriminator and the loss function of GAN <ref type="bibr" target="#b5">[6]</ref>, L GAN , is employed to train the network. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the discriminator network attempts to distinguish whether the 3D coordinate image is rendered by a 3D model or is estimated. The loss is defined as,</p><formula xml:id="formula_3">L GAN = log D(I gt ) + log(1 − D(G(I src ))),<label>(4)</label></formula><p>where D denotes the discriminator network. Finally, the objective of the training with GAN is formulated as,</p><formula xml:id="formula_4">G * = arg min G max D L GAN (G, D) + λ 1 L 3D (G) + λ 2 L e (G),<label>(5)</label></formula><p>where λ 1 and λ 2 denote weights to balance different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Pose prediction</head><p>This section gives a description of the process that computes a pose using the output of the Pix2Pose network. The overview of the process is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Before the estimation, the center, width, and height of each bounding box are used to crop the region of interest and resize it to the input size, 128×128px. The width and height of the region are set to the same size to keep the aspect ratio by taking the larger value. Then, they are multiplied by a factor of 1.5 so that the cropped region potentially includes occluded parts. The pose prediction is performed in two stages and the identical network is used in both stages. The first stage aligns the input bounding box to the center of the object which could be shifted due to different 2D detection methods. It also removes unnecessary pixels (background and uncertain) that are not preferred by the network. The second stage predicts a final estimation using the refined input from the first stage and computes the final pose.</p><p>Stage 1: Mask prediction and Bbox Adjustment In this stage, the predicted coordinate image I 3D is used for specifying pixels that belong to the object including the occluded parts by taking pixels with non-zero values. The error prediction is used to remove the uncertain pixels if an error for a pixel is larger than the outlier threshold θ o . The valid object mask is computed by taking the union of pixels that have non-zero values and pixels that have lower errors than θ o . The new center of the bounding box is determined with the centroid of the valid mask. As a result, the output of the first stage is a refined input that only contains pixels in the valid mask cropped from a new bounding box. Examples of outputs of the first stage are shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. The refined input possibly contains the occluded parts when the error prediction is below the outlier threshold θ o , which means the coordinates of these pixels are easy to predict despite occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 2: Pixel-wise 3D coordinate regression with errors</head><p>The second estimation with the network is performed to predict a coordinate image and expected error values using the refined input as depicted in <ref type="figure" target="#fig_1">Fig. 3</ref>. Black pixels in the 3D coordinate samples denote points that are removed when the error prediction is larger than the inlier threshold θ i even though points have non-zero coordinate values. In other words, pixels that have non-zero coordinate values with smaller error predictions than θ i are used to build 2D-3D correspondences. Since each pixel already has a value for a 3D point in the object coordinate, the 2D image coordinates and predicted 3D coordinates directly form correspondences. Then, applying the PnP algorithm <ref type="bibr" target="#b17">[18]</ref> with RANdom SAmple Consensus (RANSAC) <ref type="bibr" target="#b4">[5]</ref> iteration computes the final pose by maximizing the number of inliers that have lower re-projection errors than a threshold θ re . It is worth mentioning that there is no rendering involved during the pose estimation since Pix2Pose does not assume textured 3D models. This also makes the estimation process fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>In this section, experiments on three different datasets are performed to compare the performance of Pix2Pose to state-of-the-art methods.</p><p>The evaluation using LineMOD <ref type="bibr" target="#b8">[9]</ref> shows the performance for objects without occlusion in the single object scenario. For the multiple object scenario with occlusions, LineMOD Occlusion <ref type="bibr" target="#b0">[1]</ref> and T-Less <ref type="bibr" target="#b9">[10]</ref> are used. The evaluation on T-Less shows the most significant benefit of Pix2Pose since T-Less provides texture-less CAD models and most of the objects are symmetric, which is more challenging and common in industrial domains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Augmentation of training data</head><p>A small number of real images are used for training with various augmentations. Image pixels of objects are extracted from real images and pasted to background images that are randomly picked from the Coco dataset <ref type="bibr" target="#b20">[21]</ref>. After applying the color augmentations on the image, the borderlines between the object and the background are blurred to make smooth boundaries. A part of the object area is replaced by the background image to simulate occlusion. Lastly, a random rotation is applied to both the augmented color image and the target coordinate image. The same augmentation is applied to all evaluations except sizes of occluded areas that need to be larger for datasets with occlusions, LineMOD Occlusion and T-Less. Sample augmentated images are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. As explained in Sec. 4, the network recognizes two types of inputs, with background in the first stage and without background pixels in the second stage. Thus, a mini-batch is altered for every iteration as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Target coordinate images are rendered before training by placing the object in the ground truth poses using the colored coordinate model as in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>For training, the batch size of each iteration is set to 50, the Adam optimizer <ref type="bibr" target="#b16">[17]</ref> is used with initial learning rate of 0.0001 for 25K iterations. The learning rate is multiplied by a factor of 0.1 for every 12K iterations. Weights of loss functions in Eq. 1 and Eq. 5 are: β=3, λ 1 =100 and λ 2 =50. For evaluation, a 2D detection network and Pix2Pose networks of all object candidates in test sequences are loaded to the GPU memory, which requires approximately 2.2GB for the LineMOD Occlusion experiment with eight objects. The standard parameters for the inference are: θ i =0.1, θ o =[0.1, 0.2, 0.3], and θ re =3. Since the values of error predictions are biased by the level of occlusion in the online augmentation and the shape and size of each object, the outlier threshold θ o in the first stage is determined among three values to include more numbers of visible pixels while excluding noisy pixels using samples of training images with artificial occlusions. More details about param-  <ref type="bibr" target="#b25">[26]</ref> - 2D detection network An improved Faster R-CNN <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref> with Resnet-101 <ref type="bibr" target="#b7">[8]</ref> and Retinanet <ref type="bibr" target="#b19">[20]</ref> with Resnet-50 are employed to provide classes of detected objects with 2D bounding boxes for all target objects of each evaluation. The networks are initialized with pre-trained weights using the Coco dataset <ref type="bibr" target="#b20">[21]</ref>. The same set of real training images is used to generate training images. Cropped patches of objects in real images are pasted to random background images to generate training images that contain multiple classes in each image.</p><formula xml:id="formula_5">- - - - - - - - - - - - 78.7</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Metrics</head><p>A standard metric for LineMOD, AD{D|I}, is mainly used for the evaluation <ref type="bibr" target="#b8">[9]</ref>. This measures the average distance of vertices between a ground truth pose and an estimated pose. For symmetric objects, the average distance to the nearest vertices is used instead. The pose is considered correct when the error is less than 10% of the maximum 3D diameter of an object.</p><p>For T-Less, the Visible Surface Discrepancy (VSD) is used as a metric since the metric is employed to benchmark various 6D pose estimation methods in <ref type="bibr" target="#b10">[11]</ref>. This metric measures distance errors of visible parts only, which makes the metric invariant to ambiguities caused by symmetries and occlusion. As in previous work, the pose is regarded as correct when the error is less than 0.3 with τ =20mm and δ=15mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">LineMOD</head><p>For training, test sequences are separated into a training and test set. The divided set of each sequence is identical to the work of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref> the highest score in each scene is used for pose estimation since the detection network produces multiple results for all 13 objects. For the symmetric objects, marked with (*) in <ref type="table" target="#tab_1">Table 1</ref>, the pool of symmetric poses sym is defined as, sym= [I, R π z ], where R π z represents a transformation matrix of rotation with π about the z-axis.</p><p>The upper part of <ref type="table" target="#tab_1">Table 1</ref> shows Pix2Pose significantly outperforms state-of-the-art methods that use the same amount of real training images without textured 3D models. Even though methods on the bottom of <ref type="table" target="#tab_1">Table 1</ref> use a larger portion of training images, use textured 3D models for training or pose refinement, our method shows competitive results against these methods. The results on symmetric objects show the best performance among methods that do not perform pose refinement. This verifies the benefit of the transformer loss, which improves the robustness of initial pose predictions for symmetric objects.   <ref type="table">Table 3</ref>. T-Less: object recall (eVSD &lt; 0.3, τ = 20mm) on all test scenes using PrimeSense. Results of <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b1">[2]</ref> are cited from <ref type="bibr" target="#b10">[11]</ref>. Object-wise results are included in the supplement material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">LineMOD Occlusion</head><p>quences of eight objects in LineMOD are used for training without overlapping with test images. Faster R-CNN is used as a 2D detection pipeline. As shown in <ref type="table" target="#tab_2">Table 2</ref>, Pix2Pose significantly outperforms the method of <ref type="bibr" target="#b29">[30]</ref> using only real images for training. Furthermore, Pix2Pose outperforms the state of the art on three out of eight objects. On average it performs best even though methods of <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b32">[33]</ref> use more images that are synthetically rendered by using textured 3D models of objects. Although these methods cover more various poses than the given small number of images, Pix2Pose robustly estimates poses with less coverage of training poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">T-Less</head><p>In this dataset, a CAD model without textures and a reconstructed 3D model with textures are given for each object. Even though previous work uses reconstructed models for training, to show the advantage of our method, CAD models are used for training (as shown in <ref type="figure">Fig. 1</ref>) with real training images provided by the dataset. To minimize the gap of object masks between a real image and a rendered scene using a CAD model, the object mask of the real image is used to remove pixels outside of the mask in the rendered coordinate images. The pool of symmetric poses sym of objects is defined manually similar to the eggbox in the LineMOD evaluation for box-like objects such as obj-05. For cylindrical objects such as obj-01, the rotation component of the z-axis is simply ignored and regarded as a non-symmetric object. The experiment is performed based on the protocol of <ref type="bibr" target="#b10">[11]</ref>. Instead of a subset of the test sequences in <ref type="bibr" target="#b10">[11]</ref>, full test images are used to compare with the state of the art <ref type="bibr" target="#b28">[29]</ref>. Retinanet is used as a 2D detection method and objects visible more than 10% are considered as estimation targets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>The result in <ref type="table">Table 3</ref> shows Pix2Pose outperforms thestate-of-the-art method that uses RGB images only by a significant margin. The performance is also better than the best learning-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref> in the benchmark <ref type="bibr" target="#b10">[11]</ref>. Although these methods use color and depth images to refine poses or to derive the best pose among multiple hypotheses, our method, that predicts a single pose per detected object, performs better than these methods without refinement using depth images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Ablation studies</head><p>In this section, we present ablation studies by answering four important questions that clarify the contribution of each component in the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How does the transformer loss perform?</head><p>The obj-05 in T-Less is used to analyze the variation of loss values with respect to symmetric poses and to show the contribution of the transformer loss. To see the variation of loss values, 3D coordinate images are rendered while rotating the object around the z-axis. Loss values are computed using the coordinate image of a reference pose as a target output I gt and images of other poses as predicted outputs I 3D in Eq. 1 and Eq. 2. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, the L1 loss in Eq. 1 produces large errors for symmetric poses around π, which is the reason why the handling of symmetric objects is required. On the other hand, the value of the transformer loss produces minimum values on 0 and π, which is expected for obj-05 with an angle of symmetry of π. The result denoted by view limits shows the value of the L1 loss while limiting the zcomponent of rotations between 0 and π. The pose that exceeds this limit is rotated to a symmetric pose. As discussed in Sec. 1, values are significantly changed at the angles of view limits and over-penalize poses under areas with red in <ref type="figure" target="#fig_4">Fig. 5</ref>, which causes noisy predictions of poses around these angles. The results in <ref type="table">Table 4</ref> show the transformer loss significantly improves the performance compared to the L1 loss with the view limiting strategy and the L1 loss without handling symmetries.</p><p>What if the 3D model is not precise? The evaluation on T-Less already shows the robustness to 3D CAD models that have small geometric differences with real objects. However, it is often difficult to build a 3D model or a CAD model with refined meshes and precise geometries of a tar- <ref type="figure">Figure 6</ref>. Top: the fraction of frames within AD{D|I} thresholds for the cat in LineMOD. The larger area under a curve means better performance. Bottom: qualitative results with/without GAN. get object. Thus, a simpler 3D model, a convex hull covering out-bounds of the object, is used in this experiment as shown in <ref type="figure">Fig. 6</ref>. The training and evaluation are performed in the same way for the LineMOD evaluation with synchronization of object masks using annotated masks of real images. As shown in the top-left of <ref type="figure">Fig. 6</ref>, the performance slightly drops when using the convex hull. However, the performance is still competitive with methods that use 3D bounding boxes of objects, which means that Pix2Pose uses the details of 3D coordinates for robust estimation even though 3D models are roughly reconstructed.</p><p>Does GAN improve results? The network of Pix2Pose can be trained without GAN by removing the GAN loss in the final loss function in Eq. 5. Thus, the network only attempts to reconstruct the target image without trying to trick the discriminator. To compare the performance, the same training procedure is performed without GAN until the loss value excluding the GAN loss reaches the same level. Results in the top-left in <ref type="figure">Fig. 6</ref> shows the fraction of correctly estimated poses with varied thresholds for the ADD metric. Solid lines show the performance on the original LineMOD test images, which contains fully visible objects, and dashed lines represent the performance on the same test images with artificial occlusions that are made by replacing 50% of areas in each bounding box with zero. There is no significant change in the performance when objects are fully visible. However, the performance drops significantly without GAN when objects are occluded. Examples in the bottom of <ref type="figure">Fig. 6</ref> also show training with GAN produces robust predictions on occluded parts.</p><p>Is Pix2Pose robust to different 2D detection networks? <ref type="table">Table 5</ref> reports the results using different 2D detection networks on LineMOD. Retinanet and Faster R-CNN are trained using the same training images used in the SSD-6D <ref type="bibr" target="#b14">[15]</ref> Retina <ref type="bibr" target="#b19">[20]</ref> R-CNN <ref type="bibr" target="#b26">[27]</ref> GT bbox 2D bbox 89.  <ref type="table">Table 5</ref>. Average percentages of correct 2D bounding boxes (IoU&gt;0.5) and correct 6D poses (ADD-10%) on LineMOD using different 2D detection methods. The last row reports the percentage of correctly estimated poses on scenes that have correct bounding boxes (IoU&gt;0.5).</p><p>LineMOD evaluation. In addition, the public code and trained weights of SSD-6D <ref type="bibr" target="#b14">[15]</ref> are used to derive 2D detection results while ignoring pose predictions of the network. It is obvious that pose estimation results are proportional to 2D detection performances. On the other hand, the portion of correct poses on good bounding boxes (those that overlap more than 50% with ground truth) does not change significantly. This shows that Pix2Pose is robust to different 2D detection results when a bounding box overlaps the target object sufficiently. This robustness is accomplished by the refinement in the first stage that extracts useful pixels with a re-centered bounding box from a test image. Without the two stage approach, the performance significantly drops to 41% on LineMOD when the output of the network in the first stage is used directly for the PnP computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Inference time</head><p>The inference time varies according to the 2D detection networks. Faster R-CNN takes 127ms and Retinanet takes 76ms to detect objects from an image with 640×480px. The pose estimation for each bounding box takes approximately 25-45ms per region. Thus, our method is able to estimate poses at 8-10 fps with Retinanet and 6-7 fps with Faster R-CNN in the single object scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper presented a novel architecture, Pix2Pose, for 6D object pose estimation from RGB images. Pix2Pose addresses several practical problems that arise during pose estimation: the difficulty of generating real-world 3D models with high-quality texture as well as robust pose estimation of occluded and symmetric objects. Evaluations with three challenging benchmark datasets show that Pix2Pose significantly outperforms state-of-the-art methods while solving these aforementioned problems.</p><p>Our results reveal that many failure cases are related to unseen poses that are not sufficiently covered by training images or the augmentation process. Therefore, future work will investigate strategies to improve data augmentation to more broadly cover pose variations using real images in order to improve estimation performance. Another avenue for future work is to generalize the approach to use a single network to estimate poses of various objects in a class that have similar geometry but different local shape or scale.    <ref type="table" target="#tab_1">Table 11</ref>. Object reall (evsd &lt; 0.3, τ = 20mm, δ = 15mm) on all test scenes of Primesense in T-Less. Objects visible more than 10% are considered. The bounding box of an object with the highest score is used for estimation in order to follow the test protocol of 6D pose benchmark <ref type="bibr" target="#b10">[11]</ref>. <ref type="figure">Figure 8</ref> and <ref type="figure">Figure 9</ref> present example outputs of the Pix2Pose network after training of the network with/without using the transformer loss. The obj-05 in T-less is used. <ref type="figure">Figure 8</ref>. Prediction results of varied rotations with the z-axis. As discussed in the paper, limiting a view range causes noisy predictions at boundaries, 0 and π, as denoted with red boxes. The transformer loss implicitly guides the network to predict a single side consistently. For the network trained by the L1 loss, the prediction is accurate when the object is fully visible. This is because the upper part of the object provides a hint for a pose. <ref type="figure">Figure 9</ref>. Prediction results with/without occlusion. For the network trained by the L1 loss, it is difficult to predict the exact pose when the upper part, which is a clue to determine the pose, is not visible. The prediction of the network using the transformer loss is robust to this occlusion since the network consistently predicts a single side.  <ref type="figure">Figure 11</ref>. Example results on LineMOD Occlusion. The precise prediction of occluded parts enhances robustness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detail parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Qualitative examples of the transformer loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Example results on LineMOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Example results on LineMOD Occlusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Example results on T-Less</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Failure cases</head><p>Primary reasons of failure cases: (1) Poses that are not covered by real training images and the augmentation. (2) Ambiguous poses due to severe occlusion. (3) Not sufficiently overlapped bounding boxes, which cannot be recovered by the bounding box adjustment in the first stage. The second row of <ref type="figure" target="#fig_1">Fig. 13</ref> shows that the random augmentation of in-plane rotation during the training is not sufficient to cover various poses. Thus, the uniform augmentation of in-plane rotation has to performed for further improvement. <ref type="figure" target="#fig_1">Figure 13</ref>. Examples of failure cases due to unseen poses. The closest poses are obtained from training images using geodesic distances between two transformations (rotation only).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>An overview of the architecture of Pix2Pose and the training pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>An example of the pose estimation process. An image and 2D detection results are the input. In the first stage, the predicted results are used to specify important pixels and adjust bounding boxes while removing backgrounds and uncertain pixels. In the second stage, pixels with valid coordinate values and small error predictions are used to estimate poses using the PnP algorithm with RANSAC. Green and blue lines in the result represent 3D bounding boxes of objects in ground truth poses and estimated poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Examples of mini-batches for training. A mini-batch is altered for every training iteration. Left: images for the first stage, Right: images for the second stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>LineMOD</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Variation of the reconstruction loss for a symmetric object with respect to z-axis rotation using obj-05 in T-Less<ref type="bibr" target="#b9">[10]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Table 10 .Figure 7 .</head><label>107</label><figDesc>Outlier thresholds θo for objects in T-Less Examples of refined inputs in the first stage with varied values for the outlier threshold. Values are determined to maximize the number of visible pixels while excluding noisy predictions in refined inputs. Training images are used with artificial occlusions. The brighter pixel in images of the third column represents the larger error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Example results on LineMOD. The result marked with sym represents that the prediction is the symmetric pose of the ground truth pose, which shows the effect of the proposed transformer loss. Green: 3D bounding boxes of ground truth poses, blue: 3D bounding boxes of predicted poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 .</head><label>12</label><figDesc>Example results on T-Less. For visualization, ground-truth bounding boxes are used to show pose estimation results regardless of the 2D detection performance. Results with rot denote estimations of objects with cylindrical shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ape bvise cam can cat driller duck e.box* glue* holep iron lamp phone avg Pix2Pose 58.1 91.0 60.9 84.4 65.0 76.3 43.8 96.8 79.4 74.8 83.4 82.0 45.0 72.4 Tekin [30] 21.6 81.8 36.6 68.8 41.8 63.5 27.2 69.6 80.0 42.6 75.0 71.1 47.7 56.0 Brachmann [2] 33.2 64.8 38.4 62.9 42.7 61.9 30.2 49.9 31.2 52.8 80.0 67.0 38.1 50.2 BB8 [25] 27.9 62.0 40.1 48.1 45.2 58.6 32.8 40.0 27.0 42.4 67.0 39.9 35.2 43.6 Lienet</figDesc><table><row><cell>30% [4]</cell><cell cols="8">38.8 71.2 52.5 86.1 66.2 82.3 32.5 79.4</cell><cell cols="6">63.7 56.4 65.1 89.4 65.0 65.2</cell></row><row><cell>BB8 ref [25]</cell><cell cols="8">40.4 91.8 55.7 64.1 62.6 74.4 44.3 57.8</cell><cell cols="6">41.2 67.2 84.7 76.5 54.0 62.7</cell></row><row><cell>Implicit syn [29]</cell><cell cols="6">4.0 20.9 30.5 35.9 17.9 24.0</cell><cell>4.9</cell><cell>81.0</cell><cell cols="6">45.5 17.6 32.0 60.5 33.8 31.4</cell></row><row><cell cols="2">SSD-6D syn/ref [15] 65</cell><cell>80</cell><cell>78</cell><cell>86</cell><cell>70</cell><cell>73</cell><cell>66</cell><cell>100</cell><cell>100</cell><cell>49</cell><cell>78</cell><cell>73</cell><cell>79</cell><cell>76.7</cell></row><row><cell>Rad syn/ref</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>LineMOD: Percentages of correctly estimated poses (AD{D|I}-10%). ( 30% ) means the training images are obtained from 30% of test sequences that are two times larger than ours. ( ref ) denotes the results are derived after iterative refinement using textured 3D models for rendering. ( syn ) indicates the method uses synthetically rendered images for training that also needs textured 3D models.</figDesc><table /><note>eters are given in the supplementary material. The training and evaluations are performed with an Nvidia GTX 1080 GPU and i7-6700K CPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>, which uses 15% of test scenes, approximately less than 200 images per object, for training. A detection result, using Faster R-CNN, of an object with LineMOD Occlusion: object recall (AD{D|I}-10%). ( † )indicates the method uses synthetically rendered images and real images for training, which has better coverage of viewpoints.</figDesc><table><row><cell cols="2">Method Pix2Pose</cell><cell>Oberweger  † [23]</cell><cell>PoseCNN  † [33]</cell><cell>Tekin [30]</cell></row><row><cell>ape</cell><cell>22.0</cell><cell>17.6</cell><cell>9.6</cell><cell>2.48</cell></row><row><cell>can</cell><cell>44.7</cell><cell>53.9</cell><cell>45.2</cell><cell>17.48</cell></row><row><cell>cat</cell><cell>22.7</cell><cell>3.31</cell><cell>0.93</cell><cell>0.67</cell></row><row><cell>driller</cell><cell>44.7</cell><cell>62.4</cell><cell>41.4</cell><cell>7.66</cell></row><row><cell>duck</cell><cell>15.0</cell><cell>19.2</cell><cell>19.6</cell><cell>1.14</cell></row><row><cell>eggbox*</cell><cell>25.2</cell><cell>25.9</cell><cell>22.0</cell><cell>-</cell></row><row><cell>glue*</cell><cell>32.4</cell><cell>39.6</cell><cell>38.5</cell><cell>10.08</cell></row><row><cell>holep</cell><cell>49.5</cell><cell>21.3</cell><cell>22.1</cell><cell>5.45</cell></row><row><cell>Avg</cell><cell>32.0</cell><cell>30.4</cell><cell>24.9</cell><cell>6.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Occlusion is created by annotating eight objects in a test sequence of LineMOD. Thus, the test se-</figDesc><table><row><cell>Input</cell><cell cols="2">RGB only</cell><cell></cell><cell>RGB-D</cell></row><row><cell cols="2">Method Pix2Pose</cell><cell>Implicit [29]</cell><cell>Kehl [16]</cell><cell>Brachmann [2]</cell></row><row><cell>Avg</cell><cell>29.5</cell><cell>18.4</cell><cell>24.6</cell><cell>17.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>A.1. Data augmentation for training<ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30:</ref> sym = [I], the z-component of the rotation matrix is ignored. • Objects not in the list (non-symmetric): sym = [I] A.3. Pose prediction • Definition of non-zero pixels: ||I3D||2 &gt; 0.3, I3D in normalized coordinates. • PnP and RANSAC algorithm: the implementation in OpenCV 3.4.0 [3] is used with default parameters except the re-projection threshold θre= 3. Outlier thresholds θo for objects in LineMOD Occlusion 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30</figDesc><table><row><cell cols="4">Add(each channel) Contrast normalization</cell><cell></cell><cell>Multiply</cell><cell></cell><cell cols="2">Gaussian Blur</cell></row><row><cell>U(-15, 15)</cell><cell cols="2">U(0.8, 1.3)</cell><cell cols="6">U(0.8, 1.2)(per channel chance=0.3) U(0.0, 0.5)</cell></row><row><cell></cell><cell></cell><cell cols="4">Table 6. Color augmentation</cell><cell></cell><cell></cell></row><row><cell cols="3">Type Random rotation</cell><cell></cell><cell cols="3">Fraction of occluded area</cell><cell></cell></row><row><cell cols="2">Dataset</cell><cell>All</cell><cell cols="5">LineMOD LineMOD Occlusion, T-Less</cell></row><row><cell cols="3">Range U(-45 • , -45 • )</cell><cell cols="2">U(0, 0.1)</cell><cell cols="2">U(0.04, 0.5)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">Table 7. Occlusion and rotation augmentation</cell><cell></cell></row><row><cell cols="5">A.2. The pools of symmetric poses for the transformer loss</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">• LineMOD and LineMOD Occlusion -eggbox and glue: sym = [I, R π z ]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">• T-Less -obj-5,6,7,8,9,10,11,12,25,26,28,29: sym = [I, R π z ]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">• T-Less -obj-19,20: sym = [I, R π y ]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">• T-Less -obj-27: sym = [I, R z , R π π 2 z , R</cell><cell>3π 2 z ]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>• T-• List of outlier thresholds</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">ape bvise cam can cat driller duck eggbox glue holep iron lamp phone</cell></row><row><cell>θ o 0.1 0.2</cell><cell cols="2">0.2 0.2 0.2 0.2</cell><cell>0.1</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2</cell><cell>0.2 0.2</cell><cell>0.2</cell></row><row><cell></cell><cell cols="6">Table 8. Outlier thresholds θo for objects in LineMOD</cell><cell></cell></row><row><cell></cell><cell cols="7">ape can cat driller duck eggbox glue holep</cell></row><row><cell></cell><cell cols="2">θ o 0.2 0.3 0.3</cell><cell>0.3</cell><cell>0.2</cell><cell>0.2</cell><cell>0.3</cell><cell>0.3</cell></row></table><note>I: Identity matrix, R Θa : Rotation matrix about the a-axis with an angle Θ.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>B.1. T-Less: Object-wise results</figDesc><table><row><cell>Obj.No</cell><cell>01</cell><cell>02</cell><cell>03</cell><cell>04</cell><cell>05</cell><cell>06</cell><cell>07</cell><cell>08</cell><cell>09</cell><cell>10</cell></row><row><cell>VSD Recall</cell><cell>38.4</cell><cell>35.3</cell><cell>40.9</cell><cell>26.3</cell><cell>55.2</cell><cell>31.5</cell><cell>1.1</cell><cell>13.1</cell><cell>33.9</cell><cell>45.8</cell></row><row><cell>Obj.No</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell><cell>19</cell><cell>20</cell></row><row><cell>VSD Recall</cell><cell>30.7</cell><cell>30.4</cell><cell>31.0</cell><cell>19.5</cell><cell>56.1</cell><cell>66.5</cell><cell>37.9</cell><cell>45.3</cell><cell>21.7</cell><cell>1.9</cell></row><row><cell>Obj.No</cell><cell>21</cell><cell>22</cell><cell>23</cell><cell>24</cell><cell>25</cell><cell>26</cell><cell>27</cell><cell>28</cell><cell>29</cell><cell>30</cell></row><row><cell>VSD Recall</cell><cell>19.4</cell><cell>9.5</cell><cell>30.7</cell><cell>18.3</cell><cell>9.5</cell><cell>13.9</cell><cell>24.4</cell><cell>43.0</cell><cell>25.8</cell><cell>28.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">c 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment The research leading to these results has received funding from the Austrian Science Foundation (FWF) under grant agreement No. I3967-N30 (BURG) and No. I3969-N30 (InDex), and Aelous Robotics, Inc.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The OpenCV Library. Dr. Dobb&apos;s Journal of Software Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time monocular object instance 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision (ACCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Jiří Matas, Manolis Lourakis, and Xenophon Zabulis. T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Hodaň</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Haluza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Štěpán</forename><surname>Obdržálek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>WACV</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bop: Benchmark for 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Hodaň</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Glentbuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Ihrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xenophon</forename><surname>Zabulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o(n) solution to the pnp problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2008-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep model-based 6d pose refinement in rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Making deep heatmaps robust to partial occlusions for 3d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature mapping for learning fast and accurate 3d pose inference from synthetic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-time seamless single shot 6d object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">6d pose estimation using an improved method based on point pair features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chyi-Yeu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Martí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Control, Automation and Robotics (ICCAR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="405" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Keep it unreal: Bridging the realism gap for 2.5 d recognition with geometry priors only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Planche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Kosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Benchmarking in manipulation research: Using the yale-cmu-berkeley object and model set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berk</forename><forename type="middle">Alli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><forename type="middle">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Magazine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="52" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
