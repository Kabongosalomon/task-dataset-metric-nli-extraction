<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FEATURE INTERTWINER FOR OBJECT DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
							<email>bdai@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Multimedia-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Multimedia-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wanli.ouyang@sydney.edu.au</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Computer Vision Research Group</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Multimedia-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FEATURE INTERTWINER FOR OBJECT DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A well-trained model should classify objects with a unanimous score for every category. This requires the high-level semantic features should be as much alike as possible among samples. To achive this, previous works focus on re-designing the loss or proposing new regularization constraints. In this paper, we provide a new perspective. For each category, it is assumed that there are two feature sets: one with reliable information and the other with less reliable source. We argue that the reliable set could guide the feature learning of the less reliable set during training -in spirit of student mimicking teacher's behavior and thus pushing towards a more compact class centroid in the feature space. Such a scheme also benefits the reliable set since samples become closer within the same category -implying that it is easier for the classifier to identify. We refer to this mutual learning process as feature intertwiner and embed it into object detection. It is well-known that objects of low resolution are more difficult to detect due to the loss of detailed information during network forward pass (e.g., RoI operation). We thus regard objects of high resolution as the reliable set and objects of low resolution as the less reliable set. Specifically, an intertwiner is designed to minimize the distribution divergence between two sets. The choice of generating an effective feature representation for the reliable set is further investigated, where we introduce the optimal transport (OT) theory into the framework. Samples in the less reliable set are better aligned with aid of OT metric. Incorporated with such a plug-and-play intertwiner, we achieve an evident improvement over previous state-of-the-arts. 1  We use the term 'large object/(more) reliable/high resolution set' interchangeably in the following to refer to the same meaning; likewise for the term 'small set/less reliable set/low-resolution set'.</p><p>2 Only top ten categories with the most number of instances in prediction is visualized. For each category, the high-resolution objects (reliable set) are shown in solid color while the low-resolution instances (less reliable set) are shown in transparent color with dashed boundary.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Classifying complex data in the high-dimensional feature space is the core of most machine learning problems, especially with the emergence of deep learning for better feature embedding <ref type="bibr" target="#b15">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b18">Li et al., 2018b;</ref><ref type="bibr" target="#b9">Guo et al., 2018)</ref> . Previous methods address the feature representation problem by the conventional cross-entropy loss, l 1 / l 2 loss, or a regularization constraint on the loss term to ensure small intra-class variation and large inter-class distance <ref type="bibr" target="#b13">(Janocha &amp; Czarneck, 2017;</ref><ref type="bibr" target="#b26">Liu et al., 2017b;</ref><ref type="bibr" target="#b42">Wen et al., 2016;</ref><ref type="bibr" target="#b25">Liu et al., 2017a)</ref>. The goal of these works is to learn more compact representation for each class in the feature space. In this paper, we also aim for such a goal and propose a new perspective to address the problem.</p><p>Our observation is that samples can be grouped into two sets in the feature space. One set is more reliable, while the other is less reliable. For example, visual samples may be less reliable due to low resolution, occlusion, adverse lighting, noise, blur, etc. The learned features for samples from the reliable set are easier to classify than those from the less reliable one. Our hypothesis is that the reliable set can guide the feature learning of the less reliable set, in the spirit of a teacher supervising the student. We refer to this mutual learning process as a feature intertwiner.</p><p>In this paper, a plug-and-play module, namely, feature intertwiner, is applied for object detection, which is the task of classifying and localizing objects in the wild. An object of lower resolution will inevitably lose detailed information during the forward pass in the network. Therefore, it is well-known that the detection accuracy drops significantly as resolutions of objects decrease. We can treat samples with high resolution (often corresponds to large objects or region proposals) as the  reliable set and samples with low resolution (small instances) as the less reliable set 1 . Equipped with these two 'prototypical' sets, we can apply the feature intertwiner where the reliable set is leveraged to help the feature learning of the less reliable set. <ref type="figure" target="#fig_0">Fig. 1</ref> on the left visualizes the learned detection features before classifier 2 . Without intertwiner in (a), samples are more scattered and separated from each other. Note there are several samples that are far from its own class and close to the samples in other categories (e.g., class person in blue), indicating a potential mistake in classification. With the aid of feature intertwiner in (b), there is barely outlier sample outside each cluster. the features in the lower resolution set approach closer to the features in the higher resolution set -achieving the goal of compact centroids in the feature space. Empirically, these two settings correspond to the baseline and intertwiner experiments (marked in gray) in <ref type="table" target="#tab_4">Table 2a</ref>. The overall mAP metric increases from 32.8% to 35.2%, with an evident improvement of 2.6% for small instances and a satisfying increase of 0.8% for large counterparts. This suggests the proposed feature intertwiner could benefit both sets.</p><p>Two important modifications are incorporated based on the preliminary intertwiner framework. The first is the use of class-dependent historical representative stored in a buffer. Since there might be no large sample for the same category in one mini-batch during training, the record of all previous features of a given category for large instances is recorded by a representative, of which value gets updated dynamically as training evolves. The second is an inclusion of the optimal transport (OT) divergence as a deluxe regularization in the feature intertwiner. OT metric maps the comparison of two distributions on high-dimensional feature space onto a lower dimension space so that it is more sensible to measure the similarity between two distributions. For the feature intertwiner, OT is capable of enforcing the less reliable set to be better aligned with the reliable set.</p><p>We name the detection system equipped with the feature intertwiner as InterNet. Full code suite is available at https://github.com/hli2020/feature intertwiner. For brevity, we put the descriptions of dividing two sets in the detection task, related work (partial), background knowledge on OT theory and additional experiments in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Object detection <ref type="bibr" target="#b17">Li et al., 2018a;</ref><ref type="bibr" target="#b27">Lu et al., 2018;</ref><ref type="bibr" target="#b35">Shi et al., 2019)</ref> is one of the most fundamental computer vision tasks and serves as a precursor step for other high-level problems. It is challenging due to the complexity of features in high-dimensional space <ref type="bibr" target="#b15">(Krizhevsky et al., 2012)</ref>, the large intra-class variation and inter-class similarity across categories in benchmarks <ref type="bibr" target="#b4">(Deng et al., 2009;</ref><ref type="bibr" target="#b40">Tsung-Yi Lin, 2015)</ref>. Thanks to the development of deep networks structure <ref type="bibr" target="#b37">(Simonyan &amp; Zisserman, 2015;</ref> and modern GPU hardware acceleration, this community has witnessed a great bloom in both performance and efficiency. The detection of small objects is addressed in concurrent literature mainly through two manners. The first is by looking at the surrounding context <ref type="bibr" target="#b29">Mottaghi et al., 2014)</ref> since a larger receptive filed in the surrounding region could well compensate for the information loss on a tiny instance during down-sampling in the network. The second is to adopt a multi-scale strategy <ref type="bibr" target="#b19">(Li et al., 2018c;</ref> to handle the scale problem. This is probably the most effective manner to identify objects in various sizes and can be seen in (almost) all detectors. Such a practice is a "sliding-window" version of warping features across different stages in the network, aiming for normalizing the sizes of features for objects of different resolutions. The proposed feature intertwiner is perpendicular to these two solutions. We provide a new perspective of addressing the detection of small objects -leveraging the feature guidance from high-resolution reliable samples.</p><p>Designing loss functions for learning better features. The standard cross-entropy loss does not have the constraint on narrowing down the intra-class variation. Several works thereafter have focused on adding new constraints to the intra-class regularization. <ref type="bibr" target="#b25">Liu et al. (Liu et al., 2017a)</ref> proposed the angular softmax loss to learn angularly discriminative features. The new loss is expected to have smaller maximal intra-class distance than minimal inter-class distance. The center loss <ref type="bibr" target="#b42">(Wen et al., 2016)</ref> approach specifically learns a centroid for each class and penalizes the distances between samples within the category and the center. Our feature intertwiner shares some spirit with this work in that, the proposed buffer is also in charge of collecting feature representatives for each class. A simple modification <ref type="bibr" target="#b26">(Liu et al., 2017b)</ref> to the inner product between the normalized feature input and the class centroid for the softmax loss also decreases the inner-class variation and improves the classification accuracy. Our work is from a new perspective in using the reliable set for guiding the less reliable set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FEATURE INTERTWINERS FOR OBJECT DETECTION</head><p>In this paper, we adopt the Faster RCNN pipeline for object detection <ref type="bibr" target="#b44">2017;</ref><ref type="bibr" target="#b7">Girshick, 2015)</ref>. In Faster RCNN, the input image is first fed into a backbone network to extract features; a region proposal network <ref type="bibr" target="#b33">(Ren et al., 2015)</ref> is built on top of it to generate potential region proposals, which are several candidate rectangular boxes that might contain objects. These region proposals vary in size. Then the features inside the region are extracted and warped into the same spatial size (by RoI-pooling). Finally, the warped features are used by the subsequent CNN layers for classifying whether an object exists in the region.  <ref type="figure">Figure 2</ref>: Feature intertwinter overview. Blue blobs stands for the less reliable set (small objects) and green for the reliable set (large ones). For current level l, feature map P l of the small set is first passed into a RoI-pooling layer. Then it is fed into a make-up layer, which fuels back the information lost during RoI; it is optimized via the intertwiner module (yellow rectangle), with aid of the reliable set (green). 'OT' (in red) stands for the optimal transport divergence, which aligns information between levels (for details see Sec. 3.3). P m|l is the input feature map of the reliable set for the RoI layer; m indicates higher level(s) than l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FEATURE INTERTWINER OVERVIEW</head><p>We now explicitly depict how the idea of feature intertwiner could be adapted into the object detection framework. <ref type="figure">Fig. 2</ref> describes the overall pipeline of the proposed InterNet.</p><p>A network is divided into several levels based on the spatial size of feature maps. For each level l, we split the set of region proposals into two categories: one is the large-region set whose size is larger than the output size of RoI-pooling layer and another the small-region set whose size is smaller. These two sets corresponds to the reliable and less reliable sets, respectively. For details on the generation of these two sets in object detection, refer to Sec. 6.2 in the appendix. Feature map P l at level l is fed into the RoI layer and then passed onto a make-up layer. This layer is designed to fuel back the lost information during RoI and compensate necessary details for instances of small resolution. The refined high-level semantics after this layer is robust to factors (such as pose, lighting, appearance, etc.) despite sample variations. It consists of one convolutional layer without altering the spatial size. The make-up unit is learned and optimized via the intertwiner unit, with aid of features from the large object set, which is shown in the upstream (green) of <ref type="figure">Fig. 2</ref>.</p><p>The feature intertwiner is essentially a data distribution measurement to evaluate divergence between two sets. For the reliable set, the input is directly the outcome of the RoI layer of the large-object feature maps P m|l , which correspond to samples of higher level/resolution. For the less reliable set, the input is the output of the make-up layer. Both inputs are fed into a critic module to extract further representation of these two sets and provide evidence for intertwiner. The critic consists of two convolutions that transfer features to a larger channel size and reduce spatial size to one, leaving out of consideration the spatial information. A simple l 2 loss can be used for comparing difference between two sets. The final loss is a combination of the standard detection losses <ref type="bibr" target="#b7">(Girshick, 2015)</ref> and the intertwiner loss across all levels.</p><p>The detailed network structure of the make-up and critic module in the feature intertwiner is shown in the appendix (Sec. 6.6). There are two problems when applying the aforementioned pipeline into application. The first is that the two sets for the same category often do not occur simultaneously in one mini-batch; the second is how to choose the input source for the reliable set, i.e., feature map P m|l for the large object set. We address these two points in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CLASS BUFFER</head><p>The goal of the feature intertwiner is to have samples from less reliable set close to the samples within the same category from the reliable set. In one mini-batch, however, it often happens that samples from the less reliable set are existent while samples of the same category from the reliable set are non-existent (or vice versa). This makes it difficult to calculate the intertwiner loss between two sets. To address this problem, we use a buffer B to store the representative (prototype) for each category. Basically the representative is the mean feature representation from large instances.</p><p>Let the set of features from the large-region object on all levels be F (large) critic ; each sample consisting of the large set F be f (j) , where j is the sample index and its feature dimension is d. The buffer could be generated as a mapping from sample features to class representative:</p><formula xml:id="formula_0">B = [b 1 , . . . , b i , . . . , b Ncls ] = M F (large,1) critic , . . . , F (large,l) critic , . . . , F (large,L) critic ,<label>(1)</label></formula><formula xml:id="formula_1">b i * = M F (large,l) critic = 1 Z l,j f (large,l,j) critic , where F (large,l) critic = {f (large,l,j) critic ∈ R d },<label>(2)</label></formula><p>where the total number of classes is denoted as N cls . Each entry b i in the buffer B is referred to as the representative of class i. Every sample, indexed by j in the large object set, contributes to the class representative i * if its label belongs to i * . Here we denote i * as the label of sample j; and Z in Eqn.</p><p>(2) denotes the total number of instances whose label is i * . The representative is deemed as a reliable source of feature representation and could be used to guide the learning of the less reliable set. There are many options to design the mapping M, e.g., the weighted average of all features in the past iterations during training within the class as shown in Eqn.</p><p>(2), feature statistics from only a period of past iterations, etc. We empirically discuss different options in <ref type="table" target="#tab_4">Table 2d</ref>.</p><p>Equipped with the class buffer, we define the intertwiner loss between two sets as:</p><formula xml:id="formula_2">L inter = l,j D f (small,l,j) critic , B ,<label>(3)</label></formula><p>where D is a divergence measurement; f (small,l,j) critic denotes the semantic feature after critic of the j-th sample at level l in the less reliable set (small instances). Note that the feature intertwiner is proposed to optimize the feature learning of the less reliable set for each level. During inference, the green flow as shown in <ref type="figure">Fig. 2</ref> for obtaining the class buffer will be removed.</p><p>Discussion on the intertwiner. (a) Through such a mutual learning, features for small-region objects gradually encode the affluent details from large-region counterparts, ensuring that the semantic features within one category should be as much similar as possible despite the visual appearance variation caused by resolution change. The resolution imperfection of small instances inherited from the RoI interpolation is compensated by mimicking a more reliable set. Such a mechanism could be seen as a teacher-student guidance in the self-supervised domain </p><formula xml:id="formula_3">. (b)</formula><p>It is observed that if the representative b i is detached in back-propagation process (i.e., no backward gradient update in buffer), performance gets better. The buffer is used as the guidance for less reliable samples. As contents in buffer are changing as training evolves, excluding the buffer from network update would favorably stabilize the model to converge. Such a practice shares similar spirit of the replay memory update in deep reinforcement learning. (c) The buffer statistics come from all levels. Note that the concept of "large" and "small" is a relative term: large proposals on current level could be deemed as "small" ones on the next level. However, the level-agnostic buffer would always receive semantic features for (strictly) large instances. This is why there are improvements across all levels (large or small objects) in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CHOOSING BEST FEATURE MAP FOR LARGE OBJECTS USING OPTIMAL TRANSPORT</head><p>How to acquire the input source, denoted as P (large,l) , i.e., feature maps of large proposals, to be fed into the RoI layer on current level l? The feature maps, denoted by P l or P m , are the output of ResNet at different stages, corresponding to different resolutions. Altogether we use four stages, i.e., P 2 to P 5 ; P 2 corresponds to feature maps of the highest resolution and P 5 has the lowest resolution. The inputs are crucial since they serve as the guidance targets to be learned by small instances. There are several choices, which is depicted in <ref type="figure">Fig. 3</ref>.  Option (a): P (large,l) = P l . The most straightforward manner would be using features on current level as input for large object set. This is inappropriate since P l is trained in RPN specifically for identifying small objects; adopting it as the source could contain noisy details of small instances.</p><formula xml:id="formula_4">Optimal Transport (a) (b) (c) (d)</formula><p>Option (b):P (large,l) = P m . Here m and l denote the index of stage/level in ResNet and m &gt; l. One can utilize the higher level feature map(s), which has the proper resolution for large objects. Compared with P l , P m have lower resolution and higher semantics. For example, consider the large instances assigned to level l = 2 (how to assign large and small instances is discussed in the appendix Sec. 6.2), P m indicates three stages m = 3, 4, 5. However, among these large instances, some of them are deemed as small objects on higher level m -implying that those feature maps P m might not carry enough information. They would also have to be up-sampled during the RoI operation for updating the buffer on current level l. Take <ref type="table">Table 3</ref> in the appendix for example, among the assigned 98 proposals on level 2, there are 31 (11 on level 3 and 20 on level 4) objects that have insufficient size (smaller than RoI's output). Hence it might be inappropriate to directly use the high-level feature map as well.</p><p>Option (c): P (large,l) = P m|l F(P m ). P m is first up-sampled to match the size at P l and then is RoI-pooled with outcome denoted as P m|l . The up-sampling operation aims at optimizing a mapping F : P m → P m|l that can recover the information of large objects on a shallow level. F could be as simple as a bilinear interpolation or a neural network.</p><p>These three options are empirically reported in <ref type="table" target="#tab_2">Table 1</ref>. The baseline model in (b) corresponds to the default setting in cases 2d, 2e of <ref type="table" target="#tab_4">Table 2</ref>, where the feature intertwiner is adopted already. There is a 0.8% AP boost from option (b) to (c), suggesting that P m for large objects should be converted back to the feature space of P l . The gain from (a) to (c) is more evident, which verifies that it might not be good to use P l directly. More analysis is provided in the appendix.</p><p>Option (c) is a better choice for using the reliable feature set of large-region objects. Furthermore, we build on top of this choice and introduce a better alternative to build the connection between P l and P m|l , since the intertwiner is designed to guide the feature learning of the less reliable set on the current level. If some constraint is introduced to keep information better aligned between two sets, the modified input source P m|l for large instance would be more proper for the other set to learn.</p><p>Option (d): P (large,l) = OT(P l , P m|l ). The spirit of moving one distribution into another distribution optimally in the most effective manner fits well into the optimal transport (OT) domain <ref type="bibr" target="#b30">(Peyr &amp; Cuturi, 2018)</ref>. In this work, we incorporate the OT unit between feature map P l and P m|l , which serve as inputs before the RoI-pooling operation. A discretized version <ref type="bibr" target="#b6">(Genevay et al., 2017;</ref><ref type="bibr" target="#b2">Cuturi, 2013)</ref> of the OT divergence is employed as an additional regularization to the loss:</p><formula xml:id="formula_5">OT(P l , P m|l ) W Q (P ψ , P r ) discrete ← −−− − min P ∈R C 2 ×C 1 + Q, P ,<label>(4)</label></formula><p>where the non-positive P serves as a proxy for the coupling and satisfies P T 1 C2 = 1 C1 , P 1 C1 = 1 C2 . ·, · indicates the Frobenius dot-product for two matrices and 1 m := (1/m, . . . , 1/m) ∈ R m + . Now the problem boils down to computing P given some ground cost Q. We adopt the Sinkhorn algorithm <ref type="bibr" target="#b38">(Sinkhorn, 1964)</ref> in an iterative manner to compute W Q , which is promised to have a differentiable loss function. The OT divergence is hence referred to as Sinkhorn divergence.</p><p>Given features maps P m from higher level, the generator network F up-samples them to match the size of P l and outputs P m|l . The channel dimension of P l and P m|l is denoted as C. The critic unit H (not the proposed critic unit in the feature intertwiner) is designed to reduce the spatial dimensionality of input to a lower dimension k while keeping the channel dimension unchanged. The number of samples in each distribution is C. The outcome of the critic unit in OT module is denoted as p l , p m|l , respectively. We choose cosine distance as the measurement to calculate the distance between manifolds. The output is known as the ground cost Q x,y , where x, y indexes the sample in these two distributions. The complete workflow to compute the Sinkhorn divergence is summarized in Alg. 1. Note that each level owns their own OT module W l Q (P l , P m ) = OT(P l , P m|l ). The total loss for the detector is summarized as:</p><formula xml:id="formula_6">L = L inter + l W (l) Q (P l , P m ) + L standard ,<label>(5)</label></formula><p>where L standard is the classification and regression losses defined in most detectors <ref type="bibr" target="#b7">(Girshick, 2015)</ref>.</p><p>Why prefer OT to other alternatives. As proved in <ref type="bibr" target="#b0">(Arjovsky et al., 2017)</ref>, the OT metric converges while other variants (KL or JS divergence) do not in some scenarios. OT provides sensible cost functions when learning distributions supported by low-dim manifolds (in our case, p l and p m|l ) while other alternatives do not. As verified via experiments in <ref type="table" target="#tab_2">Table 1</ref>, such a property could facilitate the training towards a larger gap between positive and false samples. In essence, OT metric maps the comparison of two distributions on high-dimensional feature space onto a lower dimension space. The use of Euclidean distance could improve AP by around 0.5% (see <ref type="table" target="#tab_2">Table 1</ref>, (d) l 2 case), but does not gain as much as OT does. This is probably due to the complexity of feature representations in high-dimension space, especially learned by deep models.</p><p>Algorithm 1 Sinkhorn divergence W Q adapted for object detection (red rectangle in <ref type="figure">Fig.2)</ref> Input: Feature maps on current and higher levels, P l , P m The generator network F and the critic unit in OT module H Output: Sinkhorn loss W l Q (P l , P m ) = OT(P l , P m|l ) Upsample via generator P m|l = F(P m ) Feed both inputs into critic p l = H(P l ), p m|l = H(P m|l )</p><formula xml:id="formula_7">p (·) size C × k ∀(x, y) in p l , p m|l , define the ground cost Qx,y = cosine dist(p l , p m|l ) Q size C × C Initialize coefficients b (0) = 1C</formula><p>Compute Gibbs kernel Kx,y = exp(−Qx,y/ε) controlling factor ε = 0.1 for l = 0 to L do iteration budget L = 10 a (l+1) :</p><formula xml:id="formula_8">= 1 C K T b (l) , b (l+1) := 1 C K a (l)</formula><p>, known as Sinkhorn iterate end for Compute the proxy matrix P (L) = diag(b (L) ) · K · diag(a (L) ) Compute WQ based on the dot-product in Eqn. (4): Q, P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>We evaluate InterNet on the object detection track of the challenging COCO benchmark (Tsung-Yi <ref type="bibr" target="#b40">Lin, 2015)</ref>. For training, we follow common practice as in <ref type="bibr" target="#b33">(Ren et al., 2015;</ref> and use the trainval35k split (union of 80k images from train and a random 35k subset of images from 40k val split) for training. The lesion and sensitivity studies are reported by evaluating on the minival split (the remaining 5k images from val). For all experiments, we use depth 50 or 101 ResNet  with FPN (Lin et al., 2017a) constructed on top. We base the framework on Mask-RCNN  without the segmentation branch. All ablative analysis adopt austere settings: training and test image scale only at 512; no multi-scale and data augmentation (except for horizontal flip). Details on the training and test procedure are provided in the appendix (Sec. 6.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ABLATION STUDY ON INTERTWINER MODULE</head><p>Baseline comparison. <ref type="table" target="#tab_4">Table 2a</ref> lists the comparison of InterNet to baseline, where both methods shares the same setting. On average it improves by 2 points in terms of mAP. The gain for small objects is much more evident. Note that our method also enhances the detection of large objects (by 0.8%), since the last level also participates in the intertwiner update by comparing its similarity feature to the history buffer, which requires features of the same category to be closer to each other. The last level does not contribute to the buffer update though.</p><p>Assignment strategy (analysis based on Sec. 6.2). Table 2a also investigates the effect of different region proposal allocations. 'by RoI size' divides proposals whose area is below the RoI threshold in <ref type="table">Table 3</ref> as small and above as large; 'more on higher' indicates the base value in Eqn. (6) is smaller (=40); the default setting follows  where the base is set to 224. Preliminary, we think putting more proposals on higher levels (the first two cases) would balance the workload of the intertwiner; since the default setting leans towards too many proposals on level 2. However, there is no gain due to the mis-alignment with RPN training. The distribution of anchor templates in RPN does not alter accordingly, resulting in the inappropriate use of backbone feature maps. <ref type="table" target="#tab_4">Table 2b</ref> shows a factor of 1.0 to be merged on the total loss whereas lower block depicts a specific factor that achieves better AP than others. The simple l 2 loss achieves slightly better than the KL divergence, where the latter is computed as L inter = b · log(b/f ). The l 1 option is by around 1 point inferior than these two and yet still verifies the effectiveness of the intertwiner module compared with baseline (34.2 vs 32.8) -implying the generalization ability of our method in different loss options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intertwinter loss. Upper block in</head><p>How does the intertwiner module affect learning? By measuring the divergence between two sets (i.e., small proposals in the batch and large references in the buffer), we have gradients, as the influence, back-propagated from the critic to make-up layer. In the end, the make-up layer is optimized to enforce raw RoI outputs recovering details even after the loss from reduced resolution. The naive design denoted by 'separate' achieves 34.0% AP as shown in  influence of the intertwiner stronger, we linearly combine the features after critic with the original detection feature (with equal weights, aka 0.5; not shown in <ref type="figure">Fig. 2</ref>) and feed this new combination into the final detection heads. This improves AP by 1 point (denoted as 'linear' in <ref type="table" target="#tab_3">Table 2c</ref>). The 'naive add' case with equal weights 1 does not work (loss suddenly explodes during training), since the amplitude of features among these two sources vary differently if we simply add them.</p><p>Does buffer size matter? <ref type="table" target="#tab_4">Table 2d</ref> shows that it does not. A natural thought could be having a window size of K and sliding the window to keep the most recent features recorded. In general, larger size improves performance (see case '2000' vs the size of 'one epoch' where batch size is 8, 37.3% → 38.8%). In these cases, statistics of large object features for one category cannot reflect the whole training set and it keeps alternating as network is updated. Using 'all history' data by running averaging not only saves memory but also has the whole picture of the data. Preliminary, we choose a decayed scheme that weighs more to recent features than ones in the long run, hoping that the model would be optimized better as training evolves. However, experiments does not accord with such an assumption: AP is better where features are equally averaged (c.f., 40.5% and 39.2%) in terms of network evolution.</p><p>Unified or level-based buffer? Unified. <ref type="table" target="#tab_4">Table 2e</ref> upper block reports such a perspective. In early experiments, we only have one unified buffer in order to let objects on the last level also involved in the intertwiner. Besides, the visual features of large objects should be irrelevant of scale variation. This achieves a satisfying AP already. We also try applying different buffers on each level 3 . The performance improvement is slight, although the additional memory cost is minor.</p><p>Other investigations. As discussed at the end of Sec. 3.1, detaching buffer transaction from gradient update attracts improvement (40.5% vs 40.1% in <ref type="table" target="#tab_4">Table 2e</ref>). Moreover, we tried imposing stronger supervision on the similarity feature of large proposals by branching out a cross-entropy loss, for purpose of diversifying the critic outputs among different categories. However, it does not work and this additional loss seems to dominate the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COMPARISON TO STATE-OF-THE-ARTS</head><p>Performance. We list a comparison of our InterNet with previous state-of-the-arts in <ref type="table">Table 4</ref> in the appendix. Without multi-scale technique, ours (42.5%) still favorably outperforms other two-stage detectors (e.g., <ref type="bibr">39</ref>.2%) as well as one-stage detector (SSD, 31.2%). Moreover, we showcase in <ref type="figure" target="#fig_2">Fig. 4</ref> the per-class improvement between the baseline and the improved model after adopting feature intertwiner in  distinct drop for the 'couch' class, we find that for a large couch among samples on COCO, usually there sit a bunch of people, stuff, pets, etc. And yet the annotations in these cases would cover the whole scenario including these noises, making the feature representation of the large couch quite inaccurate. The less accurate features would guide the learning of their small counterparts, resulting in a lower AP for this class.</p><p>Model complexity and timing. The feature intertwiner only increases three light-weight conv. layers at the make-up and critic units. The usage of class buffer could take up a few GPU memory on-the-fly; however, since we adopt an 'all-history' strategy, the window size is just 1 instead of a much larger K. The additional cost to the overall model parameters is also from the OT module for each level; however, we find using just one conv. layer for the critic H and two conv. layers with small kernels for generator F is enough to achieve good result. Training on 8 GPUs with batch size of 8 takes around 3.4 days; this is slower than Mask-RCNN reported in . The memory cost on each card is 9.6 GB, compared with baseline 8.3 GB. The inference runs at 325ms per image (input size is 800) on a Titan Pascal X, increasing around 5% time compared to baseline (308 ms). We do not intentionally optimize the codebase, however.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose a feature intertwiner module to leverage the features from a more reliable set to help guide the feature learning of another less reliable set. This is a better solution for generating a more compact centroid representation in the high-dimensional space. It is assumed that the high-level semantic features within the same category should resemble as much as possible among samples with different visual variations. The mutual learning process helps two sets to have closer distance within the cluster in each class. The intertwiner is applied on the object detection task, where a historical buffer is proposed to address the sample missing problem during one mini-batch and the optimal transport (OT) theory is introduced to enforce the similarity among the two sets.</p><p>Since the features in the reliable set serve as teacher in the feature learning, careful preparation of such features is required so that they would match the information in the small-object set. This is why we design different options for the large set and finally choose OT as a solution. With aid of the feature intertwiner, we improve the detection performance by a large margin compared to previous state-of-the-arts, especially for small instances.</p><p>Feature intertwiner is positioned as a general alternative to feature learning. As long as there exists proper division of one reliable set and the other less reliable set, one can apply the idea of utilizing the reliable set guide the feature learning of another, based on the hypothesis that these two sets share similar distribution in some feature space. One direction in the future work would be applying feature intertwiner into other domains, e.g., data classification, if proper set division are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">MORE RELATED WORK</head><p>Self-supervised learning. The buffer in the feature intertwiner can be seen as utilizing non-visual domain knowledge on a set of data to help supervise the feature learning for another set in highdimensional space. Such a spirit falls into the self-supervised learning domain. In , Chen et al. proposed a knowledge distillation framework to learn compact and accurate object detectors. A teacher model with more capacity is designed to provide strong information and guide the learning of a lite-weight student model. The center loss <ref type="bibr" target="#b42">(Wen et al., 2016)</ref> is formulated to learn a class center and penalize samples that have a larger distance with the centroid. It aims at enlarging inter-class resemblance with cross-entropy (CE) loss as well as narrowing down innerclass divergence for face recognition. In our work, the feature intertwiner gradually aggregates statistics of a meta-subset and utilizes them as targets during the feature learning of a less accurate (yet holding a majority) subset. We are inspired by the proposal-split mechanism in object detection domain to learn recognition at separate scales in the network. The self-paced learning framework <ref type="bibr" target="#b16">(Kumar et al., 2010</ref>) deals with two sets as well, where the easy examples are first introduced to optimize the hidden variable and later on during training, the hard examples are involved. There is no interaction between the two sets. The division is based on splitting different samples. In our framework, the two sets mutually help and interact with each other. The goal is towards optimizing a more compact class centroid in the feature space. These are two different branches of work.</p><p>Optimal transport (OT) has been applied in two important tasks. One is for transfer learning in the domain adaption problem. <ref type="bibr" target="#b28">Lu et al. (Lu et al., 2017)</ref> explored prior knowledge in the cost matrix and applied OT loss as a soft penalty for bridging the gap between target and source predictions.</p><p>Another is for estimating generative models. In <ref type="bibr" target="#b34">(Salimans et al., 2018)</ref>, a metric combined with OT in primal form with an energy distance results in a highly discriminative feature representation with unbiased gradients. Genevay et al. <ref type="bibr" target="#b6">(Genevay et al., 2017)</ref> presents the first tractable method to train large-scale generative models using an OT-based loss. We are inspired by these works in sense that OT metric is favorably competitive to measure the divergence between two distributions supported on low-dimensional manifolds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">ASSIGNMENT OF LARGE AND SMALL SETS IN OBJECT DETECTION</head><p>In this paper we adopt the ResNet model  with feature pyramid dressings ) constructed on top. It generates five levels of feature maps to serve as inputs for the subsequent RPN and detection branches. Denote the level index as l = {1, . . . , 5} and the corresponding feature maps as P l . Level l = 1 is the most shallow stage with more local details for detecting tiny objects and level l = 5 is the deepest stage with high-level semantics.</p><p>Let A = {a j } denote the whole set of proposals generated by RPN from l 2 to l 6 (level six is generated from l 5 , for details refer to ). The region proposals are divided into different levels from l 2 to l 5 :</p><formula xml:id="formula_9">a (l) j → l = a 0 + log( Area(a j )/base),<label>(6)</label></formula><p>where a 0 =4 as in ; base=224 is the canonical ImageNet pre-training setting. <ref type="table">Table 3</ref> shows a detailed breakdown 4 of the proposal allocation based on Eqn. (6). We can see most proposals from RPN focus on identifying small objects and hence are allocated at shallow level l = 2. The threshold is set to be the ratio of RoI output's area over the area of feature map. For example, threshold on l = 3 is obtained by (14/64) 2 , where 14 is the RoI output size as default setting. Proposals whose area is below the threshold suffer from the inherent design during RoI operation -these feature outputs are up-sampled by a simple interpolation. The information of small regions is already lost and RoI layer does not help much to recover them back. As is shown on the fourth row ("below # / above #"), such a case holds the majority. This observation brings in the necessity of designing a meta-learner to provide guidance on feature learning of small objects due to the loophole during the RoI layer.  <ref type="table">Table 3</ref>: Proposal assignment on each level before RoI operation. 'below #' indicates how many proposals are there whose size is below the size of RoI output. 'intertwiner large #' stands for how many proposals are used for supervising the learning of small objects.</p><p>For level l in the network, we define small proposals (or RoIs) to be those already assigned by <ref type="formula" target="#formula_9">(6)</ref> and large to be those above l:</p><formula xml:id="formula_10">a (l,s) ← a (l) j , a (l,b) = m&gt;l a (m) j ,<label>(7)</label></formula><p>where the superscript s,b denotes the set of small and large proposals, respectively. The last two rows in <ref type="table">Table 3</ref> show an example of the assignment. These RoIs are then fed into the RoI-pooling layer 5 to generate output features maps for the subsequent detection pipeline to process.</p><p>One may wonder the last level do not have large objects for reference based on Eqn. <ref type="formula" target="#formula_10">(7)</ref>. In preliminary experiments, leaving proposals on the last level out of the intertwiner could already improve the overall performance; however, if the last level is also involved (since the buffer is shared across all levels), AP for large objects also improves. See the experiments in Sec. 4.1 for detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">SINKHORN DIVERGENCE</head><p>Let u , u indicate the individual sample after degenerating high-dimensional features P m|l , P l from two spaces into low manifolds. u , u are vectors of dimension k. The number of samples in these two distributions is denoted by C 1 and C 2 , respectively. The OT metric between two joint probability distributions supported on two spaces (U, U) is defined as the solution of the linear program <ref type="bibr" target="#b2">(Cuturi, 2013)</ref>. Denote the data and reference distribution as P ψ , P r ∈ Prob(U) 6 , respectively, we have the continuous form of OT divergence:</p><formula xml:id="formula_11">W Q (P ψ , P r ) = inf γ∈Γ(P ψ ,Pr) E U ×U Q(u , u)dγ(u , u) ,<label>(8)</label></formula><p>where γ is a coupling; Γ is the set of couplings that consists of joint distributions.</p><p>Intuitively, γ(u , u) implies how much "mass" must be transported from u to u in order to transform the distribution P ψ into P r ; Q is the "ground cost" to move a unit mass. Eqn. (8) above becomes the p-Wasserstein distance (or loss, divergence) between probability measures when U is equipped with a distance D U and Q = D U (u , u) p , for some exponent p.</p><p>The biased version of Sinkhorn divergence used in <ref type="table" target="#tab_2">Table 1</ref> is defined by:</p><formula xml:id="formula_12">2W Q (P ψ , P r ) − W Q (P r , P r ) − W Q (P ψ , P ψ ).</formula><p>More analysis on <ref type="table" target="#tab_2">Table 1</ref>. All these options have been discussed explicitly at the beginning of Sec. 3.3. Option (a) is inferior due to the inappropriateness of feature maps; (b) serves as the baseline and used as the default setting in <ref type="table" target="#tab_4">Table 2</ref>. Options in (c) verifies that up-sampling feature maps from higher-level onto current level is preferable; F being a neural net ensures better improvement. Options in (d) illustrates the case where a supervision signal is imposed onto pair (P l , P m|l ) to make better alignment between them. We can observe that OT outperforms other variants in this setup. Moreover, we tried a biased version <ref type="bibr" target="#b6">(Genevay et al., 2017)</ref> of the Sinkhorn divergence. However, it does not bring in much gain compared to the previous setup. Besides, it could burden system efficiency during training (although it is minor considering the total time per iteration). Such a phenomenon could result from an improper update of critic and generator inside the OT module, since the gradient flow would be iterated twice more for the last two terms above.</p><p>Extending OT divergence to image classification. We also testify OT divergence on CIFAR-10 <ref type="bibr" target="#b14">(Krizhevsky &amp; Hinton, 2009)</ref> where feature maps between stages are aligned via OT. Test error decreases by around 1.3%. This suggests the potential application of OT in various vision tasks. Different from OT in generative models, we deem the channel dimension as different samples to compare, instead of batch-wise manner as in <ref type="bibr" target="#b34">(Salimans et al., 2018)</ref>; and treat the optimization of F and H in a unified min problem, as opposed to the adversarial training <ref type="bibr" target="#b6">(Genevay et al., 2017)</ref>.</p><p>6.4 COMPARISON TO STATE-OF-THE-ARTS ON COCO AND PASCAL VOC <ref type="table">Table 4</ref> reports the performance of our model compared with other state-of-the-arts on COCO dataset. We can observe that it outperforms all previous one-stage or two-stage detectors by a large margin. The multi-scale technique bundled with data augmentation increases detection accuracy in a more evident manner, which is commonly adopted in most detectors. The updated result in Mask-RCNN is reported as well. It increases the original performance from 38.2% to 43.5% by switching the backbone structure to ResNetX, an updated baseline model, ImageNet-5k pre-training and train-time augmentation. It is better than ours (42.5% without multi-scale version). This is probably mainly due to the change of network structure. Our multi-scale version (44.2%) is better than the updated Mask-RCNN result, however. To further verify the effectiveness of the feature intertwiner, we further conduct experiments on the PASCAL VOC 2007 dataset. The results are shown in <ref type="table">Table 5</ref>. Two network structures are adopted.</p><p>For ResNet-101, the division of the four levels are similar as ResNet-101-FPN on COCO; for VGG-16, we take the division similarly as stated in SSD . Specifically, the output of layer 'conv7', 'conv8 2', 'conv9 2' and 'conv10 2' are used for P 2 to P 5 , respectively. Our method performs favorably against others in both backbone structures on the PASCAL dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">TRAINING AND TEST DETAILS</head><p>We adopt the stochastic gradient descent as optimizer. Initial learning rate is 0.01 with momentum 0.9 and weight decay 0.0001. Altogether there are 13 epoches for most models where the learning rate is dropped by 90% at epoch 6 and 10. We find the warm-up strategy <ref type="bibr" target="#b8">(Goyal et al., 2017)</ref> barely improves the performance and hence do not adopt it. The gradient clip is introduced to prevent training loss to explode in the first few iterations, with maximum gradient norm to be 5. Batch size is set to 8 and the system is running on 8 GPUs.</p><p>The object detector is based on <ref type="bibr">Mask-RCNN (or Faster-RCNN)</ref>. RoIAlign is adopted for better performance. The model is initialized with the corresponding ResNet model pretrained on ImageNet.</p><p>The new proposed feature intertwiner module is trained from scratch with standard initialization. The basic backbone structure for extracting features is based on FPN network , where five ResNet blocks are employed with up-sampling layers. The region proposal network consists of one convolutional layer with one classification and regression layer. The classifier structure is similar as RPN's -one convolution plus one additional classification/regression head.</p><p>Non-maximum suppression (NMS) is used during RPN generation and detection test phase. Threshold for RPN is set to 0.7 while the value is 0.3 during test. We do not adopt a dense allocation of anchor templates as in some literature ; each pixel on a level only has the number of anchors the same as the number of aspect ratios (set to 0.5, 1 and 2). Each level l among the five stages owns a unique anchor size: 32, 64, 128, 256, and 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">NETWORK STRUCTURE IN FEATURE INTERTWINER</head><p>The detailed network architecture on the make-up layer and critic layer are shown below.</p><p>Output size Layers in the make-up module B × C l × 14 × 14 conv2d(C l , C l , k = 3, padding = 1) B × C l × 14 × 14 batchnorm2d(C l ) B × C l × 14 × 14 relu(·) <ref type="table">Table 6</ref>: Network structure of the make-up unit, which consists of one convolutional layer without altering the spatial size. Input: RoI output of the small-set feature map P l . We denote the output of the make-up layer as P l . B is the batch size in one mini-batch; C l is the number of channels after the feature extractor in ResNet blocks for each level. For example, when l = 2, C l = 256, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output size</head><p>Layers in the critic module B × 512 × 7 × 7 conv2d(C l , 512, k = 3, padding = 1, stride = 2) B × 512 × 7 × 7 batchnorm2d(512) B × 512 × 7 × 7 relu(·) B × 1024 × 1 × 1 conv2d(512, 1024, k = 7) B × 1024 × 1 × 1 batchnorm1d(1024) B × 1024 × 1 × 1 relu(·) B × 1024 × 1 × 1 sigmoid(·) <ref type="table">Table 7</ref>: Network structure of the critic unit. Input: for large set, it is the RoI output of the large-set feature map P m|l and for small set, it is the output of the make-up layer P l . B is the batch size in one mini-batch; C l is the number of channels in ResNet blocks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(Zoom in for better view) Visualization of the features in object detection using t-SNE (van der Maaten &amp; Hinton, 2008) (a) without and (b) with feature intertwiner on COCO. Each point is a sample mapped onto the low-dim manifold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Improvement per category after embedding the feature intertwiner on COCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Numeric results on different input sources for the reliable set (using ResNet-101-FPN model). F is the up-sampling layer; we use option (d), OT as the final candidate. The biased version of optimal transport is detailed in appendix, Sec. 6.3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2c .</head><label>2c</label><figDesc>To further make the proposal split AP AP50 AP75 APS APM APL</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AP AP50 AP75</cell></row><row><cell></cell><cell></cell><cell>by RoI size 30.9</cell><cell>53.7</cell><cell>35.1</cell><cell>10.8</cell><cell>34.7</cell><cell>46.6</cell><cell></cell><cell cols="2">l1 34.2</cell><cell>57.1</cell><cell>37.2</cell></row><row><cell>baseline</cell><cell cols="2">more on higher 31.3</cell><cell>54.0</cell><cell>35.8</cell><cell>11.4</cell><cell>35.1</cell><cell>47.5</cell><cell></cell><cell cols="2">l2 (default) 35.2</cell><cell>57.6</cell><cell>38.0</cell></row><row><cell></cell><cell></cell><cell>default  *  32.8</cell><cell>55.3</cell><cell>37.2</cell><cell>12.7</cell><cell>36.8</cell><cell>49.3</cell><cell></cell><cell cols="2">KL div 34.6</cell><cell>57.8</cell><cell>37.4</cell></row><row><cell></cell><cell></cell><cell>by RoI size 33.7</cell><cell>56.1</cell><cell>37.6</cell><cell>13.5</cell><cell>37.4</cell><cell>50.8</cell><cell></cell><cell cols="2">l1 (fac=10) 34.4</cell><cell>57.6</cell><cell>37.8</cell></row><row><cell>intertwiner</cell><cell cols="2">more on higher 32.3</cell><cell>55.7</cell><cell>37.1</cell><cell>12.9</cell><cell>36.2</cell><cell>49.5</cell><cell></cell><cell cols="2">l2 (fac=0.1) 34.8</cell><cell>58.0</cell><cell>37.5</cell></row><row><cell></cell><cell></cell><cell>default  *  *  35.2</cell><cell>57.6</cell><cell>38.0</cell><cell>15.3</cell><cell>38.7</cell><cell>51.1</cell><cell cols="3">KL div (fac=10) 35.6</cell><cell>58.2</cell><cell>38.01</cell></row><row><cell cols="8">increase from  *  to  *  *  +2.4 +2.1 +0.8 +2.6 +1.9 +0.8</cell><cell></cell><cell></cell></row><row><cell cols="8">(a) Baseline and proposal assignment strategy: intertwiner in-</cell><cell cols="3">(b) Feature intertwiner loss: upper</cell></row><row><cell cols="8">creases detection of both small and large objects compared to base-</cell><cell cols="3">block uses a factor of 1.0. l2 performs</cell></row><row><cell cols="8">line. Putting more proposals on lower level brings more gain.</cell><cell cols="3">slightly better than KL divergence.</cell></row><row><cell></cell><cell cols="2">AP AP50 AP75</cell><cell></cell><cell cols="5">size/weight AP AP50 AP75</cell><cell></cell><cell>yes? AP AP50 AP75</cell></row><row><cell cols="3">separate 34.0 57.1 37.3 naive add -fail -</cell><cell>partial</cell><cell cols="3">2000 37.3 15k (epoch) 38.8</cell><cell>58.5 59.9</cell><cell>44.7 46.1</cell><cell>multiple B</cell><cell>40.58 62.83 47.62 40.54 62.81 47.61</cell></row><row><cell cols="2">linear 35.2</cell><cell>57.6 38.0</cell><cell>all history</cell><cell cols="3">decay weight 39.2 equal weight 40.5</cell><cell>60.6 62.8</cell><cell>45.4 47.6</cell><cell>detach bi</cell><cell>40.5 40.1</cell><cell>62.8 62.4</cell><cell>47.6 47.3</cell></row><row><cell cols="3">(c) Boosted detection fea-</cell><cell cols="6">(d) Buffer choice design (101-layer):</cell><cell cols="2">(e) Workflow design (101-layer):</cell></row><row><cell cols="3">ture source: merging fcritic</cell><cell cols="6">buffer taking in all history with equal</cell><cell cols="2">applying different buffers on each</cell></row><row><cell cols="3">into the detection folllow-</cell><cell cols="6">weight ensures best accuracy. Longer size</cell><cell cols="2">level barely matters; detaching bi</cell></row><row><cell cols="3">up pipeline increases result.</cell><cell cols="6">in 'partial' block enhances result and yet</cell><cell cols="2">during back-propagation is better.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">possesses more parameters.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Ablation study on the component design of feature intertwiner. Gray background is the final default setting adopted in each case. Network is either ResNet-50-FPN or 101.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2a</head><label>2a</label><figDesc>(two gray rows). The most improved classes are microwave, truck while the results in couch, bat decrease. Most small-size categories get improved. As for the</figDesc><table><row><cell></cell><cell>0.14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Change in AP</cell><cell>0.04 0.06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-0.04</cell><cell>microwave</cell><cell>truck</cell><cell>cow</cell><cell>car</cell><cell>zebra</cell><cell>sheep</cell><cell>hair drier</cell><cell>remote</cell><cell>bench</cell><cell>fork</cell><cell>dog</cell><cell>dining table</cell><cell>wine glass</cell><cell>book</cell><cell>bird</cell><cell>sink</cell><cell>skis</cell><cell>backpack</cell><cell>parking meter</cell><cell>bear</cell><cell>teddy bear</cell><cell>orange</cell><cell>bicycle</cell><cell>banana</cell><cell>motorcycle</cell><cell>elephant</cell><cell>bottle</cell><cell>handbag</cell><cell>cup</cell><cell>clock</cell><cell>donut</cell><cell>refrigerator</cell><cell>cell phone</cell><cell>frisbee</cell><cell>potted plant</cell><cell>scissors</cell><cell>cat</cell><cell>umbrella</cell><cell>carrot</cell><cell>baseball glove</cell><cell>train</cell><cell>giraffe</cell><cell>boat</cell><cell>skateboard</cell><cell>traffic light</cell><cell>keyboard</cell><cell>surfboard</cell><cell>laptop</cell><cell>cake</cell><cell>pizza</cell><cell>bowl</cell><cell>person</cell><cell>toothbrush</cell><cell>suitcase</cell><cell>stop sign</cell><cell>airplane</cell><cell>oven</cell><cell>vase</cell><cell>apple</cell><cell>toaster</cell><cell>tie</cell><cell>kite</cell><cell>chair</cell><cell>spoon</cell><cell>tennis racket</cell><cell>snowboard</cell><cell>sports ball</cell><cell>fire hydrant</cell><cell>horse</cell><cell>bus</cell><cell>tv</cell><cell>toilet</cell><cell>mouse</cell><cell>knife</cell><cell>hot dog</cell><cell>bed</cell><cell>sandwich</cell><cell>broccoli</cell><cell>baseball bat</cell><cell>couch</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :Table 5 :</head><label>45</label><figDesc>Object detection single-model performance (bounding box AP) on the COCO test-dev. We show two versions of InterNet that incorporates both the feature intertwiner module and OT agreement. The latter is achieved with data augmentation, 1.5× longer training time and multi-scale training. 'F-R-CNN' stands for Faster R-CNN. Our InterNet is also a two-stage detector. Comparison of our model with feature intertwiner to other methods on PASCAL VOC 2007 test set. Here we adopt two backbone options: ResNet-101 and VGG-16 without FPN to fairly compare with others. The number of levels is 4, the same as on COCO benchmark.</figDesc><table><row><cell>Model Structure</cell><cell cols="2">Training data mAP</cell></row><row><cell>Fast R-CNN (Girshick, 2015) VGG-16</cell><cell>07</cell><cell>66.9</cell></row><row><cell>Faster R-CNN (He et al., 2016) VGG-16</cell><cell>07</cell><cell>69.9</cell></row><row><cell>SSD512 (Liu et al., 2015) VGG-16</cell><cell>07</cell><cell>71.6</cell></row><row><cell>InterNet (ours) VGG-16</cell><cell>07</cell><cell>73.1</cell></row><row><cell cols="2">Faster R-CNN (He et al., 2016) ResNet-101 07+12</cell><cell>76.4</cell></row><row><cell cols="2">R-FCN (Dai et al., 2016) ResNet-101 07+12</cell><cell>80.5</cell></row><row><cell cols="2">InterNet (ours) ResNet-101 07+12</cell><cell>82.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In such case, the last level adopts the buffer on level 2 since it contains the most number of large objects.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Each sample has 200 proposals with input size being 512. Batch size is 2, resulting in 400 proposals in total. Statistics are averaged per iteration, based on the output of RPN network during training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In this paper, we opt for the RoIAlign option in the RoI layer; one can resort to other options nonetheless. We use term RoI layer, RoI-pooling layer, RoI operation, to refer to the same process. 6 Prob(U) is the set of probability distributions over a metric space U.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Buyu Li for helpful comments in a preliminary version of this work. H. Li and S. Shi are supported by the Hong Kong PhD Fellowship Scheme. This project is also supported by the Research Grants Council of Hong Kong under grant CUHK14208417, CUHK14202217, and the Hong Kong Innovation and Technology Support Programme Grant ITS/121/15FX.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno>arXiv preprint:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning Efficient Object Detection Models with Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guobin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kaiming He, and Jian Sun. R-FCN: Object Detection via Region-based Fully Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">DSSD : Deconvolutional Single Shot Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>arXiv preprint:1701.06659</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning generative models with sinkhorn divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<idno>arXiv preprint:1706.00292</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno>arXiv preprint:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On loss functions for deep neural networks in classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katarzyna</forename><surname>Janocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">Marian</forename><surname>Czarneck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Pawan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gradient Harmonized Single-stage Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05181</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural network encapsulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Zoom out-and-in network with map attention decision for region proposal and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="225" to="238" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finding Task-relevant Features for Few-shot Learning by Category Traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Attentive Contexts for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno>arXiv preprint:1603.07415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollr. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<idno>arXiv preprint:1512.02325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking feature discrimination and polymerization for large-scale recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12030</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Grid R-CNN. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Optimal transport for deep joint transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Saidi</surname></persName>
		</author>
		<idno>arXiv preprint:1709.02995</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Computational optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<idno>arXiv preprint:1803.00567</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>arXiv preprint:1612.08242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving GANs using optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Beyond skip connections: Top-down modulation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno>arXiv preprint:1612.06851</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A relationship between arbitrary positive matrices and doubly stochastic matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sinkhorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="876" to="879" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename></persName>
		</author>
		<imprint>
			<pubPlace>Bourdev Ross Girshick James Hays Pietro Perona Deva Ramanan C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence Zitnick Piotr Dollar Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<idno>arXiv preprint:1405.0312</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2016. backbone AP AP50 AP75 APS APM APL One-stage detector YOLOv2</title>
		<imprint>
			<publisher>Redmon &amp; Farhadi</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssd513 (liu</surname></persName>
		</author>
		<idno>ResNet-101-SSD 31.2 50.4 33.3 10.2 34.5 49.8</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dssd513 (fu</surname></persName>
		</author>
		<idno>ResNet-101-DSSD 33.2 53.3 35.2 13.0 35.4 51.1</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Two-Stage Detector F-R-Cnn+++ (he</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="101" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F-R-Cnn W Fpn (</forename><surname>Lin</surname></persName>
		</author>
		<idno>ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F-R-Cnn By G-Rmi (</forename><surname>Huang</surname></persName>
		</author>
		<idno>34.7 55.5 36.7 13.5 38.1 52.0</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Incept.-ResNet-v2</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F-R-Cnn W Tdm (</forename><surname>Shrivastava</surname></persName>
		</author>
		<idno>36.8 57.7 39.2 16.2 39.8 52.1</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">101</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Incept.-ResNet-v2-TDM</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rcnn (</forename><surname>Mask</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<idno>ResNet-101-FPN 38.2 60.3 41.7 20.1 41.1 50.2</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Retinanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno>ResNet-101-FPN 39.1 59.1 42.3 21.8 42.7 50.2</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rcnn ;</forename><surname>Mask</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<idno>101-FPN 42.5 65.1 49.4 25.4 46.6 54.3</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ResNetX-101-FPN 43.5 65.9 47.2 ---InterNet (ours) ResNet-</note>
	<note>updated in</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Internet</surname></persName>
		</author>
		<idno>ours) multi-scale ResNet-101-FPN 44.2 67.5 51.1 27.2 50.3 57.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
