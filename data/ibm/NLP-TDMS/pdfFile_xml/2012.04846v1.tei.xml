<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SnapMix: Semantically Proportional Mixing for Augmenting Fine-grained Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
							<email>shaoli.huang@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
							<email>xinchao.wang@stevens.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stevens Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SnapMix: Semantically Proportional Mixing for Augmenting Fine-grained Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data mixing augmentation has proved effective in training deep models. Recent methods mix labels mainly based on the mixture proportion of image pixels. As the main discriminative information of a fine-grained image usually resides in subtle regions, methods along this line are prone to heavy label noise in fine-grained recognition. We propose in this paper a novel scheme, termed as Semantically Proportional Mixing (SnapMix), which exploits class activation map (CAM) to lessen the label noise in augmenting fine-grained data. Snap-Mix generates the target label for a mixed image by estimating its intrinsic semantic composition, and allows for asymmetric mixing operations and ensures semantic correspondence between synthetic images and target labels. Experiments show that our method consistently outperforms existing mixed-based approaches on various datasets and under different network depths. Furthermore, by incorporating the mid-level features, the proposed SnapMix achieves top-level performance, demonstrating its potential to serve as a solid baseline for fine-grained recognition. Our code is available at https://github.com/Shaoli-Huang/SnapMix.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Despite the remarkable success of deep neural networks, its overfitting problem persists, particularly in encountering limited training data. Data Augmentation methods can alleviate this by effectively exploiting existing data. Among them, mixing-based methods <ref type="bibr" target="#b19">(Tokozume, Ushiku, and Harada 2018;</ref><ref type="bibr" target="#b9">Inoue 2018;</ref><ref type="bibr" target="#b31">Zhang et al. 2018;</ref><ref type="bibr" target="#b30">Yun et al. 2019)</ref> have recently gained increasing attention. These approaches generate new data by blending images and fusing their labels according to the statistics of mixed pixels. For instance, Mixup <ref type="bibr" target="#b31">(Zhang et al. 2018</ref>) combines images linearly and mix their targets using the same combination coefficients. CutMix <ref type="bibr" target="#b30">(Yun et al. 2019</ref>), on the other hand, cuts out one image area, pastes it on another image, and mix their labels according to the area proportion. By extending the training distribution, mixing-based techniques reduce memorizing data and improve model generalization.</p><p>However, their superiority decreases with the increasing risk of label noise in augmenting fine-grained data. In fine-grained object recognition, discriminative information Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. mainly lies in some small regions of images. Mixing labels based on mixture pixel-based statistics such as area size, therefore, tends to introduce severe label noise in this task. In the example of <ref type="figure">Fig. 1</ref>, CutMix cuts out a small region covering critical information about the label, in this case a red shoulder and yellow wing bar of the red-winged blackbird. The remaining part of the image, as a result, are left with only much less informative image evidences, yet still take up a high coefficient due to its large area size. This indicates that mixing labels based on area proportion is not able to effectively reflect intrinsic semantic composition of the combined image, thereby deteriorating the data augmentation effectiveness and confusing the model training.</p><p>In this paper, we propose a novel Semantically Proportional Mixing (SnapMix) strategy to address this issue. SnapMix exploits a class activation map (CAM)  to estimate the label composition of the mixedimages. Specifically, by normalizing the CAM of each image to sum to 1, we first obtain its Semantic Percent Map (SPM) to quantify the relatedness percentage between each pixel and the label, and then compute the semantic ratio of any image region by summing values in the corresponding area of the SPM. For an image composed of multiple areas from multiple images, we can estimate its semantic composition through the semantic ratios corresponding to these regions. Compared to methods based on statistics of mixture pixels, our label mixing strategy incorporates neural activations as prior knowledge to ensure the semantic correspondence between the synthetic images and the generated supervision signals.</p><p>Moreover, existing techniques rely on symmetrically blending image regions, meaning that the selected areas to be mixed are restricted to be complementary, and hence limit the diversity of augmented data. By contrast, the proposed approach enables asymmetric cut-and-paste operations, allowing us to incorporate various factors such as deformation and scale into the data augmentation to boost the data diversity. The current label-mixing strategies are designed based on the complementary principle. Thus they are not suitable for the asymmetric operation that selects noncomplementary regions to mix.</p><p>To validate the proposed approach, we adopt various network architectures <ref type="bibr">(Resnet-18,34,50,101 (He et al. 2016</ref> </p><formula xml:id="formula_0">= = 0.4 0.4 = ! ! ! " ! ! ! "</formula><p>Figure 1: Comparison of Mixup, CutMix, and SnapMix. The figure gives an example where SnapMix's generated label is visually more consistent with the mixed image's semantic structure comparing to CutMix and Mixup.</p><p>augmentation approaches on three fine-grained datasets. Results indicate that prior methods lead to unstable performances, sometimes even harmful, when using shallow network architecture. This can be in part explained by the fact shallow neural networks are not able to well tackle label noises, which significantly degrade data augmentation effectiveness. The proposed method, on the other hand, consistently outperforms compared methods on various datasets and with different network depths. Furthermore, we show that even a simple model can achieve comparable state-ofthe-art performance when applying our proposed data augmentation. This indicates that our method can well serve as a solid baseline for advancing fine-grained recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-Grained Classification</head><p>Fine-grained recognition has been an active research area in recent years. This task is more challenging than general image classification <ref type="bibr" target="#b13">(Liu and Tao 2016;</ref><ref type="bibr" target="#b21">Wang, Li, and Tao 2011;</ref><ref type="bibr">Yang et al. 2020b,a)</ref>, as the critical information to distinguish categories usually lies in subtle object parts. Part-based methods thereby, are extensively explored to address the problem. Early works <ref type="bibr" target="#b33">Zhang et al. 2014;</ref><ref type="bibr" target="#b23">Xiao et al. 2015;</ref><ref type="bibr" target="#b12">Lin et al. 2015;</ref><ref type="bibr" target="#b24">Xu et al. 2015</ref><ref type="bibr" target="#b25">Xu et al. , 2016</ref> mainly rely on strongly supervised learning to localize object part for subsequent feature learning. Due to part annotations are expensive to acquire, the later methods <ref type="bibr" target="#b35">Zheng et al. 2017;</ref><ref type="bibr" target="#b16">Sun et al. 2018;</ref><ref type="bibr" target="#b36">Zheng et al. 2019)</ref> attempts to find discriminative part regions in a weakly supervised manner. For example, Zhang et al. ) first picks distinctive filters and then use them to learn part detectors through an iteratively alternating strategy. MA-CNN ) obtains part regions by clustering feature maps of intermediate convolutional layers, MAMC <ref type="bibr" target="#b16">(Sun et al. 2018)</ref>. In recent years, fine-grained approaches have developed in the direction of enforcing the neural networks to acquire rich information <ref type="bibr" target="#b4">Ding et al. 2019;</ref><ref type="bibr" target="#b1">Chen et al. 2019;</ref><ref type="bibr" target="#b32">Zhang et al. 2019)</ref>. For instance, Zhang et al.,  progressively crop out discriminative regions to generate diversified data sets for training network experts. Chen et al., <ref type="bibr" target="#b1">(Chen et al. 2019</ref>) destruct the images and then learn a region alignment network to restore the original spatial layout of local regions. These works implicitly integrate data augmentation practices into their methodologies, which relate to our proposed method mostly. However, our proposed method SnapMix differs from them in two aspects. First, SnapMix is a pure data augmentation based technique that does not require an extra computational process in the testing stage. Besides, our approach builds on recent advances from the data mixing strategy. In contrast, those methods are mainly based on conventional data augmentation strategy, which typically processes a single image and retains the original label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data augmentation</head><p>Recent advances <ref type="bibr" target="#b18">(Takahashi, Matsubara, and Uehara 2019;</ref><ref type="bibr" target="#b37">Zhong et al. 2017;</ref><ref type="bibr" target="#b3">DeVries and Taylor 2017;</ref><ref type="bibr" target="#b19">Tokozume, Ushiku, and Harada 2018;</ref><ref type="bibr" target="#b9">Inoue 2018;</ref><ref type="bibr" target="#b31">Zhang et al. 2018;</ref><ref type="bibr" target="#b30">Yun et al. 2019)</ref> in data augmentation can be divided into two groups: region-erasing based and data mixing. The former <ref type="bibr" target="#b37">(Zhong et al. 2017</ref>; DeVries and Taylor 2017) erases partial region of images in training, aiming to encourage the neural networks to find more discriminative regions. The typical method is CutOut that augments data by cutting a rectangle region out of an image. The other line of methods is data mixing based <ref type="bibr" target="#b19">(Tokozume, Ushiku, and Harada 2018;</ref><ref type="bibr" target="#b9">Inoue 2018;</ref><ref type="bibr" target="#b31">Zhang et al. 2018</ref>) that have recently gained increasing attention in the field of image classification. Compared with region-erasing augmentation, these methods generate new data by combing multiple images and fusing their labels accordingly. Among those works, Zhang et al., <ref type="bibr" target="#b31">(Zhang et al. 2018</ref>) first proposed mixing data to extend the training distribution. Their proposed method termed as MixUp, generated images by linearly combining images and fusing their labels using the same coefficients. MixUp showed its superiorities in handling corrupted targets and improving model performance. Summers and Dineen (Summers and Dinneen 2019) further improved Mixup by introducing a more generalized form of data mixing that considered non-linear mixing operations. In very recent work, <ref type="bibr">Yun et al. proposed CutMix (Yun et al. 2019</ref>) that produces a new image by cutting out one image patch and pasting to another image. Similar to Mixup, the labels are also mixed but proportionally to the area of the patches. By taking advantage of both types of methods, CutMix showed impressive performance in classification tasks and weakly-supervised localization tasks.</p><p>Our proposed method falls into the second category. However, it differs significantly from the previous techniques in the way of mixing labels. Current mixing-data based approaches combine labels mainly depending on the statistic of mixture pixels, such as the ratio of pixel number or intensity values. In comparison, our method estimates the semantic structure of a synthetic image by exploiting class activation maps. This new characteristic allows our approach to augment fine-grained data without introducing severe label noise. Another slight difference is that SnapMix blends images using asymmetric patches, resulting in better data randomness and diversity than those using symmetric regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantically Proportional Mixing</head><p>Data augmentation has become an indispensable step for training deep neural networks. The standard augmentation methods mainly apply a composition of image preprocessing techniques on an input image, such as flipping, rotations, color jittering, and random cropping. Recent works demonstrated the great potential of mixing-based techniques for training deep models. Unlike standard practice, these methods generate new data by combining images and also mixing the corresponding labels. In the following, we first provide some notations used in this paper. We then briefly introduce two representative mix-based approaches Mixup and Cut-Mix. Next, we describe in detail our proposed method Snap-Mix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations</head><p>We use the following notations throughout this paper. The original training data set</p><formula xml:id="formula_1">{(I i , y i )|i ∈ [0, 1, ..., N − 1]},</formula><p>where I i ∈ R 3×W ×H and y i refer to an input image and the label respectively. Given a data pair ((I a , y a ), (I b , y b )) and hyperpatemer α, mixing-based methods first draw a random value λ from a beta distribution Beta(α, α). Then they generate a new imageĨ and two label weights ρ a and ρ b according to λ. Here, ρ a and ρ b are corresponding to the label y a and y b respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixup and CutMix</head><p>Recent mixing-based methods essentially stem from two representative techniques Mixup and cutMix.</p><p>MixUp mixes images and combines labels using linear combination, which is expressed as</p><formula xml:id="formula_2">I = λ × I a + (1 − λ) × I b , ρ a = λ, ρ b = 1 − λ,<label>(1)</label></formula><p>CutMix adopts cut-and-paste operation for mixing images and mixes the labels according to the area ratio. That is</p><formula xml:id="formula_3">I = (1 − M λ ) I a + M λ I b , ρ a = 1 − λ, ρ b = λ,<label>(2)</label></formula><p>where denotes element-wise multiplication and M λ ∈ R W ×H is a binary mask of a random box region whose area ratio to the image is λ.</p><p>On one hand, these two methods mainly differ in the way they mix images. Mixup mixes image by linear combination and therefore improves the neural networks' robustness to adversarial examples. By integrating Mixup and regional dropout strategy, the cut-and-paste regime of Cutmix naturally inherits this advantage but also enhances models' capabilities of object localization. On the other hand, they share two similarities: 1) mixing labels by using the statistic of mixture pixels, and 2) performing image mixing in symmetric locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SnapMix for Fine-grained Recognition</head><p>In fine-grained recognition, the category difference usually resides in subtle object parts, making part localization ability plays an important role. Therefore, the cut-and-paste mechanism is more favorable in augmenting fine-grained data. However, mixing labels by the region area ratio is unreliable and will increase the risk of label noise, particularly in combining image at asymmetric locations. Motivated by work that used class activation maps (CAMs) to describe the class-specific discriminative regions, we propose to exploit CAMs to estimate the semantic composition of a mixed image. <ref type="figure" target="#fig_0">Fig. 2</ref> shows an overview of our proposed method Snap-Mix. Our proposed method differs existing methods in two folds: 1) fusing labels based on semantic composition estimation, 2) mixing images asymmetrically. Given an input pair of data, we first extract their semantic percentage maps used to compute the semantic percentage of any image area. We then mix the images by cut-and-paste at asymmetrical locations. Finally, we calculate each mixture component's semantic proportion as guidance to fuse the one-hot labels. In the following, we further describe in detail our method in terms of image mixing and label generation.</p><p>Mixing images. As discussed previously, current existing methods blend images at symmetric locations, limiting the diversity of synthetic images. Our approach removes this constrain to increase the randomness of data augmentation further. Specifically, instead of using a single random location, we crop an area at a random location in one image and transform and paste it to another random place in another image. Such mixing operation is expressed as</p><formula xml:id="formula_4">I = (1 − M λ a ) I a + T θ (M λ b I b ),<label>(3)</label></formula><p>where M λ a and M λ b are two binary masks containing random box regions with the area ratios λ a and λ b , and T θ is a Class Activation Mapping Label generation. To estimate the semantic composition of a mixed image, we need to measure each original image pixel's semantic relatedness to the corresponding label. One alternative to do this can resort to class activation map, as it is proved useful to interpret how a region correlates with a semantic class. Thus, we first employ the attention method  to compute the class activation maps of input images. For a given image I i , we denote F (I i ) ∈ R d×h×w the output of the last convolutional layer, F l (I i ) the l th feature map of F (I i ), and w yi ∈ R d the classifier weight corresponding to class y i . Then we can obtain I i 's class activation map CAM (I i ) by</p><formula xml:id="formula_5">-% &amp; -' &amp; -( &amp; + + ⋯ * * * Resized Feature Maps , " ! , " " #$% # #$% $ ! ! ! " #$% # #$% $ &amp; # &amp; $ Input '()*(,, ,) → &amp; # , &amp; $<label>Random</label></formula><formula xml:id="formula_6">CAM (I i ) = Φ( d l=0 w l yi F l (I i )),<label>(4)</label></formula><p>where Φ(·) denotes a operation that upsamples a feature map to match dimensions with input image size. Here, we ignore the bias term for simplicity. We can now obtain a Semantic Percent Map (SPM) by normalizing the CAM to sum to one. Here, we define SPM as a semantic information measure map to quantify the relatedness percentage between a pixel and the label. We compute the SPM of an image S(I i ) by</p><formula xml:id="formula_7">S(I i ) = CAM (I i ) sum(CAM (I i )) ,<label>(5)</label></formula><p>Finally, for an image produced using Eq.3, we compute the corresponding label weights ρ a and ρ b as</p><formula xml:id="formula_8">ρ a = 1 − sum(M λ a S(I a )), ρ b = sum(M λ b S(I b )).<label>(6)</label></formula><p>By doing so, the generated supervision information for a mixed image can better reflect its intrinsic semantic composition. Therefore, in fine-grained recognition, despite the image's discriminative information is extremely uneven in spatial distribution, our method prevent introducing heavy noise in the augmented data. It is also worth noting that the two components of a mixed image generally do not complement each other in terms of semantic proportion. A case of this would be when a cutout is a background patch and pasted over the object area of another image, and then the synthesized image would not contain any foreground object. Therefore, unlike CutMix, our method does not restrict the label coefficients (ρ a and ρ b ) to sum up to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we extensively evaluated the performance of SnapMix on three fine-grained datasets. We evaluated our method using multiple network structures (Resnet-18,34,50,101) as baselines. We compared the performance of our approach and related data augmentation methods on each network architecture. Further, we tested our method using a strong baseline that integrated mid-level features and compared the results with those of the current state-of-theart methods of fine-grained recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We conduct experiments on three standard fine-grained datasets, which are CUB-200-2011 <ref type="bibr" target="#b20">(Wah et al. 2011</ref>), Stanford-Cars <ref type="bibr" target="#b10">(Krause et al. 2013)</ref>, and FGVC-Aircraft <ref type="bibr" target="#b14">(Maji et al. 2013)</ref>. For each dataset, We first resized images to 512 × 512 and cropped them with size 448 × 448. In the rest of the paper, we used the short names CUB, Cars, and Aircraft to simplify the notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Setup</head><p>Backbone networks and baselines. To extensively compare our method with other approaches, we used four network backbones as baselines in performance comparison. Here, if not specified, we refer baseline as a neural network  model that was pre-trained on Imagenet dataset and finetuned on a target dataset. The used network structures include Resnet-18,34,50 and 101. Here, we adapted their implementation from the TorchVision package to our experiments.</p><p>We also used a strong baseline that incorporates midlevel features in performance evaluation. Here, we termed it baseline † . This baseline was used in recent works <ref type="bibr" target="#b22">(Wang, Morariu, and Davis 2018;</ref><ref type="bibr" target="#b32">Zhang et al. 2019)</ref> to push the performance limits of fine-grained recognition. Compared with the standard baseline that contains a single classification branch, Baseline † adds another mid-level classification branch on top of the intermediate layers. In our experiments, we followed the implementation from . Specifically, the mid-level branch included a Conv1×1, Max Pooling, and a Linear Classifier layer and was placed after 4 th block of ResNet. We blocked the gradients passing the mid-level branch to backbone networks in training. In testing, we fused the predictions from two classification branches. Data augmentation methods. We compared our method with three representative data augmentation methods namely CutOut (DeVries and Taylor 2017), MixUp <ref type="bibr" target="#b31">(Zhang et al. 2018)</ref>, and CutMix <ref type="bibr" target="#b30">(Yun et al. 2019)</ref>. Since these works did not officially report results on fine-grained datasets, we implemented these methods based on the released codes and run experiments on fine-grained datasets. We first tested different hyperparameters for each method and then selected the optimal one for all network structures. We set the probability of performing augmentation 0.5 for CutOut and MixUp and 1.0 for CutMix. We used the α values of 1.0 and 3.0 for MixUp and CutMix, respectively.</p><p>Training details. We used stochastic gradient descent (SGD) with momentum 0.9, base learning rate 0.001 for the pre-trained weights, and 0.01 for new parameters. We trained our model for 200 epochs and decayed the learning rate by factor 0.1 every 80 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance evaluation</head><p>In this section, we presented the results of our method and performance comparisons with existing approaches. We first made comparisons between SnapMix and other data augmentation methods. Further, we tested our approach using the two baselines and compared the results with those of the current state-of-the-art methods. We used top-1 accuracy as the performance measure and provided both the best accuracy and average accuracy (the mean result of the final 10 epochs) of our proposed method. Comparison with data augmentation methods. We listed the results of performance comparisons in <ref type="table">Table.</ref> 1-2. Here, <ref type="table" target="#tab_1">Table 1</ref>-2 shows each method's average accuracy and improvement over the baseline. First, we can observe that our proposed method SnapMix consistently outperforms its counterparts on three datasets. We can further find that existing methods mostly yield limited, even negative improvement on the CUB dataset. This might mainly because the CUB dataset exhibits more subtle category differences, making those methods increase the risk of noise labels. Besides, the effectiveness of these methods is relatively sensitive to the network depth. For example, both Mixup and CutMix achieve significant improvements on the CUB dataset only using the deeper networks Resnet-101, and CutMix even suffers a performance drop when using Resnet-18. We hypothesis the reason is that the deeper models have better ca-  <ref type="table">Table 3</ref>: The accuracy (%) comparison with state-of-the-art methods on CUB, Cars, and Aircraft. For the baselines and our approach, we reported their average accuracy of the final ten epochs and showed their best accuracy in the brackets.</p><p>pacities in handling label noise. In comparison, SnapMix significantly improves the baseline regardless of network depth.</p><p>Comparison with state-of-the-art methods.</p><p>In this section, we compared the performance of SnapMix and other state-of-the-art techniques of fine-grained recognition. In <ref type="table">Table.</ref> 3, first, we can observe that the baseline † achieved higher accuracy than the baseline on three datasets, and the performance gain on the CUB dataset is the most significant. This result indicates mid-level features can effectively complement the capacity of the global-level features in fine-grained recognition. It is also worth mentioning that some top-performing works, such as DFL-CNN <ref type="bibr" target="#b22">(Wang, Morariu, and Davis 2018)</ref> and MGN-CNN ) also embedded the baseline+ into their methods.</p><p>Secondly, SnapMix enhances both baselines to obtain comparable performance even to some latest approaches with intricate designs and high inference time. S3N <ref type="bibr" target="#b4">(Ding et al. 2019</ref>) and MGN-CNN ) are two of the state-of-the-art methods. S3N adopted a selective sparse sampling strategy to construct multiple features. MGN-CNN exploit attention mechanisms to construct different inputs for multiple expert networks. Both methods require a similar data processing pipeline with the training stage and the need for multiple feed-forward passes of the backbone network in the testing stage.</p><p>In contrast, using a standard baseline with a single Resnet-101 backbone, SnapMix, without bells and whistles in the testing stage, achieves the accuracy of 88.45%, 94.44%, and 93.74% on CUB, Cars, and Aircraft respectively, which outperforms most of the existing techniques. Even using the baseline † (a more powerful baseline), SnapMix still demonstrates its promise and effectiveness in performance improvement, pushing the accuracy to the next level. For example, SnapMix achieves 89.32% accuracy (close to the result of MGN-CNN 89.4%) on the CUB dataset and exhibits superior performance than all the comparing techniques on both Cars and Aircraft dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>Training from scratch. Tab. 4 shows that our approach is also effective without using ImageNet pre-trained weights. In this experiment, we used the 'switch' probability (the probability of applying the mixing augmentation) of 0.5 for each mixing method. This allows the networks to learn from both clean and mixed data, preventing the mixed data from excessively affecting the model's initial learning stage. Therefore, despite SnapMix may introduce noise labels in the early training stage, it would not hinder the network from learning a good CAM in the subsequent stage. This is because the network tends to first learn from easy samples (clean data ) other than difficult samples (mixed data with label noise) <ref type="bibr" target="#b0">(Arpit et al. 2017)</ref>. With the continuous learning of the network and the improvement of CAM quality, the more reasonable the label estimated by SnapMix will be to enhance subsequent model learning. Effectiveness of using other network backbones. We evaluated the performance of our method with two other network  Influence of hyperparameters. The hyperparameter α of snapMix decides a beta distribution that is used to generate a random patch in mixing. To investigate its impact on the performance, we tested seven values of α. <ref type="table">Table.</ref>6 showed that the accuracy increased slightly with the increase of α value and peaked at the number of 5, which suggests the importance of using the medium-size boxes to mix images on this dataset. Besides, the accuracy of setting different α values inconsiderably fluctuates around the mean value of 87.37%, indicating that snapMix is not very sensitive to the α value.</p><p>Effectiveness of each component of SnapMix. We performed experiments using combinations of different image mixing operations and label mixing strategies. As shown in <ref type="figure">Fig. 3</ref>, the asymmetric mixing provides a slight improvement over the symmetric mixing, and the label mixing strategy of SnapMix is the primary contributor to the performance gain. More importantly, the Semantic-Ratio consistently shows improvement in using three image mixing operations.</p><p>Visualization. <ref type="figure">Fig. 4</ref> shows CAMs of some examples correctly predicted by SnapMix but misclassified by MixUp and CutMix. We can observe the attention of MixUp and CutMix are distracted by some background patterns, which might be a reason for the misprediction. By comparison, the network attention of SnapMix tends to lie in object regions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-Mix + SR-Label</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AS-Mix + AR-Label</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-Mix + AR-Label (CutMix)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drop Region + SR-Label</head><p>Drop Region (CutOut) <ref type="figure">Figure 3</ref>: Accuracy comparison of six different combination techniques (%). Here, S-Mixing, AS-Mix, AR-label, and SR-label are short for symmetric mixing, asymmetric mixing, area ratio label, and semantic ratio label respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MixUp CutMix SnapMix</head><p>Figure 4: CAM visualization of different augmentation methods.</p><p>These results imply mixing labels by pixel statistics may cause the neural networks more sensitive to background visual patterns, while our proposed method avoids this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we present a new method SnapMix for augmenting fine-grained data. SnapMix generates new training data with more reasonable supervision signals by considering the semantic correspondence. Our experiments showed the importance of estimating semantic composition for a synthetic image. Our proposed method might also benefit other tasks (e.g., indoor scene recognition or person reidentification), where a small image region contains significant discriminative information. The proposed label mixing strategy is mainly applicable to cut-and-paste mixing. Further work might explore how better to estimate the semantic structure of a linearly combined image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An overview of proposed method. function that transforms the cutout region of I b to match the box region of I a .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison(Mean Acc.%) of methods using backbone networks Resnet-18 and Resnet-34 on fine-grained datasets. Each method's improvement over the baseline is shown in the brackets.</figDesc><table><row><cell></cell><cell>CUB</cell><cell></cell><cell>Cars</cell><cell></cell><cell>Aircraft</cell><cell></cell></row><row><cell></cell><cell>Res18</cell><cell>Res34</cell><cell>Res18</cell><cell>Res34</cell><cell>Res18</cell><cell>Res34</cell></row><row><cell>Baseline</cell><cell>82.35</cell><cell>84.98</cell><cell>91.15</cell><cell>92.02</cell><cell>87.80</cell><cell>89.92</cell></row><row><cell>CutOut</cell><cell cols="6">80.54 (-1.81) 83.36 (-1.62) 91.83(+0.68) 92.84 (+0.82) 88.58 (+0.78) 89.90 (-0.02)</cell></row><row><cell>MixUp</cell><cell cols="6">83.17 (+0.82) 85.22 (+0.24) 91.57 (+0.42) 93.28 (+1.26) 89.82 (+2.02) 91.02 (+1.1)</cell></row><row><cell>CutMix</cell><cell cols="6">80.16 (-2.19) 85.69 (+0.71) 92.65 (+1.50) 93.61 (+1.59) 89.44 (+1.64) 91.26 (+1.34)</cell></row><row><cell cols="7">SnapMix 84.29 (+1.94) 87.06 (+2.08) 93.12(+1.97) 93.95 (+1.93) 90.17 (+2.37) 92.36 (+2.44)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison(Mean Acc.%) of methods using backbone networks Resnet-50 and Resnet-101 on finegrained datasets. Each method's improvement over the baseline is shown in the brackets.</figDesc><table><row><cell></cell><cell>CUB</cell><cell></cell><cell>Cars</cell><cell></cell><cell>Aircraft</cell><cell></cell></row><row><cell></cell><cell>Res50</cell><cell>Res101</cell><cell>Res50</cell><cell>Res101</cell><cell>Res50</cell><cell>Res101</cell></row><row><cell>Baseline</cell><cell>85.49</cell><cell>85.62</cell><cell>93.04</cell><cell>93.09</cell><cell>91.07</cell><cell>91.59</cell></row><row><cell>CutOut</cell><cell cols="6">83.55 (-1.94) 84.70 (-0.92) 93.76 (+0.72) 94.16 (+1.07) 91.23 (+0.16) 91.79 (+0.2)</cell></row><row><cell>MixUp</cell><cell cols="6">86.23 (+0.74) 87.72 (+2.1) 93.96 (+0.92) 94.22 (+1.13) 92.24 (+1.17) 92.89 (+1.3)</cell></row><row><cell>CutMix</cell><cell cols="6">86.15 (+0.66) 87.92 (+2.3) 94.18 (+1.14) 94.27 (+1.18) 92.23 (+1.16) 92.29 (+0.7)</cell></row><row><cell cols="7">SnapMix 87.75 (+2.26) 88.45 (+2.83) 94.30 (+1.21) 94.44 (+1.35) 92.80 (+1.73) 93.74 (+2.15)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison of training from scratch on the CUB dataset (Acc.%).</figDesc><table><row><cell></cell><cell cols="4">Baseline CutMix MixUp SnapMix</cell></row><row><cell>Res-18</cell><cell>64.98</cell><cell>60.03</cell><cell>67.63</cell><cell>70.31</cell></row><row><cell>Res-50</cell><cell>66.92</cell><cell>65.28</cell><cell>72.39</cell><cell>72.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison of using other network backbones on the CUB dataset (Acc.%).</figDesc><table><row><cell></cell><cell cols="4">Baseline Cutmix Mixup Snapmix</cell></row><row><cell>InceptionV3</cell><cell>82.22</cell><cell>84.31</cell><cell>83.83</cell><cell>85.54</cell></row><row><cell>DenseNet121</cell><cell>84.23</cell><cell>86.11</cell><cell>86.65</cell><cell>87.42</cell></row><row><cell cols="5">backbones including IceptionV3 (Szegedy et al. 2016) and</cell></row><row><cell cols="5">DenseNet121 (Huang et al. 2017). As shown in Tab 5, our</cell></row><row><cell cols="5">method surpasses both CutMix and MixUp approaches and</cell></row><row><cell cols="5">improves the baseline by a large margin. This result demon-</cell></row><row><cell cols="5">strates SnapMix's consistent effectiveness when applied to</cell></row><row><cell cols="2">various CNN architecture.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Influence of hyperparameters Acc.(%)</figDesc><table><row><cell>α= 0.2</cell><cell>0.5</cell><cell>1.0</cell><cell>3.0</cell><cell>5.0</cell><cell>7</cell><cell>8</cell></row><row><cell cols="7">87.22 87.23 87.25 87.30 87.75 87.30 87.54</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the Australian Laureate Fellowship project FL170100117, DP-180103424, IH-180100002, and Stevens Institute of Technology Startup Funding.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Closer Look at Memorization in Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Destruction and Construction Learning for Fine-grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selective Sparse Sampling for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6599" to="6608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Part-stacked cnn for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inoue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02929</idno>
		<title level="m">Data augmentation by pairing samples for images classification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">Dynamic Computational Time for Visual Attention</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep lac: Deep localization, alignment and classification for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classification with Noisy Labels by Importance Reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved mixedexample data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Dinneen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1262" to="1270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Data augmentation using random image cropping and patching for deep CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Betweenclass learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5486" to="5494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Subspaces Indexing Model on Grassmann Manifold for Image Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2627" to="2635" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a CNN for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="842" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Augmenting strong supervision using web data for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2524" to="2532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Friend or foe: Fine-grained categorization with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shared Predictive Cross-Modal Deep Quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5292" to="5303" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Factorizable Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distilling Knowledge From Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to Navigate for Fine-grained Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">mixup: Beyond Empirical Risk Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning a Mixture of Granularity-Specific Experts for Fine-Grained Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8331" to="8340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Part-based R-CNNs for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Picking deep filter responses for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning multiattention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Looking for the Devil in the Details: Learning Trilinear Attention Sampling Network for Fine-grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5012" to="5021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Random erasing data augmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
